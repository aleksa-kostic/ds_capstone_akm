{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c5923a4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.preprocessing import MinMaxScaler, MaxAbsScaler, StandardScaler \n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn import preprocessing\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8f9a1a2a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PhysHlth</th>\n",
       "      <th>BMI</th>\n",
       "      <th>MentHlth</th>\n",
       "      <th>Income</th>\n",
       "      <th>Diabetes_binary</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-6.667397</td>\n",
       "      <td>-6.392123</td>\n",
       "      <td>-1.111675</td>\n",
       "      <td>-0.458052</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-3.863936</td>\n",
       "      <td>-7.178304</td>\n",
       "      <td>3.095224</td>\n",
       "      <td>-0.072141</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.834798</td>\n",
       "      <td>0.081819</td>\n",
       "      <td>-3.494618</td>\n",
       "      <td>-4.087203</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>7.085908</td>\n",
       "      <td>-6.207388</td>\n",
       "      <td>-6.370345</td>\n",
       "      <td>-2.893083</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-4.274580</td>\n",
       "      <td>7.404026</td>\n",
       "      <td>0.413793</td>\n",
       "      <td>1.106787</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>253659</th>\n",
       "      <td>-3.320172</td>\n",
       "      <td>9.505420</td>\n",
       "      <td>0.697311</td>\n",
       "      <td>0.792966</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>253668</th>\n",
       "      <td>-5.022866</td>\n",
       "      <td>1.523446</td>\n",
       "      <td>-0.484450</td>\n",
       "      <td>-2.206810</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>253670</th>\n",
       "      <td>2.529385</td>\n",
       "      <td>-4.995248</td>\n",
       "      <td>11.196980</td>\n",
       "      <td>-6.451818</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>253676</th>\n",
       "      <td>-6.559798</td>\n",
       "      <td>-9.188122</td>\n",
       "      <td>-1.661467</td>\n",
       "      <td>-3.481757</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>253679</th>\n",
       "      <td>-5.507642</td>\n",
       "      <td>-2.370729</td>\n",
       "      <td>-0.740533</td>\n",
       "      <td>-1.636047</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>88365 rows Ã— 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        PhysHlth       BMI   MentHlth    Income  Diabetes_binary\n",
       "0      -6.667397 -6.392123  -1.111675 -0.458052                0\n",
       "1      -3.863936 -7.178304   3.095224 -0.072141                0\n",
       "2      -0.834798  0.081819  -3.494618 -4.087203                0\n",
       "3       7.085908 -6.207388  -6.370345 -2.893083                0\n",
       "4      -4.274580  7.404026   0.413793  1.106787                0\n",
       "...          ...       ...        ...       ...              ...\n",
       "253659 -3.320172  9.505420   0.697311  0.792966                1\n",
       "253668 -5.022866  1.523446  -0.484450 -2.206810                1\n",
       "253670  2.529385 -4.995248  11.196980 -6.451818                1\n",
       "253676 -6.559798 -9.188122  -1.661467 -3.481757                1\n",
       "253679 -5.507642 -2.370729  -0.740533 -1.636047                1\n",
       "\n",
       "[88365 rows x 5 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('Preprocessed_pca.csv', index_col = 0)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "185b32db",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df[['PhysHlth', 'BMI', 'MentHlth', 'Income']]\n",
    "y = df.loc[:, 'Diabetes_binary']\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, y, test_size=0.2, shuffle=True, random_state = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9e763e27",
   "metadata": {},
   "outputs": [],
   "source": [
    "acc_list = []\n",
    "roc_auc_ovr_List = []\n",
    "f1_micro_List = []"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62155024",
   "metadata": {},
   "source": [
    "## Trial 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cc362efc",
   "metadata": {},
   "outputs": [],
   "source": [
    "start = datetime.now()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a65d5cf2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ENT\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\ENT\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\ENT\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\ENT\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\ENT\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\ENT\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\ENT\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\ENT\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\ENT\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\ENT\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\ENT\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1183: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\ENT\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1183: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\ENT\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1183: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\ENT\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1183: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\ENT\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1183: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\ENT\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1183: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\ENT\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1183: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\ENT\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1183: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\ENT\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1183: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\ENT\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1183: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\ENT\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1183: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\ENT\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1183: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\ENT\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1183: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\ENT\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1183: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\ENT\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1183: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\ENT\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1183: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\ENT\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1183: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\ENT\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1183: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\ENT\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1183: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\ENT\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1183: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Create a pipeline - RF is a stand in, we will populate the classifier part below\n",
    "pipe = Pipeline([('std', StandardScaler()),\n",
    "                 ('classifier', LogisticRegression())])\n",
    "\n",
    "# Create search space of candidate learning algorithms and their hyperparameters\n",
    "# note lbfgs can't do l1, and if you pass penalty='none' it expects no C value\n",
    "search_space = [{'classifier': [LogisticRegression(max_iter=10000)],\n",
    "                 'classifier__solver': ['saga'],\n",
    "                 'classifier__penalty': ['l1', 'l2'],\n",
    "                 'classifier__C': np.logspace(-4, 4, 9)},\n",
    "                {'classifier': [LogisticRegression(max_iter=10000)],\n",
    "                 'classifier__solver': ['lbfgs'],\n",
    "                 'classifier__penalty': ['l2'],\n",
    "                 'classifier__C': np.logspace(-4, 4, 9)},\n",
    "                {'classifier': [LogisticRegression(max_iter=10000)],\n",
    "                 'classifier__solver': ['lbfgs','saga'],\n",
    "                 'classifier__penalty': ['none']},\n",
    "                {'classifier': [LogisticRegression(max_iter=10000)]}\n",
    "                ]\n",
    "\n",
    "# Create grid search\n",
    "clf = GridSearchCV(pipe, search_space, cv=StratifiedKFold(n_splits=10),\n",
    "                   scoring=['accuracy', 'precision', 'f1_micro'], refit=False,\n",
    "                   verbose=0)\n",
    "\n",
    "# Fit grid search\n",
    "best_model = clf.fit(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8390b96b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'mean_fit_time': array([0.05000498, 0.10266774, 0.11528485, 0.10219669, 0.11557832,\n",
       "        0.10265124, 0.11612074, 0.10003376, 0.12404246, 0.10399759,\n",
       "        0.11845224, 0.10547323, 0.12610879, 0.11175604, 0.12384446,\n",
       "        0.11199782, 0.13726242, 0.12367868, 0.02431118, 0.02323394,\n",
       "        0.02302573, 0.02340813, 0.02270339, 0.02307572, 0.02314734,\n",
       "        0.02421052, 0.02304683, 0.02414787, 0.11753125, 0.02328396]),\n",
       " 'std_fit_time': array([0.00828888, 0.00840852, 0.00507814, 0.0044828 , 0.00749576,\n",
       "        0.00895794, 0.00782027, 0.00528426, 0.01065537, 0.00778991,\n",
       "        0.00780167, 0.00986117, 0.0165836 , 0.00927526, 0.01594894,\n",
       "        0.0055158 , 0.01556274, 0.01288251, 0.00092476, 0.00101769,\n",
       "        0.00105979, 0.00105457, 0.00052992, 0.00096601, 0.00164706,\n",
       "        0.00213683, 0.00112265, 0.00170457, 0.00855235, 0.0005618 ]),\n",
       " 'mean_score_time': array([0.00740664, 0.00781064, 0.00792811, 0.00770681, 0.00820727,\n",
       "        0.00780694, 0.00790708, 0.00827262, 0.00785735, 0.00760663,\n",
       "        0.00780618, 0.00790701, 0.00826421, 0.00830741, 0.00790119,\n",
       "        0.00811889, 0.00834413, 0.00826738, 0.00814137, 0.00772104,\n",
       "        0.00786784, 0.00811334, 0.00814512, 0.00788803, 0.00798533,\n",
       "        0.0076407 , 0.00801425, 0.00780694, 0.0081069 , 0.00804484]),\n",
       " 'std_score_time': array([0.00066398, 0.0004024 , 0.00050648, 0.00064107, 0.00040045,\n",
       "        0.00040051, 0.00070077, 0.00066525, 0.00032016, 0.00066408,\n",
       "        0.00040019, 0.00030036, 0.00067373, 0.00100627, 0.0002206 ,\n",
       "        0.00029818, 0.00077083, 0.00060752, 0.00020093, 0.00063224,\n",
       "        0.00063281, 0.00072748, 0.00085607, 0.0003666 , 0.00072958,\n",
       "        0.00052714, 0.00013184, 0.00060059, 0.00073363, 0.00046089]),\n",
       " 'param_classifier': masked_array(data=[LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000)],\n",
       "              mask=[False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'param_classifier__C': masked_array(data=[0.0001, 0.0001, 0.001, 0.001, 0.01, 0.01, 0.1, 0.1,\n",
       "                    1.0, 1.0, 10.0, 10.0, 100.0, 100.0, 1000.0, 1000.0,\n",
       "                    10000.0, 10000.0, 0.0001, 0.001, 0.01, 0.1, 1.0, 10.0,\n",
       "                    100.0, 1000.0, 10000.0, --, --, --],\n",
       "              mask=[False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False,  True,  True,  True],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'param_classifier__penalty': masked_array(data=['l1', 'l2', 'l1', 'l2', 'l1', 'l2', 'l1', 'l2', 'l1',\n",
       "                    'l2', 'l1', 'l2', 'l1', 'l2', 'l1', 'l2', 'l1', 'l2',\n",
       "                    'l2', 'l2', 'l2', 'l2', 'l2', 'l2', 'l2', 'l2', 'l2',\n",
       "                    'none', 'none', --],\n",
       "              mask=[False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False,  True],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'param_classifier__solver': masked_array(data=['saga', 'saga', 'saga', 'saga', 'saga', 'saga', 'saga',\n",
       "                    'saga', 'saga', 'saga', 'saga', 'saga', 'saga', 'saga',\n",
       "                    'saga', 'saga', 'saga', 'saga', 'lbfgs', 'lbfgs',\n",
       "                    'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs',\n",
       "                    'lbfgs', 'lbfgs', 'saga', --],\n",
       "              mask=[False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False,  True],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'params': [{'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 0.0001,\n",
       "   'classifier__penalty': 'l1',\n",
       "   'classifier__solver': 'saga'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 0.0001,\n",
       "   'classifier__penalty': 'l2',\n",
       "   'classifier__solver': 'saga'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 0.001,\n",
       "   'classifier__penalty': 'l1',\n",
       "   'classifier__solver': 'saga'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 0.001,\n",
       "   'classifier__penalty': 'l2',\n",
       "   'classifier__solver': 'saga'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 0.01,\n",
       "   'classifier__penalty': 'l1',\n",
       "   'classifier__solver': 'saga'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 0.01,\n",
       "   'classifier__penalty': 'l2',\n",
       "   'classifier__solver': 'saga'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 0.1,\n",
       "   'classifier__penalty': 'l1',\n",
       "   'classifier__solver': 'saga'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 0.1,\n",
       "   'classifier__penalty': 'l2',\n",
       "   'classifier__solver': 'saga'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 1.0,\n",
       "   'classifier__penalty': 'l1',\n",
       "   'classifier__solver': 'saga'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 1.0,\n",
       "   'classifier__penalty': 'l2',\n",
       "   'classifier__solver': 'saga'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 10.0,\n",
       "   'classifier__penalty': 'l1',\n",
       "   'classifier__solver': 'saga'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 10.0,\n",
       "   'classifier__penalty': 'l2',\n",
       "   'classifier__solver': 'saga'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 100.0,\n",
       "   'classifier__penalty': 'l1',\n",
       "   'classifier__solver': 'saga'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 100.0,\n",
       "   'classifier__penalty': 'l2',\n",
       "   'classifier__solver': 'saga'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 1000.0,\n",
       "   'classifier__penalty': 'l1',\n",
       "   'classifier__solver': 'saga'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 1000.0,\n",
       "   'classifier__penalty': 'l2',\n",
       "   'classifier__solver': 'saga'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 10000.0,\n",
       "   'classifier__penalty': 'l1',\n",
       "   'classifier__solver': 'saga'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 10000.0,\n",
       "   'classifier__penalty': 'l2',\n",
       "   'classifier__solver': 'saga'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 0.0001,\n",
       "   'classifier__penalty': 'l2',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 0.001,\n",
       "   'classifier__penalty': 'l2',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 0.01,\n",
       "   'classifier__penalty': 'l2',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 0.1,\n",
       "   'classifier__penalty': 'l2',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 1.0,\n",
       "   'classifier__penalty': 'l2',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 10.0,\n",
       "   'classifier__penalty': 'l2',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 100.0,\n",
       "   'classifier__penalty': 'l2',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 1000.0,\n",
       "   'classifier__penalty': 'l2',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 10000.0,\n",
       "   'classifier__penalty': 'l2',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__penalty': 'none',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__penalty': 'none',\n",
       "   'classifier__solver': 'saga'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000)}],\n",
       " 'split0_test_accuracy': array([0.60056577, 0.67991513, 0.69575672, 0.69844413, 0.69886846,\n",
       "        0.69915134, 0.69816124, 0.69816124, 0.6980198 , 0.6980198 ,\n",
       "        0.6980198 , 0.6980198 , 0.6980198 , 0.6980198 , 0.6980198 ,\n",
       "        0.6980198 , 0.6980198 , 0.6980198 , 0.67991513, 0.69844413,\n",
       "        0.69915134, 0.69816124, 0.6980198 , 0.6980198 , 0.6980198 ,\n",
       "        0.6980198 , 0.6980198 , 0.6980198 , 0.6980198 , 0.6980198 ]),\n",
       " 'split1_test_accuracy': array([0.60056577, 0.67977369, 0.69943423, 0.7009901 , 0.70042433,\n",
       "        0.69971711, 0.70014144, 0.70028289, 0.70042433, 0.70042433,\n",
       "        0.70042433, 0.70042433, 0.70042433, 0.70042433, 0.70042433,\n",
       "        0.70042433, 0.70042433, 0.70042433, 0.67977369, 0.7009901 ,\n",
       "        0.69971711, 0.70028289, 0.70042433, 0.70042433, 0.70042433,\n",
       "        0.70042433, 0.70042433, 0.70042433, 0.70042433, 0.70042433]),\n",
       " 'split2_test_accuracy': array([0.60065073, 0.6831235 , 0.69557222, 0.69345028, 0.69627953,\n",
       "        0.69613807, 0.69642099, 0.69642099, 0.69656246, 0.69656246,\n",
       "        0.69656246, 0.69656246, 0.69656246, 0.69656246, 0.69656246,\n",
       "        0.69656246, 0.69656246, 0.69656246, 0.6831235 , 0.69345028,\n",
       "        0.69613807, 0.69642099, 0.69656246, 0.69656246, 0.69656246,\n",
       "        0.69656246, 0.69656246, 0.69656246, 0.69656246, 0.69656246]),\n",
       " 'split3_test_accuracy': array([0.60050927, 0.6747772 , 0.69274296, 0.69189419, 0.69076248,\n",
       "        0.69175272, 0.69161126, 0.69161126, 0.69161126, 0.69161126,\n",
       "        0.69161126, 0.6914698 , 0.6914698 , 0.6914698 , 0.69161126,\n",
       "        0.6914698 , 0.6914698 , 0.6914698 , 0.6747772 , 0.69189419,\n",
       "        0.69175272, 0.69175272, 0.69161126, 0.6914698 , 0.6914698 ,\n",
       "        0.6914698 , 0.6914698 , 0.6914698 , 0.6914698 , 0.69161126]),\n",
       " 'split4_test_accuracy': array([0.60050927, 0.69401613, 0.70985995, 0.71014288, 0.70985995,\n",
       "        0.70957703, 0.70886971, 0.70901118, 0.70901118, 0.70886971,\n",
       "        0.70886971, 0.70886971, 0.70886971, 0.70886971, 0.70886971,\n",
       "        0.70886971, 0.70901118, 0.70901118, 0.69401613, 0.71014288,\n",
       "        0.70957703, 0.70901118, 0.70886971, 0.70886971, 0.70886971,\n",
       "        0.70886971, 0.70886971, 0.70886971, 0.70886971, 0.70886971]),\n",
       " 'split5_test_accuracy': array([0.60050927, 0.68340642, 0.6959966 , 0.69910878, 0.70038195,\n",
       "        0.70123073, 0.70137219, 0.70137219, 0.70137219, 0.70137219,\n",
       "        0.70137219, 0.70137219, 0.70123073, 0.70123073, 0.70123073,\n",
       "        0.70137219, 0.70123073, 0.70123073, 0.68340642, 0.69910878,\n",
       "        0.70123073, 0.70137219, 0.70137219, 0.70123073, 0.70123073,\n",
       "        0.70123073, 0.70123073, 0.70123073, 0.70123073, 0.70137219]),\n",
       " 'split6_test_accuracy': array([0.60050927, 0.69005517, 0.70462583, 0.70759655, 0.71014288,\n",
       "        0.71085019, 0.71099165, 0.71099165, 0.71099165, 0.71099165,\n",
       "        0.71099165, 0.71099165, 0.71099165, 0.71099165, 0.71099165,\n",
       "        0.71099165, 0.71099165, 0.71099165, 0.69005517, 0.70759655,\n",
       "        0.71085019, 0.71099165, 0.71099165, 0.71099165, 0.71099165,\n",
       "        0.71099165, 0.71099165, 0.71099165, 0.71099165, 0.71099165]),\n",
       " 'split7_test_accuracy': array([0.60050927, 0.68595275, 0.70405998, 0.70462583, 0.70391852,\n",
       "        0.70363559, 0.70363559, 0.70349413, 0.70363559, 0.70363559,\n",
       "        0.70349413, 0.70363559, 0.70349413, 0.70349413, 0.70349413,\n",
       "        0.70349413, 0.70335267, 0.70349413, 0.68595275, 0.70462583,\n",
       "        0.70363559, 0.70349413, 0.70349413, 0.70349413, 0.70349413,\n",
       "        0.70349413, 0.70349413, 0.70349413, 0.70349413, 0.70349413]),\n",
       " 'split8_test_accuracy': array([0.60050927, 0.679304  , 0.69925025, 0.69967464, 0.70179658,\n",
       "        0.70137219, 0.70236243, 0.70236243, 0.70236243, 0.70236243,\n",
       "        0.70236243, 0.70236243, 0.70236243, 0.70236243, 0.70236243,\n",
       "        0.70236243, 0.70236243, 0.70236243, 0.679304  , 0.69967464,\n",
       "        0.70137219, 0.70236243, 0.70236243, 0.70236243, 0.70236243,\n",
       "        0.70236243, 0.70236243, 0.70236243, 0.70236243, 0.70236243]),\n",
       " 'split9_test_accuracy': array([0.60050927, 0.67491866, 0.69359174, 0.69613807, 0.69698684,\n",
       "        0.69684538, 0.69642099, 0.69656246, 0.69642099, 0.69642099,\n",
       "        0.69656246, 0.69642099, 0.69642099, 0.69656246, 0.69642099,\n",
       "        0.69642099, 0.69656246, 0.69656246, 0.67491866, 0.69613807,\n",
       "        0.69684538, 0.69656246, 0.69642099, 0.69642099, 0.69642099,\n",
       "        0.69642099, 0.69642099, 0.69642099, 0.69656246, 0.69642099]),\n",
       " 'mean_test_accuracy': array([0.60053471, 0.68252427, 0.69908905, 0.70020654, 0.70094215,\n",
       "        0.70102704, 0.70099875, 0.70102704, 0.70104119, 0.70102704,\n",
       "        0.70102704, 0.7010129 , 0.7009846 , 0.70099875, 0.70099875,\n",
       "        0.70099875, 0.70099875, 0.7010129 , 0.68252427, 0.70020654,\n",
       "        0.70102704, 0.70104119, 0.7010129 , 0.7009846 , 0.7009846 ,\n",
       "        0.7009846 , 0.7009846 , 0.7009846 , 0.70099875, 0.7010129 ]),\n",
       " 'std_test_accuracy': array([4.46337344e-05, 5.86749387e-03, 5.24454557e-03, 5.56215787e-03,\n",
       "        5.65386249e-03, 5.56054977e-03, 5.56403880e-03, 5.56417557e-03,\n",
       "        5.57634920e-03, 5.55625595e-03, 5.53821841e-03, 5.58033849e-03,\n",
       "        5.57306122e-03, 5.56162713e-03, 5.54901923e-03, 5.57384750e-03,\n",
       "        5.57564235e-03, 5.58177275e-03, 5.86749387e-03, 5.56215787e-03,\n",
       "        5.56054977e-03, 5.54034786e-03, 5.54977285e-03, 5.57306122e-03,\n",
       "        5.57306122e-03, 5.57306122e-03, 5.57306122e-03, 5.57306122e-03,\n",
       "        5.56162713e-03, 5.54977285e-03]),\n",
       " 'rank_test_accuracy': array([30, 28, 27, 25, 24,  6, 12,  3,  1,  4,  4,  8, 18, 13, 13, 13, 13,\n",
       "         9, 28, 25,  6,  2, 10, 18, 18, 18, 18, 18, 13, 10]),\n",
       " 'split0_test_precision': array([0.        , 0.69629111, 0.66519391, 0.66398104, 0.6590389 ,\n",
       "        0.65920512, 0.65653358, 0.65639166, 0.65609425, 0.65609425,\n",
       "        0.65609425, 0.65609425, 0.65609425, 0.65609425, 0.65609425,\n",
       "        0.65609425, 0.65609425, 0.65609425, 0.69629111, 0.66398104,\n",
       "        0.65920512, 0.65639166, 0.65609425, 0.65609425, 0.65609425,\n",
       "        0.65609425, 0.65609425, 0.65609425, 0.65609425, 0.65609425]),\n",
       " 'split1_test_precision': array([0.        , 0.70172911, 0.67448827, 0.669532  , 0.66163004,\n",
       "        0.6602652 , 0.66043756, 0.66059226, 0.66074681, 0.66074681,\n",
       "        0.66074681, 0.66074681, 0.66074681, 0.66074681, 0.66074681,\n",
       "        0.66074681, 0.66074681, 0.66074681, 0.70172911, 0.669532  ,\n",
       "        0.6602652 , 0.66059226, 0.66074681, 0.66074681, 0.66074681,\n",
       "        0.66074681, 0.66074681, 0.66074681, 0.66074681, 0.66074681]),\n",
       " 'split2_test_precision': array([0.        , 0.70896057, 0.66567901, 0.65678776, 0.65419708,\n",
       "        0.65361857, 0.65365411, 0.65365411, 0.65381125, 0.65381125,\n",
       "        0.65381125, 0.65381125, 0.65381125, 0.65381125, 0.65381125,\n",
       "        0.65381125, 0.65381125, 0.65381125, 0.70896057, 0.65678776,\n",
       "        0.65361857, 0.65365411, 0.65381125, 0.65381125, 0.65381125,\n",
       "        0.65381125, 0.65381125, 0.65381125, 0.65381125, 0.65381125]),\n",
       " 'split3_test_precision': array([0.        , 0.68525053, 0.65764023, 0.64912281, 0.64065256,\n",
       "        0.6421331 , 0.64110429, 0.64110429, 0.64110429, 0.64110429,\n",
       "        0.64110429, 0.64082348, 0.64082348, 0.64082348, 0.64110429,\n",
       "        0.64082348, 0.64082348, 0.64082348, 0.68525053, 0.64912281,\n",
       "        0.6421331 , 0.64138536, 0.64110429, 0.64082348, 0.64082348,\n",
       "        0.64082348, 0.64082348, 0.64082348, 0.64082348, 0.64110429]),\n",
       " 'split4_test_precision': array([0.        , 0.72225958, 0.67985109, 0.67353336, 0.66681053,\n",
       "        0.6659492 , 0.66423671, 0.66423983, 0.66423983, 0.66395548,\n",
       "        0.66395548, 0.66395548, 0.66395548, 0.66395548, 0.66395548,\n",
       "        0.66395548, 0.66423983, 0.66423983, 0.72225958, 0.67353336,\n",
       "        0.6659492 , 0.66423983, 0.66395548, 0.66395548, 0.66395548,\n",
       "        0.66395548, 0.66395548, 0.66395548, 0.66395548, 0.66395548]),\n",
       " 'split5_test_precision': array([0.        , 0.70662906, 0.66079085, 0.66096998, 0.65619469,\n",
       "        0.65738285, 0.65711767, 0.65711767, 0.65711767, 0.65711767,\n",
       "        0.65711767, 0.65711767, 0.65682819, 0.65682819, 0.65682819,\n",
       "        0.65711767, 0.65682819, 0.65682819, 0.70662906, 0.66096998,\n",
       "        0.65738285, 0.65711767, 0.65711767, 0.65682819, 0.65682819,\n",
       "        0.65682819, 0.65682819, 0.65682819, 0.65682819, 0.65711767]),\n",
       " 'split6_test_precision': array([0.        , 0.71574642, 0.67473884, 0.67291   , 0.6692879 ,\n",
       "        0.66986063, 0.66926745, 0.66912083, 0.66912083, 0.66912083,\n",
       "        0.66912083, 0.66912083, 0.66912083, 0.66912083, 0.66912083,\n",
       "        0.66912083, 0.66912083, 0.66912083, 0.71574642, 0.67291   ,\n",
       "        0.66986063, 0.66912083, 0.66912083, 0.66912083, 0.66912083,\n",
       "        0.66912083, 0.66912083, 0.66912083, 0.66912083, 0.66912083]),\n",
       " 'split7_test_precision': array([0.        , 0.72205882, 0.68373494, 0.67864078, 0.66976312,\n",
       "        0.66867191, 0.66820489, 0.66789668, 0.66789498, 0.66789498,\n",
       "        0.66758748, 0.66789498, 0.66758748, 0.66758748, 0.66758748,\n",
       "        0.66758748, 0.66728026, 0.66758748, 0.72205882, 0.67864078,\n",
       "        0.66867191, 0.66789668, 0.66758748, 0.66758748, 0.66758748,\n",
       "        0.66758748, 0.66758748, 0.66758748, 0.66758748, 0.66758748]),\n",
       " 'split8_test_precision': array([0.        , 0.69681979, 0.66908915, 0.66204346, 0.65854739,\n",
       "        0.65739514, 0.65817223, 0.65817223, 0.65817223, 0.65817223,\n",
       "        0.65817223, 0.65817223, 0.65817223, 0.65817223, 0.65817223,\n",
       "        0.65817223, 0.65817223, 0.65817223, 0.69681979, 0.66204346,\n",
       "        0.65739514, 0.65817223, 0.65817223, 0.65817223, 0.65817223,\n",
       "        0.65817223, 0.65817223, 0.65817223, 0.65817223, 0.65817223]),\n",
       " 'split9_test_precision': array([0.        , 0.68443198, 0.66238894, 0.65898401, 0.65332734,\n",
       "        0.65248545, 0.650935  , 0.65095598, 0.65066667, 0.65066667,\n",
       "        0.65082186, 0.65066667, 0.65066667, 0.65082186, 0.65066667,\n",
       "        0.65066667, 0.65082186, 0.65082186, 0.68443198, 0.65898401,\n",
       "        0.65248545, 0.65095598, 0.65066667, 0.65066667, 0.65066667,\n",
       "        0.65066667, 0.65066667, 0.65066667, 0.65082186, 0.65066667]),\n",
       " 'mean_test_precision': array([0.        , 0.7040177 , 0.66935952, 0.66465052, 0.65894495,\n",
       "        0.65869672, 0.65796635, 0.65792455, 0.65789688, 0.65786845,\n",
       "        0.65785322, 0.65784037, 0.65778067, 0.65779619, 0.65780875,\n",
       "        0.65780961, 0.6577939 , 0.65782462, 0.7040177 , 0.66465052,\n",
       "        0.65869672, 0.65795266, 0.6578377 , 0.65778067, 0.65778067,\n",
       "        0.65778067, 0.65778067, 0.65778067, 0.65779619, 0.6578377 ]),\n",
       " 'std_test_precision': array([0.        , 0.01296001, 0.00813813, 0.00850008, 0.00830379,\n",
       "        0.00788413, 0.00795705, 0.00790304, 0.00793212, 0.00790981,\n",
       "        0.00785727, 0.00796955, 0.0079342 , 0.00792041, 0.0078744 ,\n",
       "        0.0079312 , 0.00790564, 0.00794295, 0.01296001, 0.00850008,\n",
       "        0.00788413, 0.00784344, 0.00787128, 0.0079342 , 0.0079342 ,\n",
       "        0.0079342 , 0.0079342 , 0.0079342 , 0.00792041, 0.00787128]),\n",
       " 'rank_test_precision': array([30,  1,  3,  4,  6,  7,  9, 11, 12, 13, 14, 15, 24, 21, 20, 19, 23,\n",
       "        18,  1,  4,  7, 10, 16, 24, 24, 24, 24, 24, 21, 16]),\n",
       " 'split0_test_f1_micro': array([0.60056577, 0.67991513, 0.69575672, 0.69844413, 0.69886846,\n",
       "        0.69915134, 0.69816124, 0.69816124, 0.6980198 , 0.6980198 ,\n",
       "        0.6980198 , 0.6980198 , 0.6980198 , 0.6980198 , 0.6980198 ,\n",
       "        0.6980198 , 0.6980198 , 0.6980198 , 0.67991513, 0.69844413,\n",
       "        0.69915134, 0.69816124, 0.6980198 , 0.6980198 , 0.6980198 ,\n",
       "        0.6980198 , 0.6980198 , 0.6980198 , 0.6980198 , 0.6980198 ]),\n",
       " 'split1_test_f1_micro': array([0.60056577, 0.67977369, 0.69943423, 0.7009901 , 0.70042433,\n",
       "        0.69971711, 0.70014144, 0.70028289, 0.70042433, 0.70042433,\n",
       "        0.70042433, 0.70042433, 0.70042433, 0.70042433, 0.70042433,\n",
       "        0.70042433, 0.70042433, 0.70042433, 0.67977369, 0.7009901 ,\n",
       "        0.69971711, 0.70028289, 0.70042433, 0.70042433, 0.70042433,\n",
       "        0.70042433, 0.70042433, 0.70042433, 0.70042433, 0.70042433]),\n",
       " 'split2_test_f1_micro': array([0.60065073, 0.6831235 , 0.69557222, 0.69345028, 0.69627953,\n",
       "        0.69613807, 0.69642099, 0.69642099, 0.69656246, 0.69656246,\n",
       "        0.69656246, 0.69656246, 0.69656246, 0.69656246, 0.69656246,\n",
       "        0.69656246, 0.69656246, 0.69656246, 0.6831235 , 0.69345028,\n",
       "        0.69613807, 0.69642099, 0.69656246, 0.69656246, 0.69656246,\n",
       "        0.69656246, 0.69656246, 0.69656246, 0.69656246, 0.69656246]),\n",
       " 'split3_test_f1_micro': array([0.60050927, 0.6747772 , 0.69274296, 0.69189419, 0.69076248,\n",
       "        0.69175272, 0.69161126, 0.69161126, 0.69161126, 0.69161126,\n",
       "        0.69161126, 0.6914698 , 0.6914698 , 0.6914698 , 0.69161126,\n",
       "        0.6914698 , 0.6914698 , 0.6914698 , 0.6747772 , 0.69189419,\n",
       "        0.69175272, 0.69175272, 0.69161126, 0.6914698 , 0.6914698 ,\n",
       "        0.6914698 , 0.6914698 , 0.6914698 , 0.6914698 , 0.69161126]),\n",
       " 'split4_test_f1_micro': array([0.60050927, 0.69401613, 0.70985995, 0.71014288, 0.70985995,\n",
       "        0.70957703, 0.70886971, 0.70901118, 0.70901118, 0.70886971,\n",
       "        0.70886971, 0.70886971, 0.70886971, 0.70886971, 0.70886971,\n",
       "        0.70886971, 0.70901118, 0.70901118, 0.69401613, 0.71014288,\n",
       "        0.70957703, 0.70901118, 0.70886971, 0.70886971, 0.70886971,\n",
       "        0.70886971, 0.70886971, 0.70886971, 0.70886971, 0.70886971]),\n",
       " 'split5_test_f1_micro': array([0.60050927, 0.68340642, 0.6959966 , 0.69910878, 0.70038195,\n",
       "        0.70123073, 0.70137219, 0.70137219, 0.70137219, 0.70137219,\n",
       "        0.70137219, 0.70137219, 0.70123073, 0.70123073, 0.70123073,\n",
       "        0.70137219, 0.70123073, 0.70123073, 0.68340642, 0.69910878,\n",
       "        0.70123073, 0.70137219, 0.70137219, 0.70123073, 0.70123073,\n",
       "        0.70123073, 0.70123073, 0.70123073, 0.70123073, 0.70137219]),\n",
       " 'split6_test_f1_micro': array([0.60050927, 0.69005517, 0.70462583, 0.70759655, 0.71014288,\n",
       "        0.71085019, 0.71099165, 0.71099165, 0.71099165, 0.71099165,\n",
       "        0.71099165, 0.71099165, 0.71099165, 0.71099165, 0.71099165,\n",
       "        0.71099165, 0.71099165, 0.71099165, 0.69005517, 0.70759655,\n",
       "        0.71085019, 0.71099165, 0.71099165, 0.71099165, 0.71099165,\n",
       "        0.71099165, 0.71099165, 0.71099165, 0.71099165, 0.71099165]),\n",
       " 'split7_test_f1_micro': array([0.60050927, 0.68595275, 0.70405998, 0.70462583, 0.70391852,\n",
       "        0.70363559, 0.70363559, 0.70349413, 0.70363559, 0.70363559,\n",
       "        0.70349413, 0.70363559, 0.70349413, 0.70349413, 0.70349413,\n",
       "        0.70349413, 0.70335267, 0.70349413, 0.68595275, 0.70462583,\n",
       "        0.70363559, 0.70349413, 0.70349413, 0.70349413, 0.70349413,\n",
       "        0.70349413, 0.70349413, 0.70349413, 0.70349413, 0.70349413]),\n",
       " 'split8_test_f1_micro': array([0.60050927, 0.679304  , 0.69925025, 0.69967464, 0.70179658,\n",
       "        0.70137219, 0.70236243, 0.70236243, 0.70236243, 0.70236243,\n",
       "        0.70236243, 0.70236243, 0.70236243, 0.70236243, 0.70236243,\n",
       "        0.70236243, 0.70236243, 0.70236243, 0.679304  , 0.69967464,\n",
       "        0.70137219, 0.70236243, 0.70236243, 0.70236243, 0.70236243,\n",
       "        0.70236243, 0.70236243, 0.70236243, 0.70236243, 0.70236243]),\n",
       " 'split9_test_f1_micro': array([0.60050927, 0.67491866, 0.69359174, 0.69613807, 0.69698684,\n",
       "        0.69684538, 0.69642099, 0.69656246, 0.69642099, 0.69642099,\n",
       "        0.69656246, 0.69642099, 0.69642099, 0.69656246, 0.69642099,\n",
       "        0.69642099, 0.69656246, 0.69656246, 0.67491866, 0.69613807,\n",
       "        0.69684538, 0.69656246, 0.69642099, 0.69642099, 0.69642099,\n",
       "        0.69642099, 0.69642099, 0.69642099, 0.69656246, 0.69642099]),\n",
       " 'mean_test_f1_micro': array([0.60053471, 0.68252427, 0.69908905, 0.70020654, 0.70094215,\n",
       "        0.70102704, 0.70099875, 0.70102704, 0.70104119, 0.70102704,\n",
       "        0.70102704, 0.7010129 , 0.7009846 , 0.70099875, 0.70099875,\n",
       "        0.70099875, 0.70099875, 0.7010129 , 0.68252427, 0.70020654,\n",
       "        0.70102704, 0.70104119, 0.7010129 , 0.7009846 , 0.7009846 ,\n",
       "        0.7009846 , 0.7009846 , 0.7009846 , 0.70099875, 0.7010129 ]),\n",
       " 'std_test_f1_micro': array([4.46337344e-05, 5.86749387e-03, 5.24454557e-03, 5.56215787e-03,\n",
       "        5.65386249e-03, 5.56054977e-03, 5.56403880e-03, 5.56417557e-03,\n",
       "        5.57634920e-03, 5.55625595e-03, 5.53821841e-03, 5.58033849e-03,\n",
       "        5.57306122e-03, 5.56162713e-03, 5.54901923e-03, 5.57384750e-03,\n",
       "        5.57564235e-03, 5.58177275e-03, 5.86749387e-03, 5.56215787e-03,\n",
       "        5.56054977e-03, 5.54034786e-03, 5.54977285e-03, 5.57306122e-03,\n",
       "        5.57306122e-03, 5.57306122e-03, 5.57306122e-03, 5.57306122e-03,\n",
       "        5.56162713e-03, 5.54977285e-03]),\n",
       " 'rank_test_f1_micro': array([30, 28, 27, 25, 24,  6, 12,  3,  1,  4,  4,  8, 18, 13, 13, 13, 13,\n",
       "         9, 28, 25,  6,  2, 10, 18, 18, 18, 18, 18, 13, 10])}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_model.cv_results_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a4578782",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([30, 28, 27, 25, 24,  6, 12,  3,  1,  4,  4,  8, 18, 13, 13, 13, 13,\n",
       "        9, 28, 25,  6,  2, 10, 18, 18, 18, 18, 18, 13, 10])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_model.cv_results_['rank_test_accuracy']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "29844e0d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'classifier': LogisticRegression(max_iter=10000),\n",
       " 'classifier__C': 0.0001,\n",
       " 'classifier__penalty': 'l1',\n",
       " 'classifier__solver': 'saga'}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_model.cv_results_['params'][ np.argmin(best_model.cv_results_['mean_test_accuracy'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "da854c46",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>classifier</th>\n",
       "      <th>C</th>\n",
       "      <th>penalty</th>\n",
       "      <th>solver</th>\n",
       "      <th>score_acc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.600535</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.682524</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.699089</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.700207</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0100</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.700942</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0100</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.701027</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.1000</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.700999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.1000</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.701027</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.701041</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.701027</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>10.0000</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.701027</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>10.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.701013</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>100.0000</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.700985</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>100.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.700999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>1000.0000</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.700999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>1000.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.700999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>10000.0000</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.700999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>10000.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.701013</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.682524</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.700207</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0100</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.701027</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.1000</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.701041</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.701013</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>10.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.700985</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>100.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.700985</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>1000.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.700985</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>10000.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.700985</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>NaN</td>\n",
       "      <td>none</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.700985</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>NaN</td>\n",
       "      <td>none</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.700999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.701013</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                            classifier           C penalty solver  score_acc\n",
       "0   LogisticRegression(max_iter=10000)      0.0001      l1   saga   0.600535\n",
       "1   LogisticRegression(max_iter=10000)      0.0001      l2   saga   0.682524\n",
       "2   LogisticRegression(max_iter=10000)      0.0010      l1   saga   0.699089\n",
       "3   LogisticRegression(max_iter=10000)      0.0010      l2   saga   0.700207\n",
       "4   LogisticRegression(max_iter=10000)      0.0100      l1   saga   0.700942\n",
       "5   LogisticRegression(max_iter=10000)      0.0100      l2   saga   0.701027\n",
       "6   LogisticRegression(max_iter=10000)      0.1000      l1   saga   0.700999\n",
       "7   LogisticRegression(max_iter=10000)      0.1000      l2   saga   0.701027\n",
       "8   LogisticRegression(max_iter=10000)      1.0000      l1   saga   0.701041\n",
       "9   LogisticRegression(max_iter=10000)      1.0000      l2   saga   0.701027\n",
       "10  LogisticRegression(max_iter=10000)     10.0000      l1   saga   0.701027\n",
       "11  LogisticRegression(max_iter=10000)     10.0000      l2   saga   0.701013\n",
       "12  LogisticRegression(max_iter=10000)    100.0000      l1   saga   0.700985\n",
       "13  LogisticRegression(max_iter=10000)    100.0000      l2   saga   0.700999\n",
       "14  LogisticRegression(max_iter=10000)   1000.0000      l1   saga   0.700999\n",
       "15  LogisticRegression(max_iter=10000)   1000.0000      l2   saga   0.700999\n",
       "16  LogisticRegression(max_iter=10000)  10000.0000      l1   saga   0.700999\n",
       "17  LogisticRegression(max_iter=10000)  10000.0000      l2   saga   0.701013\n",
       "18  LogisticRegression(max_iter=10000)      0.0001      l2  lbfgs   0.682524\n",
       "19  LogisticRegression(max_iter=10000)      0.0010      l2  lbfgs   0.700207\n",
       "20  LogisticRegression(max_iter=10000)      0.0100      l2  lbfgs   0.701027\n",
       "21  LogisticRegression(max_iter=10000)      0.1000      l2  lbfgs   0.701041\n",
       "22  LogisticRegression(max_iter=10000)      1.0000      l2  lbfgs   0.701013\n",
       "23  LogisticRegression(max_iter=10000)     10.0000      l2  lbfgs   0.700985\n",
       "24  LogisticRegression(max_iter=10000)    100.0000      l2  lbfgs   0.700985\n",
       "25  LogisticRegression(max_iter=10000)   1000.0000      l2  lbfgs   0.700985\n",
       "26  LogisticRegression(max_iter=10000)  10000.0000      l2  lbfgs   0.700985\n",
       "27  LogisticRegression(max_iter=10000)         NaN    none  lbfgs   0.700985\n",
       "28  LogisticRegression(max_iter=10000)         NaN    none   saga   0.700999\n",
       "29  LogisticRegression(max_iter=10000)         NaN     NaN    NaN   0.701013"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results = pd.DataFrame( best_model.cv_results_['params'] )\n",
    "\n",
    "results['score_acc'] = best_model.cv_results_['mean_test_accuracy']\n",
    "cols = results.columns.to_series().str.split('__').apply(lambda x: x[-1])\n",
    "results.columns = cols\n",
    "\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cec21d10",
   "metadata": {},
   "outputs": [],
   "source": [
    "acc_list.append(results[0:1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8e72c958",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'classifier': LogisticRegression(max_iter=10000),\n",
       " 'classifier__C': 0.0001,\n",
       " 'classifier__penalty': 'l2',\n",
       " 'classifier__solver': 'saga'}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_model.cv_results_['params'][ np.argmin(best_model.cv_results_['rank_test_precision'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "426600e6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>classifier</th>\n",
       "      <th>C</th>\n",
       "      <th>penalty</th>\n",
       "      <th>solver</th>\n",
       "      <th>score_precision</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.704018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.669360</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.664651</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0100</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.658945</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0100</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.658697</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.1000</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.657966</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.1000</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.657925</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.657897</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.657868</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>10.0000</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.657853</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>10.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.657840</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>100.0000</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.657781</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>100.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.657796</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>1000.0000</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.657809</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>1000.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.657810</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>10000.0000</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.657794</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>10000.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.657825</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.704018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.664651</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0100</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.658697</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.1000</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.657953</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.657838</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>10.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.657781</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>100.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.657781</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>1000.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.657781</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>10000.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.657781</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>NaN</td>\n",
       "      <td>none</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.657781</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>NaN</td>\n",
       "      <td>none</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.657796</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.657838</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                            classifier           C penalty solver  \\\n",
       "0   LogisticRegression(max_iter=10000)      0.0001      l1   saga   \n",
       "1   LogisticRegression(max_iter=10000)      0.0001      l2   saga   \n",
       "2   LogisticRegression(max_iter=10000)      0.0010      l1   saga   \n",
       "3   LogisticRegression(max_iter=10000)      0.0010      l2   saga   \n",
       "4   LogisticRegression(max_iter=10000)      0.0100      l1   saga   \n",
       "5   LogisticRegression(max_iter=10000)      0.0100      l2   saga   \n",
       "6   LogisticRegression(max_iter=10000)      0.1000      l1   saga   \n",
       "7   LogisticRegression(max_iter=10000)      0.1000      l2   saga   \n",
       "8   LogisticRegression(max_iter=10000)      1.0000      l1   saga   \n",
       "9   LogisticRegression(max_iter=10000)      1.0000      l2   saga   \n",
       "10  LogisticRegression(max_iter=10000)     10.0000      l1   saga   \n",
       "11  LogisticRegression(max_iter=10000)     10.0000      l2   saga   \n",
       "12  LogisticRegression(max_iter=10000)    100.0000      l1   saga   \n",
       "13  LogisticRegression(max_iter=10000)    100.0000      l2   saga   \n",
       "14  LogisticRegression(max_iter=10000)   1000.0000      l1   saga   \n",
       "15  LogisticRegression(max_iter=10000)   1000.0000      l2   saga   \n",
       "16  LogisticRegression(max_iter=10000)  10000.0000      l1   saga   \n",
       "17  LogisticRegression(max_iter=10000)  10000.0000      l2   saga   \n",
       "18  LogisticRegression(max_iter=10000)      0.0001      l2  lbfgs   \n",
       "19  LogisticRegression(max_iter=10000)      0.0010      l2  lbfgs   \n",
       "20  LogisticRegression(max_iter=10000)      0.0100      l2  lbfgs   \n",
       "21  LogisticRegression(max_iter=10000)      0.1000      l2  lbfgs   \n",
       "22  LogisticRegression(max_iter=10000)      1.0000      l2  lbfgs   \n",
       "23  LogisticRegression(max_iter=10000)     10.0000      l2  lbfgs   \n",
       "24  LogisticRegression(max_iter=10000)    100.0000      l2  lbfgs   \n",
       "25  LogisticRegression(max_iter=10000)   1000.0000      l2  lbfgs   \n",
       "26  LogisticRegression(max_iter=10000)  10000.0000      l2  lbfgs   \n",
       "27  LogisticRegression(max_iter=10000)         NaN    none  lbfgs   \n",
       "28  LogisticRegression(max_iter=10000)         NaN    none   saga   \n",
       "29  LogisticRegression(max_iter=10000)         NaN     NaN    NaN   \n",
       "\n",
       "    score_precision  \n",
       "0          0.000000  \n",
       "1          0.704018  \n",
       "2          0.669360  \n",
       "3          0.664651  \n",
       "4          0.658945  \n",
       "5          0.658697  \n",
       "6          0.657966  \n",
       "7          0.657925  \n",
       "8          0.657897  \n",
       "9          0.657868  \n",
       "10         0.657853  \n",
       "11         0.657840  \n",
       "12         0.657781  \n",
       "13         0.657796  \n",
       "14         0.657809  \n",
       "15         0.657810  \n",
       "16         0.657794  \n",
       "17         0.657825  \n",
       "18         0.704018  \n",
       "19         0.664651  \n",
       "20         0.658697  \n",
       "21         0.657953  \n",
       "22         0.657838  \n",
       "23         0.657781  \n",
       "24         0.657781  \n",
       "25         0.657781  \n",
       "26         0.657781  \n",
       "27         0.657781  \n",
       "28         0.657796  \n",
       "29         0.657838  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results = pd.DataFrame( best_model.cv_results_['params'] )\n",
    "\n",
    "results['score_precision'] = best_model.cv_results_['mean_test_precision']\n",
    "cols = results.columns.to_series().str.split('__').apply(lambda x: x[-1])\n",
    "results.columns = cols\n",
    "\n",
    "\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "46a6f899",
   "metadata": {},
   "outputs": [],
   "source": [
    "roc_auc_ovr_List.append(results[1:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6ceb54de",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'classifier': LogisticRegression(max_iter=10000),\n",
       " 'classifier__C': 1.0,\n",
       " 'classifier__penalty': 'l1',\n",
       " 'classifier__solver': 'saga'}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_model.cv_results_['params'][ np.argmin(best_model.cv_results_['rank_test_f1_micro'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "886a47b2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>classifier</th>\n",
       "      <th>C</th>\n",
       "      <th>penalty</th>\n",
       "      <th>solver</th>\n",
       "      <th>score_f1_micro</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.600535</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.682524</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.699089</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.700207</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0100</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.700942</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0100</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.701027</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.1000</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.700999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.1000</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.701027</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.701041</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.701027</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>10.0000</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.701027</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>10.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.701013</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>100.0000</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.700985</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>100.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.700999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>1000.0000</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.700999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>1000.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.700999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>10000.0000</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.700999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>10000.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.701013</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.682524</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.700207</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0100</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.701027</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.1000</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.701041</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.701013</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>10.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.700985</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>100.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.700985</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>1000.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.700985</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>10000.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.700985</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>NaN</td>\n",
       "      <td>none</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.700985</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>NaN</td>\n",
       "      <td>none</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.700999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.701013</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                            classifier           C penalty solver  \\\n",
       "0   LogisticRegression(max_iter=10000)      0.0001      l1   saga   \n",
       "1   LogisticRegression(max_iter=10000)      0.0001      l2   saga   \n",
       "2   LogisticRegression(max_iter=10000)      0.0010      l1   saga   \n",
       "3   LogisticRegression(max_iter=10000)      0.0010      l2   saga   \n",
       "4   LogisticRegression(max_iter=10000)      0.0100      l1   saga   \n",
       "5   LogisticRegression(max_iter=10000)      0.0100      l2   saga   \n",
       "6   LogisticRegression(max_iter=10000)      0.1000      l1   saga   \n",
       "7   LogisticRegression(max_iter=10000)      0.1000      l2   saga   \n",
       "8   LogisticRegression(max_iter=10000)      1.0000      l1   saga   \n",
       "9   LogisticRegression(max_iter=10000)      1.0000      l2   saga   \n",
       "10  LogisticRegression(max_iter=10000)     10.0000      l1   saga   \n",
       "11  LogisticRegression(max_iter=10000)     10.0000      l2   saga   \n",
       "12  LogisticRegression(max_iter=10000)    100.0000      l1   saga   \n",
       "13  LogisticRegression(max_iter=10000)    100.0000      l2   saga   \n",
       "14  LogisticRegression(max_iter=10000)   1000.0000      l1   saga   \n",
       "15  LogisticRegression(max_iter=10000)   1000.0000      l2   saga   \n",
       "16  LogisticRegression(max_iter=10000)  10000.0000      l1   saga   \n",
       "17  LogisticRegression(max_iter=10000)  10000.0000      l2   saga   \n",
       "18  LogisticRegression(max_iter=10000)      0.0001      l2  lbfgs   \n",
       "19  LogisticRegression(max_iter=10000)      0.0010      l2  lbfgs   \n",
       "20  LogisticRegression(max_iter=10000)      0.0100      l2  lbfgs   \n",
       "21  LogisticRegression(max_iter=10000)      0.1000      l2  lbfgs   \n",
       "22  LogisticRegression(max_iter=10000)      1.0000      l2  lbfgs   \n",
       "23  LogisticRegression(max_iter=10000)     10.0000      l2  lbfgs   \n",
       "24  LogisticRegression(max_iter=10000)    100.0000      l2  lbfgs   \n",
       "25  LogisticRegression(max_iter=10000)   1000.0000      l2  lbfgs   \n",
       "26  LogisticRegression(max_iter=10000)  10000.0000      l2  lbfgs   \n",
       "27  LogisticRegression(max_iter=10000)         NaN    none  lbfgs   \n",
       "28  LogisticRegression(max_iter=10000)         NaN    none   saga   \n",
       "29  LogisticRegression(max_iter=10000)         NaN     NaN    NaN   \n",
       "\n",
       "    score_f1_micro  \n",
       "0         0.600535  \n",
       "1         0.682524  \n",
       "2         0.699089  \n",
       "3         0.700207  \n",
       "4         0.700942  \n",
       "5         0.701027  \n",
       "6         0.700999  \n",
       "7         0.701027  \n",
       "8         0.701041  \n",
       "9         0.701027  \n",
       "10        0.701027  \n",
       "11        0.701013  \n",
       "12        0.700985  \n",
       "13        0.700999  \n",
       "14        0.700999  \n",
       "15        0.700999  \n",
       "16        0.700999  \n",
       "17        0.701013  \n",
       "18        0.682524  \n",
       "19        0.700207  \n",
       "20        0.701027  \n",
       "21        0.701041  \n",
       "22        0.701013  \n",
       "23        0.700985  \n",
       "24        0.700985  \n",
       "25        0.700985  \n",
       "26        0.700985  \n",
       "27        0.700985  \n",
       "28        0.700999  \n",
       "29        0.701013  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results = pd.DataFrame( best_model.cv_results_['params'] )\n",
    "\n",
    "\n",
    "results['score_f1_micro'] = best_model.cv_results_['mean_test_f1_micro']\n",
    "cols = results.columns.to_series().str.split('__').apply(lambda x: x[-1])\n",
    "results.columns = cols\n",
    "\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "943aea19",
   "metadata": {},
   "outputs": [],
   "source": [
    "f1_micro_List.append(results[8:9])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02458d54",
   "metadata": {},
   "source": [
    "## Trial 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7bf26ccd",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, Y_train, Y_test = train_test_split(X, y, test_size=0.2, shuffle=True, random_state = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4628ee31",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ENT\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\ENT\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\ENT\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\ENT\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\ENT\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\ENT\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\ENT\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\ENT\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\ENT\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\ENT\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\ENT\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1183: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\ENT\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1183: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\ENT\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1183: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\ENT\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1183: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\ENT\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1183: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\ENT\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1183: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\ENT\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1183: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\ENT\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1183: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\ENT\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1183: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\ENT\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1183: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\ENT\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1183: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\ENT\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1183: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\ENT\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1183: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\ENT\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1183: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\ENT\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1183: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\ENT\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1183: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\ENT\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1183: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\ENT\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1183: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\ENT\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1183: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\ENT\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1183: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Create a pipeline - RF is a stand in, we will populate the classifier part below\n",
    "pipe = Pipeline([('std', StandardScaler()),\n",
    "                 ('classifier', LogisticRegression())])\n",
    "\n",
    "# Create search space of candidate learning algorithms and their hyperparameters\n",
    "# note lbfgs can't do l1, and if you pass penalty='none' it expects no C value\n",
    "search_space = [{'classifier': [LogisticRegression(max_iter=10000)],\n",
    "                 'classifier__solver': ['saga'],\n",
    "                 'classifier__penalty': ['l1', 'l2'],\n",
    "                 'classifier__C': np.logspace(-4, 4, 9)},\n",
    "                {'classifier': [LogisticRegression(max_iter=10000)],\n",
    "                 'classifier__solver': ['lbfgs'],\n",
    "                 'classifier__penalty': ['l2'],\n",
    "                 'classifier__C': np.logspace(-4, 4, 9)},\n",
    "                {'classifier': [LogisticRegression(max_iter=10000)],\n",
    "                 'classifier__solver': ['lbfgs','saga'],\n",
    "                 'classifier__penalty': ['none']},\n",
    "                {'classifier': [LogisticRegression(max_iter=10000)]}\n",
    "                ]\n",
    "\n",
    "# Create grid search\n",
    "clf = GridSearchCV(pipe, search_space, cv=StratifiedKFold(n_splits=10),\n",
    "                   scoring=['accuracy', 'precision', 'f1_micro'], refit=False,\n",
    "                   verbose=0)\n",
    "\n",
    "# Fit grid search\n",
    "best_model = clf.fit(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2bf2b4d0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'mean_fit_time': array([0.04892576, 0.10178926, 0.11230183, 0.10224841, 0.12948077,\n",
       "        0.11410449, 0.12701383, 0.10836613, 0.12756827, 0.12151308,\n",
       "        0.13295162, 0.11428809, 0.12567647, 0.11730998, 0.13091736,\n",
       "        0.11289821, 0.12977314, 0.11625695, 0.0244159 , 0.02319152,\n",
       "        0.02306058, 0.02285249, 0.0229697 , 0.02405984, 0.02314618,\n",
       "        0.02320399, 0.02290359, 0.02288551, 0.121999  , 0.02305312]),\n",
       " 'std_fit_time': array([0.00562913, 0.00702792, 0.00707462, 0.00574355, 0.01137634,\n",
       "        0.0085454 , 0.00960634, 0.00646496, 0.0103238 , 0.01030911,\n",
       "        0.01123925, 0.00764683, 0.01095562, 0.00782899, 0.00990571,\n",
       "        0.01026722, 0.00877325, 0.0067251 , 0.0014024 , 0.00103188,\n",
       "        0.00153561, 0.00071822, 0.00075083, 0.00323503, 0.00186818,\n",
       "        0.00115851, 0.00068914, 0.00085995, 0.01007896, 0.00064953]),\n",
       " 'mean_score_time': array([0.00742869, 0.00770686, 0.00780714, 0.00790687, 0.0080569 ,\n",
       "        0.0081054 , 0.00810702, 0.00802884, 0.00810726, 0.00840886,\n",
       "        0.00790718, 0.00793431, 0.00854626, 0.00820773, 0.00800679,\n",
       "        0.00780914, 0.00820739, 0.00823669, 0.00744128, 0.0076071 ,\n",
       "        0.00812361, 0.00760796, 0.007937  , 0.00856509, 0.00802207,\n",
       "        0.00822575, 0.00750694, 0.00800884, 0.00810752, 0.00786986]),\n",
       " 'std_score_time': array([5.21082159e-04, 4.58656268e-04, 4.00424089e-04, 3.00153269e-04,\n",
       "        3.49692903e-04, 5.36066340e-04, 5.39146082e-04, 4.78053059e-04,\n",
       "        5.39146092e-04, 6.61563949e-04, 5.38898400e-04, 2.19019525e-04,\n",
       "        4.74728729e-04, 4.00341671e-04, 8.62600923e-07, 3.51675978e-04,\n",
       "        6.00592349e-04, 3.95557689e-04, 4.72642509e-04, 6.64153938e-04,\n",
       "        2.98792884e-04, 6.63309346e-04, 5.78551250e-04, 4.54709089e-04,\n",
       "        3.71774004e-05, 3.95052507e-04, 5.00679177e-04, 7.75416470e-04,\n",
       "        7.00562495e-04, 7.88418599e-04]),\n",
       " 'param_classifier': masked_array(data=[LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000)],\n",
       "              mask=[False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'param_classifier__C': masked_array(data=[0.0001, 0.0001, 0.001, 0.001, 0.01, 0.01, 0.1, 0.1,\n",
       "                    1.0, 1.0, 10.0, 10.0, 100.0, 100.0, 1000.0, 1000.0,\n",
       "                    10000.0, 10000.0, 0.0001, 0.001, 0.01, 0.1, 1.0, 10.0,\n",
       "                    100.0, 1000.0, 10000.0, --, --, --],\n",
       "              mask=[False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False,  True,  True,  True],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'param_classifier__penalty': masked_array(data=['l1', 'l2', 'l1', 'l2', 'l1', 'l2', 'l1', 'l2', 'l1',\n",
       "                    'l2', 'l1', 'l2', 'l1', 'l2', 'l1', 'l2', 'l1', 'l2',\n",
       "                    'l2', 'l2', 'l2', 'l2', 'l2', 'l2', 'l2', 'l2', 'l2',\n",
       "                    'none', 'none', --],\n",
       "              mask=[False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False,  True],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'param_classifier__solver': masked_array(data=['saga', 'saga', 'saga', 'saga', 'saga', 'saga', 'saga',\n",
       "                    'saga', 'saga', 'saga', 'saga', 'saga', 'saga', 'saga',\n",
       "                    'saga', 'saga', 'saga', 'saga', 'lbfgs', 'lbfgs',\n",
       "                    'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs',\n",
       "                    'lbfgs', 'lbfgs', 'saga', --],\n",
       "              mask=[False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False,  True],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'params': [{'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 0.0001,\n",
       "   'classifier__penalty': 'l1',\n",
       "   'classifier__solver': 'saga'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 0.0001,\n",
       "   'classifier__penalty': 'l2',\n",
       "   'classifier__solver': 'saga'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 0.001,\n",
       "   'classifier__penalty': 'l1',\n",
       "   'classifier__solver': 'saga'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 0.001,\n",
       "   'classifier__penalty': 'l2',\n",
       "   'classifier__solver': 'saga'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 0.01,\n",
       "   'classifier__penalty': 'l1',\n",
       "   'classifier__solver': 'saga'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 0.01,\n",
       "   'classifier__penalty': 'l2',\n",
       "   'classifier__solver': 'saga'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 0.1,\n",
       "   'classifier__penalty': 'l1',\n",
       "   'classifier__solver': 'saga'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 0.1,\n",
       "   'classifier__penalty': 'l2',\n",
       "   'classifier__solver': 'saga'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 1.0,\n",
       "   'classifier__penalty': 'l1',\n",
       "   'classifier__solver': 'saga'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 1.0,\n",
       "   'classifier__penalty': 'l2',\n",
       "   'classifier__solver': 'saga'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 10.0,\n",
       "   'classifier__penalty': 'l1',\n",
       "   'classifier__solver': 'saga'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 10.0,\n",
       "   'classifier__penalty': 'l2',\n",
       "   'classifier__solver': 'saga'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 100.0,\n",
       "   'classifier__penalty': 'l1',\n",
       "   'classifier__solver': 'saga'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 100.0,\n",
       "   'classifier__penalty': 'l2',\n",
       "   'classifier__solver': 'saga'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 1000.0,\n",
       "   'classifier__penalty': 'l1',\n",
       "   'classifier__solver': 'saga'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 1000.0,\n",
       "   'classifier__penalty': 'l2',\n",
       "   'classifier__solver': 'saga'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 10000.0,\n",
       "   'classifier__penalty': 'l1',\n",
       "   'classifier__solver': 'saga'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 10000.0,\n",
       "   'classifier__penalty': 'l2',\n",
       "   'classifier__solver': 'saga'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 0.0001,\n",
       "   'classifier__penalty': 'l2',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 0.001,\n",
       "   'classifier__penalty': 'l2',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 0.01,\n",
       "   'classifier__penalty': 'l2',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 0.1,\n",
       "   'classifier__penalty': 'l2',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 1.0,\n",
       "   'classifier__penalty': 'l2',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 10.0,\n",
       "   'classifier__penalty': 'l2',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 100.0,\n",
       "   'classifier__penalty': 'l2',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 1000.0,\n",
       "   'classifier__penalty': 'l2',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 10000.0,\n",
       "   'classifier__penalty': 'l2',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__penalty': 'none',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__penalty': 'none',\n",
       "   'classifier__solver': 'saga'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000)}],\n",
       " 'split0_test_accuracy': array([0.59915134, 0.69391796, 0.71230552, 0.71301273, 0.71598303,\n",
       "        0.7155587 , 0.71499293, 0.71499293, 0.71499293, 0.71499293,\n",
       "        0.71499293, 0.71499293, 0.71499293, 0.71499293, 0.71499293,\n",
       "        0.71499293, 0.71499293, 0.71499293, 0.69391796, 0.71301273,\n",
       "        0.7155587 , 0.71499293, 0.71499293, 0.71499293, 0.71499293,\n",
       "        0.71499293, 0.71499293, 0.71499293, 0.71499293, 0.71499293]),\n",
       " 'split1_test_accuracy': array([0.59915134, 0.68019802, 0.70014144, 0.69929279, 0.70084866,\n",
       "        0.70084866, 0.70084866, 0.70084866, 0.7009901 , 0.70084866,\n",
       "        0.7009901 , 0.7009901 , 0.7009901 , 0.7009901 , 0.7009901 ,\n",
       "        0.7009901 , 0.7009901 , 0.7009901 , 0.68019802, 0.69929279,\n",
       "        0.70084866, 0.70084866, 0.7009901 , 0.7009901 , 0.7009901 ,\n",
       "        0.7009901 , 0.7009901 , 0.7009901 , 0.7009901 , 0.7009901 ]),\n",
       " 'split2_test_accuracy': array([0.5992361 , 0.68453812, 0.6998161 , 0.70264535, 0.70349413,\n",
       "        0.70349413, 0.70391852, 0.70391852, 0.70391852, 0.70391852,\n",
       "        0.70391852, 0.70391852, 0.70391852, 0.70391852, 0.70391852,\n",
       "        0.70391852, 0.70391852, 0.70391852, 0.68453812, 0.70264535,\n",
       "        0.70349413, 0.70391852, 0.70391852, 0.70391852, 0.70391852,\n",
       "        0.70391852, 0.70391852, 0.70391852, 0.70391852, 0.70391852]),\n",
       " 'split3_test_accuracy': array([0.59909464, 0.68524544, 0.69896732, 0.69712831, 0.69726977,\n",
       "        0.69741123, 0.69698684, 0.69712831, 0.69712831, 0.69712831,\n",
       "        0.69712831, 0.69712831, 0.69712831, 0.69712831, 0.69712831,\n",
       "        0.69712831, 0.69712831, 0.69712831, 0.68524544, 0.69726977,\n",
       "        0.69741123, 0.69712831, 0.69712831, 0.69712831, 0.69712831,\n",
       "        0.69712831, 0.69712831, 0.69712831, 0.69712831, 0.69712831]),\n",
       " 'split4_test_accuracy': array([0.59909464, 0.67732353, 0.68977225, 0.6914698 , 0.6937332 ,\n",
       "        0.69401613, 0.69444051, 0.69444051, 0.69444051, 0.69444051,\n",
       "        0.69444051, 0.69444051, 0.69444051, 0.69444051, 0.69444051,\n",
       "        0.69444051, 0.69444051, 0.69444051, 0.67732353, 0.6914698 ,\n",
       "        0.69401613, 0.69444051, 0.69444051, 0.69444051, 0.69444051,\n",
       "        0.69444051, 0.69444051, 0.69444051, 0.69444051, 0.69444051]),\n",
       " 'split5_test_accuracy': array([0.59909464, 0.68241618, 0.69458198, 0.69415759, 0.69543075,\n",
       "        0.69585514, 0.69571368, 0.69557222, 0.69543075, 0.69543075,\n",
       "        0.69557222, 0.69557222, 0.69557222, 0.69557222, 0.69557222,\n",
       "        0.69557222, 0.69557222, 0.69557222, 0.68241618, 0.69415759,\n",
       "        0.69585514, 0.69557222, 0.69543075, 0.69557222, 0.69557222,\n",
       "        0.69557222, 0.69557222, 0.69557222, 0.69557222, 0.69543075]),\n",
       " 'split6_test_accuracy': array([0.59909464, 0.68963078, 0.70632338, 0.71014288, 0.70886971,\n",
       "        0.7092941 , 0.7092941 , 0.7092941 , 0.7092941 , 0.7092941 ,\n",
       "        0.7092941 , 0.7092941 , 0.7092941 , 0.7092941 , 0.7092941 ,\n",
       "        0.7092941 , 0.7092941 , 0.7092941 , 0.68963078, 0.71014288,\n",
       "        0.7092941 , 0.7092941 , 0.7092941 , 0.7092941 , 0.7092941 ,\n",
       "        0.7092941 , 0.7092941 , 0.7092941 , 0.7092941 , 0.7092941 ]),\n",
       " 'split7_test_accuracy': array([0.59909464, 0.68581129, 0.70179658, 0.70405998, 0.70462583,\n",
       "        0.70420144, 0.70434291, 0.70420144, 0.70448437, 0.70448437,\n",
       "        0.70448437, 0.70448437, 0.70448437, 0.70448437, 0.70448437,\n",
       "        0.70448437, 0.70448437, 0.70448437, 0.68581129, 0.70405998,\n",
       "        0.70420144, 0.70420144, 0.70448437, 0.70448437, 0.70448437,\n",
       "        0.70448437, 0.70448437, 0.70448437, 0.70448437, 0.70448437]),\n",
       " 'split8_test_accuracy': array([0.59909464, 0.6853869 , 0.70264535, 0.70123073, 0.7020795 ,\n",
       "        0.70222096, 0.70222096, 0.70222096, 0.70222096, 0.70236243,\n",
       "        0.70236243, 0.70222096, 0.70222096, 0.70236243, 0.70236243,\n",
       "        0.70222096, 0.70236243, 0.70222096, 0.6853869 , 0.70123073,\n",
       "        0.70222096, 0.70222096, 0.70222096, 0.70236243, 0.70236243,\n",
       "        0.70236243, 0.70236243, 0.70236243, 0.70222096, 0.70222096]),\n",
       " 'split9_test_accuracy': array([0.59909464, 0.68694299, 0.70434291, 0.70547461, 0.70476729,\n",
       "        0.70476729, 0.70490876, 0.70490876, 0.70490876, 0.70462583,\n",
       "        0.70462583, 0.70462583, 0.70462583, 0.70462583, 0.70462583,\n",
       "        0.70462583, 0.70462583, 0.70462583, 0.68694299, 0.70547461,\n",
       "        0.70476729, 0.70490876, 0.70462583, 0.70462583, 0.70462583,\n",
       "        0.70462583, 0.70462583, 0.70462583, 0.70462583, 0.70462583]),\n",
       " 'mean_test_accuracy': array([0.59912013, 0.68514112, 0.70106928, 0.70186148, 0.70271019,\n",
       "        0.70276678, 0.70276679, 0.70275264, 0.70278093, 0.70275264,\n",
       "        0.70278093, 0.70276678, 0.70276678, 0.70278093, 0.70278093,\n",
       "        0.70276678, 0.70278093, 0.70276678, 0.68514112, 0.70187562,\n",
       "        0.70276678, 0.70275264, 0.70275264, 0.70278093, 0.70278093,\n",
       "        0.70278093, 0.70278093, 0.70278093, 0.70276678, 0.70275264]),\n",
       " 'std_test_accuracy': array([4.46616433e-05, 4.40789189e-03, 5.86639470e-03, 6.38882166e-03,\n",
       "        6.24228807e-03, 6.08138249e-03, 5.97445426e-03, 5.97422260e-03,\n",
       "        5.99448196e-03, 5.98827471e-03, 5.96670929e-03, 5.96785232e-03,\n",
       "        5.96785232e-03, 5.96670929e-03, 5.96670929e-03, 5.96785232e-03,\n",
       "        5.96670929e-03, 5.96785232e-03, 4.40789189e-03, 6.37847395e-03,\n",
       "        6.08138249e-03, 5.97422260e-03, 5.98503258e-03, 5.96670929e-03,\n",
       "        5.96670929e-03, 5.96670929e-03, 5.96670929e-03, 5.96670929e-03,\n",
       "        5.96785232e-03, 5.98503258e-03]),\n",
       " 'rank_test_accuracy': array([30, 28, 27, 26, 24, 17, 11, 19,  1, 21,  1, 12, 12,  1,  1, 12,  1,\n",
       "        12, 28, 25, 17, 19, 22,  1,  1,  1,  1,  1, 12, 22]),\n",
       " 'split0_test_precision': array([0.        , 0.72453083, 0.69011407, 0.6864289 , 0.68371886,\n",
       "        0.68280764, 0.68095449, 0.68095449, 0.68095449, 0.68095449,\n",
       "        0.68095449, 0.68095449, 0.68095449, 0.68095449, 0.68095449,\n",
       "        0.68095449, 0.68095449, 0.68095449, 0.72453083, 0.6864289 ,\n",
       "        0.68280764, 0.68095449, 0.68095449, 0.68095449, 0.68095449,\n",
       "        0.68095449, 0.68095449, 0.68095449, 0.68095449, 0.68095449]),\n",
       " 'split1_test_precision': array([0.        , 0.6993737 , 0.67146974, 0.66253444, 0.65802198,\n",
       "        0.65788318, 0.65733042, 0.65733042, 0.65748031, 0.65733042,\n",
       "        0.65748031, 0.65748031, 0.65748031, 0.65748031, 0.65748031,\n",
       "        0.65748031, 0.65748031, 0.65748031, 0.6993737 , 0.66253444,\n",
       "        0.65788318, 0.65733042, 0.65748031, 0.65748031, 0.65748031,\n",
       "        0.65748031, 0.65748031, 0.65748031, 0.65748031, 0.65748031]),\n",
       " 'split2_test_precision': array([0.        , 0.70721649, 0.67083133, 0.66866636, 0.66312528,\n",
       "        0.66283694, 0.66299559, 0.66299559, 0.66299559, 0.66299559,\n",
       "        0.66299559, 0.66299559, 0.66299559, 0.66299559, 0.66299559,\n",
       "        0.66299559, 0.66299559, 0.66299559, 0.70721649, 0.66866636,\n",
       "        0.66283694, 0.66299559, 0.66299559, 0.66299559, 0.66299559,\n",
       "        0.66299559, 0.66299559, 0.66299559, 0.66299559, 0.66299559]),\n",
       " 'split3_test_precision': array([0.        , 0.70784983, 0.66698202, 0.65843621, 0.6520596 ,\n",
       "        0.65194578, 0.65069686, 0.65084893, 0.65084893, 0.65084893,\n",
       "        0.65084893, 0.65084893, 0.65084893, 0.65084893, 0.65084893,\n",
       "        0.65084893, 0.65084893, 0.65084893, 0.70784983, 0.65859232,\n",
       "        0.65194578, 0.65084893, 0.65084893, 0.65084893, 0.65084893,\n",
       "        0.65084893, 0.65084893, 0.65084893, 0.65084893, 0.65084893]),\n",
       " 'split4_test_precision': array([0.        , 0.69623847, 0.6582716 , 0.65437352, 0.6518384 ,\n",
       "        0.65187868, 0.65221319, 0.65221319, 0.65221319, 0.65221319,\n",
       "        0.65221319, 0.65221319, 0.65221319, 0.65221319, 0.65221319,\n",
       "        0.65221319, 0.65221319, 0.65221319, 0.69623847, 0.65437352,\n",
       "        0.65187868, 0.65221319, 0.65221319, 0.65221319, 0.65221319,\n",
       "        0.65221319, 0.65221319, 0.65221319, 0.65221319, 0.65221319]),\n",
       " 'split5_test_precision': array([0.        , 0.69620253, 0.65531523, 0.64814815, 0.64300714,\n",
       "        0.64309623, 0.64235098, 0.64220183, 0.64193414, 0.64193414,\n",
       "        0.64208333, 0.64208333, 0.64208333, 0.64208333, 0.64208333,\n",
       "        0.64208333, 0.64208333, 0.64208333, 0.69620253, 0.64814815,\n",
       "        0.64309623, 0.64220183, 0.64193414, 0.64208333, 0.64208333,\n",
       "        0.64208333, 0.64208333, 0.64208333, 0.64208333, 0.64193414]),\n",
       " 'split6_test_precision': array([0.        , 0.72630835, 0.68415938, 0.68366869, 0.67556561,\n",
       "        0.67552952, 0.67474204, 0.67474204, 0.67474204, 0.67474204,\n",
       "        0.67474204, 0.67474204, 0.67474204, 0.67474204, 0.67474204,\n",
       "        0.67474204, 0.67474204, 0.67474204, 0.72630835, 0.68366869,\n",
       "        0.67552952, 0.67474204, 0.67474204, 0.67474204, 0.67474204,\n",
       "        0.67474204, 0.67474204, 0.67474204, 0.67474204, 0.67474204]),\n",
       " 'split7_test_precision': array([0.        , 0.70695476, 0.67418426, 0.67049632, 0.66681574,\n",
       "        0.66562639, 0.6651865 , 0.66489126, 0.66504209, 0.66504209,\n",
       "        0.66504209, 0.66504209, 0.66504209, 0.66504209, 0.66504209,\n",
       "        0.66504209, 0.66504209, 0.66504209, 0.70695476, 0.67049632,\n",
       "        0.66562639, 0.66489126, 0.66504209, 0.66504209, 0.66504209,\n",
       "        0.66504209, 0.66504209, 0.66504209, 0.66504209, 0.66504209]),\n",
       " 'split8_test_precision': array([0.        , 0.70890411, 0.67411989, 0.66605336, 0.66077739,\n",
       "        0.66021978, 0.65965834, 0.65965834, 0.65965834, 0.65980736,\n",
       "        0.65980736, 0.65965834, 0.65965834, 0.65980736, 0.65980736,\n",
       "        0.65965834, 0.65980736, 0.65965834, 0.70890411, 0.66605336,\n",
       "        0.66021978, 0.65965834, 0.65965834, 0.65980736, 0.65980736,\n",
       "        0.65980736, 0.65980736, 0.65980736, 0.65965834, 0.65965834]),\n",
       " 'split9_test_precision': array([0.        , 0.712817  , 0.67936355, 0.67488372, 0.66711409,\n",
       "        0.66607381, 0.66548673, 0.66548673, 0.66548673, 0.66489832,\n",
       "        0.66489832, 0.66489832, 0.66489832, 0.66489832, 0.66489832,\n",
       "        0.66489832, 0.66489832, 0.66489832, 0.712817  , 0.67488372,\n",
       "        0.66607381, 0.66548673, 0.66489832, 0.66489832, 0.66489832,\n",
       "        0.66489832, 0.66489832, 0.66489832, 0.66489832, 0.66489832]),\n",
       " 'mean_test_precision': array([0.        , 0.70863961, 0.67248111, 0.66736897, 0.66220441,\n",
       "        0.6617898 , 0.66116151, 0.66113228, 0.66113559, 0.66107666,\n",
       "        0.66110657, 0.66109166, 0.66109166, 0.66110657, 0.66110657,\n",
       "        0.66109166, 0.66110657, 0.66109166, 0.70863961, 0.66738458,\n",
       "        0.6617898 , 0.66113228, 0.66107674, 0.66110657, 0.66110657,\n",
       "        0.66110657, 0.66110657, 0.66110657, 0.66109166, 0.66107674]),\n",
       " 'std_test_precision': array([0.        , 0.00990984, 0.01015293, 0.01157634, 0.0113404 ,\n",
       "        0.01107705, 0.0108389 , 0.01083971, 0.01088687, 0.01086799,\n",
       "        0.01083666, 0.01083854, 0.01083854, 0.01083666, 0.01083666,\n",
       "        0.01083854, 0.01083666, 0.01083854, 0.00990984, 0.01156438,\n",
       "        0.01107705, 0.01083971, 0.01086477, 0.01083666, 0.01083666,\n",
       "        0.01083666, 0.01083666, 0.01083666, 0.01083854, 0.01086477]),\n",
       " 'rank_test_precision': array([30,  1,  3,  5,  6,  7,  9, 11, 10, 29, 13, 22, 22, 13, 13, 22, 13,\n",
       "        22,  1,  4,  7, 11, 27, 13, 13, 13, 13, 13, 22, 27]),\n",
       " 'split0_test_f1_micro': array([0.59915134, 0.69391796, 0.71230552, 0.71301273, 0.71598303,\n",
       "        0.7155587 , 0.71499293, 0.71499293, 0.71499293, 0.71499293,\n",
       "        0.71499293, 0.71499293, 0.71499293, 0.71499293, 0.71499293,\n",
       "        0.71499293, 0.71499293, 0.71499293, 0.69391796, 0.71301273,\n",
       "        0.7155587 , 0.71499293, 0.71499293, 0.71499293, 0.71499293,\n",
       "        0.71499293, 0.71499293, 0.71499293, 0.71499293, 0.71499293]),\n",
       " 'split1_test_f1_micro': array([0.59915134, 0.68019802, 0.70014144, 0.69929279, 0.70084866,\n",
       "        0.70084866, 0.70084866, 0.70084866, 0.7009901 , 0.70084866,\n",
       "        0.7009901 , 0.7009901 , 0.7009901 , 0.7009901 , 0.7009901 ,\n",
       "        0.7009901 , 0.7009901 , 0.7009901 , 0.68019802, 0.69929279,\n",
       "        0.70084866, 0.70084866, 0.7009901 , 0.7009901 , 0.7009901 ,\n",
       "        0.7009901 , 0.7009901 , 0.7009901 , 0.7009901 , 0.7009901 ]),\n",
       " 'split2_test_f1_micro': array([0.5992361 , 0.68453812, 0.6998161 , 0.70264535, 0.70349413,\n",
       "        0.70349413, 0.70391852, 0.70391852, 0.70391852, 0.70391852,\n",
       "        0.70391852, 0.70391852, 0.70391852, 0.70391852, 0.70391852,\n",
       "        0.70391852, 0.70391852, 0.70391852, 0.68453812, 0.70264535,\n",
       "        0.70349413, 0.70391852, 0.70391852, 0.70391852, 0.70391852,\n",
       "        0.70391852, 0.70391852, 0.70391852, 0.70391852, 0.70391852]),\n",
       " 'split3_test_f1_micro': array([0.59909464, 0.68524544, 0.69896732, 0.69712831, 0.69726977,\n",
       "        0.69741123, 0.69698684, 0.69712831, 0.69712831, 0.69712831,\n",
       "        0.69712831, 0.69712831, 0.69712831, 0.69712831, 0.69712831,\n",
       "        0.69712831, 0.69712831, 0.69712831, 0.68524544, 0.69726977,\n",
       "        0.69741123, 0.69712831, 0.69712831, 0.69712831, 0.69712831,\n",
       "        0.69712831, 0.69712831, 0.69712831, 0.69712831, 0.69712831]),\n",
       " 'split4_test_f1_micro': array([0.59909464, 0.67732353, 0.68977225, 0.6914698 , 0.6937332 ,\n",
       "        0.69401613, 0.69444051, 0.69444051, 0.69444051, 0.69444051,\n",
       "        0.69444051, 0.69444051, 0.69444051, 0.69444051, 0.69444051,\n",
       "        0.69444051, 0.69444051, 0.69444051, 0.67732353, 0.6914698 ,\n",
       "        0.69401613, 0.69444051, 0.69444051, 0.69444051, 0.69444051,\n",
       "        0.69444051, 0.69444051, 0.69444051, 0.69444051, 0.69444051]),\n",
       " 'split5_test_f1_micro': array([0.59909464, 0.68241618, 0.69458198, 0.69415759, 0.69543075,\n",
       "        0.69585514, 0.69571368, 0.69557222, 0.69543075, 0.69543075,\n",
       "        0.69557222, 0.69557222, 0.69557222, 0.69557222, 0.69557222,\n",
       "        0.69557222, 0.69557222, 0.69557222, 0.68241618, 0.69415759,\n",
       "        0.69585514, 0.69557222, 0.69543075, 0.69557222, 0.69557222,\n",
       "        0.69557222, 0.69557222, 0.69557222, 0.69557222, 0.69543075]),\n",
       " 'split6_test_f1_micro': array([0.59909464, 0.68963078, 0.70632338, 0.71014288, 0.70886971,\n",
       "        0.7092941 , 0.7092941 , 0.7092941 , 0.7092941 , 0.7092941 ,\n",
       "        0.7092941 , 0.7092941 , 0.7092941 , 0.7092941 , 0.7092941 ,\n",
       "        0.7092941 , 0.7092941 , 0.7092941 , 0.68963078, 0.71014288,\n",
       "        0.7092941 , 0.7092941 , 0.7092941 , 0.7092941 , 0.7092941 ,\n",
       "        0.7092941 , 0.7092941 , 0.7092941 , 0.7092941 , 0.7092941 ]),\n",
       " 'split7_test_f1_micro': array([0.59909464, 0.68581129, 0.70179658, 0.70405998, 0.70462583,\n",
       "        0.70420144, 0.70434291, 0.70420144, 0.70448437, 0.70448437,\n",
       "        0.70448437, 0.70448437, 0.70448437, 0.70448437, 0.70448437,\n",
       "        0.70448437, 0.70448437, 0.70448437, 0.68581129, 0.70405998,\n",
       "        0.70420144, 0.70420144, 0.70448437, 0.70448437, 0.70448437,\n",
       "        0.70448437, 0.70448437, 0.70448437, 0.70448437, 0.70448437]),\n",
       " 'split8_test_f1_micro': array([0.59909464, 0.6853869 , 0.70264535, 0.70123073, 0.7020795 ,\n",
       "        0.70222096, 0.70222096, 0.70222096, 0.70222096, 0.70236243,\n",
       "        0.70236243, 0.70222096, 0.70222096, 0.70236243, 0.70236243,\n",
       "        0.70222096, 0.70236243, 0.70222096, 0.6853869 , 0.70123073,\n",
       "        0.70222096, 0.70222096, 0.70222096, 0.70236243, 0.70236243,\n",
       "        0.70236243, 0.70236243, 0.70236243, 0.70222096, 0.70222096]),\n",
       " 'split9_test_f1_micro': array([0.59909464, 0.68694299, 0.70434291, 0.70547461, 0.70476729,\n",
       "        0.70476729, 0.70490876, 0.70490876, 0.70490876, 0.70462583,\n",
       "        0.70462583, 0.70462583, 0.70462583, 0.70462583, 0.70462583,\n",
       "        0.70462583, 0.70462583, 0.70462583, 0.68694299, 0.70547461,\n",
       "        0.70476729, 0.70490876, 0.70462583, 0.70462583, 0.70462583,\n",
       "        0.70462583, 0.70462583, 0.70462583, 0.70462583, 0.70462583]),\n",
       " 'mean_test_f1_micro': array([0.59912013, 0.68514112, 0.70106928, 0.70186148, 0.70271019,\n",
       "        0.70276678, 0.70276679, 0.70275264, 0.70278093, 0.70275264,\n",
       "        0.70278093, 0.70276678, 0.70276678, 0.70278093, 0.70278093,\n",
       "        0.70276678, 0.70278093, 0.70276678, 0.68514112, 0.70187562,\n",
       "        0.70276678, 0.70275264, 0.70275264, 0.70278093, 0.70278093,\n",
       "        0.70278093, 0.70278093, 0.70278093, 0.70276678, 0.70275264]),\n",
       " 'std_test_f1_micro': array([4.46616433e-05, 4.40789189e-03, 5.86639470e-03, 6.38882166e-03,\n",
       "        6.24228807e-03, 6.08138249e-03, 5.97445426e-03, 5.97422260e-03,\n",
       "        5.99448196e-03, 5.98827471e-03, 5.96670929e-03, 5.96785232e-03,\n",
       "        5.96785232e-03, 5.96670929e-03, 5.96670929e-03, 5.96785232e-03,\n",
       "        5.96670929e-03, 5.96785232e-03, 4.40789189e-03, 6.37847395e-03,\n",
       "        6.08138249e-03, 5.97422260e-03, 5.98503258e-03, 5.96670929e-03,\n",
       "        5.96670929e-03, 5.96670929e-03, 5.96670929e-03, 5.96670929e-03,\n",
       "        5.96785232e-03, 5.98503258e-03]),\n",
       " 'rank_test_f1_micro': array([30, 28, 27, 26, 24, 17, 11, 19,  1, 21,  2, 12, 12,  2,  2, 12,  2,\n",
       "        12, 28, 25, 17, 19, 22,  2,  2,  2,  2,  2, 12, 22])}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_model.cv_results_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6e3313e2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([30, 28, 27, 26, 24, 17, 11, 19,  1, 21,  1, 12, 12,  1,  1, 12,  1,\n",
       "       12, 28, 25, 17, 19, 22,  1,  1,  1,  1,  1, 12, 22])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_model.cv_results_['rank_test_accuracy']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "3d08f33e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'classifier': LogisticRegression(max_iter=10000),\n",
       " 'classifier__C': 0.0001,\n",
       " 'classifier__penalty': 'l1',\n",
       " 'classifier__solver': 'saga'}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_model.cv_results_['params'][ np.argmin(best_model.cv_results_['mean_test_accuracy'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c8da90d4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>classifier</th>\n",
       "      <th>C</th>\n",
       "      <th>penalty</th>\n",
       "      <th>solver</th>\n",
       "      <th>score_acc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.599120</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.685141</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.701069</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.701861</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0100</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.702710</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0100</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.702767</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.1000</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.702767</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.1000</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.702753</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.702781</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.702753</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>10.0000</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.702781</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>10.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.702767</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>100.0000</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.702767</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>100.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.702781</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>1000.0000</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.702781</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>1000.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.702767</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>10000.0000</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.702781</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>10000.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.702767</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.685141</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.701876</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0100</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.702767</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.1000</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.702753</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.702753</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>10.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.702781</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>100.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.702781</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>1000.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.702781</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>10000.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.702781</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>NaN</td>\n",
       "      <td>none</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.702781</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>NaN</td>\n",
       "      <td>none</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.702767</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.702753</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                            classifier           C penalty solver  score_acc\n",
       "0   LogisticRegression(max_iter=10000)      0.0001      l1   saga   0.599120\n",
       "1   LogisticRegression(max_iter=10000)      0.0001      l2   saga   0.685141\n",
       "2   LogisticRegression(max_iter=10000)      0.0010      l1   saga   0.701069\n",
       "3   LogisticRegression(max_iter=10000)      0.0010      l2   saga   0.701861\n",
       "4   LogisticRegression(max_iter=10000)      0.0100      l1   saga   0.702710\n",
       "5   LogisticRegression(max_iter=10000)      0.0100      l2   saga   0.702767\n",
       "6   LogisticRegression(max_iter=10000)      0.1000      l1   saga   0.702767\n",
       "7   LogisticRegression(max_iter=10000)      0.1000      l2   saga   0.702753\n",
       "8   LogisticRegression(max_iter=10000)      1.0000      l1   saga   0.702781\n",
       "9   LogisticRegression(max_iter=10000)      1.0000      l2   saga   0.702753\n",
       "10  LogisticRegression(max_iter=10000)     10.0000      l1   saga   0.702781\n",
       "11  LogisticRegression(max_iter=10000)     10.0000      l2   saga   0.702767\n",
       "12  LogisticRegression(max_iter=10000)    100.0000      l1   saga   0.702767\n",
       "13  LogisticRegression(max_iter=10000)    100.0000      l2   saga   0.702781\n",
       "14  LogisticRegression(max_iter=10000)   1000.0000      l1   saga   0.702781\n",
       "15  LogisticRegression(max_iter=10000)   1000.0000      l2   saga   0.702767\n",
       "16  LogisticRegression(max_iter=10000)  10000.0000      l1   saga   0.702781\n",
       "17  LogisticRegression(max_iter=10000)  10000.0000      l2   saga   0.702767\n",
       "18  LogisticRegression(max_iter=10000)      0.0001      l2  lbfgs   0.685141\n",
       "19  LogisticRegression(max_iter=10000)      0.0010      l2  lbfgs   0.701876\n",
       "20  LogisticRegression(max_iter=10000)      0.0100      l2  lbfgs   0.702767\n",
       "21  LogisticRegression(max_iter=10000)      0.1000      l2  lbfgs   0.702753\n",
       "22  LogisticRegression(max_iter=10000)      1.0000      l2  lbfgs   0.702753\n",
       "23  LogisticRegression(max_iter=10000)     10.0000      l2  lbfgs   0.702781\n",
       "24  LogisticRegression(max_iter=10000)    100.0000      l2  lbfgs   0.702781\n",
       "25  LogisticRegression(max_iter=10000)   1000.0000      l2  lbfgs   0.702781\n",
       "26  LogisticRegression(max_iter=10000)  10000.0000      l2  lbfgs   0.702781\n",
       "27  LogisticRegression(max_iter=10000)         NaN    none  lbfgs   0.702781\n",
       "28  LogisticRegression(max_iter=10000)         NaN    none   saga   0.702767\n",
       "29  LogisticRegression(max_iter=10000)         NaN     NaN    NaN   0.702753"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results = pd.DataFrame( best_model.cv_results_['params'] )\n",
    "\n",
    "results['score_acc'] = best_model.cv_results_['mean_test_accuracy']\n",
    "cols = results.columns.to_series().str.split('__').apply(lambda x: x[-1])\n",
    "results.columns = cols\n",
    "\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "eb7e7292",
   "metadata": {},
   "outputs": [],
   "source": [
    "acc_list.append(results[0:1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ce1a13cb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'classifier': LogisticRegression(max_iter=10000),\n",
       " 'classifier__C': 0.0001,\n",
       " 'classifier__penalty': 'l2',\n",
       " 'classifier__solver': 'saga'}"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_model.cv_results_['params'][ np.argmin(best_model.cv_results_['rank_test_precision'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e6654f1a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>classifier</th>\n",
       "      <th>C</th>\n",
       "      <th>penalty</th>\n",
       "      <th>solver</th>\n",
       "      <th>score_precision</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.708640</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.672481</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.667369</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0100</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.662204</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0100</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.661790</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.1000</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.661162</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.1000</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.661132</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.661136</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.661077</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>10.0000</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.661107</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>10.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.661092</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>100.0000</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.661092</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>100.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.661107</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>1000.0000</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.661107</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>1000.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.661092</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>10000.0000</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.661107</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>10000.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.661092</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.708640</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.667385</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0100</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.661790</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.1000</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.661132</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.661077</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>10.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.661107</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>100.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.661107</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>1000.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.661107</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>10000.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.661107</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>NaN</td>\n",
       "      <td>none</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.661107</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>NaN</td>\n",
       "      <td>none</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.661092</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.661077</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                            classifier           C penalty solver  \\\n",
       "0   LogisticRegression(max_iter=10000)      0.0001      l1   saga   \n",
       "1   LogisticRegression(max_iter=10000)      0.0001      l2   saga   \n",
       "2   LogisticRegression(max_iter=10000)      0.0010      l1   saga   \n",
       "3   LogisticRegression(max_iter=10000)      0.0010      l2   saga   \n",
       "4   LogisticRegression(max_iter=10000)      0.0100      l1   saga   \n",
       "5   LogisticRegression(max_iter=10000)      0.0100      l2   saga   \n",
       "6   LogisticRegression(max_iter=10000)      0.1000      l1   saga   \n",
       "7   LogisticRegression(max_iter=10000)      0.1000      l2   saga   \n",
       "8   LogisticRegression(max_iter=10000)      1.0000      l1   saga   \n",
       "9   LogisticRegression(max_iter=10000)      1.0000      l2   saga   \n",
       "10  LogisticRegression(max_iter=10000)     10.0000      l1   saga   \n",
       "11  LogisticRegression(max_iter=10000)     10.0000      l2   saga   \n",
       "12  LogisticRegression(max_iter=10000)    100.0000      l1   saga   \n",
       "13  LogisticRegression(max_iter=10000)    100.0000      l2   saga   \n",
       "14  LogisticRegression(max_iter=10000)   1000.0000      l1   saga   \n",
       "15  LogisticRegression(max_iter=10000)   1000.0000      l2   saga   \n",
       "16  LogisticRegression(max_iter=10000)  10000.0000      l1   saga   \n",
       "17  LogisticRegression(max_iter=10000)  10000.0000      l2   saga   \n",
       "18  LogisticRegression(max_iter=10000)      0.0001      l2  lbfgs   \n",
       "19  LogisticRegression(max_iter=10000)      0.0010      l2  lbfgs   \n",
       "20  LogisticRegression(max_iter=10000)      0.0100      l2  lbfgs   \n",
       "21  LogisticRegression(max_iter=10000)      0.1000      l2  lbfgs   \n",
       "22  LogisticRegression(max_iter=10000)      1.0000      l2  lbfgs   \n",
       "23  LogisticRegression(max_iter=10000)     10.0000      l2  lbfgs   \n",
       "24  LogisticRegression(max_iter=10000)    100.0000      l2  lbfgs   \n",
       "25  LogisticRegression(max_iter=10000)   1000.0000      l2  lbfgs   \n",
       "26  LogisticRegression(max_iter=10000)  10000.0000      l2  lbfgs   \n",
       "27  LogisticRegression(max_iter=10000)         NaN    none  lbfgs   \n",
       "28  LogisticRegression(max_iter=10000)         NaN    none   saga   \n",
       "29  LogisticRegression(max_iter=10000)         NaN     NaN    NaN   \n",
       "\n",
       "    score_precision  \n",
       "0          0.000000  \n",
       "1          0.708640  \n",
       "2          0.672481  \n",
       "3          0.667369  \n",
       "4          0.662204  \n",
       "5          0.661790  \n",
       "6          0.661162  \n",
       "7          0.661132  \n",
       "8          0.661136  \n",
       "9          0.661077  \n",
       "10         0.661107  \n",
       "11         0.661092  \n",
       "12         0.661092  \n",
       "13         0.661107  \n",
       "14         0.661107  \n",
       "15         0.661092  \n",
       "16         0.661107  \n",
       "17         0.661092  \n",
       "18         0.708640  \n",
       "19         0.667385  \n",
       "20         0.661790  \n",
       "21         0.661132  \n",
       "22         0.661077  \n",
       "23         0.661107  \n",
       "24         0.661107  \n",
       "25         0.661107  \n",
       "26         0.661107  \n",
       "27         0.661107  \n",
       "28         0.661092  \n",
       "29         0.661077  "
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results = pd.DataFrame( best_model.cv_results_['params'] )\n",
    "\n",
    "results['score_precision'] = best_model.cv_results_['mean_test_precision']\n",
    "cols = results.columns.to_series().str.split('__').apply(lambda x: x[-1])\n",
    "results.columns = cols\n",
    "\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "71d804bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "roc_auc_ovr_List.append(results[1:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "c8742a4b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'classifier': LogisticRegression(max_iter=10000),\n",
       " 'classifier__C': 1.0,\n",
       " 'classifier__penalty': 'l1',\n",
       " 'classifier__solver': 'saga'}"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_model.cv_results_['params'][ np.argmin(best_model.cv_results_['rank_test_f1_micro'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "4bf5cd96",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>classifier</th>\n",
       "      <th>C</th>\n",
       "      <th>penalty</th>\n",
       "      <th>solver</th>\n",
       "      <th>score_f1_micro</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.599120</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.685141</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.701069</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.701861</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0100</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.702710</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0100</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.702767</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.1000</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.702767</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.1000</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.702753</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.702781</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.702753</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>10.0000</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.702781</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>10.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.702767</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>100.0000</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.702767</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>100.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.702781</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>1000.0000</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.702781</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>1000.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.702767</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>10000.0000</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.702781</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>10000.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.702767</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.685141</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.701876</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0100</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.702767</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.1000</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.702753</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.702753</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>10.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.702781</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>100.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.702781</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>1000.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.702781</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>10000.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.702781</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>NaN</td>\n",
       "      <td>none</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.702781</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>NaN</td>\n",
       "      <td>none</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.702767</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.702753</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                            classifier           C penalty solver  \\\n",
       "0   LogisticRegression(max_iter=10000)      0.0001      l1   saga   \n",
       "1   LogisticRegression(max_iter=10000)      0.0001      l2   saga   \n",
       "2   LogisticRegression(max_iter=10000)      0.0010      l1   saga   \n",
       "3   LogisticRegression(max_iter=10000)      0.0010      l2   saga   \n",
       "4   LogisticRegression(max_iter=10000)      0.0100      l1   saga   \n",
       "5   LogisticRegression(max_iter=10000)      0.0100      l2   saga   \n",
       "6   LogisticRegression(max_iter=10000)      0.1000      l1   saga   \n",
       "7   LogisticRegression(max_iter=10000)      0.1000      l2   saga   \n",
       "8   LogisticRegression(max_iter=10000)      1.0000      l1   saga   \n",
       "9   LogisticRegression(max_iter=10000)      1.0000      l2   saga   \n",
       "10  LogisticRegression(max_iter=10000)     10.0000      l1   saga   \n",
       "11  LogisticRegression(max_iter=10000)     10.0000      l2   saga   \n",
       "12  LogisticRegression(max_iter=10000)    100.0000      l1   saga   \n",
       "13  LogisticRegression(max_iter=10000)    100.0000      l2   saga   \n",
       "14  LogisticRegression(max_iter=10000)   1000.0000      l1   saga   \n",
       "15  LogisticRegression(max_iter=10000)   1000.0000      l2   saga   \n",
       "16  LogisticRegression(max_iter=10000)  10000.0000      l1   saga   \n",
       "17  LogisticRegression(max_iter=10000)  10000.0000      l2   saga   \n",
       "18  LogisticRegression(max_iter=10000)      0.0001      l2  lbfgs   \n",
       "19  LogisticRegression(max_iter=10000)      0.0010      l2  lbfgs   \n",
       "20  LogisticRegression(max_iter=10000)      0.0100      l2  lbfgs   \n",
       "21  LogisticRegression(max_iter=10000)      0.1000      l2  lbfgs   \n",
       "22  LogisticRegression(max_iter=10000)      1.0000      l2  lbfgs   \n",
       "23  LogisticRegression(max_iter=10000)     10.0000      l2  lbfgs   \n",
       "24  LogisticRegression(max_iter=10000)    100.0000      l2  lbfgs   \n",
       "25  LogisticRegression(max_iter=10000)   1000.0000      l2  lbfgs   \n",
       "26  LogisticRegression(max_iter=10000)  10000.0000      l2  lbfgs   \n",
       "27  LogisticRegression(max_iter=10000)         NaN    none  lbfgs   \n",
       "28  LogisticRegression(max_iter=10000)         NaN    none   saga   \n",
       "29  LogisticRegression(max_iter=10000)         NaN     NaN    NaN   \n",
       "\n",
       "    score_f1_micro  \n",
       "0         0.599120  \n",
       "1         0.685141  \n",
       "2         0.701069  \n",
       "3         0.701861  \n",
       "4         0.702710  \n",
       "5         0.702767  \n",
       "6         0.702767  \n",
       "7         0.702753  \n",
       "8         0.702781  \n",
       "9         0.702753  \n",
       "10        0.702781  \n",
       "11        0.702767  \n",
       "12        0.702767  \n",
       "13        0.702781  \n",
       "14        0.702781  \n",
       "15        0.702767  \n",
       "16        0.702781  \n",
       "17        0.702767  \n",
       "18        0.685141  \n",
       "19        0.701876  \n",
       "20        0.702767  \n",
       "21        0.702753  \n",
       "22        0.702753  \n",
       "23        0.702781  \n",
       "24        0.702781  \n",
       "25        0.702781  \n",
       "26        0.702781  \n",
       "27        0.702781  \n",
       "28        0.702767  \n",
       "29        0.702753  "
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results = pd.DataFrame( best_model.cv_results_['params'] )\n",
    "\n",
    "results['score_f1_micro'] = best_model.cv_results_['mean_test_f1_micro']\n",
    "cols = results.columns.to_series().str.split('__').apply(lambda x: x[-1])\n",
    "results.columns = cols\n",
    "\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "8cebbf79",
   "metadata": {},
   "outputs": [],
   "source": [
    "f1_micro_List.append(results[8:9])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b315f477",
   "metadata": {},
   "source": [
    "## Trial 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "8e5bcf1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, Y_train, Y_test = train_test_split(X, y, test_size=0.2, shuffle=True, random_state = 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "e91ba61f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ENT\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\ENT\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\ENT\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\ENT\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\ENT\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\ENT\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\ENT\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\ENT\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\ENT\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\ENT\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\ENT\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1183: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\ENT\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1183: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\ENT\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1183: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\ENT\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1183: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\ENT\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1183: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\ENT\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1183: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\ENT\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1183: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\ENT\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1183: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\ENT\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1183: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\ENT\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1183: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\ENT\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1183: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\ENT\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1183: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\ENT\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1183: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\ENT\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1183: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\ENT\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1183: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\ENT\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1183: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\ENT\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1183: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\ENT\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1183: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\ENT\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1183: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\ENT\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1183: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Create a pipeline - RF is a stand in, we will populate the classifier part below\n",
    "pipe = Pipeline([('std', StandardScaler()),\n",
    "                 ('classifier', LogisticRegression())])\n",
    "\n",
    "# Create search space of candidate learning algorithms and their hyperparameters\n",
    "# note lbfgs can't do l1, and if you pass penalty='none' it expects no C value\n",
    "search_space = [{'classifier': [LogisticRegression(max_iter=10000)],\n",
    "                 'classifier__solver': ['saga'],\n",
    "                 'classifier__penalty': ['l1', 'l2'],\n",
    "                 'classifier__C': np.logspace(-4, 4, 9)},\n",
    "                {'classifier': [LogisticRegression(max_iter=10000)],\n",
    "                 'classifier__solver': ['lbfgs'],\n",
    "                 'classifier__penalty': ['l2'],\n",
    "                 'classifier__C': np.logspace(-4, 4, 9)},\n",
    "                {'classifier': [LogisticRegression(max_iter=10000)],\n",
    "                 'classifier__solver': ['lbfgs','saga'],\n",
    "                 'classifier__penalty': ['none']},\n",
    "                {'classifier': [LogisticRegression(max_iter=10000)]}\n",
    "                ]\n",
    "\n",
    "# Create grid search\n",
    "clf = GridSearchCV(pipe, search_space, cv=StratifiedKFold(n_splits=10),\n",
    "                   scoring=['accuracy', 'precision', 'f1_micro'], refit=False,\n",
    "                   verbose=0)\n",
    "\n",
    "# Fit grid search\n",
    "best_model = clf.fit(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "36bb3f82",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'mean_fit_time': array([0.05211456, 0.10302429, 0.11541595, 0.10655594, 0.12156851,\n",
       "        0.12497203, 0.13688803, 0.11625423, 0.13023713, 0.12108722,\n",
       "        0.13497989, 0.12046669, 0.13083308, 0.11973825, 0.1348619 ,\n",
       "        0.12107439, 0.13136995, 0.12131386, 0.02448106, 0.02340496,\n",
       "        0.02384844, 0.02312098, 0.02332084, 0.02491109, 0.02287152,\n",
       "        0.02242005, 0.02292049, 0.02262018, 0.11305141, 0.02292085]),\n",
       " 'std_fit_time': array([0.01004523, 0.00902707, 0.00885405, 0.01644106, 0.00877499,\n",
       "        0.01629365, 0.0145322 , 0.00979008, 0.00787046, 0.00621978,\n",
       "        0.01411478, 0.01108994, 0.01631147, 0.00703108, 0.01265161,\n",
       "        0.01078701, 0.00931226, 0.00877696, 0.00122502, 0.00126349,\n",
       "        0.00209977, 0.00144703, 0.00110132, 0.00375906, 0.00128037,\n",
       "        0.0010207 , 0.00137621, 0.0004902 , 0.00611312, 0.00130118]),\n",
       " 'mean_score_time': array([0.00727501, 0.00758989, 0.00770669, 0.00801365, 0.00850773,\n",
       "        0.00810738, 0.00810752, 0.00830746, 0.0078068 , 0.00828433,\n",
       "        0.00799832, 0.00811813, 0.00802178, 0.00810733, 0.00830727,\n",
       "        0.0082077 , 0.00810673, 0.00834935, 0.00803649, 0.00788651,\n",
       "        0.00763106, 0.00780721, 0.00770731, 0.00760708, 0.00766814,\n",
       "        0.00790734, 0.00750701, 0.00770724, 0.00775166, 0.00765417]),\n",
       " 'std_score_time': array([6.60932908e-04, 4.83874334e-04, 4.58651117e-04, 1.98941840e-05,\n",
       "        6.71447447e-04, 3.00345770e-04, 7.00766953e-04, 4.58672101e-04,\n",
       "        6.00783327e-04, 3.94979404e-04, 4.48448301e-04, 5.55949149e-04,\n",
       "        6.34527834e-04, 5.39178745e-04, 4.58848852e-04, 6.00490456e-04,\n",
       "        5.39289645e-04, 6.33820894e-04, 5.29624485e-04, 2.48600624e-04,\n",
       "        4.98119294e-04, 6.00600297e-04, 4.59108944e-04, 4.90349229e-04,\n",
       "        4.32583180e-04, 5.39106245e-04, 5.00750707e-04, 6.40977336e-04,\n",
       "        4.07012844e-04, 4.55053378e-04]),\n",
       " 'param_classifier': masked_array(data=[LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000)],\n",
       "              mask=[False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'param_classifier__C': masked_array(data=[0.0001, 0.0001, 0.001, 0.001, 0.01, 0.01, 0.1, 0.1,\n",
       "                    1.0, 1.0, 10.0, 10.0, 100.0, 100.0, 1000.0, 1000.0,\n",
       "                    10000.0, 10000.0, 0.0001, 0.001, 0.01, 0.1, 1.0, 10.0,\n",
       "                    100.0, 1000.0, 10000.0, --, --, --],\n",
       "              mask=[False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False,  True,  True,  True],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'param_classifier__penalty': masked_array(data=['l1', 'l2', 'l1', 'l2', 'l1', 'l2', 'l1', 'l2', 'l1',\n",
       "                    'l2', 'l1', 'l2', 'l1', 'l2', 'l1', 'l2', 'l1', 'l2',\n",
       "                    'l2', 'l2', 'l2', 'l2', 'l2', 'l2', 'l2', 'l2', 'l2',\n",
       "                    'none', 'none', --],\n",
       "              mask=[False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False,  True],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'param_classifier__solver': masked_array(data=['saga', 'saga', 'saga', 'saga', 'saga', 'saga', 'saga',\n",
       "                    'saga', 'saga', 'saga', 'saga', 'saga', 'saga', 'saga',\n",
       "                    'saga', 'saga', 'saga', 'saga', 'lbfgs', 'lbfgs',\n",
       "                    'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs',\n",
       "                    'lbfgs', 'lbfgs', 'saga', --],\n",
       "              mask=[False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False,  True],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'params': [{'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 0.0001,\n",
       "   'classifier__penalty': 'l1',\n",
       "   'classifier__solver': 'saga'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 0.0001,\n",
       "   'classifier__penalty': 'l2',\n",
       "   'classifier__solver': 'saga'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 0.001,\n",
       "   'classifier__penalty': 'l1',\n",
       "   'classifier__solver': 'saga'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 0.001,\n",
       "   'classifier__penalty': 'l2',\n",
       "   'classifier__solver': 'saga'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 0.01,\n",
       "   'classifier__penalty': 'l1',\n",
       "   'classifier__solver': 'saga'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 0.01,\n",
       "   'classifier__penalty': 'l2',\n",
       "   'classifier__solver': 'saga'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 0.1,\n",
       "   'classifier__penalty': 'l1',\n",
       "   'classifier__solver': 'saga'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 0.1,\n",
       "   'classifier__penalty': 'l2',\n",
       "   'classifier__solver': 'saga'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 1.0,\n",
       "   'classifier__penalty': 'l1',\n",
       "   'classifier__solver': 'saga'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 1.0,\n",
       "   'classifier__penalty': 'l2',\n",
       "   'classifier__solver': 'saga'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 10.0,\n",
       "   'classifier__penalty': 'l1',\n",
       "   'classifier__solver': 'saga'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 10.0,\n",
       "   'classifier__penalty': 'l2',\n",
       "   'classifier__solver': 'saga'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 100.0,\n",
       "   'classifier__penalty': 'l1',\n",
       "   'classifier__solver': 'saga'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 100.0,\n",
       "   'classifier__penalty': 'l2',\n",
       "   'classifier__solver': 'saga'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 1000.0,\n",
       "   'classifier__penalty': 'l1',\n",
       "   'classifier__solver': 'saga'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 1000.0,\n",
       "   'classifier__penalty': 'l2',\n",
       "   'classifier__solver': 'saga'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 10000.0,\n",
       "   'classifier__penalty': 'l1',\n",
       "   'classifier__solver': 'saga'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 10000.0,\n",
       "   'classifier__penalty': 'l2',\n",
       "   'classifier__solver': 'saga'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 0.0001,\n",
       "   'classifier__penalty': 'l2',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 0.001,\n",
       "   'classifier__penalty': 'l2',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 0.01,\n",
       "   'classifier__penalty': 'l2',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 0.1,\n",
       "   'classifier__penalty': 'l2',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 1.0,\n",
       "   'classifier__penalty': 'l2',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 10.0,\n",
       "   'classifier__penalty': 'l2',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 100.0,\n",
       "   'classifier__penalty': 'l2',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 1000.0,\n",
       "   'classifier__penalty': 'l2',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 10000.0,\n",
       "   'classifier__penalty': 'l2',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__penalty': 'none',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__penalty': 'none',\n",
       "   'classifier__solver': 'saga'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000)}],\n",
       " 'split0_test_accuracy': array([0.59957567, 0.67906648, 0.69674682, 0.69943423, 0.70155587,\n",
       "        0.70155587, 0.70169731, 0.70155587, 0.70113154, 0.70113154,\n",
       "        0.70113154, 0.70113154, 0.70113154, 0.70113154, 0.70113154,\n",
       "        0.70113154, 0.70113154, 0.70113154, 0.67906648, 0.69943423,\n",
       "        0.70155587, 0.70155587, 0.70113154, 0.70113154, 0.70113154,\n",
       "        0.70113154, 0.70113154, 0.70113154, 0.70113154, 0.70113154]),\n",
       " 'split1_test_accuracy': array([0.59957567, 0.69335219, 0.70650636, 0.70763791, 0.70523338,\n",
       "        0.70551627, 0.70551627, 0.70551627, 0.70551627, 0.70551627,\n",
       "        0.70551627, 0.70551627, 0.70551627, 0.70551627, 0.70551627,\n",
       "        0.70537482, 0.70537482, 0.70551627, 0.69335219, 0.70763791,\n",
       "        0.70551627, 0.70551627, 0.70551627, 0.70551627, 0.70551627,\n",
       "        0.70551627, 0.70551627, 0.70551627, 0.70551627, 0.70551627]),\n",
       " 'split2_test_accuracy': array([0.59951903, 0.69189419, 0.70872825, 0.71184043, 0.71169897,\n",
       "        0.71127458, 0.71056727, 0.7104258 , 0.71070873, 0.71070873,\n",
       "        0.71070873, 0.71070873, 0.71070873, 0.71070873, 0.71070873,\n",
       "        0.71070873, 0.71070873, 0.71070873, 0.69189419, 0.71184043,\n",
       "        0.71127458, 0.7104258 , 0.71070873, 0.71070873, 0.71070873,\n",
       "        0.71070873, 0.71070873, 0.71070873, 0.71070873, 0.71070873]),\n",
       " 'split3_test_accuracy': array([0.59951903, 0.67506012, 0.68934786, 0.69019663, 0.69274296,\n",
       "        0.69316735, 0.69401613, 0.69401613, 0.69387466, 0.69387466,\n",
       "        0.69387466, 0.69387466, 0.69387466, 0.69387466, 0.69387466,\n",
       "        0.69387466, 0.69387466, 0.69387466, 0.67506012, 0.69019663,\n",
       "        0.69316735, 0.69401613, 0.69387466, 0.69387466, 0.69387466,\n",
       "        0.69387466, 0.69387466, 0.69387466, 0.69387466, 0.69387466]),\n",
       " 'split4_test_accuracy': array([0.59951903, 0.68510398, 0.69840147, 0.70052341, 0.69953317,\n",
       "        0.70024049, 0.6998161 , 0.6998161 , 0.6998161 , 0.6998161 ,\n",
       "        0.6998161 , 0.6998161 , 0.6998161 , 0.6998161 , 0.6998161 ,\n",
       "        0.6998161 , 0.6998161 , 0.6998161 , 0.68510398, 0.70052341,\n",
       "        0.70024049, 0.6998161 , 0.6998161 , 0.6998161 , 0.6998161 ,\n",
       "        0.6998161 , 0.6998161 , 0.6998161 , 0.6998161 , 0.6998161 ]),\n",
       " 'split5_test_accuracy': array([0.59966049, 0.68213326, 0.69896732, 0.69811855, 0.7009478 ,\n",
       "        0.70052341, 0.70137219, 0.70151365, 0.70151365, 0.70151365,\n",
       "        0.70151365, 0.70151365, 0.70151365, 0.70151365, 0.70151365,\n",
       "        0.70151365, 0.70151365, 0.70151365, 0.68213326, 0.69811855,\n",
       "        0.70066487, 0.70151365, 0.70151365, 0.70151365, 0.70151365,\n",
       "        0.70151365, 0.70151365, 0.70151365, 0.70151365, 0.70151365]),\n",
       " 'split6_test_accuracy': array([0.59966049, 0.679304  , 0.69585514, 0.69458198, 0.69528929,\n",
       "        0.69557222, 0.69557222, 0.69571368, 0.69585514, 0.69585514,\n",
       "        0.69571368, 0.69585514, 0.69585514, 0.69585514, 0.69585514,\n",
       "        0.69585514, 0.69585514, 0.69585514, 0.679304  , 0.69458198,\n",
       "        0.69557222, 0.69571368, 0.69585514, 0.69585514, 0.69585514,\n",
       "        0.69585514, 0.69585514, 0.69585514, 0.69585514, 0.69585514]),\n",
       " 'split7_test_accuracy': array([0.59966049, 0.68368935, 0.70363559, 0.70405998, 0.70646485,\n",
       "        0.70674777, 0.70674777, 0.70674777, 0.70674777, 0.70674777,\n",
       "        0.70674777, 0.70674777, 0.70674777, 0.70674777, 0.70674777,\n",
       "        0.70674777, 0.70674777, 0.70674777, 0.68368935, 0.70405998,\n",
       "        0.70674777, 0.70674777, 0.70674777, 0.70674777, 0.70674777,\n",
       "        0.70674777, 0.70674777, 0.70674777, 0.70674777, 0.70674777]),\n",
       " 'split8_test_accuracy': array([0.59966049, 0.68694299, 0.70476729, 0.70519168, 0.70872825,\n",
       "        0.70844532, 0.70844532, 0.70844532, 0.70830386, 0.7081624 ,\n",
       "        0.7081624 , 0.7081624 , 0.7081624 , 0.7081624 , 0.7081624 ,\n",
       "        0.70830386, 0.7081624 , 0.7081624 , 0.68694299, 0.70519168,\n",
       "        0.70844532, 0.70844532, 0.7081624 , 0.7081624 , 0.7081624 ,\n",
       "        0.7081624 , 0.7081624 , 0.7081624 , 0.7081624 , 0.7081624 ]),\n",
       " 'split9_test_accuracy': array([0.59966049, 0.67916254, 0.68892347, 0.68906493, 0.69104541,\n",
       "        0.69104541, 0.69161126, 0.69175272, 0.69175272, 0.69175272,\n",
       "        0.69175272, 0.69175272, 0.69175272, 0.69175272, 0.69175272,\n",
       "        0.69175272, 0.69175272, 0.69175272, 0.67916254, 0.68892347,\n",
       "        0.69104541, 0.69175272, 0.69175272, 0.69175272, 0.69175272,\n",
       "        0.69175272, 0.69175272, 0.69175272, 0.69175272, 0.69175272]),\n",
       " 'mean_test_accuracy': array([0.59960109, 0.68357091, 0.69918796, 0.70006497, 0.701324  ,\n",
       "        0.70140887, 0.70153618, 0.70155033, 0.70152204, 0.7015079 ,\n",
       "        0.70149375, 0.7015079 , 0.7015079 , 0.7015079 , 0.7015079 ,\n",
       "        0.7015079 , 0.70149375, 0.7015079 , 0.68357091, 0.70005083,\n",
       "        0.70142301, 0.70155033, 0.7015079 , 0.7015079 , 0.7015079 ,\n",
       "        0.7015079 , 0.7015079 , 0.7015079 , 0.7015079 , 0.7015079 ]),\n",
       " 'std_test_accuracy': array([6.25593973e-05, 5.57777850e-03, 6.44521599e-03, 7.00784423e-03,\n",
       "        6.50598276e-03, 6.35436646e-03, 6.04924942e-03, 5.99074088e-03,\n",
       "        6.02286396e-03, 6.00706388e-03, 6.02051064e-03, 6.00706388e-03,\n",
       "        6.00706388e-03, 6.00706388e-03, 6.00706388e-03, 6.01362620e-03,\n",
       "        5.99776843e-03, 6.00706388e-03, 5.57777850e-03, 7.03014232e-03,\n",
       "        6.35253668e-03, 5.99074088e-03, 6.00706388e-03, 6.00706388e-03,\n",
       "        6.00706388e-03, 6.00706388e-03, 6.00706388e-03, 6.00706388e-03,\n",
       "        6.00706388e-03, 6.00706388e-03]),\n",
       " 'rank_test_accuracy': array([30, 28, 27, 25, 24, 23,  3,  1,  4,  6, 21,  6,  6,  6,  6,  5, 20,\n",
       "         6, 28, 26, 22,  1,  6,  6,  6,  6,  6,  6,  6,  6]),\n",
       " 'split0_test_precision': array([0.        , 0.70186782, 0.66846493, 0.6657277 , 0.66393815,\n",
       "        0.66349206, 0.6630533 , 0.66275395, 0.66185753, 0.66185753,\n",
       "        0.66185753, 0.66185753, 0.66185753, 0.66185753, 0.66185753,\n",
       "        0.66185753, 0.66185753, 0.66185753, 0.70186782, 0.6657277 ,\n",
       "        0.66349206, 0.66275395, 0.66185753, 0.66185753, 0.66185753,\n",
       "        0.66185753, 0.66185753, 0.66185753, 0.66185753, 0.66185753]),\n",
       " 'split1_test_precision': array([0.        , 0.73036831, 0.6815562 , 0.67685185, 0.66622163,\n",
       "        0.66592822, 0.66534216, 0.66534216, 0.66534216, 0.66534216,\n",
       "        0.66534216, 0.66534216, 0.66534216, 0.66534216, 0.66534216,\n",
       "        0.66504854, 0.66504854, 0.66534216, 0.73036831, 0.67685185,\n",
       "        0.66592822, 0.66534216, 0.66534216, 0.66534216, 0.66534216,\n",
       "        0.66534216, 0.66534216, 0.66534216, 0.66534216, 0.66534216]),\n",
       " 'split2_test_precision': array([0.        , 0.72016183, 0.68293839, 0.68078324, 0.67306853,\n",
       "        0.67158992, 0.66954644, 0.66925734, 0.66954271, 0.66954271,\n",
       "        0.66954271, 0.66954271, 0.66954271, 0.66954271, 0.66954271,\n",
       "        0.66954271, 0.66954271, 0.66954271, 0.72016183, 0.68078324,\n",
       "        0.67158992, 0.66925734, 0.66954271, 0.66954271, 0.66954271,\n",
       "        0.66954271, 0.66954271, 0.66954271, 0.66954271, 0.66954271]),\n",
       " 'split3_test_precision': array([0.        , 0.68262654, 0.64913105, 0.64469526, 0.64208711,\n",
       "        0.64254953, 0.64334764, 0.64334764, 0.64307164, 0.64307164,\n",
       "        0.64307164, 0.64307164, 0.64307164, 0.64307164, 0.64307164,\n",
       "        0.64307164, 0.64307164, 0.64307164, 0.68262654, 0.64469526,\n",
       "        0.64254953, 0.64334764, 0.64307164, 0.64307164, 0.64307164,\n",
       "        0.64307164, 0.64307164, 0.64307164, 0.64307164, 0.64307164]),\n",
       " 'split4_test_precision': array([0.        , 0.70761839, 0.67073766, 0.66935484, 0.66090123,\n",
       "        0.66167121, 0.66047986, 0.66047986, 0.66047986, 0.66047986,\n",
       "        0.66047986, 0.66047986, 0.66047986, 0.66047986, 0.66047986,\n",
       "        0.66047986, 0.66047986, 0.66047986, 0.70761839, 0.66935484,\n",
       "        0.66167121, 0.66047986, 0.66047986, 0.66047986, 0.66047986,\n",
       "        0.66047986, 0.66047986, 0.66047986, 0.66047986, 0.66047986]),\n",
       " 'split5_test_precision': array([0.        , 0.70747331, 0.67307692, 0.66508539, 0.66257947,\n",
       "        0.661678  , 0.66259611, 0.66274864, 0.66260163, 0.66260163,\n",
       "        0.66260163, 0.66260163, 0.66260163, 0.66260163, 0.66260163,\n",
       "        0.66260163, 0.66260163, 0.66260163, 0.70747331, 0.66508539,\n",
       "        0.66183137, 0.66274864, 0.66260163, 0.66260163, 0.66260163,\n",
       "        0.66260163, 0.66260163, 0.66260163, 0.66260163, 0.66260163]),\n",
       " 'split6_test_precision': array([0.        , 0.69865914, 0.66393443, 0.65655623, 0.65062389,\n",
       "        0.65066667, 0.65026596, 0.65055432, 0.65070922, 0.65070922,\n",
       "        0.65042091, 0.65070922, 0.65070922, 0.65070922, 0.65070922,\n",
       "        0.65070922, 0.65070922, 0.65070922, 0.69865914, 0.65655623,\n",
       "        0.65066667, 0.65055432, 0.65070922, 0.65070922, 0.65070922,\n",
       "        0.65070922, 0.65070922, 0.65070922, 0.65070922, 0.65070922]),\n",
       " 'split7_test_precision': array([0.        , 0.70539419, 0.68076734, 0.67471591, 0.67166894,\n",
       "        0.67165533, 0.67088036, 0.67088036, 0.67088036, 0.67088036,\n",
       "        0.67088036, 0.67088036, 0.67088036, 0.67088036, 0.67088036,\n",
       "        0.67088036, 0.67088036, 0.67088036, 0.70539419, 0.67471591,\n",
       "        0.67165533, 0.67088036, 0.67088036, 0.67088036, 0.67088036,\n",
       "        0.67088036, 0.67088036, 0.67088036, 0.67088036, 0.67088036]),\n",
       " 'split8_test_precision': array([0.        , 0.71832979, 0.67498822, 0.66908432, 0.66724512,\n",
       "        0.66637819, 0.66566135, 0.66551873, 0.66523236, 0.66494624,\n",
       "        0.66494624, 0.66494624, 0.66494624, 0.66494624, 0.66494624,\n",
       "        0.66523236, 0.66494624, 0.66494624, 0.71832979, 0.66908432,\n",
       "        0.66637819, 0.66551873, 0.66494624, 0.66494624, 0.66494624,\n",
       "        0.66494624, 0.66494624, 0.66494624, 0.66494624, 0.66494624]),\n",
       " 'split9_test_precision': array([0.        , 0.69677871, 0.6559565 , 0.64962121, 0.64641886,\n",
       "        0.64588979, 0.64613309, 0.64629213, 0.64629213, 0.64629213,\n",
       "        0.64629213, 0.64629213, 0.64629213, 0.64629213, 0.64629213,\n",
       "        0.64629213, 0.64629213, 0.64629213, 0.69677871, 0.64931377,\n",
       "        0.64588979, 0.64629213, 0.64629213, 0.64629213, 0.64629213,\n",
       "        0.64629213, 0.64629213, 0.64629213, 0.64629213, 0.64629213]),\n",
       " 'mean_test_precision': array([0.        , 0.7069278 , 0.67015516, 0.66524759, 0.66047529,\n",
       "        0.66014989, 0.65973063, 0.65971751, 0.65960096, 0.65957235,\n",
       "        0.65954352, 0.65957235, 0.65957235, 0.65957235, 0.65957235,\n",
       "        0.6595716 , 0.65954299, 0.65957235, 0.7069278 , 0.66521685,\n",
       "        0.66016523, 0.65971751, 0.65957235, 0.65957235, 0.65957235,\n",
       "        0.65957235, 0.65957235, 0.65957235, 0.65957235, 0.65957235]),\n",
       " 'std_test_precision': array([0.        , 0.01282676, 0.01059914, 0.01113388, 0.01006031,\n",
       "        0.00976885, 0.00921964, 0.00912181, 0.0091379 , 0.00912065,\n",
       "        0.00914904, 0.00912065, 0.00912065, 0.00912065, 0.00912065,\n",
       "        0.00911986, 0.00910249, 0.00912065, 0.01282676, 0.01117733,\n",
       "        0.00977136, 0.00912181, 0.00912065, 0.00912065, 0.00912065,\n",
       "        0.00912065, 0.00912065, 0.00912065, 0.00912065, 0.00912065]),\n",
       " 'rank_test_precision': array([30,  1,  3,  4,  6,  8,  9, 10, 12, 13, 28, 13, 13, 13, 13, 27, 29,\n",
       "        13,  1,  5,  7, 10, 13, 13, 13, 13, 13, 13, 13, 13]),\n",
       " 'split0_test_f1_micro': array([0.59957567, 0.67906648, 0.69674682, 0.69943423, 0.70155587,\n",
       "        0.70155587, 0.70169731, 0.70155587, 0.70113154, 0.70113154,\n",
       "        0.70113154, 0.70113154, 0.70113154, 0.70113154, 0.70113154,\n",
       "        0.70113154, 0.70113154, 0.70113154, 0.67906648, 0.69943423,\n",
       "        0.70155587, 0.70155587, 0.70113154, 0.70113154, 0.70113154,\n",
       "        0.70113154, 0.70113154, 0.70113154, 0.70113154, 0.70113154]),\n",
       " 'split1_test_f1_micro': array([0.59957567, 0.69335219, 0.70650636, 0.70763791, 0.70523338,\n",
       "        0.70551627, 0.70551627, 0.70551627, 0.70551627, 0.70551627,\n",
       "        0.70551627, 0.70551627, 0.70551627, 0.70551627, 0.70551627,\n",
       "        0.70537482, 0.70537482, 0.70551627, 0.69335219, 0.70763791,\n",
       "        0.70551627, 0.70551627, 0.70551627, 0.70551627, 0.70551627,\n",
       "        0.70551627, 0.70551627, 0.70551627, 0.70551627, 0.70551627]),\n",
       " 'split2_test_f1_micro': array([0.59951903, 0.69189419, 0.70872825, 0.71184043, 0.71169897,\n",
       "        0.71127458, 0.71056727, 0.7104258 , 0.71070873, 0.71070873,\n",
       "        0.71070873, 0.71070873, 0.71070873, 0.71070873, 0.71070873,\n",
       "        0.71070873, 0.71070873, 0.71070873, 0.69189419, 0.71184043,\n",
       "        0.71127458, 0.7104258 , 0.71070873, 0.71070873, 0.71070873,\n",
       "        0.71070873, 0.71070873, 0.71070873, 0.71070873, 0.71070873]),\n",
       " 'split3_test_f1_micro': array([0.59951903, 0.67506012, 0.68934786, 0.69019663, 0.69274296,\n",
       "        0.69316735, 0.69401613, 0.69401613, 0.69387466, 0.69387466,\n",
       "        0.69387466, 0.69387466, 0.69387466, 0.69387466, 0.69387466,\n",
       "        0.69387466, 0.69387466, 0.69387466, 0.67506012, 0.69019663,\n",
       "        0.69316735, 0.69401613, 0.69387466, 0.69387466, 0.69387466,\n",
       "        0.69387466, 0.69387466, 0.69387466, 0.69387466, 0.69387466]),\n",
       " 'split4_test_f1_micro': array([0.59951903, 0.68510398, 0.69840147, 0.70052341, 0.69953317,\n",
       "        0.70024049, 0.6998161 , 0.6998161 , 0.6998161 , 0.6998161 ,\n",
       "        0.6998161 , 0.6998161 , 0.6998161 , 0.6998161 , 0.6998161 ,\n",
       "        0.6998161 , 0.6998161 , 0.6998161 , 0.68510398, 0.70052341,\n",
       "        0.70024049, 0.6998161 , 0.6998161 , 0.6998161 , 0.6998161 ,\n",
       "        0.6998161 , 0.6998161 , 0.6998161 , 0.6998161 , 0.6998161 ]),\n",
       " 'split5_test_f1_micro': array([0.59966049, 0.68213326, 0.69896732, 0.69811855, 0.7009478 ,\n",
       "        0.70052341, 0.70137219, 0.70151365, 0.70151365, 0.70151365,\n",
       "        0.70151365, 0.70151365, 0.70151365, 0.70151365, 0.70151365,\n",
       "        0.70151365, 0.70151365, 0.70151365, 0.68213326, 0.69811855,\n",
       "        0.70066487, 0.70151365, 0.70151365, 0.70151365, 0.70151365,\n",
       "        0.70151365, 0.70151365, 0.70151365, 0.70151365, 0.70151365]),\n",
       " 'split6_test_f1_micro': array([0.59966049, 0.679304  , 0.69585514, 0.69458198, 0.69528929,\n",
       "        0.69557222, 0.69557222, 0.69571368, 0.69585514, 0.69585514,\n",
       "        0.69571368, 0.69585514, 0.69585514, 0.69585514, 0.69585514,\n",
       "        0.69585514, 0.69585514, 0.69585514, 0.679304  , 0.69458198,\n",
       "        0.69557222, 0.69571368, 0.69585514, 0.69585514, 0.69585514,\n",
       "        0.69585514, 0.69585514, 0.69585514, 0.69585514, 0.69585514]),\n",
       " 'split7_test_f1_micro': array([0.59966049, 0.68368935, 0.70363559, 0.70405998, 0.70646485,\n",
       "        0.70674777, 0.70674777, 0.70674777, 0.70674777, 0.70674777,\n",
       "        0.70674777, 0.70674777, 0.70674777, 0.70674777, 0.70674777,\n",
       "        0.70674777, 0.70674777, 0.70674777, 0.68368935, 0.70405998,\n",
       "        0.70674777, 0.70674777, 0.70674777, 0.70674777, 0.70674777,\n",
       "        0.70674777, 0.70674777, 0.70674777, 0.70674777, 0.70674777]),\n",
       " 'split8_test_f1_micro': array([0.59966049, 0.68694299, 0.70476729, 0.70519168, 0.70872825,\n",
       "        0.70844532, 0.70844532, 0.70844532, 0.70830386, 0.7081624 ,\n",
       "        0.7081624 , 0.7081624 , 0.7081624 , 0.7081624 , 0.7081624 ,\n",
       "        0.70830386, 0.7081624 , 0.7081624 , 0.68694299, 0.70519168,\n",
       "        0.70844532, 0.70844532, 0.7081624 , 0.7081624 , 0.7081624 ,\n",
       "        0.7081624 , 0.7081624 , 0.7081624 , 0.7081624 , 0.7081624 ]),\n",
       " 'split9_test_f1_micro': array([0.59966049, 0.67916254, 0.68892347, 0.68906493, 0.69104541,\n",
       "        0.69104541, 0.69161126, 0.69175272, 0.69175272, 0.69175272,\n",
       "        0.69175272, 0.69175272, 0.69175272, 0.69175272, 0.69175272,\n",
       "        0.69175272, 0.69175272, 0.69175272, 0.67916254, 0.68892347,\n",
       "        0.69104541, 0.69175272, 0.69175272, 0.69175272, 0.69175272,\n",
       "        0.69175272, 0.69175272, 0.69175272, 0.69175272, 0.69175272]),\n",
       " 'mean_test_f1_micro': array([0.59960109, 0.68357091, 0.69918796, 0.70006497, 0.701324  ,\n",
       "        0.70140887, 0.70153618, 0.70155033, 0.70152204, 0.7015079 ,\n",
       "        0.70149375, 0.7015079 , 0.7015079 , 0.7015079 , 0.7015079 ,\n",
       "        0.7015079 , 0.70149375, 0.7015079 , 0.68357091, 0.70005083,\n",
       "        0.70142301, 0.70155033, 0.7015079 , 0.7015079 , 0.7015079 ,\n",
       "        0.7015079 , 0.7015079 , 0.7015079 , 0.7015079 , 0.7015079 ]),\n",
       " 'std_test_f1_micro': array([6.25593973e-05, 5.57777850e-03, 6.44521599e-03, 7.00784423e-03,\n",
       "        6.50598276e-03, 6.35436646e-03, 6.04924942e-03, 5.99074088e-03,\n",
       "        6.02286396e-03, 6.00706388e-03, 6.02051064e-03, 6.00706388e-03,\n",
       "        6.00706388e-03, 6.00706388e-03, 6.00706388e-03, 6.01362620e-03,\n",
       "        5.99776843e-03, 6.00706388e-03, 5.57777850e-03, 7.03014232e-03,\n",
       "        6.35253668e-03, 5.99074088e-03, 6.00706388e-03, 6.00706388e-03,\n",
       "        6.00706388e-03, 6.00706388e-03, 6.00706388e-03, 6.00706388e-03,\n",
       "        6.00706388e-03, 6.00706388e-03]),\n",
       " 'rank_test_f1_micro': array([30, 28, 27, 25, 24, 23,  3,  1,  4,  6, 21,  6,  6,  6,  6,  5, 20,\n",
       "         6, 28, 26, 22,  1,  6,  6,  6,  6,  6,  6,  6,  6])}"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_model.cv_results_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "19851888",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([30, 28, 27, 25, 24, 23,  3,  1,  4,  6, 21,  6,  6,  6,  6,  5, 20,\n",
       "        6, 28, 26, 22,  1,  6,  6,  6,  6,  6,  6,  6,  6])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_model.cv_results_['rank_test_accuracy']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "09bdbbb9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'classifier': LogisticRegression(max_iter=10000),\n",
       " 'classifier__C': 0.0001,\n",
       " 'classifier__penalty': 'l1',\n",
       " 'classifier__solver': 'saga'}"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_model.cv_results_['params'][ np.argmin(best_model.cv_results_['mean_test_accuracy'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "ff293d70",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>classifier</th>\n",
       "      <th>C</th>\n",
       "      <th>penalty</th>\n",
       "      <th>solver</th>\n",
       "      <th>score_acc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.599601</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.683571</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.699188</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.700065</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0100</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.701324</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0100</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.701409</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.1000</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.701536</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.1000</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.701550</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.701522</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.701508</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>10.0000</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.701494</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>10.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.701508</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>100.0000</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.701508</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>100.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.701508</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>1000.0000</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.701508</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>1000.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.701508</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>10000.0000</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.701494</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>10000.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.701508</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.683571</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.700051</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0100</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.701423</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.1000</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.701550</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.701508</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>10.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.701508</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>100.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.701508</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>1000.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.701508</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>10000.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.701508</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>NaN</td>\n",
       "      <td>none</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.701508</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>NaN</td>\n",
       "      <td>none</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.701508</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.701508</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                            classifier           C penalty solver  score_acc\n",
       "0   LogisticRegression(max_iter=10000)      0.0001      l1   saga   0.599601\n",
       "1   LogisticRegression(max_iter=10000)      0.0001      l2   saga   0.683571\n",
       "2   LogisticRegression(max_iter=10000)      0.0010      l1   saga   0.699188\n",
       "3   LogisticRegression(max_iter=10000)      0.0010      l2   saga   0.700065\n",
       "4   LogisticRegression(max_iter=10000)      0.0100      l1   saga   0.701324\n",
       "5   LogisticRegression(max_iter=10000)      0.0100      l2   saga   0.701409\n",
       "6   LogisticRegression(max_iter=10000)      0.1000      l1   saga   0.701536\n",
       "7   LogisticRegression(max_iter=10000)      0.1000      l2   saga   0.701550\n",
       "8   LogisticRegression(max_iter=10000)      1.0000      l1   saga   0.701522\n",
       "9   LogisticRegression(max_iter=10000)      1.0000      l2   saga   0.701508\n",
       "10  LogisticRegression(max_iter=10000)     10.0000      l1   saga   0.701494\n",
       "11  LogisticRegression(max_iter=10000)     10.0000      l2   saga   0.701508\n",
       "12  LogisticRegression(max_iter=10000)    100.0000      l1   saga   0.701508\n",
       "13  LogisticRegression(max_iter=10000)    100.0000      l2   saga   0.701508\n",
       "14  LogisticRegression(max_iter=10000)   1000.0000      l1   saga   0.701508\n",
       "15  LogisticRegression(max_iter=10000)   1000.0000      l2   saga   0.701508\n",
       "16  LogisticRegression(max_iter=10000)  10000.0000      l1   saga   0.701494\n",
       "17  LogisticRegression(max_iter=10000)  10000.0000      l2   saga   0.701508\n",
       "18  LogisticRegression(max_iter=10000)      0.0001      l2  lbfgs   0.683571\n",
       "19  LogisticRegression(max_iter=10000)      0.0010      l2  lbfgs   0.700051\n",
       "20  LogisticRegression(max_iter=10000)      0.0100      l2  lbfgs   0.701423\n",
       "21  LogisticRegression(max_iter=10000)      0.1000      l2  lbfgs   0.701550\n",
       "22  LogisticRegression(max_iter=10000)      1.0000      l2  lbfgs   0.701508\n",
       "23  LogisticRegression(max_iter=10000)     10.0000      l2  lbfgs   0.701508\n",
       "24  LogisticRegression(max_iter=10000)    100.0000      l2  lbfgs   0.701508\n",
       "25  LogisticRegression(max_iter=10000)   1000.0000      l2  lbfgs   0.701508\n",
       "26  LogisticRegression(max_iter=10000)  10000.0000      l2  lbfgs   0.701508\n",
       "27  LogisticRegression(max_iter=10000)         NaN    none  lbfgs   0.701508\n",
       "28  LogisticRegression(max_iter=10000)         NaN    none   saga   0.701508\n",
       "29  LogisticRegression(max_iter=10000)         NaN     NaN    NaN   0.701508"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results = pd.DataFrame( best_model.cv_results_['params'] )\n",
    "\n",
    "results['score_acc'] = best_model.cv_results_['mean_test_accuracy']\n",
    "cols = results.columns.to_series().str.split('__').apply(lambda x: x[-1])\n",
    "results.columns = cols\n",
    "\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "69c1c465",
   "metadata": {},
   "outputs": [],
   "source": [
    "acc_list.append(results[0:1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "05385bec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'classifier': LogisticRegression(max_iter=10000),\n",
       " 'classifier__C': 0.0001,\n",
       " 'classifier__penalty': 'l2',\n",
       " 'classifier__solver': 'saga'}"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_model.cv_results_['params'][ np.argmin(best_model.cv_results_['rank_test_precision'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "2ef6f409",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>classifier</th>\n",
       "      <th>C</th>\n",
       "      <th>penalty</th>\n",
       "      <th>solver</th>\n",
       "      <th>score_precision</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.706928</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.670155</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.665248</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0100</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.660475</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0100</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.660150</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.1000</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.659731</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.1000</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.659718</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.659601</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.659572</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>10.0000</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.659544</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>10.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.659572</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>100.0000</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.659572</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>100.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.659572</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>1000.0000</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.659572</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>1000.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.659572</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>10000.0000</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.659543</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>10000.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.659572</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.706928</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.665217</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0100</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.660165</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.1000</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.659718</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.659572</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>10.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.659572</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>100.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.659572</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>1000.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.659572</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>10000.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.659572</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>NaN</td>\n",
       "      <td>none</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.659572</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>NaN</td>\n",
       "      <td>none</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.659572</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.659572</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                            classifier           C penalty solver  \\\n",
       "0   LogisticRegression(max_iter=10000)      0.0001      l1   saga   \n",
       "1   LogisticRegression(max_iter=10000)      0.0001      l2   saga   \n",
       "2   LogisticRegression(max_iter=10000)      0.0010      l1   saga   \n",
       "3   LogisticRegression(max_iter=10000)      0.0010      l2   saga   \n",
       "4   LogisticRegression(max_iter=10000)      0.0100      l1   saga   \n",
       "5   LogisticRegression(max_iter=10000)      0.0100      l2   saga   \n",
       "6   LogisticRegression(max_iter=10000)      0.1000      l1   saga   \n",
       "7   LogisticRegression(max_iter=10000)      0.1000      l2   saga   \n",
       "8   LogisticRegression(max_iter=10000)      1.0000      l1   saga   \n",
       "9   LogisticRegression(max_iter=10000)      1.0000      l2   saga   \n",
       "10  LogisticRegression(max_iter=10000)     10.0000      l1   saga   \n",
       "11  LogisticRegression(max_iter=10000)     10.0000      l2   saga   \n",
       "12  LogisticRegression(max_iter=10000)    100.0000      l1   saga   \n",
       "13  LogisticRegression(max_iter=10000)    100.0000      l2   saga   \n",
       "14  LogisticRegression(max_iter=10000)   1000.0000      l1   saga   \n",
       "15  LogisticRegression(max_iter=10000)   1000.0000      l2   saga   \n",
       "16  LogisticRegression(max_iter=10000)  10000.0000      l1   saga   \n",
       "17  LogisticRegression(max_iter=10000)  10000.0000      l2   saga   \n",
       "18  LogisticRegression(max_iter=10000)      0.0001      l2  lbfgs   \n",
       "19  LogisticRegression(max_iter=10000)      0.0010      l2  lbfgs   \n",
       "20  LogisticRegression(max_iter=10000)      0.0100      l2  lbfgs   \n",
       "21  LogisticRegression(max_iter=10000)      0.1000      l2  lbfgs   \n",
       "22  LogisticRegression(max_iter=10000)      1.0000      l2  lbfgs   \n",
       "23  LogisticRegression(max_iter=10000)     10.0000      l2  lbfgs   \n",
       "24  LogisticRegression(max_iter=10000)    100.0000      l2  lbfgs   \n",
       "25  LogisticRegression(max_iter=10000)   1000.0000      l2  lbfgs   \n",
       "26  LogisticRegression(max_iter=10000)  10000.0000      l2  lbfgs   \n",
       "27  LogisticRegression(max_iter=10000)         NaN    none  lbfgs   \n",
       "28  LogisticRegression(max_iter=10000)         NaN    none   saga   \n",
       "29  LogisticRegression(max_iter=10000)         NaN     NaN    NaN   \n",
       "\n",
       "    score_precision  \n",
       "0          0.000000  \n",
       "1          0.706928  \n",
       "2          0.670155  \n",
       "3          0.665248  \n",
       "4          0.660475  \n",
       "5          0.660150  \n",
       "6          0.659731  \n",
       "7          0.659718  \n",
       "8          0.659601  \n",
       "9          0.659572  \n",
       "10         0.659544  \n",
       "11         0.659572  \n",
       "12         0.659572  \n",
       "13         0.659572  \n",
       "14         0.659572  \n",
       "15         0.659572  \n",
       "16         0.659543  \n",
       "17         0.659572  \n",
       "18         0.706928  \n",
       "19         0.665217  \n",
       "20         0.660165  \n",
       "21         0.659718  \n",
       "22         0.659572  \n",
       "23         0.659572  \n",
       "24         0.659572  \n",
       "25         0.659572  \n",
       "26         0.659572  \n",
       "27         0.659572  \n",
       "28         0.659572  \n",
       "29         0.659572  "
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results = pd.DataFrame( best_model.cv_results_['params'] )\n",
    "\n",
    "results['score_precision'] = best_model.cv_results_['mean_test_precision']\n",
    "cols = results.columns.to_series().str.split('__').apply(lambda x: x[-1])\n",
    "results.columns = cols\n",
    "\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "be514fd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "roc_auc_ovr_List.append(results[1:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "8cb4c8d2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'classifier': LogisticRegression(max_iter=10000),\n",
       " 'classifier__C': 0.1,\n",
       " 'classifier__penalty': 'l2',\n",
       " 'classifier__solver': 'saga'}"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_model.cv_results_['params'][ np.argmin(best_model.cv_results_['rank_test_f1_micro'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "e1fc8e88",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>classifier</th>\n",
       "      <th>C</th>\n",
       "      <th>penalty</th>\n",
       "      <th>solver</th>\n",
       "      <th>score_f1_micro</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.599601</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.683571</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.699188</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.700065</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0100</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.701324</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0100</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.701409</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.1000</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.701536</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.1000</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.701550</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.701522</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.701508</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>10.0000</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.701494</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>10.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.701508</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>100.0000</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.701508</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>100.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.701508</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>1000.0000</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.701508</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>1000.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.701508</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>10000.0000</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.701494</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>10000.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.701508</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.683571</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.700051</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0100</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.701423</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.1000</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.701550</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.701508</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>10.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.701508</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>100.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.701508</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>1000.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.701508</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>10000.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.701508</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>NaN</td>\n",
       "      <td>none</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.701508</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>NaN</td>\n",
       "      <td>none</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.701508</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.701508</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                            classifier           C penalty solver  \\\n",
       "0   LogisticRegression(max_iter=10000)      0.0001      l1   saga   \n",
       "1   LogisticRegression(max_iter=10000)      0.0001      l2   saga   \n",
       "2   LogisticRegression(max_iter=10000)      0.0010      l1   saga   \n",
       "3   LogisticRegression(max_iter=10000)      0.0010      l2   saga   \n",
       "4   LogisticRegression(max_iter=10000)      0.0100      l1   saga   \n",
       "5   LogisticRegression(max_iter=10000)      0.0100      l2   saga   \n",
       "6   LogisticRegression(max_iter=10000)      0.1000      l1   saga   \n",
       "7   LogisticRegression(max_iter=10000)      0.1000      l2   saga   \n",
       "8   LogisticRegression(max_iter=10000)      1.0000      l1   saga   \n",
       "9   LogisticRegression(max_iter=10000)      1.0000      l2   saga   \n",
       "10  LogisticRegression(max_iter=10000)     10.0000      l1   saga   \n",
       "11  LogisticRegression(max_iter=10000)     10.0000      l2   saga   \n",
       "12  LogisticRegression(max_iter=10000)    100.0000      l1   saga   \n",
       "13  LogisticRegression(max_iter=10000)    100.0000      l2   saga   \n",
       "14  LogisticRegression(max_iter=10000)   1000.0000      l1   saga   \n",
       "15  LogisticRegression(max_iter=10000)   1000.0000      l2   saga   \n",
       "16  LogisticRegression(max_iter=10000)  10000.0000      l1   saga   \n",
       "17  LogisticRegression(max_iter=10000)  10000.0000      l2   saga   \n",
       "18  LogisticRegression(max_iter=10000)      0.0001      l2  lbfgs   \n",
       "19  LogisticRegression(max_iter=10000)      0.0010      l2  lbfgs   \n",
       "20  LogisticRegression(max_iter=10000)      0.0100      l2  lbfgs   \n",
       "21  LogisticRegression(max_iter=10000)      0.1000      l2  lbfgs   \n",
       "22  LogisticRegression(max_iter=10000)      1.0000      l2  lbfgs   \n",
       "23  LogisticRegression(max_iter=10000)     10.0000      l2  lbfgs   \n",
       "24  LogisticRegression(max_iter=10000)    100.0000      l2  lbfgs   \n",
       "25  LogisticRegression(max_iter=10000)   1000.0000      l2  lbfgs   \n",
       "26  LogisticRegression(max_iter=10000)  10000.0000      l2  lbfgs   \n",
       "27  LogisticRegression(max_iter=10000)         NaN    none  lbfgs   \n",
       "28  LogisticRegression(max_iter=10000)         NaN    none   saga   \n",
       "29  LogisticRegression(max_iter=10000)         NaN     NaN    NaN   \n",
       "\n",
       "    score_f1_micro  \n",
       "0         0.599601  \n",
       "1         0.683571  \n",
       "2         0.699188  \n",
       "3         0.700065  \n",
       "4         0.701324  \n",
       "5         0.701409  \n",
       "6         0.701536  \n",
       "7         0.701550  \n",
       "8         0.701522  \n",
       "9         0.701508  \n",
       "10        0.701494  \n",
       "11        0.701508  \n",
       "12        0.701508  \n",
       "13        0.701508  \n",
       "14        0.701508  \n",
       "15        0.701508  \n",
       "16        0.701494  \n",
       "17        0.701508  \n",
       "18        0.683571  \n",
       "19        0.700051  \n",
       "20        0.701423  \n",
       "21        0.701550  \n",
       "22        0.701508  \n",
       "23        0.701508  \n",
       "24        0.701508  \n",
       "25        0.701508  \n",
       "26        0.701508  \n",
       "27        0.701508  \n",
       "28        0.701508  \n",
       "29        0.701508  "
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results = pd.DataFrame( best_model.cv_results_['params'] )\n",
    "\n",
    "results['score_f1_micro'] = best_model.cv_results_['mean_test_f1_micro']\n",
    "cols = results.columns.to_series().str.split('__').apply(lambda x: x[-1])\n",
    "results.columns = cols\n",
    "\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "a8c83f8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "f1_micro_List.append(results[7:8])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cd3a031",
   "metadata": {},
   "source": [
    "## Trial 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "f0c67633",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, Y_train, Y_test = train_test_split(X, y, test_size=0.2, shuffle=True, random_state = 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "382815ad",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ENT\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\ENT\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\ENT\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\ENT\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\ENT\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\ENT\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\ENT\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\ENT\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\ENT\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\ENT\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\ENT\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1183: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\ENT\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1183: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\ENT\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1183: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\ENT\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1183: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\ENT\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1183: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\ENT\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1183: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\ENT\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1183: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\ENT\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1183: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\ENT\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1183: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\ENT\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1183: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\ENT\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1183: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\ENT\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1183: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\ENT\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1183: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\ENT\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1183: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\ENT\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1183: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\ENT\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1183: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\ENT\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1183: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\ENT\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1183: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\ENT\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1183: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\ENT\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1183: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Create a pipeline - RF is a stand in, we will populate the classifier part below\n",
    "pipe = Pipeline([('std', StandardScaler()),\n",
    "                 ('classifier', LogisticRegression())])\n",
    "\n",
    "# Create search space of candidate learning algorithms and their hyperparameters\n",
    "# note lbfgs can't do l1, and if you pass penalty='none' it expects no C value\n",
    "search_space = [{'classifier': [LogisticRegression(max_iter=10000)],\n",
    "                 'classifier__solver': ['saga'],\n",
    "                 'classifier__penalty': ['l1', 'l2'],\n",
    "                 'classifier__C': np.logspace(-4, 4, 9)},\n",
    "                {'classifier': [LogisticRegression(max_iter=10000)],\n",
    "                 'classifier__solver': ['lbfgs'],\n",
    "                 'classifier__penalty': ['l2'],\n",
    "                 'classifier__C': np.logspace(-4, 4, 9)},\n",
    "                {'classifier': [LogisticRegression(max_iter=10000)],\n",
    "                 'classifier__solver': ['lbfgs','saga'],\n",
    "                 'classifier__penalty': ['none']},\n",
    "                {'classifier': [LogisticRegression(max_iter=10000)]}\n",
    "                ]\n",
    "\n",
    "# Create grid search\n",
    "clf = GridSearchCV(pipe, search_space, cv=StratifiedKFold(n_splits=10),\n",
    "                   scoring=['accuracy', 'precision', 'f1_micro'], refit=False,\n",
    "                   verbose=0)\n",
    "\n",
    "# Fit grid search\n",
    "best_model = clf.fit(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "befaa8ab",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'mean_fit_time': array([0.05357742, 0.10627964, 0.12110317, 0.1061116 , 0.13594875,\n",
       "        0.12120986, 0.13368142, 0.11096139, 0.13990142, 0.11657648,\n",
       "        0.13975701, 0.11654179, 0.13539028, 0.11123261, 0.13176444,\n",
       "        0.10723565, 0.13515763, 0.10974636, 0.02428343, 0.02259071,\n",
       "        0.02261226, 0.02259984, 0.02242928, 0.02218862, 0.0229208 ,\n",
       "        0.0227211 , 0.02361941, 0.02297261, 0.10743001, 0.02244291]),\n",
       " 'std_fit_time': array([0.01038546, 0.00818391, 0.00897846, 0.00584824, 0.01463121,\n",
       "        0.01316838, 0.01361176, 0.00867428, 0.01446897, 0.01120118,\n",
       "        0.01364252, 0.01363537, 0.00833739, 0.00837242, 0.01458079,\n",
       "        0.01124388, 0.01210242, 0.00798822, 0.00140652, 0.00098355,\n",
       "        0.00062885, 0.00108545, 0.0004805 , 0.00069655, 0.00083158,\n",
       "        0.00122564, 0.00190915, 0.00121429, 0.00655359, 0.00061435]),\n",
       " 'mean_score_time': array([0.0074796 , 0.00750649, 0.0077539 , 0.00820341, 0.00850766,\n",
       "        0.00800743, 0.00794704, 0.00790691, 0.0080631 , 0.0081073 ,\n",
       "        0.00800693, 0.00820718, 0.00830681, 0.00820725, 0.00796745,\n",
       "        0.00790725, 0.00765755, 0.00780902, 0.00776446, 0.00760705,\n",
       "        0.00741007, 0.00759909, 0.00776491, 0.00762737, 0.00752652,\n",
       "        0.00780706, 0.00806236, 0.00780709, 0.00806556, 0.00765586]),\n",
       " 'std_score_time': array([5.01939663e-04, 5.00750744e-04, 4.05797034e-04, 9.81467357e-04,\n",
       "        5.00703006e-04, 4.67203091e-07, 4.82980400e-04, 5.39070903e-04,\n",
       "        3.41182042e-04, 5.39092954e-04, 3.88117328e-07, 4.00080260e-04,\n",
       "        4.59094015e-04, 4.00519768e-04, 5.52152377e-04, 3.00439920e-04,\n",
       "        5.51956101e-04, 4.01722220e-04, 7.81807445e-04, 6.64089272e-04,\n",
       "        4.88106741e-04, 5.20791008e-04, 4.14682308e-04, 5.10610883e-04,\n",
       "        4.83978302e-04, 4.00388307e-04, 1.38616880e-03, 6.00600309e-04,\n",
       "        4.79309980e-04, 7.48392927e-04]),\n",
       " 'param_classifier': masked_array(data=[LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000)],\n",
       "              mask=[False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'param_classifier__C': masked_array(data=[0.0001, 0.0001, 0.001, 0.001, 0.01, 0.01, 0.1, 0.1,\n",
       "                    1.0, 1.0, 10.0, 10.0, 100.0, 100.0, 1000.0, 1000.0,\n",
       "                    10000.0, 10000.0, 0.0001, 0.001, 0.01, 0.1, 1.0, 10.0,\n",
       "                    100.0, 1000.0, 10000.0, --, --, --],\n",
       "              mask=[False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False,  True,  True,  True],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'param_classifier__penalty': masked_array(data=['l1', 'l2', 'l1', 'l2', 'l1', 'l2', 'l1', 'l2', 'l1',\n",
       "                    'l2', 'l1', 'l2', 'l1', 'l2', 'l1', 'l2', 'l1', 'l2',\n",
       "                    'l2', 'l2', 'l2', 'l2', 'l2', 'l2', 'l2', 'l2', 'l2',\n",
       "                    'none', 'none', --],\n",
       "              mask=[False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False,  True],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'param_classifier__solver': masked_array(data=['saga', 'saga', 'saga', 'saga', 'saga', 'saga', 'saga',\n",
       "                    'saga', 'saga', 'saga', 'saga', 'saga', 'saga', 'saga',\n",
       "                    'saga', 'saga', 'saga', 'saga', 'lbfgs', 'lbfgs',\n",
       "                    'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs',\n",
       "                    'lbfgs', 'lbfgs', 'saga', --],\n",
       "              mask=[False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False,  True],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'params': [{'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 0.0001,\n",
       "   'classifier__penalty': 'l1',\n",
       "   'classifier__solver': 'saga'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 0.0001,\n",
       "   'classifier__penalty': 'l2',\n",
       "   'classifier__solver': 'saga'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 0.001,\n",
       "   'classifier__penalty': 'l1',\n",
       "   'classifier__solver': 'saga'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 0.001,\n",
       "   'classifier__penalty': 'l2',\n",
       "   'classifier__solver': 'saga'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 0.01,\n",
       "   'classifier__penalty': 'l1',\n",
       "   'classifier__solver': 'saga'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 0.01,\n",
       "   'classifier__penalty': 'l2',\n",
       "   'classifier__solver': 'saga'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 0.1,\n",
       "   'classifier__penalty': 'l1',\n",
       "   'classifier__solver': 'saga'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 0.1,\n",
       "   'classifier__penalty': 'l2',\n",
       "   'classifier__solver': 'saga'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 1.0,\n",
       "   'classifier__penalty': 'l1',\n",
       "   'classifier__solver': 'saga'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 1.0,\n",
       "   'classifier__penalty': 'l2',\n",
       "   'classifier__solver': 'saga'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 10.0,\n",
       "   'classifier__penalty': 'l1',\n",
       "   'classifier__solver': 'saga'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 10.0,\n",
       "   'classifier__penalty': 'l2',\n",
       "   'classifier__solver': 'saga'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 100.0,\n",
       "   'classifier__penalty': 'l1',\n",
       "   'classifier__solver': 'saga'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 100.0,\n",
       "   'classifier__penalty': 'l2',\n",
       "   'classifier__solver': 'saga'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 1000.0,\n",
       "   'classifier__penalty': 'l1',\n",
       "   'classifier__solver': 'saga'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 1000.0,\n",
       "   'classifier__penalty': 'l2',\n",
       "   'classifier__solver': 'saga'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 10000.0,\n",
       "   'classifier__penalty': 'l1',\n",
       "   'classifier__solver': 'saga'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 10000.0,\n",
       "   'classifier__penalty': 'l2',\n",
       "   'classifier__solver': 'saga'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 0.0001,\n",
       "   'classifier__penalty': 'l2',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 0.001,\n",
       "   'classifier__penalty': 'l2',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 0.01,\n",
       "   'classifier__penalty': 'l2',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 0.1,\n",
       "   'classifier__penalty': 'l2',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 1.0,\n",
       "   'classifier__penalty': 'l2',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 10.0,\n",
       "   'classifier__penalty': 'l2',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 100.0,\n",
       "   'classifier__penalty': 'l2',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 1000.0,\n",
       "   'classifier__penalty': 'l2',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 10000.0,\n",
       "   'classifier__penalty': 'l2',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__penalty': 'none',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__penalty': 'none',\n",
       "   'classifier__solver': 'saga'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000)}],\n",
       " 'split0_test_accuracy': array([0.59929279, 0.68189533, 0.69844413, 0.7       , 0.70113154,\n",
       "        0.70084866, 0.70141443, 0.70141443, 0.70127298, 0.70127298,\n",
       "        0.70127298, 0.70127298, 0.70127298, 0.70127298, 0.70127298,\n",
       "        0.70127298, 0.70127298, 0.70127298, 0.68189533, 0.7       ,\n",
       "        0.70084866, 0.70141443, 0.70127298, 0.70127298, 0.70127298,\n",
       "        0.70127298, 0.70127298, 0.70127298, 0.70127298, 0.70127298]),\n",
       " 'split1_test_accuracy': array([0.59929279, 0.68048091, 0.69589816, 0.69688826, 0.69632249,\n",
       "        0.69618105, 0.69561528, 0.69575672, 0.69575672, 0.69575672,\n",
       "        0.69575672, 0.69575672, 0.69575672, 0.69575672, 0.69575672,\n",
       "        0.69575672, 0.69575672, 0.69575672, 0.68033946, 0.69688826,\n",
       "        0.69618105, 0.69575672, 0.69575672, 0.69575672, 0.69575672,\n",
       "        0.69575672, 0.69575672, 0.69575672, 0.69575672, 0.69575672]),\n",
       " 'split2_test_accuracy': array([0.5992361 , 0.68467959, 0.69953317, 0.69925025, 0.70250389,\n",
       "        0.7020795 , 0.70278682, 0.70264535, 0.70278682, 0.70278682,\n",
       "        0.70278682, 0.70278682, 0.70292828, 0.70292828, 0.70278682,\n",
       "        0.70292828, 0.70292828, 0.70292828, 0.68467959, 0.69925025,\n",
       "        0.7020795 , 0.70264535, 0.70278682, 0.70292828, 0.70292828,\n",
       "        0.70292828, 0.70292828, 0.70292828, 0.70278682, 0.70278682]),\n",
       " 'split3_test_accuracy': array([0.5992361 , 0.67562597, 0.69458198, 0.69811855, 0.69854293,\n",
       "        0.6986844 , 0.69882586, 0.69882586, 0.6986844 , 0.69882586,\n",
       "        0.6986844 , 0.6986844 , 0.6986844 , 0.6986844 , 0.6986844 ,\n",
       "        0.6986844 , 0.6986844 , 0.6986844 , 0.67562597, 0.69811855,\n",
       "        0.6986844 , 0.69882586, 0.6986844 , 0.6986844 , 0.6986844 ,\n",
       "        0.6986844 , 0.6986844 , 0.6986844 , 0.6986844 , 0.6986844 ]),\n",
       " 'split4_test_accuracy': array([0.59937756, 0.68057717, 0.69543075, 0.69712831, 0.69939171,\n",
       "        0.69953317, 0.69939171, 0.69939171, 0.69939171, 0.69939171,\n",
       "        0.69939171, 0.69939171, 0.69939171, 0.69939171, 0.69939171,\n",
       "        0.69939171, 0.69939171, 0.69939171, 0.68057717, 0.69712831,\n",
       "        0.69953317, 0.69939171, 0.69939171, 0.69939171, 0.69939171,\n",
       "        0.69939171, 0.69939171, 0.69939171, 0.69939171, 0.69939171]),\n",
       " 'split5_test_accuracy': array([0.59937756, 0.68963078, 0.70519168, 0.7070307 , 0.70872825,\n",
       "        0.7081624 , 0.70858679, 0.70858679, 0.70858679, 0.70858679,\n",
       "        0.70858679, 0.70858679, 0.70858679, 0.70858679, 0.70858679,\n",
       "        0.70858679, 0.70858679, 0.70858679, 0.68977225, 0.7070307 ,\n",
       "        0.7081624 , 0.70858679, 0.70858679, 0.70858679, 0.70858679,\n",
       "        0.70858679, 0.70858679, 0.70858679, 0.70858679, 0.70858679]),\n",
       " 'split6_test_accuracy': array([0.59937756, 0.69076248, 0.70802094, 0.70547461, 0.70844532,\n",
       "        0.70844532, 0.7081624 , 0.7081624 , 0.70844532, 0.70844532,\n",
       "        0.70844532, 0.70844532, 0.70844532, 0.70844532, 0.70844532,\n",
       "        0.70844532, 0.70844532, 0.70844532, 0.69076248, 0.70547461,\n",
       "        0.70844532, 0.7081624 , 0.70844532, 0.70844532, 0.70844532,\n",
       "        0.70844532, 0.70844532, 0.70844532, 0.70830386, 0.70844532]),\n",
       " 'split7_test_accuracy': array([0.59937756, 0.68043571, 0.70066487, 0.6998161 , 0.69939171,\n",
       "        0.69910878, 0.6998161 , 0.70009902, 0.70024049, 0.70024049,\n",
       "        0.70024049, 0.70024049, 0.70024049, 0.70024049, 0.70024049,\n",
       "        0.70024049, 0.70024049, 0.70024049, 0.68043571, 0.6998161 ,\n",
       "        0.69910878, 0.70009902, 0.70024049, 0.70024049, 0.70024049,\n",
       "        0.70024049, 0.70024049, 0.70024049, 0.70024049, 0.70024049]),\n",
       " 'split8_test_accuracy': array([0.59937756, 0.68736738, 0.70335267, 0.70264535, 0.70434291,\n",
       "        0.70476729, 0.70519168, 0.70519168, 0.70490876, 0.70490876,\n",
       "        0.70505022, 0.70505022, 0.70505022, 0.70505022, 0.70505022,\n",
       "        0.70505022, 0.70505022, 0.70505022, 0.68736738, 0.70264535,\n",
       "        0.70476729, 0.70505022, 0.70490876, 0.70505022, 0.70505022,\n",
       "        0.70505022, 0.70505022, 0.70505022, 0.70505022, 0.70490876]),\n",
       " 'split9_test_accuracy': array([0.59937756, 0.67859669, 0.69783562, 0.70024049, 0.70123073,\n",
       "        0.7009478 , 0.70108926, 0.70123073, 0.70108926, 0.70123073,\n",
       "        0.70123073, 0.70123073, 0.70123073, 0.70123073, 0.70123073,\n",
       "        0.70123073, 0.70123073, 0.70123073, 0.67859669, 0.70024049,\n",
       "        0.7009478 , 0.70123073, 0.70123073, 0.70123073, 0.70123073,\n",
       "        0.70123073, 0.70123073, 0.70123073, 0.70123073, 0.70123073]),\n",
       " 'mean_test_accuracy': array([0.59933232, 0.6830052 , 0.6998954 , 0.70065926, 0.70200315,\n",
       "        0.70187584, 0.70208803, 0.70213047, 0.70211632, 0.70214462,\n",
       "        0.70214462, 0.70214462, 0.70215876, 0.70215876, 0.70214462,\n",
       "        0.70215876, 0.70215876, 0.70215876, 0.6830052 , 0.70065926,\n",
       "        0.70187584, 0.70211632, 0.70213047, 0.70215876, 0.70215876,\n",
       "        0.70215876, 0.70215876, 0.70215876, 0.70213047, 0.70213047]),\n",
       " 'std_test_accuracy': array([5.82443238e-05, 4.68930107e-03, 4.21002676e-03, 3.22764438e-03,\n",
       "        3.89477032e-03, 3.85691544e-03, 3.94555648e-03, 3.90127315e-03,\n",
       "        3.93829218e-03, 3.92265102e-03, 3.94503409e-03, 3.94503409e-03,\n",
       "        3.94756437e-03, 3.94756437e-03, 3.94503409e-03, 3.94756437e-03,\n",
       "        3.94756437e-03, 3.94756437e-03, 4.71724597e-03, 3.22764438e-03,\n",
       "        3.85691544e-03, 3.89038863e-03, 3.93483013e-03, 3.94756437e-03,\n",
       "        3.94756437e-03, 3.94756437e-03, 3.94756437e-03, 3.94756437e-03,\n",
       "        3.92260526e-03, 3.93483013e-03]),\n",
       " 'rank_test_accuracy': array([30, 29, 27, 25, 22, 23, 21, 18, 19, 11, 11, 11,  1,  1, 11,  1,  1,\n",
       "         1, 28, 25, 23, 20, 15,  1,  1,  1,  1,  1, 17, 15]),\n",
       " 'split0_test_precision': array([0.        , 0.70738636, 0.67039378, 0.66573557, 0.6618705 ,\n",
       "        0.66098655, 0.66130474, 0.66130474, 0.66100938, 0.66100938,\n",
       "        0.66100938, 0.66100938, 0.66100938, 0.66100938, 0.66100938,\n",
       "        0.66100938, 0.66100938, 0.66100938, 0.70738636, 0.66573557,\n",
       "        0.66098655, 0.66130474, 0.66100938, 0.66100938, 0.66100938,\n",
       "        0.66100938, 0.66100938, 0.66100938, 0.66100938, 0.66100938]),\n",
       " 'split1_test_precision': array([0.        , 0.69820442, 0.66331899, 0.65913284, 0.65367384,\n",
       "        0.6533811 , 0.65167038, 0.65182547, 0.65169039, 0.65169039,\n",
       "        0.65169039, 0.65169039, 0.65169039, 0.65169039, 0.65169039,\n",
       "        0.65169039, 0.65169039, 0.65169039, 0.69799585, 0.65913284,\n",
       "        0.6533811 , 0.65182547, 0.65169039, 0.65169039, 0.65169039,\n",
       "        0.65169039, 0.65169039, 0.65169039, 0.65169039, 0.65169039]),\n",
       " 'split2_test_precision': array([0.        , 0.70106525, 0.66761229, 0.65930599, 0.65759931,\n",
       "        0.65701944, 0.65735168, 0.65706919, 0.65721649, 0.65721649,\n",
       "        0.65721649, 0.65721649, 0.65736368, 0.65736368, 0.65721649,\n",
       "        0.65736368, 0.65736368, 0.65736368, 0.70106525, 0.65930599,\n",
       "        0.65701944, 0.65706919, 0.65721649, 0.65736368, 0.65736368,\n",
       "        0.65736368, 0.65736368, 0.65736368, 0.65721649, 0.65721649]),\n",
       " 'split3_test_precision': array([0.        , 0.68802228, 0.66124402, 0.66054203, 0.65340909,\n",
       "        0.65342645, 0.65264527, 0.65264527, 0.65236238, 0.65264527,\n",
       "        0.65236238, 0.65236238, 0.65236238, 0.65236238, 0.65236238,\n",
       "        0.65236238, 0.65236238, 0.65236238, 0.68802228, 0.66054203,\n",
       "        0.65342645, 0.65264527, 0.65236238, 0.65236238, 0.65236238,\n",
       "        0.65236238, 0.65236238, 0.65236238, 0.65236238, 0.65236238]),\n",
       " 'split4_test_precision': array([0.        , 0.69986072, 0.66036845, 0.65740319, 0.65429943,\n",
       "        0.65431561, 0.65349544, 0.65336226, 0.65336226, 0.65336226,\n",
       "        0.65336226, 0.65336226, 0.65336226, 0.65336226, 0.65336226,\n",
       "        0.65336226, 0.65336226, 0.65336226, 0.69986072, 0.65740319,\n",
       "        0.65431561, 0.65336226, 0.65336226, 0.65336226, 0.65336226,\n",
       "        0.65336226, 0.65336226, 0.65336226, 0.65336226, 0.65336226]),\n",
       " 'split5_test_precision': array([0.        , 0.71909341, 0.67809524, 0.67382366, 0.66944323,\n",
       "        0.66768426, 0.66768028, 0.66768028, 0.66768028, 0.66768028,\n",
       "        0.66768028, 0.66768028, 0.66768028, 0.66768028, 0.66768028,\n",
       "        0.66768028, 0.66768028, 0.66768028, 0.7192862 , 0.67382366,\n",
       "        0.66768426, 0.66768028, 0.66768028, 0.66768028, 0.66768028,\n",
       "        0.66768028, 0.66768028, 0.66768028, 0.66768028, 0.66768028]),\n",
       " 'split6_test_precision': array([0.        , 0.72368421, 0.68461538, 0.67313019, 0.67110519,\n",
       "        0.67049978, 0.66960741, 0.66960741, 0.66989863, 0.66989863,\n",
       "        0.66989863, 0.66989863, 0.66989863, 0.66989863, 0.66989863,\n",
       "        0.66989863, 0.66989863, 0.66989863, 0.72368421, 0.67313019,\n",
       "        0.67049978, 0.66960741, 0.66989863, 0.66989863, 0.66989863,\n",
       "        0.66989863, 0.66989863, 0.66989863, 0.66975309, 0.66989863]),\n",
       " 'split7_test_precision': array([0.        , 0.6982699 , 0.66966825, 0.66269478, 0.65634675,\n",
       "        0.65549184, 0.65583845, 0.65614035, 0.6562911 , 0.6562911 ,\n",
       "        0.6562911 , 0.6562911 , 0.6562911 , 0.6562911 , 0.6562911 ,\n",
       "        0.6562911 , 0.6562911 , 0.6562911 , 0.6982699 , 0.66269478,\n",
       "        0.65549184, 0.65614035, 0.6562911 , 0.6562911 , 0.6562911 ,\n",
       "        0.6562911 , 0.6562911 , 0.6562911 , 0.6562911 , 0.6562911 ]),\n",
       " 'split8_test_precision': array([0.        , 0.7135989 , 0.67710843, 0.66898148, 0.66488889,\n",
       "        0.66518847, 0.664903  , 0.66475771, 0.66431718, 0.66431718,\n",
       "        0.66446499, 0.66446499, 0.66446499, 0.66446499, 0.66446499,\n",
       "        0.66446499, 0.66446499, 0.66446499, 0.7135989 , 0.66898148,\n",
       "        0.66518847, 0.66460996, 0.66431718, 0.66446499, 0.66446499,\n",
       "        0.66446499, 0.66446499, 0.66446499, 0.66446499, 0.66431718]),\n",
       " 'split9_test_precision': array([0.        , 0.69525802, 0.67008798, 0.66776471, 0.66143498,\n",
       "        0.66084229, 0.66013363, 0.66042781, 0.6599911 , 0.66028495,\n",
       "        0.66028495, 0.66028495, 0.66028495, 0.66028495, 0.66028495,\n",
       "        0.66028495, 0.66028495, 0.66028495, 0.69525802, 0.66776471,\n",
       "        0.66084229, 0.66042781, 0.66028495, 0.66028495, 0.66028495,\n",
       "        0.66028495, 0.66028495, 0.66028495, 0.66028495, 0.66028495]),\n",
       " 'mean_test_precision': array([0.        , 0.70444435, 0.67025128, 0.66485144, 0.66040712,\n",
       "        0.65988358, 0.65946303, 0.65948205, 0.65938192, 0.65943959,\n",
       "        0.65942609, 0.65942609, 0.6594408 , 0.6594408 , 0.65942609,\n",
       "        0.6594408 , 0.6594408 , 0.6594408 , 0.70444277, 0.66485144,\n",
       "        0.65988358, 0.65946727, 0.6594113 , 0.6594408 , 0.6594408 ,\n",
       "        0.6594408 , 0.6594408 , 0.6594408 , 0.65941153, 0.6594113 ]),\n",
       " 'std_test_precision': array([0.        , 0.01068078, 0.00741003, 0.00563487, 0.00612673,\n",
       "        0.00588474, 0.00603557, 0.00601322, 0.00604775, 0.00601891,\n",
       "        0.00606348, 0.00606348, 0.00605828, 0.00605828, 0.00606348,\n",
       "        0.00605828, 0.00605828, 0.00605828, 0.01071971, 0.00563487,\n",
       "        0.00588474, 0.00600041, 0.00605135, 0.00605828, 0.00605828,\n",
       "        0.00605828, 0.00605828, 0.00605828, 0.00603845, 0.00605135]),\n",
       " 'rank_test_precision': array([30,  1,  3,  4,  6,  7, 11,  9, 29, 22, 23, 23, 12, 12, 23, 12, 12,\n",
       "        12,  2,  4,  7, 10, 27, 12, 12, 12, 12, 12, 26, 27]),\n",
       " 'split0_test_f1_micro': array([0.59929279, 0.68189533, 0.69844413, 0.7       , 0.70113154,\n",
       "        0.70084866, 0.70141443, 0.70141443, 0.70127298, 0.70127298,\n",
       "        0.70127298, 0.70127298, 0.70127298, 0.70127298, 0.70127298,\n",
       "        0.70127298, 0.70127298, 0.70127298, 0.68189533, 0.7       ,\n",
       "        0.70084866, 0.70141443, 0.70127298, 0.70127298, 0.70127298,\n",
       "        0.70127298, 0.70127298, 0.70127298, 0.70127298, 0.70127298]),\n",
       " 'split1_test_f1_micro': array([0.59929279, 0.68048091, 0.69589816, 0.69688826, 0.69632249,\n",
       "        0.69618105, 0.69561528, 0.69575672, 0.69575672, 0.69575672,\n",
       "        0.69575672, 0.69575672, 0.69575672, 0.69575672, 0.69575672,\n",
       "        0.69575672, 0.69575672, 0.69575672, 0.68033946, 0.69688826,\n",
       "        0.69618105, 0.69575672, 0.69575672, 0.69575672, 0.69575672,\n",
       "        0.69575672, 0.69575672, 0.69575672, 0.69575672, 0.69575672]),\n",
       " 'split2_test_f1_micro': array([0.5992361 , 0.68467959, 0.69953317, 0.69925025, 0.70250389,\n",
       "        0.7020795 , 0.70278682, 0.70264535, 0.70278682, 0.70278682,\n",
       "        0.70278682, 0.70278682, 0.70292828, 0.70292828, 0.70278682,\n",
       "        0.70292828, 0.70292828, 0.70292828, 0.68467959, 0.69925025,\n",
       "        0.7020795 , 0.70264535, 0.70278682, 0.70292828, 0.70292828,\n",
       "        0.70292828, 0.70292828, 0.70292828, 0.70278682, 0.70278682]),\n",
       " 'split3_test_f1_micro': array([0.5992361 , 0.67562597, 0.69458198, 0.69811855, 0.69854293,\n",
       "        0.6986844 , 0.69882586, 0.69882586, 0.6986844 , 0.69882586,\n",
       "        0.6986844 , 0.6986844 , 0.6986844 , 0.6986844 , 0.6986844 ,\n",
       "        0.6986844 , 0.6986844 , 0.6986844 , 0.67562597, 0.69811855,\n",
       "        0.6986844 , 0.69882586, 0.6986844 , 0.6986844 , 0.6986844 ,\n",
       "        0.6986844 , 0.6986844 , 0.6986844 , 0.6986844 , 0.6986844 ]),\n",
       " 'split4_test_f1_micro': array([0.59937756, 0.68057717, 0.69543075, 0.69712831, 0.69939171,\n",
       "        0.69953317, 0.69939171, 0.69939171, 0.69939171, 0.69939171,\n",
       "        0.69939171, 0.69939171, 0.69939171, 0.69939171, 0.69939171,\n",
       "        0.69939171, 0.69939171, 0.69939171, 0.68057717, 0.69712831,\n",
       "        0.69953317, 0.69939171, 0.69939171, 0.69939171, 0.69939171,\n",
       "        0.69939171, 0.69939171, 0.69939171, 0.69939171, 0.69939171]),\n",
       " 'split5_test_f1_micro': array([0.59937756, 0.68963078, 0.70519168, 0.7070307 , 0.70872825,\n",
       "        0.7081624 , 0.70858679, 0.70858679, 0.70858679, 0.70858679,\n",
       "        0.70858679, 0.70858679, 0.70858679, 0.70858679, 0.70858679,\n",
       "        0.70858679, 0.70858679, 0.70858679, 0.68977225, 0.7070307 ,\n",
       "        0.7081624 , 0.70858679, 0.70858679, 0.70858679, 0.70858679,\n",
       "        0.70858679, 0.70858679, 0.70858679, 0.70858679, 0.70858679]),\n",
       " 'split6_test_f1_micro': array([0.59937756, 0.69076248, 0.70802094, 0.70547461, 0.70844532,\n",
       "        0.70844532, 0.7081624 , 0.7081624 , 0.70844532, 0.70844532,\n",
       "        0.70844532, 0.70844532, 0.70844532, 0.70844532, 0.70844532,\n",
       "        0.70844532, 0.70844532, 0.70844532, 0.69076248, 0.70547461,\n",
       "        0.70844532, 0.7081624 , 0.70844532, 0.70844532, 0.70844532,\n",
       "        0.70844532, 0.70844532, 0.70844532, 0.70830386, 0.70844532]),\n",
       " 'split7_test_f1_micro': array([0.59937756, 0.68043571, 0.70066487, 0.6998161 , 0.69939171,\n",
       "        0.69910878, 0.6998161 , 0.70009902, 0.70024049, 0.70024049,\n",
       "        0.70024049, 0.70024049, 0.70024049, 0.70024049, 0.70024049,\n",
       "        0.70024049, 0.70024049, 0.70024049, 0.68043571, 0.6998161 ,\n",
       "        0.69910878, 0.70009902, 0.70024049, 0.70024049, 0.70024049,\n",
       "        0.70024049, 0.70024049, 0.70024049, 0.70024049, 0.70024049]),\n",
       " 'split8_test_f1_micro': array([0.59937756, 0.68736738, 0.70335267, 0.70264535, 0.70434291,\n",
       "        0.70476729, 0.70519168, 0.70519168, 0.70490876, 0.70490876,\n",
       "        0.70505022, 0.70505022, 0.70505022, 0.70505022, 0.70505022,\n",
       "        0.70505022, 0.70505022, 0.70505022, 0.68736738, 0.70264535,\n",
       "        0.70476729, 0.70505022, 0.70490876, 0.70505022, 0.70505022,\n",
       "        0.70505022, 0.70505022, 0.70505022, 0.70505022, 0.70490876]),\n",
       " 'split9_test_f1_micro': array([0.59937756, 0.67859669, 0.69783562, 0.70024049, 0.70123073,\n",
       "        0.7009478 , 0.70108926, 0.70123073, 0.70108926, 0.70123073,\n",
       "        0.70123073, 0.70123073, 0.70123073, 0.70123073, 0.70123073,\n",
       "        0.70123073, 0.70123073, 0.70123073, 0.67859669, 0.70024049,\n",
       "        0.7009478 , 0.70123073, 0.70123073, 0.70123073, 0.70123073,\n",
       "        0.70123073, 0.70123073, 0.70123073, 0.70123073, 0.70123073]),\n",
       " 'mean_test_f1_micro': array([0.59933232, 0.6830052 , 0.6998954 , 0.70065926, 0.70200315,\n",
       "        0.70187584, 0.70208803, 0.70213047, 0.70211632, 0.70214462,\n",
       "        0.70214462, 0.70214462, 0.70215876, 0.70215876, 0.70214462,\n",
       "        0.70215876, 0.70215876, 0.70215876, 0.6830052 , 0.70065926,\n",
       "        0.70187584, 0.70211632, 0.70213047, 0.70215876, 0.70215876,\n",
       "        0.70215876, 0.70215876, 0.70215876, 0.70213047, 0.70213047]),\n",
       " 'std_test_f1_micro': array([5.82443238e-05, 4.68930107e-03, 4.21002676e-03, 3.22764438e-03,\n",
       "        3.89477032e-03, 3.85691544e-03, 3.94555648e-03, 3.90127315e-03,\n",
       "        3.93829218e-03, 3.92265102e-03, 3.94503409e-03, 3.94503409e-03,\n",
       "        3.94756437e-03, 3.94756437e-03, 3.94503409e-03, 3.94756437e-03,\n",
       "        3.94756437e-03, 3.94756437e-03, 4.71724597e-03, 3.22764438e-03,\n",
       "        3.85691544e-03, 3.89038863e-03, 3.93483013e-03, 3.94756437e-03,\n",
       "        3.94756437e-03, 3.94756437e-03, 3.94756437e-03, 3.94756437e-03,\n",
       "        3.92260526e-03, 3.93483013e-03]),\n",
       " 'rank_test_f1_micro': array([30, 29, 27, 25, 22, 23, 21, 18, 19, 11, 11, 11,  1,  1, 11,  1,  1,\n",
       "         1, 28, 25, 23, 20, 15,  1,  1,  1,  1,  1, 17, 15])}"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_model.cv_results_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "9aa08645",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([30, 29, 27, 25, 22, 23, 21, 18, 19, 11, 11, 11,  1,  1, 11,  1,  1,\n",
       "        1, 28, 25, 23, 20, 15,  1,  1,  1,  1,  1, 17, 15])"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_model.cv_results_['rank_test_accuracy']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "abdb8b79",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'classifier': LogisticRegression(max_iter=10000),\n",
       " 'classifier__C': 0.0001,\n",
       " 'classifier__penalty': 'l1',\n",
       " 'classifier__solver': 'saga'}"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_model.cv_results_['params'][ np.argmin(best_model.cv_results_['mean_test_accuracy'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "f4602352",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>classifier</th>\n",
       "      <th>C</th>\n",
       "      <th>penalty</th>\n",
       "      <th>solver</th>\n",
       "      <th>score_acc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.599332</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.683005</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.699895</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.700659</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0100</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.702003</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0100</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.701876</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.1000</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.702088</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.1000</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.702130</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.702116</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.702145</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>10.0000</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.702145</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>10.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.702145</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>100.0000</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.702159</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>100.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.702159</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>1000.0000</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.702145</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>1000.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.702159</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>10000.0000</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.702159</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>10000.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.702159</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.683005</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.700659</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0100</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.701876</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.1000</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.702116</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.702130</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>10.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.702159</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>100.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.702159</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>1000.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.702159</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>10000.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.702159</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>NaN</td>\n",
       "      <td>none</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.702159</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>NaN</td>\n",
       "      <td>none</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.702130</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.702130</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                            classifier           C penalty solver  score_acc\n",
       "0   LogisticRegression(max_iter=10000)      0.0001      l1   saga   0.599332\n",
       "1   LogisticRegression(max_iter=10000)      0.0001      l2   saga   0.683005\n",
       "2   LogisticRegression(max_iter=10000)      0.0010      l1   saga   0.699895\n",
       "3   LogisticRegression(max_iter=10000)      0.0010      l2   saga   0.700659\n",
       "4   LogisticRegression(max_iter=10000)      0.0100      l1   saga   0.702003\n",
       "5   LogisticRegression(max_iter=10000)      0.0100      l2   saga   0.701876\n",
       "6   LogisticRegression(max_iter=10000)      0.1000      l1   saga   0.702088\n",
       "7   LogisticRegression(max_iter=10000)      0.1000      l2   saga   0.702130\n",
       "8   LogisticRegression(max_iter=10000)      1.0000      l1   saga   0.702116\n",
       "9   LogisticRegression(max_iter=10000)      1.0000      l2   saga   0.702145\n",
       "10  LogisticRegression(max_iter=10000)     10.0000      l1   saga   0.702145\n",
       "11  LogisticRegression(max_iter=10000)     10.0000      l2   saga   0.702145\n",
       "12  LogisticRegression(max_iter=10000)    100.0000      l1   saga   0.702159\n",
       "13  LogisticRegression(max_iter=10000)    100.0000      l2   saga   0.702159\n",
       "14  LogisticRegression(max_iter=10000)   1000.0000      l1   saga   0.702145\n",
       "15  LogisticRegression(max_iter=10000)   1000.0000      l2   saga   0.702159\n",
       "16  LogisticRegression(max_iter=10000)  10000.0000      l1   saga   0.702159\n",
       "17  LogisticRegression(max_iter=10000)  10000.0000      l2   saga   0.702159\n",
       "18  LogisticRegression(max_iter=10000)      0.0001      l2  lbfgs   0.683005\n",
       "19  LogisticRegression(max_iter=10000)      0.0010      l2  lbfgs   0.700659\n",
       "20  LogisticRegression(max_iter=10000)      0.0100      l2  lbfgs   0.701876\n",
       "21  LogisticRegression(max_iter=10000)      0.1000      l2  lbfgs   0.702116\n",
       "22  LogisticRegression(max_iter=10000)      1.0000      l2  lbfgs   0.702130\n",
       "23  LogisticRegression(max_iter=10000)     10.0000      l2  lbfgs   0.702159\n",
       "24  LogisticRegression(max_iter=10000)    100.0000      l2  lbfgs   0.702159\n",
       "25  LogisticRegression(max_iter=10000)   1000.0000      l2  lbfgs   0.702159\n",
       "26  LogisticRegression(max_iter=10000)  10000.0000      l2  lbfgs   0.702159\n",
       "27  LogisticRegression(max_iter=10000)         NaN    none  lbfgs   0.702159\n",
       "28  LogisticRegression(max_iter=10000)         NaN    none   saga   0.702130\n",
       "29  LogisticRegression(max_iter=10000)         NaN     NaN    NaN   0.702130"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results = pd.DataFrame( best_model.cv_results_['params'] )\n",
    "\n",
    "results['score_acc'] = best_model.cv_results_['mean_test_accuracy']\n",
    "cols = results.columns.to_series().str.split('__').apply(lambda x: x[-1])\n",
    "results.columns = cols\n",
    "\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "dce721ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "acc_list.append(results[0:1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "ab7ddcc5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'classifier': LogisticRegression(max_iter=10000),\n",
       " 'classifier__C': 0.0001,\n",
       " 'classifier__penalty': 'l2',\n",
       " 'classifier__solver': 'saga'}"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_model.cv_results_['params'][ np.argmin(best_model.cv_results_['rank_test_precision'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "c6c37019",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>classifier</th>\n",
       "      <th>C</th>\n",
       "      <th>penalty</th>\n",
       "      <th>solver</th>\n",
       "      <th>score_precision</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.704444</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.670251</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.664851</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0100</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.660407</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0100</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.659884</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.1000</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.659463</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.1000</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.659482</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.659382</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.659440</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>10.0000</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.659426</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>10.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.659426</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>100.0000</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.659441</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>100.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.659441</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>1000.0000</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.659426</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>1000.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.659441</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>10000.0000</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.659441</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>10000.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.659441</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.704443</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.664851</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0100</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.659884</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.1000</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.659467</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.659411</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>10.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.659441</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>100.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.659441</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>1000.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.659441</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>10000.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.659441</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>NaN</td>\n",
       "      <td>none</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.659441</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>NaN</td>\n",
       "      <td>none</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.659412</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.659411</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                            classifier           C penalty solver  \\\n",
       "0   LogisticRegression(max_iter=10000)      0.0001      l1   saga   \n",
       "1   LogisticRegression(max_iter=10000)      0.0001      l2   saga   \n",
       "2   LogisticRegression(max_iter=10000)      0.0010      l1   saga   \n",
       "3   LogisticRegression(max_iter=10000)      0.0010      l2   saga   \n",
       "4   LogisticRegression(max_iter=10000)      0.0100      l1   saga   \n",
       "5   LogisticRegression(max_iter=10000)      0.0100      l2   saga   \n",
       "6   LogisticRegression(max_iter=10000)      0.1000      l1   saga   \n",
       "7   LogisticRegression(max_iter=10000)      0.1000      l2   saga   \n",
       "8   LogisticRegression(max_iter=10000)      1.0000      l1   saga   \n",
       "9   LogisticRegression(max_iter=10000)      1.0000      l2   saga   \n",
       "10  LogisticRegression(max_iter=10000)     10.0000      l1   saga   \n",
       "11  LogisticRegression(max_iter=10000)     10.0000      l2   saga   \n",
       "12  LogisticRegression(max_iter=10000)    100.0000      l1   saga   \n",
       "13  LogisticRegression(max_iter=10000)    100.0000      l2   saga   \n",
       "14  LogisticRegression(max_iter=10000)   1000.0000      l1   saga   \n",
       "15  LogisticRegression(max_iter=10000)   1000.0000      l2   saga   \n",
       "16  LogisticRegression(max_iter=10000)  10000.0000      l1   saga   \n",
       "17  LogisticRegression(max_iter=10000)  10000.0000      l2   saga   \n",
       "18  LogisticRegression(max_iter=10000)      0.0001      l2  lbfgs   \n",
       "19  LogisticRegression(max_iter=10000)      0.0010      l2  lbfgs   \n",
       "20  LogisticRegression(max_iter=10000)      0.0100      l2  lbfgs   \n",
       "21  LogisticRegression(max_iter=10000)      0.1000      l2  lbfgs   \n",
       "22  LogisticRegression(max_iter=10000)      1.0000      l2  lbfgs   \n",
       "23  LogisticRegression(max_iter=10000)     10.0000      l2  lbfgs   \n",
       "24  LogisticRegression(max_iter=10000)    100.0000      l2  lbfgs   \n",
       "25  LogisticRegression(max_iter=10000)   1000.0000      l2  lbfgs   \n",
       "26  LogisticRegression(max_iter=10000)  10000.0000      l2  lbfgs   \n",
       "27  LogisticRegression(max_iter=10000)         NaN    none  lbfgs   \n",
       "28  LogisticRegression(max_iter=10000)         NaN    none   saga   \n",
       "29  LogisticRegression(max_iter=10000)         NaN     NaN    NaN   \n",
       "\n",
       "    score_precision  \n",
       "0          0.000000  \n",
       "1          0.704444  \n",
       "2          0.670251  \n",
       "3          0.664851  \n",
       "4          0.660407  \n",
       "5          0.659884  \n",
       "6          0.659463  \n",
       "7          0.659482  \n",
       "8          0.659382  \n",
       "9          0.659440  \n",
       "10         0.659426  \n",
       "11         0.659426  \n",
       "12         0.659441  \n",
       "13         0.659441  \n",
       "14         0.659426  \n",
       "15         0.659441  \n",
       "16         0.659441  \n",
       "17         0.659441  \n",
       "18         0.704443  \n",
       "19         0.664851  \n",
       "20         0.659884  \n",
       "21         0.659467  \n",
       "22         0.659411  \n",
       "23         0.659441  \n",
       "24         0.659441  \n",
       "25         0.659441  \n",
       "26         0.659441  \n",
       "27         0.659441  \n",
       "28         0.659412  \n",
       "29         0.659411  "
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results = pd.DataFrame( best_model.cv_results_['params'] )\n",
    "\n",
    "results['score_precision'] = best_model.cv_results_['mean_test_precision']\n",
    "cols = results.columns.to_series().str.split('__').apply(lambda x: x[-1])\n",
    "results.columns = cols\n",
    "\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "30976105",
   "metadata": {},
   "outputs": [],
   "source": [
    "roc_auc_ovr_List.append(results[1:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "ca7bc287",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'classifier': LogisticRegression(max_iter=10000),\n",
       " 'classifier__C': 100.0,\n",
       " 'classifier__penalty': 'l1',\n",
       " 'classifier__solver': 'saga'}"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_model.cv_results_['params'][ np.argmin(best_model.cv_results_['rank_test_f1_micro'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "affe1adf",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>classifier</th>\n",
       "      <th>C</th>\n",
       "      <th>penalty</th>\n",
       "      <th>solver</th>\n",
       "      <th>score_f1_micro</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.599332</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.683005</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.699895</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.700659</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0100</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.702003</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0100</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.701876</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.1000</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.702088</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.1000</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.702130</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.702116</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.702145</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>10.0000</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.702145</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>10.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.702145</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>100.0000</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.702159</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>100.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.702159</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>1000.0000</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.702145</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>1000.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.702159</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>10000.0000</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.702159</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>10000.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.702159</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.683005</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.700659</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0100</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.701876</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.1000</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.702116</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.702130</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>10.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.702159</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>100.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.702159</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>1000.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.702159</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>10000.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.702159</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>NaN</td>\n",
       "      <td>none</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.702159</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>NaN</td>\n",
       "      <td>none</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.702130</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.702130</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                            classifier           C penalty solver  \\\n",
       "0   LogisticRegression(max_iter=10000)      0.0001      l1   saga   \n",
       "1   LogisticRegression(max_iter=10000)      0.0001      l2   saga   \n",
       "2   LogisticRegression(max_iter=10000)      0.0010      l1   saga   \n",
       "3   LogisticRegression(max_iter=10000)      0.0010      l2   saga   \n",
       "4   LogisticRegression(max_iter=10000)      0.0100      l1   saga   \n",
       "5   LogisticRegression(max_iter=10000)      0.0100      l2   saga   \n",
       "6   LogisticRegression(max_iter=10000)      0.1000      l1   saga   \n",
       "7   LogisticRegression(max_iter=10000)      0.1000      l2   saga   \n",
       "8   LogisticRegression(max_iter=10000)      1.0000      l1   saga   \n",
       "9   LogisticRegression(max_iter=10000)      1.0000      l2   saga   \n",
       "10  LogisticRegression(max_iter=10000)     10.0000      l1   saga   \n",
       "11  LogisticRegression(max_iter=10000)     10.0000      l2   saga   \n",
       "12  LogisticRegression(max_iter=10000)    100.0000      l1   saga   \n",
       "13  LogisticRegression(max_iter=10000)    100.0000      l2   saga   \n",
       "14  LogisticRegression(max_iter=10000)   1000.0000      l1   saga   \n",
       "15  LogisticRegression(max_iter=10000)   1000.0000      l2   saga   \n",
       "16  LogisticRegression(max_iter=10000)  10000.0000      l1   saga   \n",
       "17  LogisticRegression(max_iter=10000)  10000.0000      l2   saga   \n",
       "18  LogisticRegression(max_iter=10000)      0.0001      l2  lbfgs   \n",
       "19  LogisticRegression(max_iter=10000)      0.0010      l2  lbfgs   \n",
       "20  LogisticRegression(max_iter=10000)      0.0100      l2  lbfgs   \n",
       "21  LogisticRegression(max_iter=10000)      0.1000      l2  lbfgs   \n",
       "22  LogisticRegression(max_iter=10000)      1.0000      l2  lbfgs   \n",
       "23  LogisticRegression(max_iter=10000)     10.0000      l2  lbfgs   \n",
       "24  LogisticRegression(max_iter=10000)    100.0000      l2  lbfgs   \n",
       "25  LogisticRegression(max_iter=10000)   1000.0000      l2  lbfgs   \n",
       "26  LogisticRegression(max_iter=10000)  10000.0000      l2  lbfgs   \n",
       "27  LogisticRegression(max_iter=10000)         NaN    none  lbfgs   \n",
       "28  LogisticRegression(max_iter=10000)         NaN    none   saga   \n",
       "29  LogisticRegression(max_iter=10000)         NaN     NaN    NaN   \n",
       "\n",
       "    score_f1_micro  \n",
       "0         0.599332  \n",
       "1         0.683005  \n",
       "2         0.699895  \n",
       "3         0.700659  \n",
       "4         0.702003  \n",
       "5         0.701876  \n",
       "6         0.702088  \n",
       "7         0.702130  \n",
       "8         0.702116  \n",
       "9         0.702145  \n",
       "10        0.702145  \n",
       "11        0.702145  \n",
       "12        0.702159  \n",
       "13        0.702159  \n",
       "14        0.702145  \n",
       "15        0.702159  \n",
       "16        0.702159  \n",
       "17        0.702159  \n",
       "18        0.683005  \n",
       "19        0.700659  \n",
       "20        0.701876  \n",
       "21        0.702116  \n",
       "22        0.702130  \n",
       "23        0.702159  \n",
       "24        0.702159  \n",
       "25        0.702159  \n",
       "26        0.702159  \n",
       "27        0.702159  \n",
       "28        0.702130  \n",
       "29        0.702130  "
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results = pd.DataFrame( best_model.cv_results_['params'] )\n",
    "\n",
    "results['score_f1_micro'] = best_model.cv_results_['mean_test_f1_micro']\n",
    "cols = results.columns.to_series().str.split('__').apply(lambda x: x[-1])\n",
    "results.columns = cols\n",
    "\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "7439a073",
   "metadata": {},
   "outputs": [],
   "source": [
    "f1_micro_List.append(results[12:13])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efd1babe",
   "metadata": {},
   "source": [
    "## Trial 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "1a0f0acc",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, Y_train, Y_test = train_test_split(X, y, test_size=0.2, shuffle=True, random_state = 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "7eb408a1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ENT\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\ENT\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\ENT\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\ENT\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\ENT\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\ENT\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\ENT\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\ENT\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\ENT\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\ENT\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\ENT\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1183: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\ENT\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1183: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\ENT\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1183: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\ENT\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1183: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\ENT\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1183: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\ENT\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1183: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\ENT\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1183: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\ENT\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1183: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\ENT\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1183: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\ENT\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1183: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\ENT\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1183: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\ENT\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1183: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\ENT\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1183: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\ENT\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1183: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\ENT\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1183: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\ENT\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1183: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\ENT\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1183: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\ENT\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1183: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\ENT\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1183: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\ENT\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1183: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Create a pipeline - RF is a stand in, we will populate the classifier part below\n",
    "pipe = Pipeline([('std', StandardScaler()),\n",
    "                 ('classifier', LogisticRegression())])\n",
    "\n",
    "# Create search space of candidate learning algorithms and their hyperparameters\n",
    "# note lbfgs can't do l1, and if you pass penalty='none' it expects no C value\n",
    "search_space = [{'classifier': [LogisticRegression(max_iter=10000)],\n",
    "                 'classifier__solver': ['saga'],\n",
    "                 'classifier__penalty': ['l1', 'l2'],\n",
    "                 'classifier__C': np.logspace(-4, 4, 9)},\n",
    "                {'classifier': [LogisticRegression(max_iter=10000)],\n",
    "                 'classifier__solver': ['lbfgs'],\n",
    "                 'classifier__penalty': ['l2'],\n",
    "                 'classifier__C': np.logspace(-4, 4, 9)},\n",
    "                {'classifier': [LogisticRegression(max_iter=10000)],\n",
    "                 'classifier__solver': ['lbfgs','saga'],\n",
    "                 'classifier__penalty': ['none']},\n",
    "                {'classifier': [LogisticRegression(max_iter=10000)]}\n",
    "                ]\n",
    "\n",
    "# Create grid search\n",
    "clf = GridSearchCV(pipe, search_space, cv=StratifiedKFold(n_splits=10),\n",
    "                   scoring=['accuracy', 'precision', 'f1_micro'], refit=False,\n",
    "                   verbose=0)\n",
    "\n",
    "# Fit grid search\n",
    "best_model = clf.fit(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "bfa6938c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'mean_fit_time': array([0.05160749, 0.09878242, 0.11398287, 0.10975924, 0.12118802,\n",
       "        0.10482383, 0.11972671, 0.1045399 , 0.12873909, 0.10793281,\n",
       "        0.12007725, 0.11292734, 0.12211237, 0.10488939, 0.12972534,\n",
       "        0.10506749, 0.12798007, 0.11813211, 0.02399919, 0.02280006,\n",
       "        0.0227169 , 0.02222023, 0.0227412 , 0.02247024, 0.02253256,\n",
       "        0.02212   , 0.02263489, 0.02248135, 0.11398149, 0.02221994]),\n",
       " 'std_fit_time': array([0.00938847, 0.00172497, 0.0063941 , 0.01249836, 0.01227626,\n",
       "        0.00604963, 0.01094269, 0.00865853, 0.01884232, 0.0108797 ,\n",
       "        0.00930199, 0.00899529, 0.01018915, 0.00699343, 0.00842855,\n",
       "        0.01053873, 0.011344  , 0.01558889, 0.00114743, 0.00113024,\n",
       "        0.00119068, 0.00074913, 0.00165372, 0.00131364, 0.00102083,\n",
       "        0.00083147, 0.00103008, 0.00092705, 0.01918112, 0.00074883]),\n",
       " 'mean_score_time': array([0.00730603, 0.0077054 , 0.00790722, 0.00765719, 0.00780787,\n",
       "        0.00840709, 0.00797496, 0.00850725, 0.00832963, 0.00820663,\n",
       "        0.00830166, 0.00800714, 0.00810735, 0.0079071 , 0.00824411,\n",
       "        0.00791371, 0.00792329, 0.00859876, 0.0079411 , 0.00781574,\n",
       "        0.00770698, 0.00756269, 0.00750785, 0.00762253, 0.00770686,\n",
       "        0.00770698, 0.00790939, 0.00755155, 0.00796151, 0.00760689]),\n",
       " 'std_score_time': array([0.00045799, 0.00045767, 0.00053917, 0.00045033, 0.00059928,\n",
       "        0.00091757, 0.00058813, 0.00067177, 0.00044903, 0.00087263,\n",
       "        0.00050759, 0.00044771, 0.00053953, 0.00030039, 0.00066288,\n",
       "        0.00054043, 0.00030939, 0.0007369 , 0.0005299 , 0.00058992,\n",
       "        0.00064078, 0.00053064, 0.00050149, 0.00050499, 0.00078171,\n",
       "        0.00045889, 0.00070426, 0.00055976, 0.00047002, 0.00049024]),\n",
       " 'param_classifier': masked_array(data=[LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000)],\n",
       "              mask=[False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'param_classifier__C': masked_array(data=[0.0001, 0.0001, 0.001, 0.001, 0.01, 0.01, 0.1, 0.1,\n",
       "                    1.0, 1.0, 10.0, 10.0, 100.0, 100.0, 1000.0, 1000.0,\n",
       "                    10000.0, 10000.0, 0.0001, 0.001, 0.01, 0.1, 1.0, 10.0,\n",
       "                    100.0, 1000.0, 10000.0, --, --, --],\n",
       "              mask=[False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False,  True,  True,  True],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'param_classifier__penalty': masked_array(data=['l1', 'l2', 'l1', 'l2', 'l1', 'l2', 'l1', 'l2', 'l1',\n",
       "                    'l2', 'l1', 'l2', 'l1', 'l2', 'l1', 'l2', 'l1', 'l2',\n",
       "                    'l2', 'l2', 'l2', 'l2', 'l2', 'l2', 'l2', 'l2', 'l2',\n",
       "                    'none', 'none', --],\n",
       "              mask=[False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False,  True],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'param_classifier__solver': masked_array(data=['saga', 'saga', 'saga', 'saga', 'saga', 'saga', 'saga',\n",
       "                    'saga', 'saga', 'saga', 'saga', 'saga', 'saga', 'saga',\n",
       "                    'saga', 'saga', 'saga', 'saga', 'lbfgs', 'lbfgs',\n",
       "                    'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs',\n",
       "                    'lbfgs', 'lbfgs', 'saga', --],\n",
       "              mask=[False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False,  True],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'params': [{'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 0.0001,\n",
       "   'classifier__penalty': 'l1',\n",
       "   'classifier__solver': 'saga'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 0.0001,\n",
       "   'classifier__penalty': 'l2',\n",
       "   'classifier__solver': 'saga'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 0.001,\n",
       "   'classifier__penalty': 'l1',\n",
       "   'classifier__solver': 'saga'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 0.001,\n",
       "   'classifier__penalty': 'l2',\n",
       "   'classifier__solver': 'saga'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 0.01,\n",
       "   'classifier__penalty': 'l1',\n",
       "   'classifier__solver': 'saga'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 0.01,\n",
       "   'classifier__penalty': 'l2',\n",
       "   'classifier__solver': 'saga'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 0.1,\n",
       "   'classifier__penalty': 'l1',\n",
       "   'classifier__solver': 'saga'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 0.1,\n",
       "   'classifier__penalty': 'l2',\n",
       "   'classifier__solver': 'saga'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 1.0,\n",
       "   'classifier__penalty': 'l1',\n",
       "   'classifier__solver': 'saga'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 1.0,\n",
       "   'classifier__penalty': 'l2',\n",
       "   'classifier__solver': 'saga'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 10.0,\n",
       "   'classifier__penalty': 'l1',\n",
       "   'classifier__solver': 'saga'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 10.0,\n",
       "   'classifier__penalty': 'l2',\n",
       "   'classifier__solver': 'saga'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 100.0,\n",
       "   'classifier__penalty': 'l1',\n",
       "   'classifier__solver': 'saga'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 100.0,\n",
       "   'classifier__penalty': 'l2',\n",
       "   'classifier__solver': 'saga'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 1000.0,\n",
       "   'classifier__penalty': 'l1',\n",
       "   'classifier__solver': 'saga'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 1000.0,\n",
       "   'classifier__penalty': 'l2',\n",
       "   'classifier__solver': 'saga'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 10000.0,\n",
       "   'classifier__penalty': 'l1',\n",
       "   'classifier__solver': 'saga'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 10000.0,\n",
       "   'classifier__penalty': 'l2',\n",
       "   'classifier__solver': 'saga'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 0.0001,\n",
       "   'classifier__penalty': 'l2',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 0.001,\n",
       "   'classifier__penalty': 'l2',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 0.01,\n",
       "   'classifier__penalty': 'l2',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 0.1,\n",
       "   'classifier__penalty': 'l2',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 1.0,\n",
       "   'classifier__penalty': 'l2',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 10.0,\n",
       "   'classifier__penalty': 'l2',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 100.0,\n",
       "   'classifier__penalty': 'l2',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 1000.0,\n",
       "   'classifier__penalty': 'l2',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 10000.0,\n",
       "   'classifier__penalty': 'l2',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__penalty': 'none',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__penalty': 'none',\n",
       "   'classifier__solver': 'saga'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000)}],\n",
       " 'split0_test_accuracy': array([0.60014144, 0.68585573, 0.70848656, 0.70693069, 0.70806223,\n",
       "        0.70848656, 0.70891089, 0.70891089, 0.70891089, 0.70891089,\n",
       "        0.70891089, 0.70891089, 0.70891089, 0.70891089, 0.70891089,\n",
       "        0.70891089, 0.70891089, 0.70891089, 0.68585573, 0.70693069,\n",
       "        0.70848656, 0.70891089, 0.70891089, 0.70891089, 0.70891089,\n",
       "        0.70891089, 0.70891089, 0.70891089, 0.70891089, 0.70891089]),\n",
       " 'split1_test_accuracy': array([0.60014144, 0.68981612, 0.70594059, 0.70721358, 0.70820368,\n",
       "        0.70848656, 0.70806223, 0.70834512, 0.70834512, 0.70848656,\n",
       "        0.70834512, 0.70834512, 0.70834512, 0.70862801, 0.70834512,\n",
       "        0.70834512, 0.70848656, 0.70834512, 0.68981612, 0.70721358,\n",
       "        0.70848656, 0.70834512, 0.70834512, 0.70834512, 0.70834512,\n",
       "        0.70834512, 0.70834512, 0.70834512, 0.70834512, 0.70834512]),\n",
       " 'split2_test_accuracy': array([0.60022634, 0.68255765, 0.70264535, 0.70236243, 0.70405998,\n",
       "        0.70377705, 0.70377705, 0.70363559, 0.70377705, 0.70377705,\n",
       "        0.70377705, 0.70377705, 0.70377705, 0.70377705, 0.70377705,\n",
       "        0.70377705, 0.70377705, 0.70377705, 0.68255765, 0.70236243,\n",
       "        0.70377705, 0.70363559, 0.70377705, 0.70377705, 0.70377705,\n",
       "        0.70377705, 0.70377705, 0.70377705, 0.70377705, 0.70377705]),\n",
       " 'split3_test_accuracy': array([0.60022634, 0.69288442, 0.70731362, 0.70547461, 0.70787947,\n",
       "        0.70773801, 0.70773801, 0.70773801, 0.70787947, 0.70787947,\n",
       "        0.70787947, 0.70787947, 0.70787947, 0.70787947, 0.70787947,\n",
       "        0.70787947, 0.70787947, 0.70787947, 0.69288442, 0.70547461,\n",
       "        0.70773801, 0.70787947, 0.70787947, 0.70787947, 0.70787947,\n",
       "        0.70787947, 0.70787947, 0.70787947, 0.70787947, 0.70787947]),\n",
       " 'split4_test_accuracy': array([0.60022634, 0.6831235 , 0.70024049, 0.70193804, 0.70434291,\n",
       "        0.70363559, 0.7032112 , 0.70335267, 0.70335267, 0.70349413,\n",
       "        0.70349413, 0.70349413, 0.70349413, 0.70335267, 0.70349413,\n",
       "        0.70349413, 0.70335267, 0.70349413, 0.6831235 , 0.70179658,\n",
       "        0.70363559, 0.70335267, 0.70349413, 0.70349413, 0.70349413,\n",
       "        0.70349413, 0.70349413, 0.70349413, 0.70349413, 0.70349413]),\n",
       " 'split5_test_accuracy': array([0.60022634, 0.68666007, 0.70448437, 0.70533314, 0.70858679,\n",
       "        0.70872825, 0.7092941 , 0.7092941 , 0.70915264, 0.70901118,\n",
       "        0.70901118, 0.70901118, 0.70901118, 0.70901118, 0.70901118,\n",
       "        0.70901118, 0.70901118, 0.70901118, 0.68666007, 0.70533314,\n",
       "        0.70872825, 0.7092941 , 0.70901118, 0.70901118, 0.70901118,\n",
       "        0.70901118, 0.70901118, 0.70901118, 0.70901118, 0.70901118]),\n",
       " 'split6_test_accuracy': array([0.60022634, 0.68354789, 0.6948649 , 0.6926015 , 0.6948649 ,\n",
       "        0.69514783, 0.69458198, 0.69458198, 0.69415759, 0.69415759,\n",
       "        0.69415759, 0.69415759, 0.69415759, 0.69415759, 0.69415759,\n",
       "        0.69415759, 0.69415759, 0.69415759, 0.68354789, 0.6926015 ,\n",
       "        0.69500637, 0.69444051, 0.69415759, 0.69415759, 0.69415759,\n",
       "        0.69415759, 0.69415759, 0.69415759, 0.69415759, 0.69415759]),\n",
       " 'split7_test_accuracy': array([0.60008488, 0.68156741, 0.6937332 , 0.69500637, 0.69585514,\n",
       "        0.69670392, 0.69627953, 0.69613807, 0.69627953, 0.69627953,\n",
       "        0.69627953, 0.69627953, 0.69627953, 0.69627953, 0.69627953,\n",
       "        0.69627953, 0.69627953, 0.69627953, 0.68156741, 0.69514783,\n",
       "        0.69670392, 0.69613807, 0.69627953, 0.69627953, 0.69627953,\n",
       "        0.69627953, 0.69627953, 0.69627953, 0.69627953, 0.69627953]),\n",
       " 'split8_test_accuracy': array([0.60008488, 0.68086009, 0.69769416, 0.6998161 , 0.69896732,\n",
       "        0.69882586, 0.69826001, 0.69811855, 0.69811855, 0.69811855,\n",
       "        0.69811855, 0.69811855, 0.69811855, 0.69811855, 0.69811855,\n",
       "        0.69811855, 0.69811855, 0.69811855, 0.68086009, 0.6998161 ,\n",
       "        0.69882586, 0.69811855, 0.69811855, 0.69811855, 0.69811855,\n",
       "        0.69811855, 0.69811855, 0.69811855, 0.69811855, 0.69811855]),\n",
       " 'split9_test_accuracy': array([0.60008488, 0.68835762, 0.70391852, 0.70519168, 0.70943556,\n",
       "        0.70915264, 0.70957703, 0.70957703, 0.70943556, 0.70943556,\n",
       "        0.70943556, 0.70943556, 0.70943556, 0.70943556, 0.70943556,\n",
       "        0.70943556, 0.70943556, 0.70943556, 0.68835762, 0.70519168,\n",
       "        0.70915264, 0.70957703, 0.70943556, 0.70943556, 0.70943556,\n",
       "        0.70943556, 0.70943556, 0.70943556, 0.70943556, 0.70943556]),\n",
       " 'mean_test_accuracy': array([0.60016692, 0.68552305, 0.70193218, 0.70218681, 0.7040258 ,\n",
       "        0.70406823, 0.7039692 , 0.7039692 , 0.70394091, 0.70395505,\n",
       "        0.70394091, 0.70394091, 0.70394091, 0.70395505, 0.70394091,\n",
       "        0.70394091, 0.70394091, 0.70394091, 0.68552305, 0.70218681,\n",
       "        0.70405408, 0.7039692 , 0.70394091, 0.70394091, 0.70394091,\n",
       "        0.70394091, 0.70394091, 0.70394091, 0.70394091, 0.70394091]),\n",
       " 'std_test_accuracy': array([6.25659081e-05, 3.71351060e-03, 4.86772410e-03, 4.76089357e-03,\n",
       "        5.24440675e-03, 5.11506279e-03, 5.43724910e-03, 5.49311709e-03,\n",
       "        5.52852872e-03, 5.52547967e-03, 5.51403085e-03, 5.51403085e-03,\n",
       "        5.51403085e-03, 5.53860613e-03, 5.51403085e-03, 5.51403085e-03,\n",
       "        5.52682253e-03, 5.51403085e-03, 3.71351060e-03, 4.74067455e-03,\n",
       "        5.13984914e-03, 5.52725569e-03, 5.51403085e-03, 5.51403085e-03,\n",
       "        5.51403085e-03, 5.51403085e-03, 5.51403085e-03, 5.51403085e-03,\n",
       "        5.51403085e-03, 5.51403085e-03]),\n",
       " 'rank_test_accuracy': array([30, 28, 27, 25,  3,  1,  4,  5,  9,  7,  9,  9,  9,  8,  9,  9, 24,\n",
       "         9, 28, 25,  2,  6,  9,  9,  9,  9,  9,  9,  9,  9]),\n",
       " 'split0_test_precision': array([0.        , 0.7127809 , 0.68307839, 0.67452612, 0.66902969,\n",
       "        0.66931919, 0.66945791, 0.66930867, 0.66930867, 0.66930867,\n",
       "        0.66930867, 0.66930867, 0.66930867, 0.66930867, 0.66930867,\n",
       "        0.66930867, 0.66930867, 0.66930867, 0.7127809 , 0.67452612,\n",
       "        0.66931919, 0.66930867, 0.66930867, 0.66930867, 0.66930867,\n",
       "        0.66930867, 0.66930867, 0.66930867, 0.66930867, 0.66930867]),\n",
       " 'split1_test_precision': array([0.        , 0.72323944, 0.68085106, 0.67811765, 0.67222723,\n",
       "        0.67252252, 0.67161493, 0.67206478, 0.67191011, 0.67221223,\n",
       "        0.67191011, 0.67191011, 0.67191011, 0.67235955, 0.67191011,\n",
       "        0.67191011, 0.6720575 , 0.67191011, 0.72323944, 0.67811765,\n",
       "        0.67252252, 0.67206478, 0.67191011, 0.67191011, 0.67191011,\n",
       "        0.67191011, 0.67191011, 0.67191011, 0.67191011, 0.67191011]),\n",
       " 'split2_test_precision': array([0.        , 0.70321229, 0.67188984, 0.66697502, 0.6622458 ,\n",
       "        0.66180371, 0.66109155, 0.6608007 , 0.66094987, 0.66094987,\n",
       "        0.66094987, 0.66094987, 0.66094987, 0.66094987, 0.66094987,\n",
       "        0.66094987, 0.66094987, 0.66094987, 0.70321229, 0.66697502,\n",
       "        0.66180371, 0.6608007 , 0.66094987, 0.66094987, 0.66094987,\n",
       "        0.66094987, 0.66094987, 0.66094987, 0.66094987, 0.66094987]),\n",
       " 'split3_test_precision': array([0.        , 0.72633034, 0.6808409 , 0.67190388, 0.66888593,\n",
       "        0.66829052, 0.66769638, 0.66769638, 0.66784296, 0.66784296,\n",
       "        0.66784296, 0.66784296, 0.66784296, 0.66784296, 0.66784296,\n",
       "        0.66784296, 0.66784296, 0.66784296, 0.72633034, 0.67190388,\n",
       "        0.66829052, 0.66784296, 0.66784296, 0.66784296, 0.66784296,\n",
       "        0.66784296, 0.66784296, 0.66784296, 0.66784296, 0.66784296]),\n",
       " 'split4_test_precision': array([0.        , 0.70151307, 0.6713524 , 0.66498394, 0.6622575 ,\n",
       "        0.66094232, 0.65964912, 0.65993857, 0.65979833, 0.66008772,\n",
       "        0.66008772, 0.66008772, 0.66008772, 0.65979833, 0.66008772,\n",
       "        0.66008772, 0.65979833, 0.66008772, 0.70151307, 0.6646789 ,\n",
       "        0.66094232, 0.65993857, 0.66008772, 0.66008772, 0.66008772,\n",
       "        0.66008772, 0.66008772, 0.66008772, 0.66008772, 0.66008772]),\n",
       " 'split5_test_precision': array([0.        , 0.71289199, 0.67897037, 0.67222995, 0.67052538,\n",
       "        0.67036873, 0.67049978, 0.67049978, 0.67020336, 0.6699072 ,\n",
       "        0.6699072 , 0.6699072 , 0.6699072 , 0.6699072 , 0.6699072 ,\n",
       "        0.6699072 , 0.6699072 , 0.6699072 , 0.71289199, 0.67222995,\n",
       "        0.67036873, 0.67049978, 0.6699072 , 0.6699072 , 0.6699072 ,\n",
       "        0.6699072 , 0.6699072 , 0.6699072 , 0.6699072 , 0.6699072 ]),\n",
       " 'split6_test_precision': array([0.        , 0.69568106, 0.65594406, 0.64660979, 0.64374731,\n",
       "        0.64392964, 0.64233888, 0.64233888, 0.64151748, 0.64151748,\n",
       "        0.64151748, 0.64151748, 0.64151748, 0.64151748, 0.64151748,\n",
       "        0.64151748, 0.64151748, 0.64151748, 0.69568106, 0.64660979,\n",
       "        0.64377682, 0.64206485, 0.64151748, 0.64151748, 0.64151748,\n",
       "        0.64151748, 0.64151748, 0.64151748, 0.64151748, 0.64151748]),\n",
       " 'split7_test_precision': array([0.        , 0.70779221, 0.66321499, 0.65892942, 0.65365411,\n",
       "        0.654455  , 0.65315315, 0.65285907, 0.6530153 , 0.6530153 ,\n",
       "        0.6530153 , 0.6530153 , 0.6530153 , 0.6530153 , 0.6530153 ,\n",
       "        0.6530153 , 0.6530153 , 0.6530153 , 0.70779221, 0.65909091,\n",
       "        0.654455  , 0.65285907, 0.6530153 , 0.6530153 , 0.6530153 ,\n",
       "        0.6530153 , 0.6530153 , 0.6530153 , 0.6530153 , 0.6530153 ]),\n",
       " 'split8_test_precision': array([0.        , 0.69461486, 0.66554702, 0.66281755, 0.6551265 ,\n",
       "        0.65456156, 0.65246046, 0.65230769, 0.65230769, 0.65230769,\n",
       "        0.65230769, 0.65230769, 0.65230769, 0.65230769, 0.65230769,\n",
       "        0.65230769, 0.65230769, 0.65230769, 0.69461486, 0.66281755,\n",
       "        0.65456156, 0.65230769, 0.65230769, 0.65230769, 0.65230769,\n",
       "        0.65230769, 0.65230769, 0.65230769, 0.65230769, 0.65230769]),\n",
       " 'split9_test_precision': array([0.        , 0.71340629, 0.67867575, 0.67271037, 0.67293065,\n",
       "        0.67186803, 0.672     , 0.672     , 0.67170147, 0.67170147,\n",
       "        0.67170147, 0.67170147, 0.67170147, 0.67170147, 0.67170147,\n",
       "        0.67170147, 0.67170147, 0.67170147, 0.71340629, 0.67271037,\n",
       "        0.67186803, 0.672     , 0.67170147, 0.67170147, 0.67170147,\n",
       "        0.67170147, 0.67170147, 0.67170147, 0.67170147, 0.67170147]),\n",
       " 'mean_test_precision': array([0.        , 0.70914624, 0.67303648, 0.66698037, 0.66306301,\n",
       "        0.66280612, 0.66199622, 0.66198145, 0.66185552, 0.66188506,\n",
       "        0.66185485, 0.66185485, 0.66185485, 0.66187085, 0.66185485,\n",
       "        0.66185485, 0.66184065, 0.66185485, 0.70914624, 0.66696601,\n",
       "        0.66279084, 0.66196871, 0.66185485, 0.66185485, 0.66185485,\n",
       "        0.66185485, 0.66185485, 0.66185485, 0.66185485, 0.66185485]),\n",
       " 'std_test_precision': array([0.        , 0.01013451, 0.00859468, 0.00875615, 0.00912401,\n",
       "        0.00897069, 0.00956527, 0.00963926, 0.00973202, 0.00973301,\n",
       "        0.00970133, 0.00970133, 0.00970133, 0.0097545 , 0.00970133,\n",
       "        0.00970133, 0.00972239, 0.00970133, 0.01013451, 0.00874893,\n",
       "        0.0090029 , 0.00970406, 0.00970133, 0.00970133, 0.00970133,\n",
       "        0.00970133, 0.00970133, 0.00970133, 0.00970133, 0.00970133]),\n",
       " 'rank_test_precision': array([30,  1,  3,  4,  6,  7,  9, 10, 14, 12, 15, 15, 15, 13, 15, 15, 29,\n",
       "        15,  1,  5,  8, 11, 15, 15, 15, 15, 15, 15, 15, 15]),\n",
       " 'split0_test_f1_micro': array([0.60014144, 0.68585573, 0.70848656, 0.70693069, 0.70806223,\n",
       "        0.70848656, 0.70891089, 0.70891089, 0.70891089, 0.70891089,\n",
       "        0.70891089, 0.70891089, 0.70891089, 0.70891089, 0.70891089,\n",
       "        0.70891089, 0.70891089, 0.70891089, 0.68585573, 0.70693069,\n",
       "        0.70848656, 0.70891089, 0.70891089, 0.70891089, 0.70891089,\n",
       "        0.70891089, 0.70891089, 0.70891089, 0.70891089, 0.70891089]),\n",
       " 'split1_test_f1_micro': array([0.60014144, 0.68981612, 0.70594059, 0.70721358, 0.70820368,\n",
       "        0.70848656, 0.70806223, 0.70834512, 0.70834512, 0.70848656,\n",
       "        0.70834512, 0.70834512, 0.70834512, 0.70862801, 0.70834512,\n",
       "        0.70834512, 0.70848656, 0.70834512, 0.68981612, 0.70721358,\n",
       "        0.70848656, 0.70834512, 0.70834512, 0.70834512, 0.70834512,\n",
       "        0.70834512, 0.70834512, 0.70834512, 0.70834512, 0.70834512]),\n",
       " 'split2_test_f1_micro': array([0.60022634, 0.68255765, 0.70264535, 0.70236243, 0.70405998,\n",
       "        0.70377705, 0.70377705, 0.70363559, 0.70377705, 0.70377705,\n",
       "        0.70377705, 0.70377705, 0.70377705, 0.70377705, 0.70377705,\n",
       "        0.70377705, 0.70377705, 0.70377705, 0.68255765, 0.70236243,\n",
       "        0.70377705, 0.70363559, 0.70377705, 0.70377705, 0.70377705,\n",
       "        0.70377705, 0.70377705, 0.70377705, 0.70377705, 0.70377705]),\n",
       " 'split3_test_f1_micro': array([0.60022634, 0.69288442, 0.70731362, 0.70547461, 0.70787947,\n",
       "        0.70773801, 0.70773801, 0.70773801, 0.70787947, 0.70787947,\n",
       "        0.70787947, 0.70787947, 0.70787947, 0.70787947, 0.70787947,\n",
       "        0.70787947, 0.70787947, 0.70787947, 0.69288442, 0.70547461,\n",
       "        0.70773801, 0.70787947, 0.70787947, 0.70787947, 0.70787947,\n",
       "        0.70787947, 0.70787947, 0.70787947, 0.70787947, 0.70787947]),\n",
       " 'split4_test_f1_micro': array([0.60022634, 0.6831235 , 0.70024049, 0.70193804, 0.70434291,\n",
       "        0.70363559, 0.7032112 , 0.70335267, 0.70335267, 0.70349413,\n",
       "        0.70349413, 0.70349413, 0.70349413, 0.70335267, 0.70349413,\n",
       "        0.70349413, 0.70335267, 0.70349413, 0.6831235 , 0.70179658,\n",
       "        0.70363559, 0.70335267, 0.70349413, 0.70349413, 0.70349413,\n",
       "        0.70349413, 0.70349413, 0.70349413, 0.70349413, 0.70349413]),\n",
       " 'split5_test_f1_micro': array([0.60022634, 0.68666007, 0.70448437, 0.70533314, 0.70858679,\n",
       "        0.70872825, 0.7092941 , 0.7092941 , 0.70915264, 0.70901118,\n",
       "        0.70901118, 0.70901118, 0.70901118, 0.70901118, 0.70901118,\n",
       "        0.70901118, 0.70901118, 0.70901118, 0.68666007, 0.70533314,\n",
       "        0.70872825, 0.7092941 , 0.70901118, 0.70901118, 0.70901118,\n",
       "        0.70901118, 0.70901118, 0.70901118, 0.70901118, 0.70901118]),\n",
       " 'split6_test_f1_micro': array([0.60022634, 0.68354789, 0.6948649 , 0.6926015 , 0.6948649 ,\n",
       "        0.69514783, 0.69458198, 0.69458198, 0.69415759, 0.69415759,\n",
       "        0.69415759, 0.69415759, 0.69415759, 0.69415759, 0.69415759,\n",
       "        0.69415759, 0.69415759, 0.69415759, 0.68354789, 0.6926015 ,\n",
       "        0.69500637, 0.69444051, 0.69415759, 0.69415759, 0.69415759,\n",
       "        0.69415759, 0.69415759, 0.69415759, 0.69415759, 0.69415759]),\n",
       " 'split7_test_f1_micro': array([0.60008488, 0.68156741, 0.6937332 , 0.69500637, 0.69585514,\n",
       "        0.69670392, 0.69627953, 0.69613807, 0.69627953, 0.69627953,\n",
       "        0.69627953, 0.69627953, 0.69627953, 0.69627953, 0.69627953,\n",
       "        0.69627953, 0.69627953, 0.69627953, 0.68156741, 0.69514783,\n",
       "        0.69670392, 0.69613807, 0.69627953, 0.69627953, 0.69627953,\n",
       "        0.69627953, 0.69627953, 0.69627953, 0.69627953, 0.69627953]),\n",
       " 'split8_test_f1_micro': array([0.60008488, 0.68086009, 0.69769416, 0.6998161 , 0.69896732,\n",
       "        0.69882586, 0.69826001, 0.69811855, 0.69811855, 0.69811855,\n",
       "        0.69811855, 0.69811855, 0.69811855, 0.69811855, 0.69811855,\n",
       "        0.69811855, 0.69811855, 0.69811855, 0.68086009, 0.6998161 ,\n",
       "        0.69882586, 0.69811855, 0.69811855, 0.69811855, 0.69811855,\n",
       "        0.69811855, 0.69811855, 0.69811855, 0.69811855, 0.69811855]),\n",
       " 'split9_test_f1_micro': array([0.60008488, 0.68835762, 0.70391852, 0.70519168, 0.70943556,\n",
       "        0.70915264, 0.70957703, 0.70957703, 0.70943556, 0.70943556,\n",
       "        0.70943556, 0.70943556, 0.70943556, 0.70943556, 0.70943556,\n",
       "        0.70943556, 0.70943556, 0.70943556, 0.68835762, 0.70519168,\n",
       "        0.70915264, 0.70957703, 0.70943556, 0.70943556, 0.70943556,\n",
       "        0.70943556, 0.70943556, 0.70943556, 0.70943556, 0.70943556]),\n",
       " 'mean_test_f1_micro': array([0.60016692, 0.68552305, 0.70193218, 0.70218681, 0.7040258 ,\n",
       "        0.70406823, 0.7039692 , 0.7039692 , 0.70394091, 0.70395505,\n",
       "        0.70394091, 0.70394091, 0.70394091, 0.70395505, 0.70394091,\n",
       "        0.70394091, 0.70394091, 0.70394091, 0.68552305, 0.70218681,\n",
       "        0.70405408, 0.7039692 , 0.70394091, 0.70394091, 0.70394091,\n",
       "        0.70394091, 0.70394091, 0.70394091, 0.70394091, 0.70394091]),\n",
       " 'std_test_f1_micro': array([6.25659081e-05, 3.71351060e-03, 4.86772410e-03, 4.76089357e-03,\n",
       "        5.24440675e-03, 5.11506279e-03, 5.43724910e-03, 5.49311709e-03,\n",
       "        5.52852872e-03, 5.52547967e-03, 5.51403085e-03, 5.51403085e-03,\n",
       "        5.51403085e-03, 5.53860613e-03, 5.51403085e-03, 5.51403085e-03,\n",
       "        5.52682253e-03, 5.51403085e-03, 3.71351060e-03, 4.74067455e-03,\n",
       "        5.13984914e-03, 5.52725569e-03, 5.51403085e-03, 5.51403085e-03,\n",
       "        5.51403085e-03, 5.51403085e-03, 5.51403085e-03, 5.51403085e-03,\n",
       "        5.51403085e-03, 5.51403085e-03]),\n",
       " 'rank_test_f1_micro': array([30, 28, 27, 25,  3,  1,  4,  5,  9,  7,  9,  9,  9,  8,  9,  9, 24,\n",
       "         9, 28, 25,  2,  5,  9,  9,  9,  9,  9,  9,  9,  9])}"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_model.cv_results_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "d443f4d3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([30, 28, 27, 25,  3,  1,  4,  5,  9,  7,  9,  9,  9,  8,  9,  9, 24,\n",
       "        9, 28, 25,  2,  6,  9,  9,  9,  9,  9,  9,  9,  9])"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_model.cv_results_['rank_test_accuracy']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "4f10e207",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'classifier': LogisticRegression(max_iter=10000),\n",
       " 'classifier__C': 0.0001,\n",
       " 'classifier__penalty': 'l1',\n",
       " 'classifier__solver': 'saga'}"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_model.cv_results_['params'][ np.argmin(best_model.cv_results_['mean_test_accuracy'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "51791136",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>classifier</th>\n",
       "      <th>C</th>\n",
       "      <th>penalty</th>\n",
       "      <th>solver</th>\n",
       "      <th>score_acc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.600167</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.685523</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.701932</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.702187</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0100</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.704026</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0100</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.704068</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.1000</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.703969</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.1000</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.703969</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.703941</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.703955</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>10.0000</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.703941</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>10.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.703941</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>100.0000</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.703941</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>100.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.703955</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>1000.0000</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.703941</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>1000.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.703941</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>10000.0000</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.703941</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>10000.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.703941</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.685523</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.702187</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0100</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.704054</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.1000</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.703969</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.703941</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>10.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.703941</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>100.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.703941</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>1000.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.703941</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>10000.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.703941</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>NaN</td>\n",
       "      <td>none</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.703941</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>NaN</td>\n",
       "      <td>none</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.703941</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.703941</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                            classifier           C penalty solver  score_acc\n",
       "0   LogisticRegression(max_iter=10000)      0.0001      l1   saga   0.600167\n",
       "1   LogisticRegression(max_iter=10000)      0.0001      l2   saga   0.685523\n",
       "2   LogisticRegression(max_iter=10000)      0.0010      l1   saga   0.701932\n",
       "3   LogisticRegression(max_iter=10000)      0.0010      l2   saga   0.702187\n",
       "4   LogisticRegression(max_iter=10000)      0.0100      l1   saga   0.704026\n",
       "5   LogisticRegression(max_iter=10000)      0.0100      l2   saga   0.704068\n",
       "6   LogisticRegression(max_iter=10000)      0.1000      l1   saga   0.703969\n",
       "7   LogisticRegression(max_iter=10000)      0.1000      l2   saga   0.703969\n",
       "8   LogisticRegression(max_iter=10000)      1.0000      l1   saga   0.703941\n",
       "9   LogisticRegression(max_iter=10000)      1.0000      l2   saga   0.703955\n",
       "10  LogisticRegression(max_iter=10000)     10.0000      l1   saga   0.703941\n",
       "11  LogisticRegression(max_iter=10000)     10.0000      l2   saga   0.703941\n",
       "12  LogisticRegression(max_iter=10000)    100.0000      l1   saga   0.703941\n",
       "13  LogisticRegression(max_iter=10000)    100.0000      l2   saga   0.703955\n",
       "14  LogisticRegression(max_iter=10000)   1000.0000      l1   saga   0.703941\n",
       "15  LogisticRegression(max_iter=10000)   1000.0000      l2   saga   0.703941\n",
       "16  LogisticRegression(max_iter=10000)  10000.0000      l1   saga   0.703941\n",
       "17  LogisticRegression(max_iter=10000)  10000.0000      l2   saga   0.703941\n",
       "18  LogisticRegression(max_iter=10000)      0.0001      l2  lbfgs   0.685523\n",
       "19  LogisticRegression(max_iter=10000)      0.0010      l2  lbfgs   0.702187\n",
       "20  LogisticRegression(max_iter=10000)      0.0100      l2  lbfgs   0.704054\n",
       "21  LogisticRegression(max_iter=10000)      0.1000      l2  lbfgs   0.703969\n",
       "22  LogisticRegression(max_iter=10000)      1.0000      l2  lbfgs   0.703941\n",
       "23  LogisticRegression(max_iter=10000)     10.0000      l2  lbfgs   0.703941\n",
       "24  LogisticRegression(max_iter=10000)    100.0000      l2  lbfgs   0.703941\n",
       "25  LogisticRegression(max_iter=10000)   1000.0000      l2  lbfgs   0.703941\n",
       "26  LogisticRegression(max_iter=10000)  10000.0000      l2  lbfgs   0.703941\n",
       "27  LogisticRegression(max_iter=10000)         NaN    none  lbfgs   0.703941\n",
       "28  LogisticRegression(max_iter=10000)         NaN    none   saga   0.703941\n",
       "29  LogisticRegression(max_iter=10000)         NaN     NaN    NaN   0.703941"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results = pd.DataFrame( best_model.cv_results_['params'] )\n",
    "\n",
    "results['score_acc'] = best_model.cv_results_['mean_test_accuracy']\n",
    "cols = results.columns.to_series().str.split('__').apply(lambda x: x[-1])\n",
    "results.columns = cols\n",
    "\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "153b94ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "acc_list.append(results[0:1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "a7cbb487",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'classifier': LogisticRegression(max_iter=10000),\n",
       " 'classifier__C': 0.0001,\n",
       " 'classifier__penalty': 'l2',\n",
       " 'classifier__solver': 'saga'}"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_model.cv_results_['params'][ np.argmin(best_model.cv_results_['rank_test_precision'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "155a06f9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>classifier</th>\n",
       "      <th>C</th>\n",
       "      <th>penalty</th>\n",
       "      <th>solver</th>\n",
       "      <th>score_precision</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.709146</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.673036</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.666980</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0100</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.663063</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0100</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.662806</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.1000</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.661996</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.1000</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.661981</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.661856</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.661885</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>10.0000</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.661855</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>10.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.661855</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>100.0000</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.661855</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>100.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.661871</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>1000.0000</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.661855</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>1000.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.661855</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>10000.0000</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.661841</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>10000.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.661855</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.709146</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.666966</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0100</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.662791</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.1000</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.661969</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.661855</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>10.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.661855</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>100.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.661855</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>1000.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.661855</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>10000.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.661855</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>NaN</td>\n",
       "      <td>none</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.661855</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>NaN</td>\n",
       "      <td>none</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.661855</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.661855</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                            classifier           C penalty solver  \\\n",
       "0   LogisticRegression(max_iter=10000)      0.0001      l1   saga   \n",
       "1   LogisticRegression(max_iter=10000)      0.0001      l2   saga   \n",
       "2   LogisticRegression(max_iter=10000)      0.0010      l1   saga   \n",
       "3   LogisticRegression(max_iter=10000)      0.0010      l2   saga   \n",
       "4   LogisticRegression(max_iter=10000)      0.0100      l1   saga   \n",
       "5   LogisticRegression(max_iter=10000)      0.0100      l2   saga   \n",
       "6   LogisticRegression(max_iter=10000)      0.1000      l1   saga   \n",
       "7   LogisticRegression(max_iter=10000)      0.1000      l2   saga   \n",
       "8   LogisticRegression(max_iter=10000)      1.0000      l1   saga   \n",
       "9   LogisticRegression(max_iter=10000)      1.0000      l2   saga   \n",
       "10  LogisticRegression(max_iter=10000)     10.0000      l1   saga   \n",
       "11  LogisticRegression(max_iter=10000)     10.0000      l2   saga   \n",
       "12  LogisticRegression(max_iter=10000)    100.0000      l1   saga   \n",
       "13  LogisticRegression(max_iter=10000)    100.0000      l2   saga   \n",
       "14  LogisticRegression(max_iter=10000)   1000.0000      l1   saga   \n",
       "15  LogisticRegression(max_iter=10000)   1000.0000      l2   saga   \n",
       "16  LogisticRegression(max_iter=10000)  10000.0000      l1   saga   \n",
       "17  LogisticRegression(max_iter=10000)  10000.0000      l2   saga   \n",
       "18  LogisticRegression(max_iter=10000)      0.0001      l2  lbfgs   \n",
       "19  LogisticRegression(max_iter=10000)      0.0010      l2  lbfgs   \n",
       "20  LogisticRegression(max_iter=10000)      0.0100      l2  lbfgs   \n",
       "21  LogisticRegression(max_iter=10000)      0.1000      l2  lbfgs   \n",
       "22  LogisticRegression(max_iter=10000)      1.0000      l2  lbfgs   \n",
       "23  LogisticRegression(max_iter=10000)     10.0000      l2  lbfgs   \n",
       "24  LogisticRegression(max_iter=10000)    100.0000      l2  lbfgs   \n",
       "25  LogisticRegression(max_iter=10000)   1000.0000      l2  lbfgs   \n",
       "26  LogisticRegression(max_iter=10000)  10000.0000      l2  lbfgs   \n",
       "27  LogisticRegression(max_iter=10000)         NaN    none  lbfgs   \n",
       "28  LogisticRegression(max_iter=10000)         NaN    none   saga   \n",
       "29  LogisticRegression(max_iter=10000)         NaN     NaN    NaN   \n",
       "\n",
       "    score_precision  \n",
       "0          0.000000  \n",
       "1          0.709146  \n",
       "2          0.673036  \n",
       "3          0.666980  \n",
       "4          0.663063  \n",
       "5          0.662806  \n",
       "6          0.661996  \n",
       "7          0.661981  \n",
       "8          0.661856  \n",
       "9          0.661885  \n",
       "10         0.661855  \n",
       "11         0.661855  \n",
       "12         0.661855  \n",
       "13         0.661871  \n",
       "14         0.661855  \n",
       "15         0.661855  \n",
       "16         0.661841  \n",
       "17         0.661855  \n",
       "18         0.709146  \n",
       "19         0.666966  \n",
       "20         0.662791  \n",
       "21         0.661969  \n",
       "22         0.661855  \n",
       "23         0.661855  \n",
       "24         0.661855  \n",
       "25         0.661855  \n",
       "26         0.661855  \n",
       "27         0.661855  \n",
       "28         0.661855  \n",
       "29         0.661855  "
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results = pd.DataFrame( best_model.cv_results_['params'] )\n",
    "\n",
    "results['score_precision'] = best_model.cv_results_['mean_test_precision']\n",
    "cols = results.columns.to_series().str.split('__').apply(lambda x: x[-1])\n",
    "results.columns = cols\n",
    "\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "a4de3306",
   "metadata": {},
   "outputs": [],
   "source": [
    "roc_auc_ovr_List.append(results[1:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "58513be7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'classifier': LogisticRegression(max_iter=10000),\n",
       " 'classifier__C': 0.01,\n",
       " 'classifier__penalty': 'l2',\n",
       " 'classifier__solver': 'saga'}"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_model.cv_results_['params'][ np.argmin(best_model.cv_results_['rank_test_f1_micro'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "6f4588d3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>classifier</th>\n",
       "      <th>C</th>\n",
       "      <th>penalty</th>\n",
       "      <th>solver</th>\n",
       "      <th>score_f1_micro</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.600167</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.685523</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.701932</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.702187</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0100</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.704026</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0100</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.704068</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.1000</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.703969</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.1000</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.703969</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.703941</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.703955</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>10.0000</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.703941</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>10.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.703941</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>100.0000</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.703941</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>100.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.703955</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>1000.0000</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.703941</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>1000.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.703941</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>10000.0000</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.703941</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>10000.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.703941</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.685523</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.702187</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0100</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.704054</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.1000</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.703969</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.703941</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>10.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.703941</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>100.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.703941</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>1000.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.703941</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>10000.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.703941</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>NaN</td>\n",
       "      <td>none</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.703941</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>NaN</td>\n",
       "      <td>none</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.703941</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.703941</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                            classifier           C penalty solver  \\\n",
       "0   LogisticRegression(max_iter=10000)      0.0001      l1   saga   \n",
       "1   LogisticRegression(max_iter=10000)      0.0001      l2   saga   \n",
       "2   LogisticRegression(max_iter=10000)      0.0010      l1   saga   \n",
       "3   LogisticRegression(max_iter=10000)      0.0010      l2   saga   \n",
       "4   LogisticRegression(max_iter=10000)      0.0100      l1   saga   \n",
       "5   LogisticRegression(max_iter=10000)      0.0100      l2   saga   \n",
       "6   LogisticRegression(max_iter=10000)      0.1000      l1   saga   \n",
       "7   LogisticRegression(max_iter=10000)      0.1000      l2   saga   \n",
       "8   LogisticRegression(max_iter=10000)      1.0000      l1   saga   \n",
       "9   LogisticRegression(max_iter=10000)      1.0000      l2   saga   \n",
       "10  LogisticRegression(max_iter=10000)     10.0000      l1   saga   \n",
       "11  LogisticRegression(max_iter=10000)     10.0000      l2   saga   \n",
       "12  LogisticRegression(max_iter=10000)    100.0000      l1   saga   \n",
       "13  LogisticRegression(max_iter=10000)    100.0000      l2   saga   \n",
       "14  LogisticRegression(max_iter=10000)   1000.0000      l1   saga   \n",
       "15  LogisticRegression(max_iter=10000)   1000.0000      l2   saga   \n",
       "16  LogisticRegression(max_iter=10000)  10000.0000      l1   saga   \n",
       "17  LogisticRegression(max_iter=10000)  10000.0000      l2   saga   \n",
       "18  LogisticRegression(max_iter=10000)      0.0001      l2  lbfgs   \n",
       "19  LogisticRegression(max_iter=10000)      0.0010      l2  lbfgs   \n",
       "20  LogisticRegression(max_iter=10000)      0.0100      l2  lbfgs   \n",
       "21  LogisticRegression(max_iter=10000)      0.1000      l2  lbfgs   \n",
       "22  LogisticRegression(max_iter=10000)      1.0000      l2  lbfgs   \n",
       "23  LogisticRegression(max_iter=10000)     10.0000      l2  lbfgs   \n",
       "24  LogisticRegression(max_iter=10000)    100.0000      l2  lbfgs   \n",
       "25  LogisticRegression(max_iter=10000)   1000.0000      l2  lbfgs   \n",
       "26  LogisticRegression(max_iter=10000)  10000.0000      l2  lbfgs   \n",
       "27  LogisticRegression(max_iter=10000)         NaN    none  lbfgs   \n",
       "28  LogisticRegression(max_iter=10000)         NaN    none   saga   \n",
       "29  LogisticRegression(max_iter=10000)         NaN     NaN    NaN   \n",
       "\n",
       "    score_f1_micro  \n",
       "0         0.600167  \n",
       "1         0.685523  \n",
       "2         0.701932  \n",
       "3         0.702187  \n",
       "4         0.704026  \n",
       "5         0.704068  \n",
       "6         0.703969  \n",
       "7         0.703969  \n",
       "8         0.703941  \n",
       "9         0.703955  \n",
       "10        0.703941  \n",
       "11        0.703941  \n",
       "12        0.703941  \n",
       "13        0.703955  \n",
       "14        0.703941  \n",
       "15        0.703941  \n",
       "16        0.703941  \n",
       "17        0.703941  \n",
       "18        0.685523  \n",
       "19        0.702187  \n",
       "20        0.704054  \n",
       "21        0.703969  \n",
       "22        0.703941  \n",
       "23        0.703941  \n",
       "24        0.703941  \n",
       "25        0.703941  \n",
       "26        0.703941  \n",
       "27        0.703941  \n",
       "28        0.703941  \n",
       "29        0.703941  "
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results = pd.DataFrame( best_model.cv_results_['params'] )\n",
    "\n",
    "results['score_f1_micro'] = best_model.cv_results_['mean_test_f1_micro']\n",
    "cols = results.columns.to_series().str.split('__').apply(lambda x: x[-1])\n",
    "results.columns = cols\n",
    "\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "4f243c59",
   "metadata": {},
   "outputs": [],
   "source": [
    "f1_micro_List.append(results[5:6])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94d1e17e",
   "metadata": {},
   "source": [
    "## Trial 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "82374d4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, Y_train, Y_test = train_test_split(X, y, test_size=0.2, shuffle=True, random_state = 6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "64a330b7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ENT\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\ENT\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\ENT\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\ENT\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\ENT\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\ENT\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\ENT\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\ENT\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\ENT\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\ENT\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\ENT\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1183: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\ENT\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1183: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\ENT\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1183: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\ENT\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1183: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\ENT\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1183: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\ENT\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1183: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\ENT\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1183: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\ENT\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1183: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\ENT\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1183: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\ENT\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1183: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\ENT\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1183: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\ENT\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1183: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\ENT\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1183: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\ENT\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1183: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\ENT\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1183: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\ENT\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1183: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\ENT\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1183: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\ENT\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1183: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\ENT\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1183: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\ENT\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1183: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Create a pipeline - RF is a stand in, we will populate the classifier part below\n",
    "pipe = Pipeline([('std', StandardScaler()),\n",
    "                 ('classifier', LogisticRegression())])\n",
    "\n",
    "# Create search space of candidate learning algorithms and their hyperparameters\n",
    "# note lbfgs can't do l1, and if you pass penalty='none' it expects no C value\n",
    "search_space = [{'classifier': [LogisticRegression(max_iter=10000)],\n",
    "                 'classifier__solver': ['saga'],\n",
    "                 'classifier__penalty': ['l1', 'l2'],\n",
    "                 'classifier__C': np.logspace(-4, 4, 9)},\n",
    "                {'classifier': [LogisticRegression(max_iter=10000)],\n",
    "                 'classifier__solver': ['lbfgs'],\n",
    "                 'classifier__penalty': ['l2'],\n",
    "                 'classifier__C': np.logspace(-4, 4, 9)},\n",
    "                {'classifier': [LogisticRegression(max_iter=10000)],\n",
    "                 'classifier__solver': ['lbfgs','saga'],\n",
    "                 'classifier__penalty': ['none']},\n",
    "                {'classifier': [LogisticRegression(max_iter=10000)]}\n",
    "                ]\n",
    "\n",
    "# Create grid search\n",
    "clf = GridSearchCV(pipe, search_space, cv=StratifiedKFold(n_splits=10),\n",
    "                   scoring=['accuracy', 'precision', 'f1_micro'], refit=False,\n",
    "                   verbose=0)\n",
    "\n",
    "# Fit grid search\n",
    "best_model = clf.fit(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "b0094308",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'mean_fit_time': array([0.05646436, 0.10914884, 0.11542802, 0.10506878, 0.11361196,\n",
       "        0.10064795, 0.11870692, 0.11041274, 0.11254458, 0.10054352,\n",
       "        0.1151422 , 0.10829477, 0.11674552, 0.10589635, 0.12660825,\n",
       "        0.10722167, 0.12487049, 0.11238387, 0.02295918, 0.02299523,\n",
       "        0.02300763, 0.02253525, 0.02256341, 0.02303963, 0.02253766,\n",
       "        0.02291131, 0.02398825, 0.02251158, 0.11287985, 0.02237499]),\n",
       " 'std_fit_time': array([0.00902457, 0.006758  , 0.00685167, 0.00914438, 0.01095791,\n",
       "        0.00615378, 0.0118446 , 0.00447411, 0.00638088, 0.00467788,\n",
       "        0.00789225, 0.00637137, 0.00893891, 0.00495626, 0.01173085,\n",
       "        0.00839021, 0.0115005 , 0.0061682 , 0.00065053, 0.00102456,\n",
       "        0.00141037, 0.00093191, 0.00089289, 0.00131622, 0.00105172,\n",
       "        0.00114187, 0.00251631, 0.00110324, 0.00768977, 0.00088781]),\n",
       " 'mean_score_time': array([0.00729456, 0.0076242 , 0.00800705, 0.00790701, 0.00780675,\n",
       "        0.00780721, 0.00790701, 0.00760682, 0.00760579, 0.00774555,\n",
       "        0.00770662, 0.00770681, 0.00770674, 0.00830739, 0.00792773,\n",
       "        0.00784378, 0.00800717, 0.00800443, 0.00733831, 0.00780561,\n",
       "        0.00810773, 0.00740638, 0.00790737, 0.0080102 , 0.00780754,\n",
       "        0.00762784, 0.00760736, 0.00792699, 0.00776286, 0.00750709]),\n",
       " 'std_score_time': array([4.68076762e-04, 4.66092307e-04, 4.47661012e-04, 5.39309900e-04,\n",
       "        4.00352581e-04, 4.00519765e-04, 3.00359764e-04, 4.90524381e-04,\n",
       "        4.90712816e-04, 4.11932854e-04, 4.58552499e-04, 4.58676997e-04,\n",
       "        6.40966037e-04, 4.58718759e-04, 3.13374936e-04, 3.39071520e-04,\n",
       "        4.47821069e-04, 6.24547468e-06, 4.26627173e-04, 3.99767125e-04,\n",
       "        5.39279079e-04, 4.90009024e-04, 5.39199358e-04, 7.29354036e-04,\n",
       "        4.00388449e-04, 4.68456765e-04, 4.90436779e-04, 8.13096346e-04,\n",
       "        3.98113351e-04, 5.00727424e-04]),\n",
       " 'param_classifier': masked_array(data=[LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000)],\n",
       "              mask=[False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'param_classifier__C': masked_array(data=[0.0001, 0.0001, 0.001, 0.001, 0.01, 0.01, 0.1, 0.1,\n",
       "                    1.0, 1.0, 10.0, 10.0, 100.0, 100.0, 1000.0, 1000.0,\n",
       "                    10000.0, 10000.0, 0.0001, 0.001, 0.01, 0.1, 1.0, 10.0,\n",
       "                    100.0, 1000.0, 10000.0, --, --, --],\n",
       "              mask=[False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False,  True,  True,  True],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'param_classifier__penalty': masked_array(data=['l1', 'l2', 'l1', 'l2', 'l1', 'l2', 'l1', 'l2', 'l1',\n",
       "                    'l2', 'l1', 'l2', 'l1', 'l2', 'l1', 'l2', 'l1', 'l2',\n",
       "                    'l2', 'l2', 'l2', 'l2', 'l2', 'l2', 'l2', 'l2', 'l2',\n",
       "                    'none', 'none', --],\n",
       "              mask=[False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False,  True],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'param_classifier__solver': masked_array(data=['saga', 'saga', 'saga', 'saga', 'saga', 'saga', 'saga',\n",
       "                    'saga', 'saga', 'saga', 'saga', 'saga', 'saga', 'saga',\n",
       "                    'saga', 'saga', 'saga', 'saga', 'lbfgs', 'lbfgs',\n",
       "                    'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs',\n",
       "                    'lbfgs', 'lbfgs', 'saga', --],\n",
       "              mask=[False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False,  True],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'params': [{'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 0.0001,\n",
       "   'classifier__penalty': 'l1',\n",
       "   'classifier__solver': 'saga'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 0.0001,\n",
       "   'classifier__penalty': 'l2',\n",
       "   'classifier__solver': 'saga'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 0.001,\n",
       "   'classifier__penalty': 'l1',\n",
       "   'classifier__solver': 'saga'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 0.001,\n",
       "   'classifier__penalty': 'l2',\n",
       "   'classifier__solver': 'saga'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 0.01,\n",
       "   'classifier__penalty': 'l1',\n",
       "   'classifier__solver': 'saga'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 0.01,\n",
       "   'classifier__penalty': 'l2',\n",
       "   'classifier__solver': 'saga'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 0.1,\n",
       "   'classifier__penalty': 'l1',\n",
       "   'classifier__solver': 'saga'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 0.1,\n",
       "   'classifier__penalty': 'l2',\n",
       "   'classifier__solver': 'saga'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 1.0,\n",
       "   'classifier__penalty': 'l1',\n",
       "   'classifier__solver': 'saga'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 1.0,\n",
       "   'classifier__penalty': 'l2',\n",
       "   'classifier__solver': 'saga'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 10.0,\n",
       "   'classifier__penalty': 'l1',\n",
       "   'classifier__solver': 'saga'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 10.0,\n",
       "   'classifier__penalty': 'l2',\n",
       "   'classifier__solver': 'saga'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 100.0,\n",
       "   'classifier__penalty': 'l1',\n",
       "   'classifier__solver': 'saga'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 100.0,\n",
       "   'classifier__penalty': 'l2',\n",
       "   'classifier__solver': 'saga'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 1000.0,\n",
       "   'classifier__penalty': 'l1',\n",
       "   'classifier__solver': 'saga'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 1000.0,\n",
       "   'classifier__penalty': 'l2',\n",
       "   'classifier__solver': 'saga'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 10000.0,\n",
       "   'classifier__penalty': 'l1',\n",
       "   'classifier__solver': 'saga'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 10000.0,\n",
       "   'classifier__penalty': 'l2',\n",
       "   'classifier__solver': 'saga'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 0.0001,\n",
       "   'classifier__penalty': 'l2',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 0.001,\n",
       "   'classifier__penalty': 'l2',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 0.01,\n",
       "   'classifier__penalty': 'l2',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 0.1,\n",
       "   'classifier__penalty': 'l2',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 1.0,\n",
       "   'classifier__penalty': 'l2',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 10.0,\n",
       "   'classifier__penalty': 'l2',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 100.0,\n",
       "   'classifier__penalty': 'l2',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 1000.0,\n",
       "   'classifier__penalty': 'l2',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 10000.0,\n",
       "   'classifier__penalty': 'l2',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__penalty': 'none',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__penalty': 'none',\n",
       "   'classifier__solver': 'saga'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000)}],\n",
       " 'split0_test_accuracy': array([0.59943423, 0.6844413 , 0.70028289, 0.70169731, 0.70353607,\n",
       "        0.70367751, 0.70410184, 0.7039604 , 0.70410184, 0.70410184,\n",
       "        0.70410184, 0.70410184, 0.7039604 , 0.70410184, 0.70410184,\n",
       "        0.7039604 , 0.70410184, 0.7039604 , 0.6844413 , 0.70169731,\n",
       "        0.70367751, 0.7039604 , 0.70410184, 0.70410184, 0.70410184,\n",
       "        0.70410184, 0.70410184, 0.70410184, 0.70410184, 0.70410184]),\n",
       " 'split1_test_accuracy': array([0.59943423, 0.67765205, 0.69193777, 0.69193777, 0.69264498,\n",
       "        0.69363508, 0.69391796, 0.69405941, 0.69405941, 0.69405941,\n",
       "        0.69405941, 0.69405941, 0.69420085, 0.69405941, 0.69405941,\n",
       "        0.69405941, 0.69405941, 0.69405941, 0.67765205, 0.69193777,\n",
       "        0.69363508, 0.69405941, 0.69405941, 0.69405941, 0.69405941,\n",
       "        0.69405941, 0.69405941, 0.69405941, 0.69405941, 0.69405941]),\n",
       " 'split2_test_accuracy': array([0.59937756, 0.67859669, 0.69514783, 0.69345028, 0.69429905,\n",
       "        0.69444051, 0.69543075, 0.69528929, 0.69543075, 0.69543075,\n",
       "        0.69543075, 0.69543075, 0.69543075, 0.69543075, 0.69543075,\n",
       "        0.69543075, 0.69543075, 0.69543075, 0.67859669, 0.69345028,\n",
       "        0.69444051, 0.69528929, 0.69543075, 0.69543075, 0.69543075,\n",
       "        0.69543075, 0.69543075, 0.69543075, 0.69543075, 0.69543075]),\n",
       " 'split3_test_accuracy': array([0.59937756, 0.68637714, 0.69953317, 0.705899  , 0.70533314,\n",
       "        0.70533314, 0.70618192, 0.70660631, 0.70660631, 0.70660631,\n",
       "        0.70660631, 0.70660631, 0.70660631, 0.70660631, 0.70660631,\n",
       "        0.70660631, 0.70660631, 0.70660631, 0.68637714, 0.705899  ,\n",
       "        0.70533314, 0.70660631, 0.70660631, 0.70660631, 0.70660631,\n",
       "        0.70660631, 0.70660631, 0.70660631, 0.70660631, 0.70660631]),\n",
       " 'split4_test_accuracy': array([0.59937756, 0.68552836, 0.70646485, 0.70773801, 0.70844532,\n",
       "        0.70858679, 0.70787947, 0.70787947, 0.70773801, 0.70773801,\n",
       "        0.70773801, 0.70773801, 0.70773801, 0.70773801, 0.70773801,\n",
       "        0.70773801, 0.70773801, 0.70773801, 0.68552836, 0.70773801,\n",
       "        0.70858679, 0.70787947, 0.70773801, 0.70773801, 0.70773801,\n",
       "        0.70773801, 0.70773801, 0.70773801, 0.70773801, 0.70773801]),\n",
       " 'split5_test_accuracy': array([0.59937756, 0.68185033, 0.69571368, 0.69613807, 0.69642099,\n",
       "        0.69627953, 0.69627953, 0.69613807, 0.69627953, 0.69627953,\n",
       "        0.69627953, 0.69627953, 0.69627953, 0.69627953, 0.69627953,\n",
       "        0.69627953, 0.69627953, 0.69627953, 0.68185033, 0.69613807,\n",
       "        0.69627953, 0.69613807, 0.69627953, 0.69627953, 0.69627953,\n",
       "        0.69627953, 0.69627953, 0.69627953, 0.69627953, 0.69627953]),\n",
       " 'split6_test_accuracy': array([0.59937756, 0.68241618, 0.7032112 , 0.70292828, 0.70405998,\n",
       "        0.70377705, 0.70363559, 0.70349413, 0.70335267, 0.70335267,\n",
       "        0.70335267, 0.70335267, 0.70335267, 0.70335267, 0.70335267,\n",
       "        0.70335267, 0.70335267, 0.70335267, 0.68241618, 0.70292828,\n",
       "        0.70377705, 0.70349413, 0.70335267, 0.70335267, 0.70335267,\n",
       "        0.70335267, 0.70335267, 0.70335267, 0.70335267, 0.70335267]),\n",
       " 'split7_test_accuracy': array([0.59937756, 0.68397227, 0.69840147, 0.6998161 , 0.70151365,\n",
       "        0.7009478 , 0.70123073, 0.70123073, 0.70123073, 0.70123073,\n",
       "        0.70123073, 0.70123073, 0.70123073, 0.70123073, 0.70123073,\n",
       "        0.70123073, 0.70123073, 0.70123073, 0.68397227, 0.6998161 ,\n",
       "        0.7009478 , 0.70123073, 0.70123073, 0.70123073, 0.70123073,\n",
       "        0.70123073, 0.70123073, 0.70123073, 0.70123073, 0.70123073]),\n",
       " 'split8_test_accuracy': array([0.59937756, 0.68977225, 0.70377705, 0.70405998, 0.70731362,\n",
       "        0.70731362, 0.70773801, 0.70787947, 0.70802094, 0.70802094,\n",
       "        0.70802094, 0.70802094, 0.70802094, 0.70802094, 0.70802094,\n",
       "        0.70802094, 0.70802094, 0.70802094, 0.68977225, 0.70405998,\n",
       "        0.70731362, 0.70787947, 0.70802094, 0.70802094, 0.70802094,\n",
       "        0.70802094, 0.70802094, 0.70802094, 0.70802094, 0.70802094]),\n",
       " 'split9_test_accuracy': array([0.59937756, 0.67675767, 0.69557222, 0.6948649 , 0.69444051,\n",
       "        0.69387466, 0.69458198, 0.69444051, 0.6948649 , 0.6948649 ,\n",
       "        0.6948649 , 0.6948649 , 0.6948649 , 0.6948649 , 0.6948649 ,\n",
       "        0.6948649 , 0.6948649 , 0.6948649 , 0.67675767, 0.6948649 ,\n",
       "        0.69387466, 0.69458198, 0.6948649 , 0.6948649 , 0.6948649 ,\n",
       "        0.6948649 , 0.6948649 , 0.6948649 , 0.6948649 , 0.6948649 ]),\n",
       " 'mean_test_accuracy': array([0.5993889 , 0.68273643, 0.69900421, 0.69985297, 0.70080073,\n",
       "        0.70078657, 0.70109778, 0.70109778, 0.70116851, 0.70116851,\n",
       "        0.70116851, 0.70116851, 0.70116851, 0.70116851, 0.70116851,\n",
       "        0.70115436, 0.70116851, 0.70115436, 0.68273643, 0.69985297,\n",
       "        0.70078657, 0.70111193, 0.70116851, 0.70116851, 0.70116851,\n",
       "        0.70116851, 0.70116851, 0.70116851, 0.70116851, 0.70116851]),\n",
       " 'std_test_accuracy': array([2.26660501e-05, 3.93202049e-03, 4.31080662e-03, 5.21165985e-03,\n",
       "        5.55370101e-03, 5.48587085e-03, 5.29859628e-03, 5.37100126e-03,\n",
       "        5.29302719e-03, 5.29302719e-03, 5.29302719e-03, 5.29302719e-03,\n",
       "        5.26650284e-03, 5.29302719e-03, 5.29302719e-03, 5.28535313e-03,\n",
       "        5.29302719e-03, 5.28535313e-03, 3.93202049e-03, 5.21165985e-03,\n",
       "        5.48587085e-03, 5.35360670e-03, 5.29302719e-03, 5.29302719e-03,\n",
       "        5.29302719e-03, 5.29302719e-03, 5.29302719e-03, 5.29302719e-03,\n",
       "        5.29302719e-03, 5.29302719e-03]),\n",
       " 'rank_test_accuracy': array([30, 28, 27, 25, 22, 23, 20, 20,  1,  1,  1,  1,  1,  1,  1, 17,  1,\n",
       "        17, 28, 25, 23, 19,  1,  1,  1,  1,  1,  1,  1,  1]),\n",
       " 'split0_test_precision': array([0.        , 0.70824671, 0.67180723, 0.66790525, 0.66399287,\n",
       "        0.66414254, 0.66415262, 0.66385809, 0.66400709, 0.66400709,\n",
       "        0.66400709, 0.66400709, 0.66371289, 0.66400709, 0.66400709,\n",
       "        0.66371289, 0.66400709, 0.66371289, 0.70824671, 0.66790525,\n",
       "        0.66414254, 0.66385809, 0.66400709, 0.66400709, 0.66400709,\n",
       "        0.66400709, 0.66400709, 0.66400709, 0.66400709, 0.66400709]),\n",
       " 'split1_test_precision': array([0.        , 0.68745763, 0.65889213, 0.65280374, 0.6467706 ,\n",
       "        0.648     , 0.64726631, 0.64742177, 0.64742177, 0.64742177,\n",
       "        0.64742177, 0.64742177, 0.64757709, 0.64742177, 0.64742177,\n",
       "        0.64742177, 0.64742177, 0.64742177, 0.68745763, 0.65280374,\n",
       "        0.648     , 0.64742177, 0.64742177, 0.64742177, 0.64742177,\n",
       "        0.64742177, 0.64742177, 0.64742177, 0.64742177, 0.64742177]),\n",
       " 'split2_test_precision': array([0.        , 0.6958042 , 0.66601275, 0.65765766, 0.65270824,\n",
       "        0.65286624, 0.65368945, 0.65353261, 0.65368945, 0.65368945,\n",
       "        0.65368945, 0.65368945, 0.65368945, 0.65368945, 0.65368945,\n",
       "        0.65368945, 0.65368945, 0.65368945, 0.6958042 , 0.65765766,\n",
       "        0.65286624, 0.65353261, 0.65368945, 0.65368945, 0.65368945,\n",
       "        0.65368945, 0.65368945, 0.65368945, 0.65368945, 0.65368945]),\n",
       " 'split3_test_precision': array([0.        , 0.71731449, 0.67052023, 0.67519777, 0.6671129 ,\n",
       "        0.66696389, 0.66755437, 0.66799645, 0.66799645, 0.66799645,\n",
       "        0.66799645, 0.66799645, 0.66799645, 0.66799645, 0.66799645,\n",
       "        0.66799645, 0.66799645, 0.66799645, 0.71731449, 0.67519777,\n",
       "        0.66696389, 0.66799645, 0.66799645, 0.66799645, 0.66799645,\n",
       "        0.66799645, 0.66799645, 0.66799645, 0.66799645, 0.66799645]),\n",
       " 'split4_test_precision': array([0.        , 0.71458774, 0.68240964, 0.67764378, 0.67080195,\n",
       "        0.67079646, 0.66857143, 0.66857143, 0.66827768, 0.66827768,\n",
       "        0.66827768, 0.66827768, 0.66827768, 0.66827768, 0.66827768,\n",
       "        0.66827768, 0.66827768, 0.66827768, 0.71458774, 0.67764378,\n",
       "        0.67079646, 0.66857143, 0.66827768, 0.66827768, 0.66827768,\n",
       "        0.66827768, 0.66827768, 0.66827768, 0.66827768, 0.66827768]),\n",
       " 'split5_test_precision': array([0.        , 0.70926059, 0.6642547 , 0.65921788, 0.653125  ,\n",
       "        0.65256125, 0.6518847 , 0.65159574, 0.65175011, 0.65175011,\n",
       "        0.65175011, 0.65175011, 0.65175011, 0.65175011, 0.65175011,\n",
       "        0.65175011, 0.65175011, 0.65175011, 0.70926059, 0.65921788,\n",
       "        0.65256125, 0.65159574, 0.65175011, 0.65175011, 0.65175011,\n",
       "        0.65175011, 0.65175011, 0.65175011, 0.65175011, 0.65175011]),\n",
       " 'split6_test_precision': array([0.        , 0.7068358 , 0.67850195, 0.67023256, 0.66606822,\n",
       "        0.66473214, 0.66399644, 0.66370107, 0.66340596, 0.66340596,\n",
       "        0.66340596, 0.66340596, 0.66340596, 0.66340596, 0.66340596,\n",
       "        0.66340596, 0.66340596, 0.66340596, 0.7068358 , 0.67023256,\n",
       "        0.66473214, 0.66370107, 0.66340596, 0.66340596, 0.66340596,\n",
       "        0.66340596, 0.66340596, 0.66340596, 0.66340596, 0.66340596]),\n",
       " 'split7_test_precision': array([0.        , 0.70938375, 0.67241379, 0.66698024, 0.66290614,\n",
       "        0.66156616, 0.66143498, 0.66143498, 0.66143498, 0.66143498,\n",
       "        0.66143498, 0.66143498, 0.66143498, 0.66143498, 0.66143498,\n",
       "        0.66143498, 0.66143498, 0.66143498, 0.70938375, 0.66698024,\n",
       "        0.66156616, 0.66143498, 0.66143498, 0.66143498, 0.66143498,\n",
       "        0.66143498, 0.66143498, 0.66143498, 0.66143498, 0.66143498]),\n",
       " 'split8_test_precision': array([0.        , 0.7192862 , 0.67438563, 0.66864175, 0.66637593,\n",
       "        0.66652117, 0.66666667, 0.66681166, 0.66695652, 0.66695652,\n",
       "        0.66695652, 0.66695652, 0.66695652, 0.66695652, 0.66695652,\n",
       "        0.66695652, 0.66695652, 0.66695652, 0.7192862 , 0.66864175,\n",
       "        0.66652117, 0.66681166, 0.66695652, 0.66695652, 0.66695652,\n",
       "        0.66695652, 0.66695652, 0.66695652, 0.66695652, 0.66695652]),\n",
       " 'split9_test_precision': array([0.        , 0.68694463, 0.65799257, 0.65141319, 0.64558059,\n",
       "        0.64408973, 0.64460679, 0.6443299 , 0.64466352, 0.64466352,\n",
       "        0.64466352, 0.64466352, 0.64466352, 0.64466352, 0.64466352,\n",
       "        0.64466352, 0.64466352, 0.64466352, 0.68694463, 0.65141319,\n",
       "        0.64408973, 0.64460679, 0.64466352, 0.64466352, 0.64466352,\n",
       "        0.64466352, 0.64466352, 0.64466352, 0.64466352, 0.64466352]),\n",
       " 'mean_test_precision': array([0.        , 0.70551217, 0.66971906, 0.66476938, 0.65954424,\n",
       "        0.65922396, 0.65898238, 0.65892537, 0.65896035, 0.65896035,\n",
       "        0.65896035, 0.65896035, 0.65894647, 0.65896035, 0.65896035,\n",
       "        0.65893093, 0.65896035, 0.65893093, 0.70551217, 0.66476938,\n",
       "        0.65922396, 0.65895306, 0.65896035, 0.65896035, 0.65896035,\n",
       "        0.65896035, 0.65896035, 0.65896035, 0.65896035, 0.65896035]),\n",
       " 'std_test_precision': array([0.        , 0.01102097, 0.00755354, 0.00858502, 0.00866276,\n",
       "        0.00864252, 0.00839452, 0.00848014, 0.00837298, 0.00837298,\n",
       "        0.00837298, 0.00837298, 0.0083344 , 0.00837298, 0.00837298,\n",
       "        0.00835569, 0.00837298, 0.00835569, 0.01102097, 0.00858502,\n",
       "        0.00864252, 0.00843276, 0.00837298, 0.00837298, 0.00837298,\n",
       "        0.00837298, 0.00837298, 0.00837298, 0.00837298, 0.00837298]),\n",
       " 'rank_test_precision': array([30,  1,  3,  4,  6,  7,  9, 29, 10, 10, 10, 10, 26, 10, 10, 27, 10,\n",
       "        27,  1,  4,  7, 25, 10, 10, 10, 10, 10, 10, 10, 10]),\n",
       " 'split0_test_f1_micro': array([0.59943423, 0.6844413 , 0.70028289, 0.70169731, 0.70353607,\n",
       "        0.70367751, 0.70410184, 0.7039604 , 0.70410184, 0.70410184,\n",
       "        0.70410184, 0.70410184, 0.7039604 , 0.70410184, 0.70410184,\n",
       "        0.7039604 , 0.70410184, 0.7039604 , 0.6844413 , 0.70169731,\n",
       "        0.70367751, 0.7039604 , 0.70410184, 0.70410184, 0.70410184,\n",
       "        0.70410184, 0.70410184, 0.70410184, 0.70410184, 0.70410184]),\n",
       " 'split1_test_f1_micro': array([0.59943423, 0.67765205, 0.69193777, 0.69193777, 0.69264498,\n",
       "        0.69363508, 0.69391796, 0.69405941, 0.69405941, 0.69405941,\n",
       "        0.69405941, 0.69405941, 0.69420085, 0.69405941, 0.69405941,\n",
       "        0.69405941, 0.69405941, 0.69405941, 0.67765205, 0.69193777,\n",
       "        0.69363508, 0.69405941, 0.69405941, 0.69405941, 0.69405941,\n",
       "        0.69405941, 0.69405941, 0.69405941, 0.69405941, 0.69405941]),\n",
       " 'split2_test_f1_micro': array([0.59937756, 0.67859669, 0.69514783, 0.69345028, 0.69429905,\n",
       "        0.69444051, 0.69543075, 0.69528929, 0.69543075, 0.69543075,\n",
       "        0.69543075, 0.69543075, 0.69543075, 0.69543075, 0.69543075,\n",
       "        0.69543075, 0.69543075, 0.69543075, 0.67859669, 0.69345028,\n",
       "        0.69444051, 0.69528929, 0.69543075, 0.69543075, 0.69543075,\n",
       "        0.69543075, 0.69543075, 0.69543075, 0.69543075, 0.69543075]),\n",
       " 'split3_test_f1_micro': array([0.59937756, 0.68637714, 0.69953317, 0.705899  , 0.70533314,\n",
       "        0.70533314, 0.70618192, 0.70660631, 0.70660631, 0.70660631,\n",
       "        0.70660631, 0.70660631, 0.70660631, 0.70660631, 0.70660631,\n",
       "        0.70660631, 0.70660631, 0.70660631, 0.68637714, 0.705899  ,\n",
       "        0.70533314, 0.70660631, 0.70660631, 0.70660631, 0.70660631,\n",
       "        0.70660631, 0.70660631, 0.70660631, 0.70660631, 0.70660631]),\n",
       " 'split4_test_f1_micro': array([0.59937756, 0.68552836, 0.70646485, 0.70773801, 0.70844532,\n",
       "        0.70858679, 0.70787947, 0.70787947, 0.70773801, 0.70773801,\n",
       "        0.70773801, 0.70773801, 0.70773801, 0.70773801, 0.70773801,\n",
       "        0.70773801, 0.70773801, 0.70773801, 0.68552836, 0.70773801,\n",
       "        0.70858679, 0.70787947, 0.70773801, 0.70773801, 0.70773801,\n",
       "        0.70773801, 0.70773801, 0.70773801, 0.70773801, 0.70773801]),\n",
       " 'split5_test_f1_micro': array([0.59937756, 0.68185033, 0.69571368, 0.69613807, 0.69642099,\n",
       "        0.69627953, 0.69627953, 0.69613807, 0.69627953, 0.69627953,\n",
       "        0.69627953, 0.69627953, 0.69627953, 0.69627953, 0.69627953,\n",
       "        0.69627953, 0.69627953, 0.69627953, 0.68185033, 0.69613807,\n",
       "        0.69627953, 0.69613807, 0.69627953, 0.69627953, 0.69627953,\n",
       "        0.69627953, 0.69627953, 0.69627953, 0.69627953, 0.69627953]),\n",
       " 'split6_test_f1_micro': array([0.59937756, 0.68241618, 0.7032112 , 0.70292828, 0.70405998,\n",
       "        0.70377705, 0.70363559, 0.70349413, 0.70335267, 0.70335267,\n",
       "        0.70335267, 0.70335267, 0.70335267, 0.70335267, 0.70335267,\n",
       "        0.70335267, 0.70335267, 0.70335267, 0.68241618, 0.70292828,\n",
       "        0.70377705, 0.70349413, 0.70335267, 0.70335267, 0.70335267,\n",
       "        0.70335267, 0.70335267, 0.70335267, 0.70335267, 0.70335267]),\n",
       " 'split7_test_f1_micro': array([0.59937756, 0.68397227, 0.69840147, 0.6998161 , 0.70151365,\n",
       "        0.7009478 , 0.70123073, 0.70123073, 0.70123073, 0.70123073,\n",
       "        0.70123073, 0.70123073, 0.70123073, 0.70123073, 0.70123073,\n",
       "        0.70123073, 0.70123073, 0.70123073, 0.68397227, 0.6998161 ,\n",
       "        0.7009478 , 0.70123073, 0.70123073, 0.70123073, 0.70123073,\n",
       "        0.70123073, 0.70123073, 0.70123073, 0.70123073, 0.70123073]),\n",
       " 'split8_test_f1_micro': array([0.59937756, 0.68977225, 0.70377705, 0.70405998, 0.70731362,\n",
       "        0.70731362, 0.70773801, 0.70787947, 0.70802094, 0.70802094,\n",
       "        0.70802094, 0.70802094, 0.70802094, 0.70802094, 0.70802094,\n",
       "        0.70802094, 0.70802094, 0.70802094, 0.68977225, 0.70405998,\n",
       "        0.70731362, 0.70787947, 0.70802094, 0.70802094, 0.70802094,\n",
       "        0.70802094, 0.70802094, 0.70802094, 0.70802094, 0.70802094]),\n",
       " 'split9_test_f1_micro': array([0.59937756, 0.67675767, 0.69557222, 0.6948649 , 0.69444051,\n",
       "        0.69387466, 0.69458198, 0.69444051, 0.6948649 , 0.6948649 ,\n",
       "        0.6948649 , 0.6948649 , 0.6948649 , 0.6948649 , 0.6948649 ,\n",
       "        0.6948649 , 0.6948649 , 0.6948649 , 0.67675767, 0.6948649 ,\n",
       "        0.69387466, 0.69458198, 0.6948649 , 0.6948649 , 0.6948649 ,\n",
       "        0.6948649 , 0.6948649 , 0.6948649 , 0.6948649 , 0.6948649 ]),\n",
       " 'mean_test_f1_micro': array([0.5993889 , 0.68273643, 0.69900421, 0.69985297, 0.70080073,\n",
       "        0.70078657, 0.70109778, 0.70109778, 0.70116851, 0.70116851,\n",
       "        0.70116851, 0.70116851, 0.70116851, 0.70116851, 0.70116851,\n",
       "        0.70115436, 0.70116851, 0.70115436, 0.68273643, 0.69985297,\n",
       "        0.70078657, 0.70111193, 0.70116851, 0.70116851, 0.70116851,\n",
       "        0.70116851, 0.70116851, 0.70116851, 0.70116851, 0.70116851]),\n",
       " 'std_test_f1_micro': array([2.26660501e-05, 3.93202049e-03, 4.31080662e-03, 5.21165985e-03,\n",
       "        5.55370101e-03, 5.48587085e-03, 5.29859628e-03, 5.37100126e-03,\n",
       "        5.29302719e-03, 5.29302719e-03, 5.29302719e-03, 5.29302719e-03,\n",
       "        5.26650284e-03, 5.29302719e-03, 5.29302719e-03, 5.28535313e-03,\n",
       "        5.29302719e-03, 5.28535313e-03, 3.93202049e-03, 5.21165985e-03,\n",
       "        5.48587085e-03, 5.35360670e-03, 5.29302719e-03, 5.29302719e-03,\n",
       "        5.29302719e-03, 5.29302719e-03, 5.29302719e-03, 5.29302719e-03,\n",
       "        5.29302719e-03, 5.29302719e-03]),\n",
       " 'rank_test_f1_micro': array([30, 28, 27, 25, 22, 23, 20, 20,  1,  1,  1,  1,  1,  1,  1, 17,  1,\n",
       "        17, 28, 25, 23, 19,  1,  1,  1,  1,  1,  1,  1,  1])}"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_model.cv_results_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "50a53953",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([30, 28, 27, 25, 22, 23, 20, 20,  1,  1,  1,  1,  1,  1,  1, 17,  1,\n",
       "       17, 28, 25, 23, 19,  1,  1,  1,  1,  1,  1,  1,  1])"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_model.cv_results_['rank_test_accuracy']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "ed852058",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'classifier': LogisticRegression(max_iter=10000),\n",
       " 'classifier__C': 0.0001,\n",
       " 'classifier__penalty': 'l1',\n",
       " 'classifier__solver': 'saga'}"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_model.cv_results_['params'][ np.argmin(best_model.cv_results_['mean_test_accuracy'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "cb83803b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>classifier</th>\n",
       "      <th>C</th>\n",
       "      <th>penalty</th>\n",
       "      <th>solver</th>\n",
       "      <th>score_acc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.599389</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.682736</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.699004</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.699853</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0100</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.700801</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0100</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.700787</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.1000</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.701098</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.1000</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.701098</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.701169</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.701169</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>10.0000</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.701169</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>10.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.701169</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>100.0000</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.701169</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>100.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.701169</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>1000.0000</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.701169</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>1000.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.701154</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>10000.0000</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.701169</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>10000.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.701154</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.682736</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.699853</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0100</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.700787</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.1000</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.701112</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.701169</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>10.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.701169</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>100.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.701169</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>1000.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.701169</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>10000.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.701169</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>NaN</td>\n",
       "      <td>none</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.701169</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>NaN</td>\n",
       "      <td>none</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.701169</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.701169</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                            classifier           C penalty solver  score_acc\n",
       "0   LogisticRegression(max_iter=10000)      0.0001      l1   saga   0.599389\n",
       "1   LogisticRegression(max_iter=10000)      0.0001      l2   saga   0.682736\n",
       "2   LogisticRegression(max_iter=10000)      0.0010      l1   saga   0.699004\n",
       "3   LogisticRegression(max_iter=10000)      0.0010      l2   saga   0.699853\n",
       "4   LogisticRegression(max_iter=10000)      0.0100      l1   saga   0.700801\n",
       "5   LogisticRegression(max_iter=10000)      0.0100      l2   saga   0.700787\n",
       "6   LogisticRegression(max_iter=10000)      0.1000      l1   saga   0.701098\n",
       "7   LogisticRegression(max_iter=10000)      0.1000      l2   saga   0.701098\n",
       "8   LogisticRegression(max_iter=10000)      1.0000      l1   saga   0.701169\n",
       "9   LogisticRegression(max_iter=10000)      1.0000      l2   saga   0.701169\n",
       "10  LogisticRegression(max_iter=10000)     10.0000      l1   saga   0.701169\n",
       "11  LogisticRegression(max_iter=10000)     10.0000      l2   saga   0.701169\n",
       "12  LogisticRegression(max_iter=10000)    100.0000      l1   saga   0.701169\n",
       "13  LogisticRegression(max_iter=10000)    100.0000      l2   saga   0.701169\n",
       "14  LogisticRegression(max_iter=10000)   1000.0000      l1   saga   0.701169\n",
       "15  LogisticRegression(max_iter=10000)   1000.0000      l2   saga   0.701154\n",
       "16  LogisticRegression(max_iter=10000)  10000.0000      l1   saga   0.701169\n",
       "17  LogisticRegression(max_iter=10000)  10000.0000      l2   saga   0.701154\n",
       "18  LogisticRegression(max_iter=10000)      0.0001      l2  lbfgs   0.682736\n",
       "19  LogisticRegression(max_iter=10000)      0.0010      l2  lbfgs   0.699853\n",
       "20  LogisticRegression(max_iter=10000)      0.0100      l2  lbfgs   0.700787\n",
       "21  LogisticRegression(max_iter=10000)      0.1000      l2  lbfgs   0.701112\n",
       "22  LogisticRegression(max_iter=10000)      1.0000      l2  lbfgs   0.701169\n",
       "23  LogisticRegression(max_iter=10000)     10.0000      l2  lbfgs   0.701169\n",
       "24  LogisticRegression(max_iter=10000)    100.0000      l2  lbfgs   0.701169\n",
       "25  LogisticRegression(max_iter=10000)   1000.0000      l2  lbfgs   0.701169\n",
       "26  LogisticRegression(max_iter=10000)  10000.0000      l2  lbfgs   0.701169\n",
       "27  LogisticRegression(max_iter=10000)         NaN    none  lbfgs   0.701169\n",
       "28  LogisticRegression(max_iter=10000)         NaN    none   saga   0.701169\n",
       "29  LogisticRegression(max_iter=10000)         NaN     NaN    NaN   0.701169"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results = pd.DataFrame( best_model.cv_results_['params'] )\n",
    "\n",
    "results['score_acc'] = best_model.cv_results_['mean_test_accuracy']\n",
    "cols = results.columns.to_series().str.split('__').apply(lambda x: x[-1])\n",
    "results.columns = cols\n",
    "\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "e5605c9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "acc_list.append(results[0:1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "687c473b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'classifier': LogisticRegression(max_iter=10000),\n",
       " 'classifier__C': 0.0001,\n",
       " 'classifier__penalty': 'l2',\n",
       " 'classifier__solver': 'saga'}"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_model.cv_results_['params'][ np.argmin(best_model.cv_results_['rank_test_precision'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "f9394193",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>classifier</th>\n",
       "      <th>C</th>\n",
       "      <th>penalty</th>\n",
       "      <th>solver</th>\n",
       "      <th>score_precision</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.705512</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.669719</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.664769</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0100</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.659544</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0100</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.659224</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.1000</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.658982</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.1000</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.658925</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.658960</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.658960</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>10.0000</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.658960</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>10.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.658960</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>100.0000</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.658946</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>100.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.658960</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>1000.0000</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.658960</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>1000.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.658931</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>10000.0000</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.658960</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>10000.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.658931</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.705512</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.664769</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0100</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.659224</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.1000</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.658953</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.658960</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>10.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.658960</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>100.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.658960</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>1000.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.658960</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>10000.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.658960</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>NaN</td>\n",
       "      <td>none</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.658960</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>NaN</td>\n",
       "      <td>none</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.658960</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.658960</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                            classifier           C penalty solver  \\\n",
       "0   LogisticRegression(max_iter=10000)      0.0001      l1   saga   \n",
       "1   LogisticRegression(max_iter=10000)      0.0001      l2   saga   \n",
       "2   LogisticRegression(max_iter=10000)      0.0010      l1   saga   \n",
       "3   LogisticRegression(max_iter=10000)      0.0010      l2   saga   \n",
       "4   LogisticRegression(max_iter=10000)      0.0100      l1   saga   \n",
       "5   LogisticRegression(max_iter=10000)      0.0100      l2   saga   \n",
       "6   LogisticRegression(max_iter=10000)      0.1000      l1   saga   \n",
       "7   LogisticRegression(max_iter=10000)      0.1000      l2   saga   \n",
       "8   LogisticRegression(max_iter=10000)      1.0000      l1   saga   \n",
       "9   LogisticRegression(max_iter=10000)      1.0000      l2   saga   \n",
       "10  LogisticRegression(max_iter=10000)     10.0000      l1   saga   \n",
       "11  LogisticRegression(max_iter=10000)     10.0000      l2   saga   \n",
       "12  LogisticRegression(max_iter=10000)    100.0000      l1   saga   \n",
       "13  LogisticRegression(max_iter=10000)    100.0000      l2   saga   \n",
       "14  LogisticRegression(max_iter=10000)   1000.0000      l1   saga   \n",
       "15  LogisticRegression(max_iter=10000)   1000.0000      l2   saga   \n",
       "16  LogisticRegression(max_iter=10000)  10000.0000      l1   saga   \n",
       "17  LogisticRegression(max_iter=10000)  10000.0000      l2   saga   \n",
       "18  LogisticRegression(max_iter=10000)      0.0001      l2  lbfgs   \n",
       "19  LogisticRegression(max_iter=10000)      0.0010      l2  lbfgs   \n",
       "20  LogisticRegression(max_iter=10000)      0.0100      l2  lbfgs   \n",
       "21  LogisticRegression(max_iter=10000)      0.1000      l2  lbfgs   \n",
       "22  LogisticRegression(max_iter=10000)      1.0000      l2  lbfgs   \n",
       "23  LogisticRegression(max_iter=10000)     10.0000      l2  lbfgs   \n",
       "24  LogisticRegression(max_iter=10000)    100.0000      l2  lbfgs   \n",
       "25  LogisticRegression(max_iter=10000)   1000.0000      l2  lbfgs   \n",
       "26  LogisticRegression(max_iter=10000)  10000.0000      l2  lbfgs   \n",
       "27  LogisticRegression(max_iter=10000)         NaN    none  lbfgs   \n",
       "28  LogisticRegression(max_iter=10000)         NaN    none   saga   \n",
       "29  LogisticRegression(max_iter=10000)         NaN     NaN    NaN   \n",
       "\n",
       "    score_precision  \n",
       "0          0.000000  \n",
       "1          0.705512  \n",
       "2          0.669719  \n",
       "3          0.664769  \n",
       "4          0.659544  \n",
       "5          0.659224  \n",
       "6          0.658982  \n",
       "7          0.658925  \n",
       "8          0.658960  \n",
       "9          0.658960  \n",
       "10         0.658960  \n",
       "11         0.658960  \n",
       "12         0.658946  \n",
       "13         0.658960  \n",
       "14         0.658960  \n",
       "15         0.658931  \n",
       "16         0.658960  \n",
       "17         0.658931  \n",
       "18         0.705512  \n",
       "19         0.664769  \n",
       "20         0.659224  \n",
       "21         0.658953  \n",
       "22         0.658960  \n",
       "23         0.658960  \n",
       "24         0.658960  \n",
       "25         0.658960  \n",
       "26         0.658960  \n",
       "27         0.658960  \n",
       "28         0.658960  \n",
       "29         0.658960  "
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results = pd.DataFrame( best_model.cv_results_['params'] )\n",
    "\n",
    "results['score_precision'] = best_model.cv_results_['mean_test_precision']\n",
    "cols = results.columns.to_series().str.split('__').apply(lambda x: x[-1])\n",
    "results.columns = cols\n",
    "\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "f6cdf507",
   "metadata": {},
   "outputs": [],
   "source": [
    "roc_auc_ovr_List.append(results[1:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "342313d7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'classifier': LogisticRegression(max_iter=10000),\n",
       " 'classifier__C': 1.0,\n",
       " 'classifier__penalty': 'l1',\n",
       " 'classifier__solver': 'saga'}"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_model.cv_results_['params'][ np.argmin(best_model.cv_results_['rank_test_f1_micro'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "2d1e6c10",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>classifier</th>\n",
       "      <th>C</th>\n",
       "      <th>penalty</th>\n",
       "      <th>solver</th>\n",
       "      <th>score_f1_micro</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.599389</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.682736</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.699004</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.699853</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0100</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.700801</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0100</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.700787</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.1000</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.701098</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.1000</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.701098</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.701169</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.701169</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>10.0000</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.701169</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>10.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.701169</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>100.0000</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.701169</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>100.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.701169</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>1000.0000</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.701169</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>1000.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.701154</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>10000.0000</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.701169</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>10000.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.701154</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.682736</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.699853</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0100</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.700787</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.1000</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.701112</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.701169</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>10.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.701169</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>100.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.701169</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>1000.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.701169</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>10000.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.701169</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>NaN</td>\n",
       "      <td>none</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.701169</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>NaN</td>\n",
       "      <td>none</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.701169</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.701169</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                            classifier           C penalty solver  \\\n",
       "0   LogisticRegression(max_iter=10000)      0.0001      l1   saga   \n",
       "1   LogisticRegression(max_iter=10000)      0.0001      l2   saga   \n",
       "2   LogisticRegression(max_iter=10000)      0.0010      l1   saga   \n",
       "3   LogisticRegression(max_iter=10000)      0.0010      l2   saga   \n",
       "4   LogisticRegression(max_iter=10000)      0.0100      l1   saga   \n",
       "5   LogisticRegression(max_iter=10000)      0.0100      l2   saga   \n",
       "6   LogisticRegression(max_iter=10000)      0.1000      l1   saga   \n",
       "7   LogisticRegression(max_iter=10000)      0.1000      l2   saga   \n",
       "8   LogisticRegression(max_iter=10000)      1.0000      l1   saga   \n",
       "9   LogisticRegression(max_iter=10000)      1.0000      l2   saga   \n",
       "10  LogisticRegression(max_iter=10000)     10.0000      l1   saga   \n",
       "11  LogisticRegression(max_iter=10000)     10.0000      l2   saga   \n",
       "12  LogisticRegression(max_iter=10000)    100.0000      l1   saga   \n",
       "13  LogisticRegression(max_iter=10000)    100.0000      l2   saga   \n",
       "14  LogisticRegression(max_iter=10000)   1000.0000      l1   saga   \n",
       "15  LogisticRegression(max_iter=10000)   1000.0000      l2   saga   \n",
       "16  LogisticRegression(max_iter=10000)  10000.0000      l1   saga   \n",
       "17  LogisticRegression(max_iter=10000)  10000.0000      l2   saga   \n",
       "18  LogisticRegression(max_iter=10000)      0.0001      l2  lbfgs   \n",
       "19  LogisticRegression(max_iter=10000)      0.0010      l2  lbfgs   \n",
       "20  LogisticRegression(max_iter=10000)      0.0100      l2  lbfgs   \n",
       "21  LogisticRegression(max_iter=10000)      0.1000      l2  lbfgs   \n",
       "22  LogisticRegression(max_iter=10000)      1.0000      l2  lbfgs   \n",
       "23  LogisticRegression(max_iter=10000)     10.0000      l2  lbfgs   \n",
       "24  LogisticRegression(max_iter=10000)    100.0000      l2  lbfgs   \n",
       "25  LogisticRegression(max_iter=10000)   1000.0000      l2  lbfgs   \n",
       "26  LogisticRegression(max_iter=10000)  10000.0000      l2  lbfgs   \n",
       "27  LogisticRegression(max_iter=10000)         NaN    none  lbfgs   \n",
       "28  LogisticRegression(max_iter=10000)         NaN    none   saga   \n",
       "29  LogisticRegression(max_iter=10000)         NaN     NaN    NaN   \n",
       "\n",
       "    score_f1_micro  \n",
       "0         0.599389  \n",
       "1         0.682736  \n",
       "2         0.699004  \n",
       "3         0.699853  \n",
       "4         0.700801  \n",
       "5         0.700787  \n",
       "6         0.701098  \n",
       "7         0.701098  \n",
       "8         0.701169  \n",
       "9         0.701169  \n",
       "10        0.701169  \n",
       "11        0.701169  \n",
       "12        0.701169  \n",
       "13        0.701169  \n",
       "14        0.701169  \n",
       "15        0.701154  \n",
       "16        0.701169  \n",
       "17        0.701154  \n",
       "18        0.682736  \n",
       "19        0.699853  \n",
       "20        0.700787  \n",
       "21        0.701112  \n",
       "22        0.701169  \n",
       "23        0.701169  \n",
       "24        0.701169  \n",
       "25        0.701169  \n",
       "26        0.701169  \n",
       "27        0.701169  \n",
       "28        0.701169  \n",
       "29        0.701169  "
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results = pd.DataFrame( best_model.cv_results_['params'] )\n",
    "\n",
    "results['score_f1_micro'] = best_model.cv_results_['mean_test_f1_micro']\n",
    "cols = results.columns.to_series().str.split('__').apply(lambda x: x[-1])\n",
    "results.columns = cols\n",
    "\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "cd4d8e65",
   "metadata": {},
   "outputs": [],
   "source": [
    "f1_micro_List.append(results[8:9])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c967a56e",
   "metadata": {},
   "source": [
    "## Trial 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "1542f63b",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, Y_train, Y_test = train_test_split(X, y, test_size=0.2, shuffle=True, random_state = 7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "5380e5ce",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ENT\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\ENT\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\ENT\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\ENT\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\ENT\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\ENT\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\ENT\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\ENT\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\ENT\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\ENT\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\ENT\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1183: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\ENT\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1183: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\ENT\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1183: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\ENT\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1183: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\ENT\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1183: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\ENT\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1183: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\ENT\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1183: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\ENT\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1183: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\ENT\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1183: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\ENT\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1183: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\ENT\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1183: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\ENT\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1183: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\ENT\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1183: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\ENT\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1183: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\ENT\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1183: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\ENT\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1183: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\ENT\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1183: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\ENT\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1183: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\ENT\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1183: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\ENT\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1183: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Create a pipeline - RF is a stand in, we will populate the classifier part below\n",
    "pipe = Pipeline([('std', StandardScaler()),\n",
    "                 ('classifier', LogisticRegression())])\n",
    "\n",
    "# Create search space of candidate learning algorithms and their hyperparameters\n",
    "# note lbfgs can't do l1, and if you pass penalty='none' it expects no C value\n",
    "search_space = [{'classifier': [LogisticRegression(max_iter=10000)],\n",
    "                 'classifier__solver': ['saga'],\n",
    "                 'classifier__penalty': ['l1', 'l2'],\n",
    "                 'classifier__C': np.logspace(-4, 4, 9)},\n",
    "                {'classifier': [LogisticRegression(max_iter=10000)],\n",
    "                 'classifier__solver': ['lbfgs'],\n",
    "                 'classifier__penalty': ['l2'],\n",
    "                 'classifier__C': np.logspace(-4, 4, 9)},\n",
    "                {'classifier': [LogisticRegression(max_iter=10000)],\n",
    "                 'classifier__solver': ['lbfgs','saga'],\n",
    "                 'classifier__penalty': ['none']},\n",
    "                {'classifier': [LogisticRegression(max_iter=10000)]}\n",
    "                ]\n",
    "\n",
    "# Create grid search\n",
    "clf = GridSearchCV(pipe, search_space, cv=StratifiedKFold(n_splits=10),\n",
    "                   scoring=['accuracy', 'precision', 'f1_micro'], refit=False,\n",
    "                   verbose=0)\n",
    "\n",
    "# Fit grid search\n",
    "best_model = clf.fit(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "2795f9d4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'mean_fit_time': array([0.05603466, 0.10090907, 0.11907024, 0.10900588, 0.11592526,\n",
       "        0.10350008, 0.11353064, 0.10154216, 0.11814113, 0.10816143,\n",
       "        0.12410607, 0.103193  , 0.12532997, 0.107128  , 0.11577501,\n",
       "        0.1080066 , 0.12376661, 0.1053257 , 0.02266173, 0.0223201 ,\n",
       "        0.02173741, 0.021365  , 0.02136874, 0.02135756, 0.02150311,\n",
       "        0.02175233, 0.0218199 , 0.02152281, 0.10427606, 0.0215116 ]),\n",
       " 'std_fit_time': array([0.01506615, 0.00679525, 0.01141412, 0.00885118, 0.01007513,\n",
       "        0.00692923, 0.0065583 , 0.0055693 , 0.00904185, 0.00739944,\n",
       "        0.00575316, 0.00918819, 0.01560695, 0.00808041, 0.00933599,\n",
       "        0.00898281, 0.00577098, 0.006069  , 0.00072762, 0.00127004,\n",
       "        0.00101701, 0.00044771, 0.00064354, 0.00109659, 0.00068508,\n",
       "        0.00069443, 0.00107787, 0.00092961, 0.00494824, 0.00054496]),\n",
       " 'mean_score_time': array([0.00750673, 0.00736382, 0.00750656, 0.00764372, 0.00805891,\n",
       "        0.00766375, 0.00800679, 0.00781727, 0.00800717, 0.00793447,\n",
       "        0.00847018, 0.00790691, 0.0082258 , 0.00793045, 0.00780685,\n",
       "        0.00800705, 0.00774527, 0.00732987, 0.00720661, 0.00730669,\n",
       "        0.00754118, 0.00730653, 0.00758052, 0.00746865, 0.00740681,\n",
       "        0.00729806, 0.00742743, 0.00754304, 0.00766091, 0.00734637]),\n",
       " 'std_score_time': array([5.00464646e-04, 5.66174126e-04, 5.00440697e-04, 4.55879646e-04,\n",
       "        4.73946191e-04, 5.57614864e-04, 4.93820075e-07, 4.06513871e-04,\n",
       "        3.06254210e-07, 4.97986538e-04, 6.52793442e-04, 3.00248876e-04,\n",
       "        4.39143209e-04, 3.15603329e-04, 4.00400336e-04, 4.39622138e-07,\n",
       "        4.97117385e-04, 4.48593931e-04, 3.76307956e-04, 4.58593848e-04,\n",
       "        5.43339967e-04, 4.58546999e-04, 4.70631693e-04, 4.55536775e-04,\n",
       "        4.90485444e-04, 4.65021856e-04, 5.18910865e-04, 4.75560958e-04,\n",
       "        7.14661723e-04, 4.97067670e-04]),\n",
       " 'param_classifier': masked_array(data=[LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000)],\n",
       "              mask=[False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'param_classifier__C': masked_array(data=[0.0001, 0.0001, 0.001, 0.001, 0.01, 0.01, 0.1, 0.1,\n",
       "                    1.0, 1.0, 10.0, 10.0, 100.0, 100.0, 1000.0, 1000.0,\n",
       "                    10000.0, 10000.0, 0.0001, 0.001, 0.01, 0.1, 1.0, 10.0,\n",
       "                    100.0, 1000.0, 10000.0, --, --, --],\n",
       "              mask=[False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False,  True,  True,  True],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'param_classifier__penalty': masked_array(data=['l1', 'l2', 'l1', 'l2', 'l1', 'l2', 'l1', 'l2', 'l1',\n",
       "                    'l2', 'l1', 'l2', 'l1', 'l2', 'l1', 'l2', 'l1', 'l2',\n",
       "                    'l2', 'l2', 'l2', 'l2', 'l2', 'l2', 'l2', 'l2', 'l2',\n",
       "                    'none', 'none', --],\n",
       "              mask=[False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False,  True],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'param_classifier__solver': masked_array(data=['saga', 'saga', 'saga', 'saga', 'saga', 'saga', 'saga',\n",
       "                    'saga', 'saga', 'saga', 'saga', 'saga', 'saga', 'saga',\n",
       "                    'saga', 'saga', 'saga', 'saga', 'lbfgs', 'lbfgs',\n",
       "                    'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs',\n",
       "                    'lbfgs', 'lbfgs', 'saga', --],\n",
       "              mask=[False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False,  True],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'params': [{'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 0.0001,\n",
       "   'classifier__penalty': 'l1',\n",
       "   'classifier__solver': 'saga'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 0.0001,\n",
       "   'classifier__penalty': 'l2',\n",
       "   'classifier__solver': 'saga'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 0.001,\n",
       "   'classifier__penalty': 'l1',\n",
       "   'classifier__solver': 'saga'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 0.001,\n",
       "   'classifier__penalty': 'l2',\n",
       "   'classifier__solver': 'saga'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 0.01,\n",
       "   'classifier__penalty': 'l1',\n",
       "   'classifier__solver': 'saga'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 0.01,\n",
       "   'classifier__penalty': 'l2',\n",
       "   'classifier__solver': 'saga'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 0.1,\n",
       "   'classifier__penalty': 'l1',\n",
       "   'classifier__solver': 'saga'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 0.1,\n",
       "   'classifier__penalty': 'l2',\n",
       "   'classifier__solver': 'saga'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 1.0,\n",
       "   'classifier__penalty': 'l1',\n",
       "   'classifier__solver': 'saga'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 1.0,\n",
       "   'classifier__penalty': 'l2',\n",
       "   'classifier__solver': 'saga'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 10.0,\n",
       "   'classifier__penalty': 'l1',\n",
       "   'classifier__solver': 'saga'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 10.0,\n",
       "   'classifier__penalty': 'l2',\n",
       "   'classifier__solver': 'saga'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 100.0,\n",
       "   'classifier__penalty': 'l1',\n",
       "   'classifier__solver': 'saga'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 100.0,\n",
       "   'classifier__penalty': 'l2',\n",
       "   'classifier__solver': 'saga'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 1000.0,\n",
       "   'classifier__penalty': 'l1',\n",
       "   'classifier__solver': 'saga'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 1000.0,\n",
       "   'classifier__penalty': 'l2',\n",
       "   'classifier__solver': 'saga'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 10000.0,\n",
       "   'classifier__penalty': 'l1',\n",
       "   'classifier__solver': 'saga'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 10000.0,\n",
       "   'classifier__penalty': 'l2',\n",
       "   'classifier__solver': 'saga'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 0.0001,\n",
       "   'classifier__penalty': 'l2',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 0.001,\n",
       "   'classifier__penalty': 'l2',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 0.01,\n",
       "   'classifier__penalty': 'l2',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 0.1,\n",
       "   'classifier__penalty': 'l2',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 1.0,\n",
       "   'classifier__penalty': 'l2',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 10.0,\n",
       "   'classifier__penalty': 'l2',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 100.0,\n",
       "   'classifier__penalty': 'l2',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 1000.0,\n",
       "   'classifier__penalty': 'l2',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 10000.0,\n",
       "   'classifier__penalty': 'l2',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__penalty': 'none',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__penalty': 'none',\n",
       "   'classifier__solver': 'saga'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000)}],\n",
       " 'split0_test_accuracy': array([0.59971711, 0.68161245, 0.69533239, 0.69547383, 0.6990099 ,\n",
       "        0.69844413, 0.69915134, 0.6990099 , 0.69915134, 0.6990099 ,\n",
       "        0.6990099 , 0.69915134, 0.6990099 , 0.6990099 , 0.6990099 ,\n",
       "        0.6990099 , 0.6990099 , 0.6990099 , 0.681471  , 0.69547383,\n",
       "        0.69844413, 0.6990099 , 0.6990099 , 0.6990099 , 0.6990099 ,\n",
       "        0.6990099 , 0.6990099 , 0.6990099 , 0.6990099 , 0.6990099 ]),\n",
       " 'split1_test_accuracy': array([0.59971711, 0.68217822, 0.6990099 , 0.7       , 0.70042433,\n",
       "        0.70028289, 0.70056577, 0.70042433, 0.70028289, 0.70028289,\n",
       "        0.70028289, 0.70028289, 0.70028289, 0.70028289, 0.70028289,\n",
       "        0.70028289, 0.70028289, 0.70028289, 0.68231966, 0.7       ,\n",
       "        0.70028289, 0.70042433, 0.70028289, 0.70028289, 0.70028289,\n",
       "        0.70028289, 0.70028289, 0.70028289, 0.70028289, 0.70028289]),\n",
       " 'split2_test_accuracy': array([0.59980195, 0.68114302, 0.69811855, 0.69769416, 0.69698684,\n",
       "        0.69698684, 0.69769416, 0.69769416, 0.69755269, 0.69755269,\n",
       "        0.69755269, 0.69741123, 0.69741123, 0.69755269, 0.69755269,\n",
       "        0.69755269, 0.69755269, 0.69755269, 0.68114302, 0.69769416,\n",
       "        0.69698684, 0.69769416, 0.69755269, 0.69755269, 0.69755269,\n",
       "        0.69755269, 0.69755269, 0.69755269, 0.69755269, 0.69755269]),\n",
       " 'split3_test_accuracy': array([0.59980195, 0.69019663, 0.70363559, 0.70363559, 0.70618192,\n",
       "        0.70646485, 0.70674777, 0.70688923, 0.70688923, 0.70688923,\n",
       "        0.70688923, 0.70688923, 0.70688923, 0.70688923, 0.70688923,\n",
       "        0.70688923, 0.70688923, 0.70688923, 0.69005517, 0.70363559,\n",
       "        0.70646485, 0.70688923, 0.70688923, 0.70688923, 0.70688923,\n",
       "        0.70688923, 0.70688923, 0.70688923, 0.70688923, 0.70688923]),\n",
       " 'split4_test_accuracy': array([0.59980195, 0.68128448, 0.69925025, 0.70024049, 0.70123073,\n",
       "        0.70151365, 0.70236243, 0.70236243, 0.70222096, 0.70222096,\n",
       "        0.70222096, 0.70222096, 0.70222096, 0.70222096, 0.70222096,\n",
       "        0.70222096, 0.70222096, 0.70222096, 0.68128448, 0.70024049,\n",
       "        0.70151365, 0.70236243, 0.70222096, 0.70222096, 0.70222096,\n",
       "        0.70222096, 0.70222096, 0.70222096, 0.70222096, 0.70222096]),\n",
       " 'split5_test_accuracy': array([0.59980195, 0.67972839, 0.69090395, 0.6926015 , 0.69161126,\n",
       "        0.6914698 , 0.6914698 , 0.69132833, 0.69132833, 0.69132833,\n",
       "        0.69132833, 0.69132833, 0.69132833, 0.69132833, 0.69132833,\n",
       "        0.69132833, 0.69132833, 0.69132833, 0.67958693, 0.6926015 ,\n",
       "        0.6914698 , 0.69132833, 0.69132833, 0.69132833, 0.69132833,\n",
       "        0.69132833, 0.69132833, 0.69132833, 0.69132833, 0.69132833]),\n",
       " 'split6_test_accuracy': array([0.59980195, 0.68100156, 0.70038195, 0.70179658, 0.70123073,\n",
       "        0.70024049, 0.70080634, 0.7009478 , 0.7009478 , 0.7009478 ,\n",
       "        0.7009478 , 0.7009478 , 0.7009478 , 0.7009478 , 0.7009478 ,\n",
       "        0.7009478 , 0.7009478 , 0.7009478 , 0.68100156, 0.70179658,\n",
       "        0.70009902, 0.7009478 , 0.7009478 , 0.7009478 , 0.7009478 ,\n",
       "        0.7009478 , 0.7009478 , 0.7009478 , 0.7009478 , 0.7009478 ]),\n",
       " 'split7_test_accuracy': array([0.59980195, 0.68284057, 0.70038195, 0.70420144, 0.70759655,\n",
       "        0.70688923, 0.70618192, 0.70632338, 0.70618192, 0.70618192,\n",
       "        0.70632338, 0.70632338, 0.70618192, 0.70618192, 0.70618192,\n",
       "        0.70632338, 0.70632338, 0.70632338, 0.68284057, 0.70420144,\n",
       "        0.70688923, 0.70632338, 0.70618192, 0.70632338, 0.70632338,\n",
       "        0.70632338, 0.70632338, 0.70632338, 0.70618192, 0.70618192]),\n",
       " 'split8_test_accuracy': array([0.59980195, 0.68255765, 0.69613807, 0.69925025, 0.69953317,\n",
       "        0.69995756, 0.69939171, 0.69925025, 0.69910878, 0.69910878,\n",
       "        0.69910878, 0.69910878, 0.69910878, 0.69910878, 0.69910878,\n",
       "        0.69910878, 0.69910878, 0.69910878, 0.68255765, 0.69925025,\n",
       "        0.69995756, 0.69925025, 0.69910878, 0.69910878, 0.69910878,\n",
       "        0.69910878, 0.69910878, 0.69910878, 0.69910878, 0.69910878]),\n",
       " 'split9_test_accuracy': array([0.59980195, 0.68170887, 0.70038195, 0.70264535, 0.70448437,\n",
       "        0.70391852, 0.70405998, 0.70434291, 0.70448437, 0.70448437,\n",
       "        0.70448437, 0.70448437, 0.70448437, 0.70448437, 0.70448437,\n",
       "        0.70448437, 0.70448437, 0.70448437, 0.68170887, 0.70264535,\n",
       "        0.70405998, 0.70434291, 0.70448437, 0.70448437, 0.70448437,\n",
       "        0.70448437, 0.70448437, 0.70448437, 0.70448437, 0.70448437]),\n",
       " 'mean_test_accuracy': array([0.59978498, 0.68242518, 0.69835345, 0.69975392, 0.70082898,\n",
       "        0.7006168 , 0.70084312, 0.70085727, 0.70081483, 0.70080069,\n",
       "        0.70081484, 0.70081483, 0.70078654, 0.70080069, 0.70080069,\n",
       "        0.70081484, 0.70081484, 0.70081484, 0.68239689, 0.69975392,\n",
       "        0.7006168 , 0.70085727, 0.70080069, 0.70081484, 0.70081484,\n",
       "        0.70081484, 0.70081484, 0.70081484, 0.70080069, 0.70080069]),\n",
       " 'std_test_accuracy': array([3.39350468e-05, 2.72148325e-03, 3.33265097e-03, 3.48784376e-03,\n",
       "        4.39339155e-03, 4.33014085e-03, 4.22902239e-03, 4.33214685e-03,\n",
       "        4.33313191e-03, 4.33876600e-03, 4.35648255e-03, 4.36167139e-03,\n",
       "        4.34955003e-03, 4.33876600e-03, 4.33876600e-03, 4.35648255e-03,\n",
       "        4.35648255e-03, 4.35648255e-03, 2.69927842e-03, 3.48784376e-03,\n",
       "        4.34260095e-03, 4.33214685e-03, 4.33876600e-03, 4.35648255e-03,\n",
       "        4.35648255e-03, 4.35648255e-03, 4.35648255e-03, 4.35648255e-03,\n",
       "        4.33876600e-03, 4.33876600e-03]),\n",
       " 'rank_test_accuracy': array([30, 28, 27, 25,  4, 23,  3,  1, 14, 16,  5, 14, 22, 16, 16,  5,  5,\n",
       "         5, 29, 25, 23,  1, 16,  5,  5,  5,  5,  5, 16, 16]),\n",
       " 'split0_test_precision': array([0.        , 0.70258922, 0.6626564 , 0.65563218, 0.65503534,\n",
       "        0.65347405, 0.65396408, 0.65367776, 0.65382932, 0.65354331,\n",
       "        0.65354331, 0.65382932, 0.65354331, 0.65354331, 0.65354331,\n",
       "        0.65354331, 0.65354331, 0.65354331, 0.7020979 , 0.65563218,\n",
       "        0.65347405, 0.65367776, 0.65354331, 0.65354331, 0.65354331,\n",
       "        0.65354331, 0.65354331, 0.65354331, 0.65354331, 0.65354331]),\n",
       " 'split1_test_precision': array([0.        , 0.70896057, 0.67393459, 0.66840855, 0.66108597,\n",
       "        0.66035183, 0.66036887, 0.66007194, 0.65977528, 0.65977528,\n",
       "        0.65977528, 0.65977528, 0.65977528, 0.65977528, 0.65977528,\n",
       "        0.65977528, 0.65977528, 0.65977528, 0.70916905, 0.66840855,\n",
       "        0.66035183, 0.66007194, 0.65977528, 0.65977528, 0.65977528,\n",
       "        0.65977528, 0.65977528, 0.65977528, 0.65977528, 0.65977528]),\n",
       " 'split2_test_precision': array([0.        , 0.69571137, 0.66383781, 0.6575592 , 0.650724  ,\n",
       "        0.65032823, 0.65069686, 0.65069686, 0.65041358, 0.65041358,\n",
       "        0.65041358, 0.65013055, 0.65013055, 0.65041358, 0.65041358,\n",
       "        0.65041358, 0.65041358, 0.65041358, 0.69571137, 0.6575592 ,\n",
       "        0.65032823, 0.65069686, 0.65041358, 0.65041358, 0.65041358,\n",
       "        0.65041358, 0.65041358, 0.65041358, 0.65041358, 0.65041358]),\n",
       " 'split3_test_precision': array([0.        , 0.72515856, 0.68132411, 0.6747619 , 0.67263545,\n",
       "        0.67309458, 0.67244526, 0.67259462, 0.67259462, 0.67259462,\n",
       "        0.67259462, 0.67259462, 0.67259462, 0.67259462, 0.67259462,\n",
       "        0.67259462, 0.67259462, 0.67259462, 0.72496474, 0.6747619 ,\n",
       "        0.67309458, 0.67259462, 0.67259462, 0.67259462, 0.67259462,\n",
       "        0.67259462, 0.67259462, 0.67259462, 0.67259462, 0.67259462]),\n",
       " 'split4_test_precision': array([0.        , 0.70779221, 0.67375185, 0.66713748, 0.66214383,\n",
       "        0.66157303, 0.66233766, 0.66233766, 0.66189624, 0.66189624,\n",
       "        0.66189624, 0.66189624, 0.66189624, 0.66189624, 0.66189624,\n",
       "        0.66189624, 0.66189624, 0.66189624, 0.70779221, 0.66713748,\n",
       "        0.66157303, 0.66233766, 0.66189624, 0.66189624, 0.66189624,\n",
       "        0.66189624, 0.66189624, 0.66189624, 0.66189624, 0.66189624]),\n",
       " 'split5_test_precision': array([0.        , 0.703677  , 0.65692008, 0.65500945, 0.64703217,\n",
       "        0.64647378, 0.64607755, 0.64578639, 0.64578639, 0.64578639,\n",
       "        0.64578639, 0.64578639, 0.64578639, 0.64578639, 0.64578639,\n",
       "        0.64578639, 0.64578639, 0.64578639, 0.70317003, 0.65500945,\n",
       "        0.64647378, 0.64578639, 0.64578639, 0.64578639, 0.64578639,\n",
       "        0.64578639, 0.64578639, 0.64578639, 0.64578639, 0.64578639]),\n",
       " 'split6_test_precision': array([0.        , 0.70013947, 0.67001435, 0.66620562, 0.65799912,\n",
       "        0.6559754 , 0.65630473, 0.65645514, 0.65631832, 0.65631832,\n",
       "        0.65631832, 0.65631832, 0.65631832, 0.65631832, 0.65631832,\n",
       "        0.65631832, 0.65631832, 0.65631832, 0.70013947, 0.66620562,\n",
       "        0.65568731, 0.65645514, 0.65631832, 0.65631832, 0.65631832,\n",
       "        0.65631832, 0.65631832, 0.65631832, 0.65631832, 0.65631832]),\n",
       " 'split7_test_precision': array([0.        , 0.70538838, 0.67182214, 0.67083333, 0.66873339,\n",
       "        0.66681357, 0.66520211, 0.66534914, 0.66505707, 0.66505707,\n",
       "        0.66520404, 0.66520404, 0.66505707, 0.66505707, 0.66505707,\n",
       "        0.66520404, 0.66520404, 0.66520404, 0.70538838, 0.67083333,\n",
       "        0.66681357, 0.66534914, 0.66505707, 0.66520404, 0.66520404,\n",
       "        0.66520404, 0.66520404, 0.66520404, 0.66505707, 0.66505707]),\n",
       " 'split8_test_precision': array([0.        , 0.70613108, 0.66023529, 0.65840469, 0.65306122,\n",
       "        0.65337955, 0.65185505, 0.65157395, 0.65116279, 0.65116279,\n",
       "        0.65116279, 0.65116279, 0.65116279, 0.65116279, 0.65116279,\n",
       "        0.65116279, 0.65116279, 0.65116279, 0.70613108, 0.65840469,\n",
       "        0.65337955, 0.65157395, 0.65116279, 0.65116279, 0.65116279,\n",
       "        0.65116279, 0.65116279, 0.65116279, 0.65116279, 0.65116279]),\n",
       " 'split9_test_precision': array([0.        , 0.69761092, 0.67099567, 0.66774342, 0.66299559,\n",
       "        0.66182938, 0.66126915, 0.66156537, 0.66171329, 0.66171329,\n",
       "        0.66171329, 0.66171329, 0.66171329, 0.66171329, 0.66171329,\n",
       "        0.66171329, 0.66171329, 0.66171329, 0.69761092, 0.66774342,\n",
       "        0.66197802, 0.66156537, 0.66171329, 0.66171329, 0.66171329,\n",
       "        0.66171329, 0.66171329, 0.66171329, 0.66171329, 0.66171329]),\n",
       " 'mean_test_precision': array([0.        , 0.70531588, 0.66854923, 0.66416958, 0.65914461,\n",
       "        0.65832934, 0.65805213, 0.65801088, 0.65785469, 0.65782609,\n",
       "        0.65784079, 0.65784108, 0.65779779, 0.65782609, 0.65782609,\n",
       "        0.65784079, 0.65784079, 0.65784079, 0.70521752, 0.66416958,\n",
       "        0.6583154 , 0.65801088, 0.65782609, 0.65784079, 0.65784079,\n",
       "        0.65784079, 0.65784079, 0.65784079, 0.65782609, 0.65782609]),\n",
       " 'std_test_precision': array([0.        , 0.0077611 , 0.00706677, 0.00658805, 0.00757869,\n",
       "        0.00758317, 0.00740288, 0.0075346 , 0.0075393 , 0.00755505,\n",
       "        0.00756923, 0.00758182, 0.00758324, 0.00755505, 0.00755505,\n",
       "        0.00756923, 0.00756923, 0.00756923, 0.00775241, 0.00658805,\n",
       "        0.00759963, 0.0075346 , 0.00755505, 0.00756923, 0.00756923,\n",
       "        0.00756923, 0.00756923, 0.00756923, 0.00755505, 0.00755505]),\n",
       " 'rank_test_precision': array([30,  1,  3,  4,  6,  7,  9, 10, 12, 23, 14, 13, 29, 23, 23, 14, 14,\n",
       "        14,  2,  4,  8, 10, 23, 14, 14, 14, 14, 14, 23, 23]),\n",
       " 'split0_test_f1_micro': array([0.59971711, 0.68161245, 0.69533239, 0.69547383, 0.6990099 ,\n",
       "        0.69844413, 0.69915134, 0.6990099 , 0.69915134, 0.6990099 ,\n",
       "        0.6990099 , 0.69915134, 0.6990099 , 0.6990099 , 0.6990099 ,\n",
       "        0.6990099 , 0.6990099 , 0.6990099 , 0.681471  , 0.69547383,\n",
       "        0.69844413, 0.6990099 , 0.6990099 , 0.6990099 , 0.6990099 ,\n",
       "        0.6990099 , 0.6990099 , 0.6990099 , 0.6990099 , 0.6990099 ]),\n",
       " 'split1_test_f1_micro': array([0.59971711, 0.68217822, 0.6990099 , 0.7       , 0.70042433,\n",
       "        0.70028289, 0.70056577, 0.70042433, 0.70028289, 0.70028289,\n",
       "        0.70028289, 0.70028289, 0.70028289, 0.70028289, 0.70028289,\n",
       "        0.70028289, 0.70028289, 0.70028289, 0.68231966, 0.7       ,\n",
       "        0.70028289, 0.70042433, 0.70028289, 0.70028289, 0.70028289,\n",
       "        0.70028289, 0.70028289, 0.70028289, 0.70028289, 0.70028289]),\n",
       " 'split2_test_f1_micro': array([0.59980195, 0.68114302, 0.69811855, 0.69769416, 0.69698684,\n",
       "        0.69698684, 0.69769416, 0.69769416, 0.69755269, 0.69755269,\n",
       "        0.69755269, 0.69741123, 0.69741123, 0.69755269, 0.69755269,\n",
       "        0.69755269, 0.69755269, 0.69755269, 0.68114302, 0.69769416,\n",
       "        0.69698684, 0.69769416, 0.69755269, 0.69755269, 0.69755269,\n",
       "        0.69755269, 0.69755269, 0.69755269, 0.69755269, 0.69755269]),\n",
       " 'split3_test_f1_micro': array([0.59980195, 0.69019663, 0.70363559, 0.70363559, 0.70618192,\n",
       "        0.70646485, 0.70674777, 0.70688923, 0.70688923, 0.70688923,\n",
       "        0.70688923, 0.70688923, 0.70688923, 0.70688923, 0.70688923,\n",
       "        0.70688923, 0.70688923, 0.70688923, 0.69005517, 0.70363559,\n",
       "        0.70646485, 0.70688923, 0.70688923, 0.70688923, 0.70688923,\n",
       "        0.70688923, 0.70688923, 0.70688923, 0.70688923, 0.70688923]),\n",
       " 'split4_test_f1_micro': array([0.59980195, 0.68128448, 0.69925025, 0.70024049, 0.70123073,\n",
       "        0.70151365, 0.70236243, 0.70236243, 0.70222096, 0.70222096,\n",
       "        0.70222096, 0.70222096, 0.70222096, 0.70222096, 0.70222096,\n",
       "        0.70222096, 0.70222096, 0.70222096, 0.68128448, 0.70024049,\n",
       "        0.70151365, 0.70236243, 0.70222096, 0.70222096, 0.70222096,\n",
       "        0.70222096, 0.70222096, 0.70222096, 0.70222096, 0.70222096]),\n",
       " 'split5_test_f1_micro': array([0.59980195, 0.67972839, 0.69090395, 0.6926015 , 0.69161126,\n",
       "        0.6914698 , 0.6914698 , 0.69132833, 0.69132833, 0.69132833,\n",
       "        0.69132833, 0.69132833, 0.69132833, 0.69132833, 0.69132833,\n",
       "        0.69132833, 0.69132833, 0.69132833, 0.67958693, 0.6926015 ,\n",
       "        0.6914698 , 0.69132833, 0.69132833, 0.69132833, 0.69132833,\n",
       "        0.69132833, 0.69132833, 0.69132833, 0.69132833, 0.69132833]),\n",
       " 'split6_test_f1_micro': array([0.59980195, 0.68100156, 0.70038195, 0.70179658, 0.70123073,\n",
       "        0.70024049, 0.70080634, 0.7009478 , 0.7009478 , 0.7009478 ,\n",
       "        0.7009478 , 0.7009478 , 0.7009478 , 0.7009478 , 0.7009478 ,\n",
       "        0.7009478 , 0.7009478 , 0.7009478 , 0.68100156, 0.70179658,\n",
       "        0.70009902, 0.7009478 , 0.7009478 , 0.7009478 , 0.7009478 ,\n",
       "        0.7009478 , 0.7009478 , 0.7009478 , 0.7009478 , 0.7009478 ]),\n",
       " 'split7_test_f1_micro': array([0.59980195, 0.68284057, 0.70038195, 0.70420144, 0.70759655,\n",
       "        0.70688923, 0.70618192, 0.70632338, 0.70618192, 0.70618192,\n",
       "        0.70632338, 0.70632338, 0.70618192, 0.70618192, 0.70618192,\n",
       "        0.70632338, 0.70632338, 0.70632338, 0.68284057, 0.70420144,\n",
       "        0.70688923, 0.70632338, 0.70618192, 0.70632338, 0.70632338,\n",
       "        0.70632338, 0.70632338, 0.70632338, 0.70618192, 0.70618192]),\n",
       " 'split8_test_f1_micro': array([0.59980195, 0.68255765, 0.69613807, 0.69925025, 0.69953317,\n",
       "        0.69995756, 0.69939171, 0.69925025, 0.69910878, 0.69910878,\n",
       "        0.69910878, 0.69910878, 0.69910878, 0.69910878, 0.69910878,\n",
       "        0.69910878, 0.69910878, 0.69910878, 0.68255765, 0.69925025,\n",
       "        0.69995756, 0.69925025, 0.69910878, 0.69910878, 0.69910878,\n",
       "        0.69910878, 0.69910878, 0.69910878, 0.69910878, 0.69910878]),\n",
       " 'split9_test_f1_micro': array([0.59980195, 0.68170887, 0.70038195, 0.70264535, 0.70448437,\n",
       "        0.70391852, 0.70405998, 0.70434291, 0.70448437, 0.70448437,\n",
       "        0.70448437, 0.70448437, 0.70448437, 0.70448437, 0.70448437,\n",
       "        0.70448437, 0.70448437, 0.70448437, 0.68170887, 0.70264535,\n",
       "        0.70405998, 0.70434291, 0.70448437, 0.70448437, 0.70448437,\n",
       "        0.70448437, 0.70448437, 0.70448437, 0.70448437, 0.70448437]),\n",
       " 'mean_test_f1_micro': array([0.59978498, 0.68242518, 0.69835345, 0.69975392, 0.70082898,\n",
       "        0.7006168 , 0.70084312, 0.70085727, 0.70081483, 0.70080069,\n",
       "        0.70081484, 0.70081483, 0.70078654, 0.70080069, 0.70080069,\n",
       "        0.70081484, 0.70081484, 0.70081484, 0.68239689, 0.69975392,\n",
       "        0.7006168 , 0.70085727, 0.70080069, 0.70081484, 0.70081484,\n",
       "        0.70081484, 0.70081484, 0.70081484, 0.70080069, 0.70080069]),\n",
       " 'std_test_f1_micro': array([3.39350468e-05, 2.72148325e-03, 3.33265097e-03, 3.48784376e-03,\n",
       "        4.39339155e-03, 4.33014085e-03, 4.22902239e-03, 4.33214685e-03,\n",
       "        4.33313191e-03, 4.33876600e-03, 4.35648255e-03, 4.36167139e-03,\n",
       "        4.34955003e-03, 4.33876600e-03, 4.33876600e-03, 4.35648255e-03,\n",
       "        4.35648255e-03, 4.35648255e-03, 2.69927842e-03, 3.48784376e-03,\n",
       "        4.34260095e-03, 4.33214685e-03, 4.33876600e-03, 4.35648255e-03,\n",
       "        4.35648255e-03, 4.35648255e-03, 4.35648255e-03, 4.35648255e-03,\n",
       "        4.33876600e-03, 4.33876600e-03]),\n",
       " 'rank_test_f1_micro': array([30, 28, 27, 25,  4, 23,  3,  1, 14, 16,  5, 14, 22, 16, 16,  5,  5,\n",
       "         5, 29, 25, 23,  1, 16,  5,  5,  5,  5,  5, 16, 16])}"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_model.cv_results_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "5a869ea2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([30, 28, 27, 25,  4, 23,  3,  1, 14, 16,  5, 14, 22, 16, 16,  5,  5,\n",
       "        5, 29, 25, 23,  1, 16,  5,  5,  5,  5,  5, 16, 16])"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_model.cv_results_['rank_test_accuracy']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "3bcc2637",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'classifier': LogisticRegression(max_iter=10000),\n",
       " 'classifier__C': 0.0001,\n",
       " 'classifier__penalty': 'l1',\n",
       " 'classifier__solver': 'saga'}"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_model.cv_results_['params'][ np.argmin(best_model.cv_results_['mean_test_accuracy'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "0216cb96",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>classifier</th>\n",
       "      <th>C</th>\n",
       "      <th>penalty</th>\n",
       "      <th>solver</th>\n",
       "      <th>score_acc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.599785</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.682425</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.698353</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.699754</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0100</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.700829</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0100</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.700617</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.1000</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.700843</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.1000</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.700857</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.700815</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.700801</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>10.0000</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.700815</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>10.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.700815</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>100.0000</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.700787</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>100.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.700801</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>1000.0000</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.700801</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>1000.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.700815</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>10000.0000</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.700815</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>10000.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.700815</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.682397</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.699754</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0100</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.700617</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.1000</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.700857</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.700801</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>10.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.700815</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>100.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.700815</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>1000.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.700815</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>10000.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.700815</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>NaN</td>\n",
       "      <td>none</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.700815</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>NaN</td>\n",
       "      <td>none</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.700801</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.700801</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                            classifier           C penalty solver  score_acc\n",
       "0   LogisticRegression(max_iter=10000)      0.0001      l1   saga   0.599785\n",
       "1   LogisticRegression(max_iter=10000)      0.0001      l2   saga   0.682425\n",
       "2   LogisticRegression(max_iter=10000)      0.0010      l1   saga   0.698353\n",
       "3   LogisticRegression(max_iter=10000)      0.0010      l2   saga   0.699754\n",
       "4   LogisticRegression(max_iter=10000)      0.0100      l1   saga   0.700829\n",
       "5   LogisticRegression(max_iter=10000)      0.0100      l2   saga   0.700617\n",
       "6   LogisticRegression(max_iter=10000)      0.1000      l1   saga   0.700843\n",
       "7   LogisticRegression(max_iter=10000)      0.1000      l2   saga   0.700857\n",
       "8   LogisticRegression(max_iter=10000)      1.0000      l1   saga   0.700815\n",
       "9   LogisticRegression(max_iter=10000)      1.0000      l2   saga   0.700801\n",
       "10  LogisticRegression(max_iter=10000)     10.0000      l1   saga   0.700815\n",
       "11  LogisticRegression(max_iter=10000)     10.0000      l2   saga   0.700815\n",
       "12  LogisticRegression(max_iter=10000)    100.0000      l1   saga   0.700787\n",
       "13  LogisticRegression(max_iter=10000)    100.0000      l2   saga   0.700801\n",
       "14  LogisticRegression(max_iter=10000)   1000.0000      l1   saga   0.700801\n",
       "15  LogisticRegression(max_iter=10000)   1000.0000      l2   saga   0.700815\n",
       "16  LogisticRegression(max_iter=10000)  10000.0000      l1   saga   0.700815\n",
       "17  LogisticRegression(max_iter=10000)  10000.0000      l2   saga   0.700815\n",
       "18  LogisticRegression(max_iter=10000)      0.0001      l2  lbfgs   0.682397\n",
       "19  LogisticRegression(max_iter=10000)      0.0010      l2  lbfgs   0.699754\n",
       "20  LogisticRegression(max_iter=10000)      0.0100      l2  lbfgs   0.700617\n",
       "21  LogisticRegression(max_iter=10000)      0.1000      l2  lbfgs   0.700857\n",
       "22  LogisticRegression(max_iter=10000)      1.0000      l2  lbfgs   0.700801\n",
       "23  LogisticRegression(max_iter=10000)     10.0000      l2  lbfgs   0.700815\n",
       "24  LogisticRegression(max_iter=10000)    100.0000      l2  lbfgs   0.700815\n",
       "25  LogisticRegression(max_iter=10000)   1000.0000      l2  lbfgs   0.700815\n",
       "26  LogisticRegression(max_iter=10000)  10000.0000      l2  lbfgs   0.700815\n",
       "27  LogisticRegression(max_iter=10000)         NaN    none  lbfgs   0.700815\n",
       "28  LogisticRegression(max_iter=10000)         NaN    none   saga   0.700801\n",
       "29  LogisticRegression(max_iter=10000)         NaN     NaN    NaN   0.700801"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results = pd.DataFrame( best_model.cv_results_['params'] )\n",
    "\n",
    "results['score_acc'] = best_model.cv_results_['mean_test_accuracy']\n",
    "cols = results.columns.to_series().str.split('__').apply(lambda x: x[-1])\n",
    "results.columns = cols\n",
    "\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "83604c0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "acc_list.append(results[0:1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "9c89d3be",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'classifier': LogisticRegression(max_iter=10000),\n",
       " 'classifier__C': 0.0001,\n",
       " 'classifier__penalty': 'l2',\n",
       " 'classifier__solver': 'saga'}"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_model.cv_results_['params'][ np.argmin(best_model.cv_results_['rank_test_precision'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "69ab11b2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>classifier</th>\n",
       "      <th>C</th>\n",
       "      <th>penalty</th>\n",
       "      <th>solver</th>\n",
       "      <th>score_precision</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.705316</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.668549</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.664170</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0100</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.659145</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0100</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.658329</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.1000</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.658052</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.1000</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.658011</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.657855</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.657826</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>10.0000</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.657841</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>10.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.657841</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>100.0000</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.657798</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>100.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.657826</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>1000.0000</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.657826</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>1000.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.657841</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>10000.0000</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.657841</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>10000.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.657841</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.705218</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.664170</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0100</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.658315</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.1000</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.658011</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.657826</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>10.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.657841</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>100.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.657841</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>1000.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.657841</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>10000.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.657841</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>NaN</td>\n",
       "      <td>none</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.657841</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>NaN</td>\n",
       "      <td>none</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.657826</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.657826</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                            classifier           C penalty solver  \\\n",
       "0   LogisticRegression(max_iter=10000)      0.0001      l1   saga   \n",
       "1   LogisticRegression(max_iter=10000)      0.0001      l2   saga   \n",
       "2   LogisticRegression(max_iter=10000)      0.0010      l1   saga   \n",
       "3   LogisticRegression(max_iter=10000)      0.0010      l2   saga   \n",
       "4   LogisticRegression(max_iter=10000)      0.0100      l1   saga   \n",
       "5   LogisticRegression(max_iter=10000)      0.0100      l2   saga   \n",
       "6   LogisticRegression(max_iter=10000)      0.1000      l1   saga   \n",
       "7   LogisticRegression(max_iter=10000)      0.1000      l2   saga   \n",
       "8   LogisticRegression(max_iter=10000)      1.0000      l1   saga   \n",
       "9   LogisticRegression(max_iter=10000)      1.0000      l2   saga   \n",
       "10  LogisticRegression(max_iter=10000)     10.0000      l1   saga   \n",
       "11  LogisticRegression(max_iter=10000)     10.0000      l2   saga   \n",
       "12  LogisticRegression(max_iter=10000)    100.0000      l1   saga   \n",
       "13  LogisticRegression(max_iter=10000)    100.0000      l2   saga   \n",
       "14  LogisticRegression(max_iter=10000)   1000.0000      l1   saga   \n",
       "15  LogisticRegression(max_iter=10000)   1000.0000      l2   saga   \n",
       "16  LogisticRegression(max_iter=10000)  10000.0000      l1   saga   \n",
       "17  LogisticRegression(max_iter=10000)  10000.0000      l2   saga   \n",
       "18  LogisticRegression(max_iter=10000)      0.0001      l2  lbfgs   \n",
       "19  LogisticRegression(max_iter=10000)      0.0010      l2  lbfgs   \n",
       "20  LogisticRegression(max_iter=10000)      0.0100      l2  lbfgs   \n",
       "21  LogisticRegression(max_iter=10000)      0.1000      l2  lbfgs   \n",
       "22  LogisticRegression(max_iter=10000)      1.0000      l2  lbfgs   \n",
       "23  LogisticRegression(max_iter=10000)     10.0000      l2  lbfgs   \n",
       "24  LogisticRegression(max_iter=10000)    100.0000      l2  lbfgs   \n",
       "25  LogisticRegression(max_iter=10000)   1000.0000      l2  lbfgs   \n",
       "26  LogisticRegression(max_iter=10000)  10000.0000      l2  lbfgs   \n",
       "27  LogisticRegression(max_iter=10000)         NaN    none  lbfgs   \n",
       "28  LogisticRegression(max_iter=10000)         NaN    none   saga   \n",
       "29  LogisticRegression(max_iter=10000)         NaN     NaN    NaN   \n",
       "\n",
       "    score_precision  \n",
       "0          0.000000  \n",
       "1          0.705316  \n",
       "2          0.668549  \n",
       "3          0.664170  \n",
       "4          0.659145  \n",
       "5          0.658329  \n",
       "6          0.658052  \n",
       "7          0.658011  \n",
       "8          0.657855  \n",
       "9          0.657826  \n",
       "10         0.657841  \n",
       "11         0.657841  \n",
       "12         0.657798  \n",
       "13         0.657826  \n",
       "14         0.657826  \n",
       "15         0.657841  \n",
       "16         0.657841  \n",
       "17         0.657841  \n",
       "18         0.705218  \n",
       "19         0.664170  \n",
       "20         0.658315  \n",
       "21         0.658011  \n",
       "22         0.657826  \n",
       "23         0.657841  \n",
       "24         0.657841  \n",
       "25         0.657841  \n",
       "26         0.657841  \n",
       "27         0.657841  \n",
       "28         0.657826  \n",
       "29         0.657826  "
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results = pd.DataFrame( best_model.cv_results_['params'] )\n",
    "\n",
    "results['score_precision'] = best_model.cv_results_['mean_test_precision']\n",
    "cols = results.columns.to_series().str.split('__').apply(lambda x: x[-1])\n",
    "results.columns = cols\n",
    "\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "c584772f",
   "metadata": {},
   "outputs": [],
   "source": [
    "roc_auc_ovr_List.append(results[1:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "70f619b8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'classifier': LogisticRegression(max_iter=10000),\n",
       " 'classifier__C': 0.1,\n",
       " 'classifier__penalty': 'l2',\n",
       " 'classifier__solver': 'saga'}"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_model.cv_results_['params'][ np.argmin(best_model.cv_results_['rank_test_f1_micro'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "11465a39",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>classifier</th>\n",
       "      <th>C</th>\n",
       "      <th>penalty</th>\n",
       "      <th>solver</th>\n",
       "      <th>score_f1_micro</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.599785</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.682425</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.698353</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.699754</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0100</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.700829</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0100</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.700617</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.1000</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.700843</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.1000</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.700857</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.700815</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.700801</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>10.0000</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.700815</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>10.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.700815</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>100.0000</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.700787</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>100.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.700801</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>1000.0000</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.700801</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>1000.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.700815</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>10000.0000</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.700815</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>10000.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.700815</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.682397</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.699754</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0100</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.700617</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.1000</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.700857</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.700801</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>10.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.700815</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>100.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.700815</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>1000.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.700815</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>10000.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.700815</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>NaN</td>\n",
       "      <td>none</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.700815</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>NaN</td>\n",
       "      <td>none</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.700801</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.700801</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                            classifier           C penalty solver  \\\n",
       "0   LogisticRegression(max_iter=10000)      0.0001      l1   saga   \n",
       "1   LogisticRegression(max_iter=10000)      0.0001      l2   saga   \n",
       "2   LogisticRegression(max_iter=10000)      0.0010      l1   saga   \n",
       "3   LogisticRegression(max_iter=10000)      0.0010      l2   saga   \n",
       "4   LogisticRegression(max_iter=10000)      0.0100      l1   saga   \n",
       "5   LogisticRegression(max_iter=10000)      0.0100      l2   saga   \n",
       "6   LogisticRegression(max_iter=10000)      0.1000      l1   saga   \n",
       "7   LogisticRegression(max_iter=10000)      0.1000      l2   saga   \n",
       "8   LogisticRegression(max_iter=10000)      1.0000      l1   saga   \n",
       "9   LogisticRegression(max_iter=10000)      1.0000      l2   saga   \n",
       "10  LogisticRegression(max_iter=10000)     10.0000      l1   saga   \n",
       "11  LogisticRegression(max_iter=10000)     10.0000      l2   saga   \n",
       "12  LogisticRegression(max_iter=10000)    100.0000      l1   saga   \n",
       "13  LogisticRegression(max_iter=10000)    100.0000      l2   saga   \n",
       "14  LogisticRegression(max_iter=10000)   1000.0000      l1   saga   \n",
       "15  LogisticRegression(max_iter=10000)   1000.0000      l2   saga   \n",
       "16  LogisticRegression(max_iter=10000)  10000.0000      l1   saga   \n",
       "17  LogisticRegression(max_iter=10000)  10000.0000      l2   saga   \n",
       "18  LogisticRegression(max_iter=10000)      0.0001      l2  lbfgs   \n",
       "19  LogisticRegression(max_iter=10000)      0.0010      l2  lbfgs   \n",
       "20  LogisticRegression(max_iter=10000)      0.0100      l2  lbfgs   \n",
       "21  LogisticRegression(max_iter=10000)      0.1000      l2  lbfgs   \n",
       "22  LogisticRegression(max_iter=10000)      1.0000      l2  lbfgs   \n",
       "23  LogisticRegression(max_iter=10000)     10.0000      l2  lbfgs   \n",
       "24  LogisticRegression(max_iter=10000)    100.0000      l2  lbfgs   \n",
       "25  LogisticRegression(max_iter=10000)   1000.0000      l2  lbfgs   \n",
       "26  LogisticRegression(max_iter=10000)  10000.0000      l2  lbfgs   \n",
       "27  LogisticRegression(max_iter=10000)         NaN    none  lbfgs   \n",
       "28  LogisticRegression(max_iter=10000)         NaN    none   saga   \n",
       "29  LogisticRegression(max_iter=10000)         NaN     NaN    NaN   \n",
       "\n",
       "    score_f1_micro  \n",
       "0         0.599785  \n",
       "1         0.682425  \n",
       "2         0.698353  \n",
       "3         0.699754  \n",
       "4         0.700829  \n",
       "5         0.700617  \n",
       "6         0.700843  \n",
       "7         0.700857  \n",
       "8         0.700815  \n",
       "9         0.700801  \n",
       "10        0.700815  \n",
       "11        0.700815  \n",
       "12        0.700787  \n",
       "13        0.700801  \n",
       "14        0.700801  \n",
       "15        0.700815  \n",
       "16        0.700815  \n",
       "17        0.700815  \n",
       "18        0.682397  \n",
       "19        0.699754  \n",
       "20        0.700617  \n",
       "21        0.700857  \n",
       "22        0.700801  \n",
       "23        0.700815  \n",
       "24        0.700815  \n",
       "25        0.700815  \n",
       "26        0.700815  \n",
       "27        0.700815  \n",
       "28        0.700801  \n",
       "29        0.700801  "
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results = pd.DataFrame( best_model.cv_results_['params'] )\n",
    "\n",
    "results['score_f1_micro'] = best_model.cv_results_['mean_test_f1_micro']\n",
    "cols = results.columns.to_series().str.split('__').apply(lambda x: x[-1])\n",
    "results.columns = cols\n",
    "\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "4a693627",
   "metadata": {},
   "outputs": [],
   "source": [
    "f1_micro_List.append(results[7:8])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a0b1170",
   "metadata": {},
   "source": [
    "## Trial 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "d430cae5",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, Y_train, Y_test = train_test_split(X, y, test_size=0.2, shuffle=True, random_state = 8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "ec6bf17a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ENT\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\ENT\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\ENT\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\ENT\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\ENT\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\ENT\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\ENT\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\ENT\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\ENT\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\ENT\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\ENT\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1183: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\ENT\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1183: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\ENT\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1183: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\ENT\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1183: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\ENT\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1183: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\ENT\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1183: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\ENT\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1183: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\ENT\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1183: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\ENT\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1183: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\ENT\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1183: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\ENT\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1183: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\ENT\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1183: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\ENT\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1183: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\ENT\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1183: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\ENT\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1183: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\ENT\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1183: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\ENT\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1183: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\ENT\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1183: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\ENT\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1183: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\ENT\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1183: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Create a pipeline - RF is a stand in, we will populate the classifier part below\n",
    "pipe = Pipeline([('std', StandardScaler()),\n",
    "                 ('classifier', LogisticRegression())])\n",
    "\n",
    "# Create search space of candidate learning algorithms and their hyperparameters\n",
    "# note lbfgs can't do l1, and if you pass penalty='none' it expects no C value\n",
    "search_space = [{'classifier': [LogisticRegression(max_iter=10000)],\n",
    "                 'classifier__solver': ['saga'],\n",
    "                 'classifier__penalty': ['l1', 'l2'],\n",
    "                 'classifier__C': np.logspace(-4, 4, 9)},\n",
    "                {'classifier': [LogisticRegression(max_iter=10000)],\n",
    "                 'classifier__solver': ['lbfgs'],\n",
    "                 'classifier__penalty': ['l2'],\n",
    "                 'classifier__C': np.logspace(-4, 4, 9)},\n",
    "                {'classifier': [LogisticRegression(max_iter=10000)],\n",
    "                 'classifier__solver': ['lbfgs','saga'],\n",
    "                 'classifier__penalty': ['none']},\n",
    "                {'classifier': [LogisticRegression(max_iter=10000)]}\n",
    "                ]\n",
    "\n",
    "# Create grid search\n",
    "clf = GridSearchCV(pipe, search_space, cv=StratifiedKFold(n_splits=10),\n",
    "                   scoring=['accuracy', 'precision', 'f1_micro'], refit=False,\n",
    "                   verbose=0)\n",
    "\n",
    "# Fit grid search\n",
    "best_model = clf.fit(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "b6aa2042",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'mean_fit_time': array([0.04678934, 0.09954183, 0.11104252, 0.10280142, 0.11270027,\n",
       "        0.11310141, 0.13514037, 0.11243141, 0.12552671, 0.11808836,\n",
       "        0.13361847, 0.12123921, 0.13190758, 0.11197882, 0.13912077,\n",
       "        0.11503415, 0.13074541, 0.11524057, 0.02419181, 0.02254026,\n",
       "        0.02313309, 0.02313685, 0.02262032, 0.02293642, 0.02300429,\n",
       "        0.0224072 , 0.02342122, 0.02322154, 0.10895872, 0.02293863]),\n",
       " 'std_fit_time': array([0.00542952, 0.00541426, 0.00795919, 0.0095545 , 0.01043283,\n",
       "        0.01150007, 0.01140674, 0.00599009, 0.00871672, 0.00903209,\n",
       "        0.01493289, 0.01327   , 0.00826346, 0.00600628, 0.00867936,\n",
       "        0.00633688, 0.00694949, 0.00771142, 0.00128885, 0.00065932,\n",
       "        0.0020432 , 0.00128864, 0.00135762, 0.0010327 , 0.00079909,\n",
       "        0.0009025 , 0.00135798, 0.00087238, 0.00720119, 0.00082281]),\n",
       " 'mean_score_time': array([0.00740163, 0.00770652, 0.00780709, 0.00780642, 0.00800881,\n",
       "        0.00835495, 0.00852909, 0.00815818, 0.00811939, 0.00860789,\n",
       "        0.00828068, 0.00850513, 0.00800712, 0.00810726, 0.00820763,\n",
       "        0.00800674, 0.00849664, 0.0083077 , 0.00750735, 0.00770707,\n",
       "        0.00778246, 0.00775726, 0.00828435, 0.00750673, 0.00796907,\n",
       "        0.00780725, 0.00770702, 0.00780697, 0.00832205, 0.00822942]),\n",
       " 'std_score_time': array([4.85558189e-04, 4.58802002e-04, 4.00221517e-04, 4.00187659e-04,\n",
       "        6.34359128e-04, 7.13374770e-04, 1.54676879e-03, 5.50625460e-04,\n",
       "        3.36075387e-04, 9.17328119e-04, 6.38170785e-04, 4.97898686e-04,\n",
       "        4.47714607e-04, 8.31552382e-04, 4.00092604e-04, 1.12865452e-06,\n",
       "        6.79635236e-04, 6.40995939e-04, 5.00464785e-04, 4.58744716e-04,\n",
       "        5.27988761e-04, 4.03056844e-04, 5.94948907e-04, 5.00321434e-04,\n",
       "        7.51592682e-04, 4.00364757e-04, 6.41211955e-04, 4.00281176e-04,\n",
       "        4.51060402e-04, 3.94790918e-04]),\n",
       " 'param_classifier': masked_array(data=[LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000)],\n",
       "              mask=[False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'param_classifier__C': masked_array(data=[0.0001, 0.0001, 0.001, 0.001, 0.01, 0.01, 0.1, 0.1,\n",
       "                    1.0, 1.0, 10.0, 10.0, 100.0, 100.0, 1000.0, 1000.0,\n",
       "                    10000.0, 10000.0, 0.0001, 0.001, 0.01, 0.1, 1.0, 10.0,\n",
       "                    100.0, 1000.0, 10000.0, --, --, --],\n",
       "              mask=[False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False,  True,  True,  True],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'param_classifier__penalty': masked_array(data=['l1', 'l2', 'l1', 'l2', 'l1', 'l2', 'l1', 'l2', 'l1',\n",
       "                    'l2', 'l1', 'l2', 'l1', 'l2', 'l1', 'l2', 'l1', 'l2',\n",
       "                    'l2', 'l2', 'l2', 'l2', 'l2', 'l2', 'l2', 'l2', 'l2',\n",
       "                    'none', 'none', --],\n",
       "              mask=[False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False,  True],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'param_classifier__solver': masked_array(data=['saga', 'saga', 'saga', 'saga', 'saga', 'saga', 'saga',\n",
       "                    'saga', 'saga', 'saga', 'saga', 'saga', 'saga', 'saga',\n",
       "                    'saga', 'saga', 'saga', 'saga', 'lbfgs', 'lbfgs',\n",
       "                    'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs',\n",
       "                    'lbfgs', 'lbfgs', 'saga', --],\n",
       "              mask=[False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False,  True],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'params': [{'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 0.0001,\n",
       "   'classifier__penalty': 'l1',\n",
       "   'classifier__solver': 'saga'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 0.0001,\n",
       "   'classifier__penalty': 'l2',\n",
       "   'classifier__solver': 'saga'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 0.001,\n",
       "   'classifier__penalty': 'l1',\n",
       "   'classifier__solver': 'saga'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 0.001,\n",
       "   'classifier__penalty': 'l2',\n",
       "   'classifier__solver': 'saga'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 0.01,\n",
       "   'classifier__penalty': 'l1',\n",
       "   'classifier__solver': 'saga'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 0.01,\n",
       "   'classifier__penalty': 'l2',\n",
       "   'classifier__solver': 'saga'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 0.1,\n",
       "   'classifier__penalty': 'l1',\n",
       "   'classifier__solver': 'saga'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 0.1,\n",
       "   'classifier__penalty': 'l2',\n",
       "   'classifier__solver': 'saga'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 1.0,\n",
       "   'classifier__penalty': 'l1',\n",
       "   'classifier__solver': 'saga'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 1.0,\n",
       "   'classifier__penalty': 'l2',\n",
       "   'classifier__solver': 'saga'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 10.0,\n",
       "   'classifier__penalty': 'l1',\n",
       "   'classifier__solver': 'saga'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 10.0,\n",
       "   'classifier__penalty': 'l2',\n",
       "   'classifier__solver': 'saga'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 100.0,\n",
       "   'classifier__penalty': 'l1',\n",
       "   'classifier__solver': 'saga'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 100.0,\n",
       "   'classifier__penalty': 'l2',\n",
       "   'classifier__solver': 'saga'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 1000.0,\n",
       "   'classifier__penalty': 'l1',\n",
       "   'classifier__solver': 'saga'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 1000.0,\n",
       "   'classifier__penalty': 'l2',\n",
       "   'classifier__solver': 'saga'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 10000.0,\n",
       "   'classifier__penalty': 'l1',\n",
       "   'classifier__solver': 'saga'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 10000.0,\n",
       "   'classifier__penalty': 'l2',\n",
       "   'classifier__solver': 'saga'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 0.0001,\n",
       "   'classifier__penalty': 'l2',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 0.001,\n",
       "   'classifier__penalty': 'l2',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 0.01,\n",
       "   'classifier__penalty': 'l2',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 0.1,\n",
       "   'classifier__penalty': 'l2',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 1.0,\n",
       "   'classifier__penalty': 'l2',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 10.0,\n",
       "   'classifier__penalty': 'l2',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 100.0,\n",
       "   'classifier__penalty': 'l2',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 1000.0,\n",
       "   'classifier__penalty': 'l2',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 10000.0,\n",
       "   'classifier__penalty': 'l2',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__penalty': 'none',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__penalty': 'none',\n",
       "   'classifier__solver': 'saga'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000)}],\n",
       " 'split0_test_accuracy': array([0.6       , 0.68203678, 0.70466761, 0.70311174, 0.70282885,\n",
       "        0.70226308, 0.70311174, 0.70311174, 0.7029703 , 0.7029703 ,\n",
       "        0.7029703 , 0.70311174, 0.7029703 , 0.7029703 , 0.70311174,\n",
       "        0.70311174, 0.7029703 , 0.7029703 , 0.68203678, 0.70311174,\n",
       "        0.70226308, 0.70311174, 0.7029703 , 0.70311174, 0.70311174,\n",
       "        0.70311174, 0.70311174, 0.70311174, 0.70311174, 0.7029703 ]),\n",
       " 'split1_test_accuracy': array([0.6       , 0.68373409, 0.70325318, 0.70509194, 0.70466761,\n",
       "        0.7049505 , 0.70466761, 0.70466761, 0.70480905, 0.70480905,\n",
       "        0.70480905, 0.70480905, 0.70480905, 0.70480905, 0.70480905,\n",
       "        0.70480905, 0.70480905, 0.70480905, 0.68373409, 0.70509194,\n",
       "        0.7049505 , 0.70466761, 0.70480905, 0.70480905, 0.70480905,\n",
       "        0.70480905, 0.70480905, 0.70480905, 0.70480905, 0.70480905]),\n",
       " 'split2_test_accuracy': array([0.60008488, 0.67916254, 0.69712831, 0.69670392, 0.69642099,\n",
       "        0.69613807, 0.69642099, 0.69642099, 0.69656246, 0.69656246,\n",
       "        0.69642099, 0.69656246, 0.69656246, 0.69642099, 0.69656246,\n",
       "        0.69656246, 0.69656246, 0.69642099, 0.67916254, 0.69656246,\n",
       "        0.69613807, 0.69642099, 0.69642099, 0.69656246, 0.69656246,\n",
       "        0.69656246, 0.69656246, 0.69656246, 0.69656246, 0.69642099]),\n",
       " 'split3_test_accuracy': array([0.59994341, 0.6853869 , 0.69967464, 0.7009478 , 0.70377705,\n",
       "        0.70349413, 0.70405998, 0.70434291, 0.70420144, 0.70434291,\n",
       "        0.70420144, 0.70420144, 0.70420144, 0.70420144, 0.70420144,\n",
       "        0.70420144, 0.70420144, 0.70420144, 0.6853869 , 0.7009478 ,\n",
       "        0.70349413, 0.70434291, 0.70434291, 0.70420144, 0.70420144,\n",
       "        0.70420144, 0.70420144, 0.70420144, 0.70420144, 0.70434291]),\n",
       " 'split4_test_accuracy': array([0.59994341, 0.68694299, 0.70391852, 0.70646485, 0.70575753,\n",
       "        0.705899  , 0.70646485, 0.70674777, 0.70674777, 0.70674777,\n",
       "        0.70674777, 0.70674777, 0.70674777, 0.70674777, 0.70674777,\n",
       "        0.70674777, 0.70674777, 0.70674777, 0.68694299, 0.70646485,\n",
       "        0.705899  , 0.70674777, 0.70674777, 0.70674777, 0.70674777,\n",
       "        0.70674777, 0.70674777, 0.70674777, 0.70674777, 0.70674777]),\n",
       " 'split5_test_accuracy': array([0.59994341, 0.6819918 , 0.6959966 , 0.69472344, 0.69797708,\n",
       "        0.69811855, 0.69769416, 0.69769416, 0.69769416, 0.69769416,\n",
       "        0.69769416, 0.69769416, 0.69769416, 0.69769416, 0.69769416,\n",
       "        0.69769416, 0.69769416, 0.69769416, 0.6819918 , 0.69472344,\n",
       "        0.69811855, 0.69769416, 0.69769416, 0.69769416, 0.69769416,\n",
       "        0.69769416, 0.69769416, 0.69769416, 0.69769416, 0.69769416]),\n",
       " 'split6_test_accuracy': array([0.59994341, 0.6819918 , 0.69755269, 0.69882586, 0.69995756,\n",
       "        0.69925025, 0.6998161 , 0.6998161 , 0.6998161 , 0.6998161 ,\n",
       "        0.6998161 , 0.6998161 , 0.6998161 , 0.6998161 , 0.6998161 ,\n",
       "        0.6998161 , 0.6998161 , 0.6998161 , 0.6819918 , 0.69882586,\n",
       "        0.69925025, 0.6998161 , 0.6998161 , 0.6998161 , 0.6998161 ,\n",
       "        0.6998161 , 0.6998161 , 0.6998161 , 0.6998161 , 0.6998161 ]),\n",
       " 'split7_test_accuracy': array([0.59994341, 0.68213326, 0.69543075, 0.69967464, 0.70066487,\n",
       "        0.70123073, 0.70080634, 0.70080634, 0.7009478 , 0.7009478 ,\n",
       "        0.7009478 , 0.7009478 , 0.7009478 , 0.7009478 , 0.7009478 ,\n",
       "        0.7009478 , 0.7009478 , 0.7009478 , 0.6819918 , 0.69967464,\n",
       "        0.70123073, 0.70080634, 0.7009478 , 0.7009478 , 0.7009478 ,\n",
       "        0.7009478 , 0.7009478 , 0.7009478 , 0.7009478 , 0.7009478 ]),\n",
       " 'split8_test_accuracy': array([0.59994341, 0.68298203, 0.70038195, 0.69910878, 0.70024049,\n",
       "        0.70052341, 0.70080634, 0.70066487, 0.70066487, 0.70066487,\n",
       "        0.70066487, 0.70066487, 0.70066487, 0.70066487, 0.70066487,\n",
       "        0.70066487, 0.70066487, 0.70066487, 0.68298203, 0.69910878,\n",
       "        0.70052341, 0.70066487, 0.70066487, 0.70066487, 0.70066487,\n",
       "        0.70066487, 0.70066487, 0.70066487, 0.70066487, 0.70066487]),\n",
       " 'split9_test_accuracy': array([0.59994341, 0.68255765, 0.7009478 , 0.70405998, 0.70236243,\n",
       "        0.70222096, 0.70222096, 0.70236243, 0.70236243, 0.70236243,\n",
       "        0.70236243, 0.70236243, 0.70236243, 0.70236243, 0.70236243,\n",
       "        0.70236243, 0.70236243, 0.70236243, 0.68255765, 0.70405998,\n",
       "        0.70222096, 0.70236243, 0.70236243, 0.70236243, 0.70236243,\n",
       "        0.70236243, 0.70236243, 0.70236243, 0.70236243, 0.70236243]),\n",
       " 'mean_test_accuracy': array([0.59996888, 0.68289198, 0.69989521, 0.70087129, 0.70146545,\n",
       "        0.70140887, 0.70160691, 0.70166349, 0.70167764, 0.70169178,\n",
       "        0.70166349, 0.70169178, 0.70167764, 0.70166349, 0.70169178,\n",
       "        0.70169178, 0.70167764, 0.70166349, 0.68287784, 0.70085715,\n",
       "        0.70140887, 0.70166349, 0.70167764, 0.70169178, 0.70169178,\n",
       "        0.70169178, 0.70169178, 0.70169178, 0.70169178, 0.70167764]),\n",
       " 'std_test_accuracy': array([4.46448828e-05, 2.00750509e-03, 3.16078969e-03, 3.58022609e-03,\n",
       "        2.80558715e-03, 2.87271360e-03, 2.97184729e-03, 3.04998424e-03,\n",
       "        3.01791976e-03, 3.03002402e-03, 3.04209833e-03, 3.02426977e-03,\n",
       "        3.01791976e-03, 3.04209833e-03, 3.02426977e-03, 3.02426977e-03,\n",
       "        3.01791976e-03, 3.04209833e-03, 2.01329183e-03, 3.59690500e-03,\n",
       "        2.87271360e-03, 3.04998424e-03, 3.05417229e-03, 3.02426977e-03,\n",
       "        3.02426977e-03, 3.02426977e-03, 3.02426977e-03, 3.02426977e-03,\n",
       "        3.02426977e-03, 3.05417229e-03]),\n",
       " 'rank_test_accuracy': array([30, 28, 27, 25, 22, 23, 21, 16, 11,  1, 18,  2, 11, 18,  2,  2, 11,\n",
       "        18, 29, 26, 23, 16, 11,  2,  2,  2,  2,  2,  2, 11]),\n",
       " 'split0_test_precision': array([0.        , 0.71354934, 0.68226601, 0.67299478, 0.66560364,\n",
       "        0.66409442, 0.66485753, 0.66485753, 0.66440831, 0.66440831,\n",
       "        0.66440831, 0.66455982, 0.66440831, 0.66440831, 0.66455982,\n",
       "        0.66455982, 0.66440831, 0.66440831, 0.71354934, 0.67299478,\n",
       "        0.66409442, 0.66485753, 0.66440831, 0.66455982, 0.66455982,\n",
       "        0.66455982, 0.66455982, 0.66455982, 0.66455982, 0.66440831]),\n",
       " 'split1_test_precision': array([0.        , 0.71325648, 0.68015795, 0.67367929, 0.66696751,\n",
       "        0.66681655, 0.66577061, 0.66577061, 0.66592029, 0.66592029,\n",
       "        0.66592029, 0.66592029, 0.66592029, 0.66592029, 0.66592029,\n",
       "        0.66592029, 0.66592029, 0.66592029, 0.71325648, 0.67367929,\n",
       "        0.66681655, 0.66577061, 0.66592029, 0.66592029, 0.66592029,\n",
       "        0.66592029, 0.66592029, 0.66592029, 0.66592029, 0.66592029]),\n",
       " 'split2_test_precision': array([0.        , 0.70688379, 0.67253521, 0.66316292, 0.65655172,\n",
       "        0.65551993, 0.65540849, 0.65540849, 0.65556569, 0.65556569,\n",
       "        0.65540849, 0.65556569, 0.65556569, 0.65540849, 0.65556569,\n",
       "        0.65556569, 0.65556569, 0.65540849, 0.70688379, 0.66300191,\n",
       "        0.65551993, 0.65540849, 0.65540849, 0.65556569, 0.65556569,\n",
       "        0.65556569, 0.65556569, 0.65556569, 0.65556569, 0.65540849]),\n",
       " 'split3_test_precision': array([0.        , 0.71237693, 0.66761769, 0.66421343, 0.66153169,\n",
       "        0.66052632, 0.66097988, 0.66127622, 0.66098733, 0.66127622,\n",
       "        0.66112812, 0.66098733, 0.66098733, 0.66098733, 0.66098733,\n",
       "        0.66098733, 0.66098733, 0.66098733, 0.71237693, 0.66421343,\n",
       "        0.66052632, 0.66127622, 0.66127622, 0.66098733, 0.66098733,\n",
       "        0.66098733, 0.66098733, 0.66098733, 0.66098733, 0.66127622]),\n",
       " 'split4_test_precision': array([0.        , 0.70596115, 0.67343086, 0.67168263, 0.66360455,\n",
       "        0.66332316, 0.66319896, 0.66349069, 0.66349069, 0.66349069,\n",
       "        0.66349069, 0.66349069, 0.66349069, 0.66349069, 0.66349069,\n",
       "        0.66349069, 0.66349069, 0.66349069, 0.70596115, 0.67168263,\n",
       "        0.66332316, 0.66349069, 0.66349069, 0.66349069, 0.66349069,\n",
       "        0.66349069, 0.66349069, 0.66349069, 0.66349069, 0.66349069]),\n",
       " 'split5_test_precision': array([0.        , 0.69808743, 0.66143604, 0.65282847, 0.65164114,\n",
       "        0.65152838, 0.65015211, 0.65015211, 0.65015211, 0.65015211,\n",
       "        0.65015211, 0.65015211, 0.65015211, 0.65015211, 0.65015211,\n",
       "        0.65015211, 0.65015211, 0.65015211, 0.69808743, 0.65282847,\n",
       "        0.65152838, 0.65015211, 0.65015211, 0.65015211, 0.65015211,\n",
       "        0.65015211, 0.65015211, 0.65015211, 0.65015211, 0.65015211]),\n",
       " 'split6_test_precision': array([0.        , 0.70893372, 0.6740666 , 0.66892218, 0.66403712,\n",
       "        0.66295265, 0.66312384, 0.66312384, 0.66312384, 0.66312384,\n",
       "        0.66312384, 0.66312384, 0.66312384, 0.66312384, 0.66312384,\n",
       "        0.66312384, 0.66312384, 0.66312384, 0.70893372, 0.66892218,\n",
       "        0.66295265, 0.66312384, 0.66312384, 0.66312384, 0.66312384,\n",
       "        0.66312384, 0.66312384, 0.66312384, 0.66312384, 0.66312384]),\n",
       " 'split7_test_precision': array([0.        , 0.69965636, 0.65987684, 0.66044606, 0.65627744,\n",
       "        0.65687993, 0.65547318, 0.65547318, 0.65562337, 0.65562337,\n",
       "        0.65562337, 0.65562337, 0.65562337, 0.65562337, 0.65562337,\n",
       "        0.65562337, 0.65562337, 0.65562337, 0.69917582, 0.66044606,\n",
       "        0.65687993, 0.65547318, 0.65562337, 0.65562337, 0.65562337,\n",
       "        0.65562337, 0.65562337, 0.65562337, 0.65562337, 0.65562337]),\n",
       " 'split8_test_precision': array([0.        , 0.7003413 , 0.66969407, 0.6608536 , 0.65596128,\n",
       "        0.6558527 , 0.6556089 , 0.65532286, 0.65532286, 0.65532286,\n",
       "        0.65532286, 0.65532286, 0.65532286, 0.65532286, 0.65532286,\n",
       "        0.65532286, 0.65532286, 0.65532286, 0.7003413 , 0.6608536 ,\n",
       "        0.6558527 , 0.65532286, 0.65532286, 0.65532286, 0.65532286,\n",
       "        0.65532286, 0.65532286, 0.65532286, 0.65532286, 0.65532286]),\n",
       " 'split9_test_precision': array([0.        , 0.69109948, 0.66512488, 0.66546763, 0.65496575,\n",
       "        0.65428937, 0.65376436, 0.65391156, 0.65391156, 0.65391156,\n",
       "        0.65391156, 0.65391156, 0.65391156, 0.65391156, 0.65391156,\n",
       "        0.65391156, 0.65391156, 0.65391156, 0.69109948, 0.66546763,\n",
       "        0.65428937, 0.65391156, 0.65391156, 0.65391156, 0.65391156,\n",
       "        0.65391156, 0.65391156, 0.65391156, 0.65391156, 0.65391156]),\n",
       " 'mean_test_precision': array([0.        , 0.7050146 , 0.67062062, 0.6654251 , 0.65971419,\n",
       "        0.65917834, 0.65883379, 0.65887871, 0.65885061, 0.65887949,\n",
       "        0.65884896, 0.65886576, 0.65885061, 0.65883489, 0.65886576,\n",
       "        0.65886576, 0.65885061, 0.65883489, 0.70496654, 0.665409  ,\n",
       "        0.65917834, 0.65887871, 0.65886377, 0.65886576, 0.65886576,\n",
       "        0.65886576, 0.65886576, 0.65886576, 0.65886576, 0.65886377]),\n",
       " 'std_test_precision': array([0.        , 0.00711962, 0.00699913, 0.00622712, 0.00498037,\n",
       "        0.00477391, 0.00510849, 0.00515164, 0.00508855, 0.00510141,\n",
       "        0.00510502, 0.00510528, 0.00508855, 0.00509891, 0.00510528,\n",
       "        0.00510528, 0.00508855, 0.00509891, 0.00715715, 0.00623315,\n",
       "        0.00477391, 0.00515164, 0.00511182, 0.00510528, 0.00510528,\n",
       "        0.00510528, 0.00510528, 0.00510528, 0.00510528, 0.00511182]),\n",
       " 'rank_test_precision': array([30,  1,  3,  4,  6,  7, 29, 10, 23,  9, 26, 12, 23, 27, 12, 12, 23,\n",
       "        27,  2,  5,  7, 10, 21, 12, 12, 12, 12, 12, 12, 21]),\n",
       " 'split0_test_f1_micro': array([0.6       , 0.68203678, 0.70466761, 0.70311174, 0.70282885,\n",
       "        0.70226308, 0.70311174, 0.70311174, 0.7029703 , 0.7029703 ,\n",
       "        0.7029703 , 0.70311174, 0.7029703 , 0.7029703 , 0.70311174,\n",
       "        0.70311174, 0.7029703 , 0.7029703 , 0.68203678, 0.70311174,\n",
       "        0.70226308, 0.70311174, 0.7029703 , 0.70311174, 0.70311174,\n",
       "        0.70311174, 0.70311174, 0.70311174, 0.70311174, 0.7029703 ]),\n",
       " 'split1_test_f1_micro': array([0.6       , 0.68373409, 0.70325318, 0.70509194, 0.70466761,\n",
       "        0.7049505 , 0.70466761, 0.70466761, 0.70480905, 0.70480905,\n",
       "        0.70480905, 0.70480905, 0.70480905, 0.70480905, 0.70480905,\n",
       "        0.70480905, 0.70480905, 0.70480905, 0.68373409, 0.70509194,\n",
       "        0.7049505 , 0.70466761, 0.70480905, 0.70480905, 0.70480905,\n",
       "        0.70480905, 0.70480905, 0.70480905, 0.70480905, 0.70480905]),\n",
       " 'split2_test_f1_micro': array([0.60008488, 0.67916254, 0.69712831, 0.69670392, 0.69642099,\n",
       "        0.69613807, 0.69642099, 0.69642099, 0.69656246, 0.69656246,\n",
       "        0.69642099, 0.69656246, 0.69656246, 0.69642099, 0.69656246,\n",
       "        0.69656246, 0.69656246, 0.69642099, 0.67916254, 0.69656246,\n",
       "        0.69613807, 0.69642099, 0.69642099, 0.69656246, 0.69656246,\n",
       "        0.69656246, 0.69656246, 0.69656246, 0.69656246, 0.69642099]),\n",
       " 'split3_test_f1_micro': array([0.59994341, 0.6853869 , 0.69967464, 0.7009478 , 0.70377705,\n",
       "        0.70349413, 0.70405998, 0.70434291, 0.70420144, 0.70434291,\n",
       "        0.70420144, 0.70420144, 0.70420144, 0.70420144, 0.70420144,\n",
       "        0.70420144, 0.70420144, 0.70420144, 0.6853869 , 0.7009478 ,\n",
       "        0.70349413, 0.70434291, 0.70434291, 0.70420144, 0.70420144,\n",
       "        0.70420144, 0.70420144, 0.70420144, 0.70420144, 0.70434291]),\n",
       " 'split4_test_f1_micro': array([0.59994341, 0.68694299, 0.70391852, 0.70646485, 0.70575753,\n",
       "        0.705899  , 0.70646485, 0.70674777, 0.70674777, 0.70674777,\n",
       "        0.70674777, 0.70674777, 0.70674777, 0.70674777, 0.70674777,\n",
       "        0.70674777, 0.70674777, 0.70674777, 0.68694299, 0.70646485,\n",
       "        0.705899  , 0.70674777, 0.70674777, 0.70674777, 0.70674777,\n",
       "        0.70674777, 0.70674777, 0.70674777, 0.70674777, 0.70674777]),\n",
       " 'split5_test_f1_micro': array([0.59994341, 0.6819918 , 0.6959966 , 0.69472344, 0.69797708,\n",
       "        0.69811855, 0.69769416, 0.69769416, 0.69769416, 0.69769416,\n",
       "        0.69769416, 0.69769416, 0.69769416, 0.69769416, 0.69769416,\n",
       "        0.69769416, 0.69769416, 0.69769416, 0.6819918 , 0.69472344,\n",
       "        0.69811855, 0.69769416, 0.69769416, 0.69769416, 0.69769416,\n",
       "        0.69769416, 0.69769416, 0.69769416, 0.69769416, 0.69769416]),\n",
       " 'split6_test_f1_micro': array([0.59994341, 0.6819918 , 0.69755269, 0.69882586, 0.69995756,\n",
       "        0.69925025, 0.6998161 , 0.6998161 , 0.6998161 , 0.6998161 ,\n",
       "        0.6998161 , 0.6998161 , 0.6998161 , 0.6998161 , 0.6998161 ,\n",
       "        0.6998161 , 0.6998161 , 0.6998161 , 0.6819918 , 0.69882586,\n",
       "        0.69925025, 0.6998161 , 0.6998161 , 0.6998161 , 0.6998161 ,\n",
       "        0.6998161 , 0.6998161 , 0.6998161 , 0.6998161 , 0.6998161 ]),\n",
       " 'split7_test_f1_micro': array([0.59994341, 0.68213326, 0.69543075, 0.69967464, 0.70066487,\n",
       "        0.70123073, 0.70080634, 0.70080634, 0.7009478 , 0.7009478 ,\n",
       "        0.7009478 , 0.7009478 , 0.7009478 , 0.7009478 , 0.7009478 ,\n",
       "        0.7009478 , 0.7009478 , 0.7009478 , 0.6819918 , 0.69967464,\n",
       "        0.70123073, 0.70080634, 0.7009478 , 0.7009478 , 0.7009478 ,\n",
       "        0.7009478 , 0.7009478 , 0.7009478 , 0.7009478 , 0.7009478 ]),\n",
       " 'split8_test_f1_micro': array([0.59994341, 0.68298203, 0.70038195, 0.69910878, 0.70024049,\n",
       "        0.70052341, 0.70080634, 0.70066487, 0.70066487, 0.70066487,\n",
       "        0.70066487, 0.70066487, 0.70066487, 0.70066487, 0.70066487,\n",
       "        0.70066487, 0.70066487, 0.70066487, 0.68298203, 0.69910878,\n",
       "        0.70052341, 0.70066487, 0.70066487, 0.70066487, 0.70066487,\n",
       "        0.70066487, 0.70066487, 0.70066487, 0.70066487, 0.70066487]),\n",
       " 'split9_test_f1_micro': array([0.59994341, 0.68255765, 0.7009478 , 0.70405998, 0.70236243,\n",
       "        0.70222096, 0.70222096, 0.70236243, 0.70236243, 0.70236243,\n",
       "        0.70236243, 0.70236243, 0.70236243, 0.70236243, 0.70236243,\n",
       "        0.70236243, 0.70236243, 0.70236243, 0.68255765, 0.70405998,\n",
       "        0.70222096, 0.70236243, 0.70236243, 0.70236243, 0.70236243,\n",
       "        0.70236243, 0.70236243, 0.70236243, 0.70236243, 0.70236243]),\n",
       " 'mean_test_f1_micro': array([0.59996888, 0.68289198, 0.69989521, 0.70087129, 0.70146545,\n",
       "        0.70140887, 0.70160691, 0.70166349, 0.70167764, 0.70169178,\n",
       "        0.70166349, 0.70169178, 0.70167764, 0.70166349, 0.70169178,\n",
       "        0.70169178, 0.70167764, 0.70166349, 0.68287784, 0.70085715,\n",
       "        0.70140887, 0.70166349, 0.70167764, 0.70169178, 0.70169178,\n",
       "        0.70169178, 0.70169178, 0.70169178, 0.70169178, 0.70167764]),\n",
       " 'std_test_f1_micro': array([4.46448828e-05, 2.00750509e-03, 3.16078969e-03, 3.58022609e-03,\n",
       "        2.80558715e-03, 2.87271360e-03, 2.97184729e-03, 3.04998424e-03,\n",
       "        3.01791976e-03, 3.03002402e-03, 3.04209833e-03, 3.02426977e-03,\n",
       "        3.01791976e-03, 3.04209833e-03, 3.02426977e-03, 3.02426977e-03,\n",
       "        3.01791976e-03, 3.04209833e-03, 2.01329183e-03, 3.59690500e-03,\n",
       "        2.87271360e-03, 3.04998424e-03, 3.05417229e-03, 3.02426977e-03,\n",
       "        3.02426977e-03, 3.02426977e-03, 3.02426977e-03, 3.02426977e-03,\n",
       "        3.02426977e-03, 3.05417229e-03]),\n",
       " 'rank_test_f1_micro': array([30, 28, 27, 25, 22, 23, 21, 16, 11,  1, 18,  2, 11, 18,  2,  2, 11,\n",
       "        18, 29, 26, 23, 16, 11,  2,  2,  2,  2,  2,  2, 11])}"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_model.cv_results_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "a76a5ef1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([30, 28, 27, 25, 22, 23, 21, 16, 11,  1, 18,  2, 11, 18,  2,  2, 11,\n",
       "       18, 29, 26, 23, 16, 11,  2,  2,  2,  2,  2,  2, 11])"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_model.cv_results_['rank_test_accuracy']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "80177251",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'classifier': LogisticRegression(max_iter=10000),\n",
       " 'classifier__C': 0.0001,\n",
       " 'classifier__penalty': 'l1',\n",
       " 'classifier__solver': 'saga'}"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_model.cv_results_['params'][ np.argmin(best_model.cv_results_['mean_test_accuracy'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "543c8047",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>classifier</th>\n",
       "      <th>C</th>\n",
       "      <th>penalty</th>\n",
       "      <th>solver</th>\n",
       "      <th>score_acc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.599969</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.682892</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.699895</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.700871</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0100</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.701465</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0100</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.701409</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.1000</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.701607</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.1000</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.701663</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.701678</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.701692</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>10.0000</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.701663</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>10.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.701692</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>100.0000</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.701678</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>100.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.701663</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>1000.0000</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.701692</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>1000.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.701692</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>10000.0000</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.701678</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>10000.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.701663</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.682878</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.700857</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0100</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.701409</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.1000</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.701663</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.701678</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>10.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.701692</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>100.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.701692</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>1000.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.701692</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>10000.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.701692</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>NaN</td>\n",
       "      <td>none</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.701692</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>NaN</td>\n",
       "      <td>none</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.701692</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.701678</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                            classifier           C penalty solver  score_acc\n",
       "0   LogisticRegression(max_iter=10000)      0.0001      l1   saga   0.599969\n",
       "1   LogisticRegression(max_iter=10000)      0.0001      l2   saga   0.682892\n",
       "2   LogisticRegression(max_iter=10000)      0.0010      l1   saga   0.699895\n",
       "3   LogisticRegression(max_iter=10000)      0.0010      l2   saga   0.700871\n",
       "4   LogisticRegression(max_iter=10000)      0.0100      l1   saga   0.701465\n",
       "5   LogisticRegression(max_iter=10000)      0.0100      l2   saga   0.701409\n",
       "6   LogisticRegression(max_iter=10000)      0.1000      l1   saga   0.701607\n",
       "7   LogisticRegression(max_iter=10000)      0.1000      l2   saga   0.701663\n",
       "8   LogisticRegression(max_iter=10000)      1.0000      l1   saga   0.701678\n",
       "9   LogisticRegression(max_iter=10000)      1.0000      l2   saga   0.701692\n",
       "10  LogisticRegression(max_iter=10000)     10.0000      l1   saga   0.701663\n",
       "11  LogisticRegression(max_iter=10000)     10.0000      l2   saga   0.701692\n",
       "12  LogisticRegression(max_iter=10000)    100.0000      l1   saga   0.701678\n",
       "13  LogisticRegression(max_iter=10000)    100.0000      l2   saga   0.701663\n",
       "14  LogisticRegression(max_iter=10000)   1000.0000      l1   saga   0.701692\n",
       "15  LogisticRegression(max_iter=10000)   1000.0000      l2   saga   0.701692\n",
       "16  LogisticRegression(max_iter=10000)  10000.0000      l1   saga   0.701678\n",
       "17  LogisticRegression(max_iter=10000)  10000.0000      l2   saga   0.701663\n",
       "18  LogisticRegression(max_iter=10000)      0.0001      l2  lbfgs   0.682878\n",
       "19  LogisticRegression(max_iter=10000)      0.0010      l2  lbfgs   0.700857\n",
       "20  LogisticRegression(max_iter=10000)      0.0100      l2  lbfgs   0.701409\n",
       "21  LogisticRegression(max_iter=10000)      0.1000      l2  lbfgs   0.701663\n",
       "22  LogisticRegression(max_iter=10000)      1.0000      l2  lbfgs   0.701678\n",
       "23  LogisticRegression(max_iter=10000)     10.0000      l2  lbfgs   0.701692\n",
       "24  LogisticRegression(max_iter=10000)    100.0000      l2  lbfgs   0.701692\n",
       "25  LogisticRegression(max_iter=10000)   1000.0000      l2  lbfgs   0.701692\n",
       "26  LogisticRegression(max_iter=10000)  10000.0000      l2  lbfgs   0.701692\n",
       "27  LogisticRegression(max_iter=10000)         NaN    none  lbfgs   0.701692\n",
       "28  LogisticRegression(max_iter=10000)         NaN    none   saga   0.701692\n",
       "29  LogisticRegression(max_iter=10000)         NaN     NaN    NaN   0.701678"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results = pd.DataFrame( best_model.cv_results_['params'] )\n",
    "\n",
    "results['score_acc'] = best_model.cv_results_['mean_test_accuracy']\n",
    "cols = results.columns.to_series().str.split('__').apply(lambda x: x[-1])\n",
    "results.columns = cols\n",
    "\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "557091b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "acc_list.append(results[0:1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "b765827e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'classifier': LogisticRegression(max_iter=10000),\n",
       " 'classifier__C': 0.0001,\n",
       " 'classifier__penalty': 'l2',\n",
       " 'classifier__solver': 'saga'}"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_model.cv_results_['params'][ np.argmin(best_model.cv_results_['rank_test_precision'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "3d3fe844",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>classifier</th>\n",
       "      <th>C</th>\n",
       "      <th>penalty</th>\n",
       "      <th>solver</th>\n",
       "      <th>score_precision</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.705015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.670621</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.665425</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0100</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.659714</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0100</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.659178</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.1000</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.658834</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.1000</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.658879</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.658851</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.658879</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>10.0000</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.658849</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>10.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.658866</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>100.0000</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.658851</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>100.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.658835</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>1000.0000</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.658866</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>1000.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.658866</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>10000.0000</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.658851</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>10000.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.658835</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.704967</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.665409</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0100</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.659178</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.1000</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.658879</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.658864</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>10.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.658866</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>100.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.658866</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>1000.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.658866</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>10000.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.658866</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>NaN</td>\n",
       "      <td>none</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.658866</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>NaN</td>\n",
       "      <td>none</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.658866</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.658864</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                            classifier           C penalty solver  \\\n",
       "0   LogisticRegression(max_iter=10000)      0.0001      l1   saga   \n",
       "1   LogisticRegression(max_iter=10000)      0.0001      l2   saga   \n",
       "2   LogisticRegression(max_iter=10000)      0.0010      l1   saga   \n",
       "3   LogisticRegression(max_iter=10000)      0.0010      l2   saga   \n",
       "4   LogisticRegression(max_iter=10000)      0.0100      l1   saga   \n",
       "5   LogisticRegression(max_iter=10000)      0.0100      l2   saga   \n",
       "6   LogisticRegression(max_iter=10000)      0.1000      l1   saga   \n",
       "7   LogisticRegression(max_iter=10000)      0.1000      l2   saga   \n",
       "8   LogisticRegression(max_iter=10000)      1.0000      l1   saga   \n",
       "9   LogisticRegression(max_iter=10000)      1.0000      l2   saga   \n",
       "10  LogisticRegression(max_iter=10000)     10.0000      l1   saga   \n",
       "11  LogisticRegression(max_iter=10000)     10.0000      l2   saga   \n",
       "12  LogisticRegression(max_iter=10000)    100.0000      l1   saga   \n",
       "13  LogisticRegression(max_iter=10000)    100.0000      l2   saga   \n",
       "14  LogisticRegression(max_iter=10000)   1000.0000      l1   saga   \n",
       "15  LogisticRegression(max_iter=10000)   1000.0000      l2   saga   \n",
       "16  LogisticRegression(max_iter=10000)  10000.0000      l1   saga   \n",
       "17  LogisticRegression(max_iter=10000)  10000.0000      l2   saga   \n",
       "18  LogisticRegression(max_iter=10000)      0.0001      l2  lbfgs   \n",
       "19  LogisticRegression(max_iter=10000)      0.0010      l2  lbfgs   \n",
       "20  LogisticRegression(max_iter=10000)      0.0100      l2  lbfgs   \n",
       "21  LogisticRegression(max_iter=10000)      0.1000      l2  lbfgs   \n",
       "22  LogisticRegression(max_iter=10000)      1.0000      l2  lbfgs   \n",
       "23  LogisticRegression(max_iter=10000)     10.0000      l2  lbfgs   \n",
       "24  LogisticRegression(max_iter=10000)    100.0000      l2  lbfgs   \n",
       "25  LogisticRegression(max_iter=10000)   1000.0000      l2  lbfgs   \n",
       "26  LogisticRegression(max_iter=10000)  10000.0000      l2  lbfgs   \n",
       "27  LogisticRegression(max_iter=10000)         NaN    none  lbfgs   \n",
       "28  LogisticRegression(max_iter=10000)         NaN    none   saga   \n",
       "29  LogisticRegression(max_iter=10000)         NaN     NaN    NaN   \n",
       "\n",
       "    score_precision  \n",
       "0          0.000000  \n",
       "1          0.705015  \n",
       "2          0.670621  \n",
       "3          0.665425  \n",
       "4          0.659714  \n",
       "5          0.659178  \n",
       "6          0.658834  \n",
       "7          0.658879  \n",
       "8          0.658851  \n",
       "9          0.658879  \n",
       "10         0.658849  \n",
       "11         0.658866  \n",
       "12         0.658851  \n",
       "13         0.658835  \n",
       "14         0.658866  \n",
       "15         0.658866  \n",
       "16         0.658851  \n",
       "17         0.658835  \n",
       "18         0.704967  \n",
       "19         0.665409  \n",
       "20         0.659178  \n",
       "21         0.658879  \n",
       "22         0.658864  \n",
       "23         0.658866  \n",
       "24         0.658866  \n",
       "25         0.658866  \n",
       "26         0.658866  \n",
       "27         0.658866  \n",
       "28         0.658866  \n",
       "29         0.658864  "
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results = pd.DataFrame( best_model.cv_results_['params'] )\n",
    "\n",
    "results['score_precision'] = best_model.cv_results_['mean_test_precision']\n",
    "cols = results.columns.to_series().str.split('__').apply(lambda x: x[-1])\n",
    "results.columns = cols\n",
    "\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "b27ecb6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "roc_auc_ovr_List.append(results[1:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "531a42ca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'classifier': LogisticRegression(max_iter=10000),\n",
       " 'classifier__C': 1.0,\n",
       " 'classifier__penalty': 'l2',\n",
       " 'classifier__solver': 'saga'}"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_model.cv_results_['params'][ np.argmin(best_model.cv_results_['rank_test_f1_micro'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "37066f63",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>classifier</th>\n",
       "      <th>C</th>\n",
       "      <th>penalty</th>\n",
       "      <th>solver</th>\n",
       "      <th>score_f1_micro</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.599969</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.682892</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.699895</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.700871</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0100</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.701465</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0100</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.701409</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.1000</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.701607</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.1000</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.701663</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.701678</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.701692</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>10.0000</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.701663</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>10.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.701692</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>100.0000</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.701678</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>100.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.701663</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>1000.0000</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.701692</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>1000.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.701692</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>10000.0000</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.701678</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>10000.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.701663</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.682878</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.700857</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0100</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.701409</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.1000</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.701663</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.701678</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>10.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.701692</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>100.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.701692</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>1000.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.701692</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>10000.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.701692</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>NaN</td>\n",
       "      <td>none</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.701692</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>NaN</td>\n",
       "      <td>none</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.701692</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.701678</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                            classifier           C penalty solver  \\\n",
       "0   LogisticRegression(max_iter=10000)      0.0001      l1   saga   \n",
       "1   LogisticRegression(max_iter=10000)      0.0001      l2   saga   \n",
       "2   LogisticRegression(max_iter=10000)      0.0010      l1   saga   \n",
       "3   LogisticRegression(max_iter=10000)      0.0010      l2   saga   \n",
       "4   LogisticRegression(max_iter=10000)      0.0100      l1   saga   \n",
       "5   LogisticRegression(max_iter=10000)      0.0100      l2   saga   \n",
       "6   LogisticRegression(max_iter=10000)      0.1000      l1   saga   \n",
       "7   LogisticRegression(max_iter=10000)      0.1000      l2   saga   \n",
       "8   LogisticRegression(max_iter=10000)      1.0000      l1   saga   \n",
       "9   LogisticRegression(max_iter=10000)      1.0000      l2   saga   \n",
       "10  LogisticRegression(max_iter=10000)     10.0000      l1   saga   \n",
       "11  LogisticRegression(max_iter=10000)     10.0000      l2   saga   \n",
       "12  LogisticRegression(max_iter=10000)    100.0000      l1   saga   \n",
       "13  LogisticRegression(max_iter=10000)    100.0000      l2   saga   \n",
       "14  LogisticRegression(max_iter=10000)   1000.0000      l1   saga   \n",
       "15  LogisticRegression(max_iter=10000)   1000.0000      l2   saga   \n",
       "16  LogisticRegression(max_iter=10000)  10000.0000      l1   saga   \n",
       "17  LogisticRegression(max_iter=10000)  10000.0000      l2   saga   \n",
       "18  LogisticRegression(max_iter=10000)      0.0001      l2  lbfgs   \n",
       "19  LogisticRegression(max_iter=10000)      0.0010      l2  lbfgs   \n",
       "20  LogisticRegression(max_iter=10000)      0.0100      l2  lbfgs   \n",
       "21  LogisticRegression(max_iter=10000)      0.1000      l2  lbfgs   \n",
       "22  LogisticRegression(max_iter=10000)      1.0000      l2  lbfgs   \n",
       "23  LogisticRegression(max_iter=10000)     10.0000      l2  lbfgs   \n",
       "24  LogisticRegression(max_iter=10000)    100.0000      l2  lbfgs   \n",
       "25  LogisticRegression(max_iter=10000)   1000.0000      l2  lbfgs   \n",
       "26  LogisticRegression(max_iter=10000)  10000.0000      l2  lbfgs   \n",
       "27  LogisticRegression(max_iter=10000)         NaN    none  lbfgs   \n",
       "28  LogisticRegression(max_iter=10000)         NaN    none   saga   \n",
       "29  LogisticRegression(max_iter=10000)         NaN     NaN    NaN   \n",
       "\n",
       "    score_f1_micro  \n",
       "0         0.599969  \n",
       "1         0.682892  \n",
       "2         0.699895  \n",
       "3         0.700871  \n",
       "4         0.701465  \n",
       "5         0.701409  \n",
       "6         0.701607  \n",
       "7         0.701663  \n",
       "8         0.701678  \n",
       "9         0.701692  \n",
       "10        0.701663  \n",
       "11        0.701692  \n",
       "12        0.701678  \n",
       "13        0.701663  \n",
       "14        0.701692  \n",
       "15        0.701692  \n",
       "16        0.701678  \n",
       "17        0.701663  \n",
       "18        0.682878  \n",
       "19        0.700857  \n",
       "20        0.701409  \n",
       "21        0.701663  \n",
       "22        0.701678  \n",
       "23        0.701692  \n",
       "24        0.701692  \n",
       "25        0.701692  \n",
       "26        0.701692  \n",
       "27        0.701692  \n",
       "28        0.701692  \n",
       "29        0.701678  "
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results = pd.DataFrame( best_model.cv_results_['params'] )\n",
    "\n",
    "results['score_f1_micro'] = best_model.cv_results_['mean_test_f1_micro']\n",
    "cols = results.columns.to_series().str.split('__').apply(lambda x: x[-1])\n",
    "results.columns = cols\n",
    "\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "56f4f911",
   "metadata": {},
   "outputs": [],
   "source": [
    "f1_micro_List.append(results[9:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b7bf1d0",
   "metadata": {},
   "source": [
    "## Trial 9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "b5b093d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, Y_train, Y_test = train_test_split(X, y, test_size=0.2, shuffle=True, random_state = 9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "90fdec31",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ENT\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\ENT\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\ENT\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\ENT\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\ENT\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\ENT\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\ENT\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\ENT\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\ENT\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\ENT\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\ENT\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1183: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\ENT\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1183: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\ENT\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1183: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\ENT\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1183: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\ENT\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1183: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\ENT\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1183: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\ENT\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1183: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\ENT\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1183: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\ENT\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1183: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\ENT\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1183: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\ENT\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1183: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\ENT\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1183: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\ENT\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1183: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\ENT\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1183: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\ENT\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1183: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\ENT\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1183: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\ENT\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1183: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\ENT\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1183: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\ENT\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1183: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\ENT\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1183: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Create a pipeline - RF is a stand in, we will populate the classifier part below\n",
    "pipe = Pipeline([('std', StandardScaler()),\n",
    "                 ('classifier', LogisticRegression())])\n",
    "\n",
    "# Create search space of candidate learning algorithms and their hyperparameters\n",
    "# note lbfgs can't do l1, and if you pass penalty='none' it expects no C value\n",
    "search_space = [{'classifier': [LogisticRegression(max_iter=10000)],\n",
    "                 'classifier__solver': ['saga'],\n",
    "                 'classifier__penalty': ['l1', 'l2'],\n",
    "                 'classifier__C': np.logspace(-4, 4, 9)},\n",
    "                {'classifier': [LogisticRegression(max_iter=10000)],\n",
    "                 'classifier__solver': ['lbfgs'],\n",
    "                 'classifier__penalty': ['l2'],\n",
    "                 'classifier__C': np.logspace(-4, 4, 9)},\n",
    "                {'classifier': [LogisticRegression(max_iter=10000)],\n",
    "                 'classifier__solver': ['lbfgs','saga'],\n",
    "                 'classifier__penalty': ['none']},\n",
    "                {'classifier': [LogisticRegression(max_iter=10000)]}\n",
    "                ]\n",
    "\n",
    "# Create grid search\n",
    "clf = GridSearchCV(pipe, search_space, cv=StratifiedKFold(n_splits=10),\n",
    "                   scoring=['accuracy', 'precision', 'f1_micro'], refit=False,\n",
    "                   verbose=0)\n",
    "\n",
    "# Fit grid search\n",
    "best_model = clf.fit(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "079fae53",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'mean_fit_time': array([0.04809718, 0.10023692, 0.113309  , 0.10107071, 0.11666787,\n",
       "        0.10162554, 0.11393485, 0.10478768, 0.11857188, 0.10603652,\n",
       "        0.1204525 , 0.10413423, 0.11036119, 0.10189722, 0.1181031 ,\n",
       "        0.10527678, 0.1179261 , 0.10408208, 0.02292995, 0.02170815,\n",
       "        0.02179921, 0.02157373, 0.02168822, 0.02213433, 0.02208335,\n",
       "        0.02194805, 0.02187631, 0.02217865, 0.10934405, 0.02152536]),\n",
       " 'std_fit_time': array([0.00781227, 0.00855313, 0.00706955, 0.00841118, 0.00780082,\n",
       "        0.0045712 , 0.00651535, 0.00799121, 0.01541881, 0.00830545,\n",
       "        0.00797919, 0.00526679, 0.00687355, 0.0052886 , 0.00941067,\n",
       "        0.00723202, 0.00582025, 0.00526648, 0.00063922, 0.00076796,\n",
       "        0.00060905, 0.00039376, 0.00092781, 0.00095655, 0.00087093,\n",
       "        0.00068533, 0.00060121, 0.00070549, 0.01021588, 0.00045808]),\n",
       " 'mean_score_time': array([0.00740767, 0.00770922, 0.00803773, 0.0078552 , 0.00786266,\n",
       "        0.00770655, 0.00765858, 0.00800626, 0.00781729, 0.00820777,\n",
       "        0.00820367, 0.00749576, 0.00790703, 0.00794609, 0.00790703,\n",
       "        0.00790699, 0.00793183, 0.00790689, 0.00729184, 0.00726497,\n",
       "        0.00761223, 0.00775077, 0.00775754, 0.00759835, 0.00743845,\n",
       "        0.00720649, 0.00755887, 0.00750687, 0.00780268, 0.00740311]),\n",
       " 'std_score_time': array([4.91606738e-04, 4.60245930e-04, 9.14974623e-05, 3.18967839e-04,\n",
       "        6.23232853e-04, 4.58974060e-04, 6.34693313e-04, 8.20551193e-04,\n",
       "        6.04898115e-04, 6.00743549e-04, 4.02333935e-04, 5.12126996e-04,\n",
       "        3.00288314e-04, 3.34685001e-04, 3.00367732e-04, 5.37894413e-04,\n",
       "        3.33329335e-04, 8.31586720e-04, 4.02963891e-04, 4.01982806e-04,\n",
       "        4.95130540e-04, 4.37354972e-04, 4.03499581e-04, 4.39051018e-04,\n",
       "        5.36029111e-04, 4.00459812e-04, 4.71983572e-04, 5.00702931e-04,\n",
       "        3.98400924e-04, 4.85961267e-04]),\n",
       " 'param_classifier': masked_array(data=[LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000)],\n",
       "              mask=[False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'param_classifier__C': masked_array(data=[0.0001, 0.0001, 0.001, 0.001, 0.01, 0.01, 0.1, 0.1,\n",
       "                    1.0, 1.0, 10.0, 10.0, 100.0, 100.0, 1000.0, 1000.0,\n",
       "                    10000.0, 10000.0, 0.0001, 0.001, 0.01, 0.1, 1.0, 10.0,\n",
       "                    100.0, 1000.0, 10000.0, --, --, --],\n",
       "              mask=[False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False,  True,  True,  True],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'param_classifier__penalty': masked_array(data=['l1', 'l2', 'l1', 'l2', 'l1', 'l2', 'l1', 'l2', 'l1',\n",
       "                    'l2', 'l1', 'l2', 'l1', 'l2', 'l1', 'l2', 'l1', 'l2',\n",
       "                    'l2', 'l2', 'l2', 'l2', 'l2', 'l2', 'l2', 'l2', 'l2',\n",
       "                    'none', 'none', --],\n",
       "              mask=[False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False,  True],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'param_classifier__solver': masked_array(data=['saga', 'saga', 'saga', 'saga', 'saga', 'saga', 'saga',\n",
       "                    'saga', 'saga', 'saga', 'saga', 'saga', 'saga', 'saga',\n",
       "                    'saga', 'saga', 'saga', 'saga', 'lbfgs', 'lbfgs',\n",
       "                    'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs',\n",
       "                    'lbfgs', 'lbfgs', 'saga', --],\n",
       "              mask=[False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False,  True],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'params': [{'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 0.0001,\n",
       "   'classifier__penalty': 'l1',\n",
       "   'classifier__solver': 'saga'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 0.0001,\n",
       "   'classifier__penalty': 'l2',\n",
       "   'classifier__solver': 'saga'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 0.001,\n",
       "   'classifier__penalty': 'l1',\n",
       "   'classifier__solver': 'saga'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 0.001,\n",
       "   'classifier__penalty': 'l2',\n",
       "   'classifier__solver': 'saga'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 0.01,\n",
       "   'classifier__penalty': 'l1',\n",
       "   'classifier__solver': 'saga'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 0.01,\n",
       "   'classifier__penalty': 'l2',\n",
       "   'classifier__solver': 'saga'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 0.1,\n",
       "   'classifier__penalty': 'l1',\n",
       "   'classifier__solver': 'saga'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 0.1,\n",
       "   'classifier__penalty': 'l2',\n",
       "   'classifier__solver': 'saga'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 1.0,\n",
       "   'classifier__penalty': 'l1',\n",
       "   'classifier__solver': 'saga'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 1.0,\n",
       "   'classifier__penalty': 'l2',\n",
       "   'classifier__solver': 'saga'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 10.0,\n",
       "   'classifier__penalty': 'l1',\n",
       "   'classifier__solver': 'saga'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 10.0,\n",
       "   'classifier__penalty': 'l2',\n",
       "   'classifier__solver': 'saga'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 100.0,\n",
       "   'classifier__penalty': 'l1',\n",
       "   'classifier__solver': 'saga'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 100.0,\n",
       "   'classifier__penalty': 'l2',\n",
       "   'classifier__solver': 'saga'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 1000.0,\n",
       "   'classifier__penalty': 'l1',\n",
       "   'classifier__solver': 'saga'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 1000.0,\n",
       "   'classifier__penalty': 'l2',\n",
       "   'classifier__solver': 'saga'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 10000.0,\n",
       "   'classifier__penalty': 'l1',\n",
       "   'classifier__solver': 'saga'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 10000.0,\n",
       "   'classifier__penalty': 'l2',\n",
       "   'classifier__solver': 'saga'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 0.0001,\n",
       "   'classifier__penalty': 'l2',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 0.001,\n",
       "   'classifier__penalty': 'l2',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 0.01,\n",
       "   'classifier__penalty': 'l2',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 0.1,\n",
       "   'classifier__penalty': 'l2',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 1.0,\n",
       "   'classifier__penalty': 'l2',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 10.0,\n",
       "   'classifier__penalty': 'l2',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 100.0,\n",
       "   'classifier__penalty': 'l2',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 1000.0,\n",
       "   'classifier__penalty': 'l2',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 10000.0,\n",
       "   'classifier__penalty': 'l2',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__penalty': 'none',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__penalty': 'none',\n",
       "   'classifier__solver': 'saga'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000)}],\n",
       " 'split0_test_accuracy': array([0.60014144, 0.68359264, 0.70353607, 0.70353607, 0.70381895,\n",
       "        0.70353607, 0.70353607, 0.70339463, 0.70339463, 0.70339463,\n",
       "        0.70339463, 0.70339463, 0.70339463, 0.70339463, 0.70339463,\n",
       "        0.70339463, 0.70339463, 0.70339463, 0.68359264, 0.70353607,\n",
       "        0.70353607, 0.70339463, 0.70339463, 0.70339463, 0.70339463,\n",
       "        0.70339463, 0.70339463, 0.70339463, 0.70339463, 0.70339463]),\n",
       " 'split1_test_accuracy': array([0.60014144, 0.68826025, 0.69943423, 0.69844413, 0.70226308,\n",
       "        0.7019802 , 0.7019802 , 0.70212164, 0.70212164, 0.70212164,\n",
       "        0.70212164, 0.70212164, 0.70212164, 0.70212164, 0.70212164,\n",
       "        0.70212164, 0.70212164, 0.70226308, 0.68826025, 0.69844413,\n",
       "        0.7019802 , 0.70212164, 0.70212164, 0.70212164, 0.70212164,\n",
       "        0.70212164, 0.70212164, 0.70212164, 0.70212164, 0.70212164]),\n",
       " 'split2_test_accuracy': array([0.60008488, 0.67845523, 0.68666007, 0.68878201, 0.69118687,\n",
       "        0.69076248, 0.69062102, 0.69047956, 0.69062102, 0.69062102,\n",
       "        0.69062102, 0.69062102, 0.69062102, 0.69062102, 0.69062102,\n",
       "        0.69062102, 0.69062102, 0.69062102, 0.67845523, 0.68878201,\n",
       "        0.69076248, 0.69047956, 0.69062102, 0.69062102, 0.69062102,\n",
       "        0.69062102, 0.69062102, 0.69062102, 0.69062102, 0.69062102]),\n",
       " 'split3_test_accuracy': array([0.60008488, 0.67562597, 0.68835762, 0.69062102, 0.69189419,\n",
       "        0.69217711, 0.69217711, 0.69217711, 0.69203565, 0.69203565,\n",
       "        0.69203565, 0.69203565, 0.69203565, 0.69203565, 0.69203565,\n",
       "        0.69203565, 0.69203565, 0.69203565, 0.67562597, 0.69062102,\n",
       "        0.69217711, 0.69217711, 0.69203565, 0.69203565, 0.69203565,\n",
       "        0.69203565, 0.69203565, 0.69203565, 0.69203565, 0.69203565]),\n",
       " 'split4_test_accuracy': array([0.60008488, 0.68934786, 0.71070873, 0.71254774, 0.71268921,\n",
       "        0.71339652, 0.71382091, 0.71410383, 0.7142453 , 0.7142453 ,\n",
       "        0.7142453 , 0.7142453 , 0.7142453 , 0.7142453 , 0.7142453 ,\n",
       "        0.7142453 , 0.7142453 , 0.7142453 , 0.68934786, 0.71254774,\n",
       "        0.71339652, 0.71410383, 0.7142453 , 0.7142453 , 0.7142453 ,\n",
       "        0.7142453 , 0.7142453 , 0.7142453 , 0.7142453 , 0.7142453 ]),\n",
       " 'split5_test_accuracy': array([0.60008488, 0.68708445, 0.70137219, 0.70363559, 0.70462583,\n",
       "        0.70476729, 0.70448437, 0.70448437, 0.70448437, 0.70448437,\n",
       "        0.70448437, 0.70448437, 0.70448437, 0.70448437, 0.70448437,\n",
       "        0.70448437, 0.70448437, 0.70448437, 0.68708445, 0.70363559,\n",
       "        0.70476729, 0.70448437, 0.70448437, 0.70448437, 0.70448437,\n",
       "        0.70448437, 0.70448437, 0.70448437, 0.70448437, 0.70448437]),\n",
       " 'split6_test_accuracy': array([0.60008488, 0.68368935, 0.70193804, 0.70066487, 0.70278682,\n",
       "        0.70278682, 0.70335267, 0.70335267, 0.70335267, 0.70335267,\n",
       "        0.70335267, 0.70335267, 0.70335267, 0.70335267, 0.70335267,\n",
       "        0.70335267, 0.70335267, 0.70335267, 0.68368935, 0.70066487,\n",
       "        0.70278682, 0.70335267, 0.70335267, 0.70335267, 0.70335267,\n",
       "        0.70335267, 0.70335267, 0.70335267, 0.70335267, 0.70335267]),\n",
       " 'split7_test_accuracy': array([0.60008488, 0.68340642, 0.70066487, 0.70151365, 0.70066487,\n",
       "        0.70009902, 0.70066487, 0.70066487, 0.70080634, 0.70080634,\n",
       "        0.70080634, 0.70080634, 0.70080634, 0.70080634, 0.70080634,\n",
       "        0.70080634, 0.70080634, 0.70080634, 0.68340642, 0.70151365,\n",
       "        0.70009902, 0.70066487, 0.70080634, 0.70080634, 0.70080634,\n",
       "        0.70080634, 0.70080634, 0.70080634, 0.70080634, 0.70080634]),\n",
       " 'split8_test_accuracy': array([0.60008488, 0.68029424, 0.69811855, 0.70222096, 0.70278682,\n",
       "        0.70292828, 0.70250389, 0.70264535, 0.70250389, 0.70264535,\n",
       "        0.70264535, 0.70264535, 0.70264535, 0.70264535, 0.70264535,\n",
       "        0.70264535, 0.70264535, 0.70264535, 0.68029424, 0.70222096,\n",
       "        0.70292828, 0.70264535, 0.70264535, 0.70264535, 0.70264535,\n",
       "        0.70264535, 0.70264535, 0.70264535, 0.70250389, 0.70264535]),\n",
       " 'split9_test_accuracy': array([0.60022634, 0.6842552 , 0.70038195, 0.69826001, 0.69925025,\n",
       "        0.69967464, 0.6998161 , 0.6998161 , 0.6998161 , 0.6998161 ,\n",
       "        0.69953317, 0.69953317, 0.69953317, 0.69953317, 0.69939171,\n",
       "        0.69953317, 0.69967464, 0.69953317, 0.6842552 , 0.69826001,\n",
       "        0.69967464, 0.69967464, 0.6998161 , 0.69967464, 0.69967464,\n",
       "        0.69967464, 0.69967464, 0.69967464, 0.69953317, 0.6998161 ]),\n",
       " 'mean_test_accuracy': array([0.60011034, 0.68340116, 0.69911723, 0.70002261, 0.70119669,\n",
       "        0.70121084, 0.70129572, 0.70132401, 0.70133816, 0.70135231,\n",
       "        0.70132401, 0.70132401, 0.70132401, 0.70132401, 0.70130987,\n",
       "        0.70132401, 0.70133816, 0.70133816, 0.68340116, 0.70002261,\n",
       "        0.70121084, 0.70130987, 0.70135231, 0.70133816, 0.70133816,\n",
       "        0.70133816, 0.70133816, 0.70133816, 0.70130987, 0.70135231]),\n",
       " 'std_test_accuracy': array([4.46420938e-05, 4.09827415e-03, 6.65562585e-03, 6.40378702e-03,\n",
       "        5.89671010e-03, 6.05758620e-03, 6.14745192e-03, 6.22955015e-03,\n",
       "        6.25096902e-03, 6.25375057e-03, 6.26127198e-03, 6.26127198e-03,\n",
       "        6.26127198e-03, 6.26127198e-03, 6.26546050e-03, 6.26127198e-03,\n",
       "        6.25736849e-03, 6.26321731e-03, 4.09827415e-03, 6.40378702e-03,\n",
       "        6.05758620e-03, 6.23311791e-03, 6.25375057e-03, 6.25736849e-03,\n",
       "        6.25736849e-03, 6.25736849e-03, 6.25736849e-03, 6.25736849e-03,\n",
       "        6.25842982e-03, 6.25375057e-03]),\n",
       " 'rank_test_accuracy': array([30, 28, 27, 25, 24, 22, 21, 12,  4,  1, 13, 13, 13, 13, 19, 13,  4,\n",
       "        11, 28, 25, 22, 18,  1,  4,  4,  4,  4,  4, 19,  1]),\n",
       " 'split0_test_precision': array([0.        , 0.71041369, 0.6815698 , 0.67346939, 0.66651522,\n",
       "        0.66591012, 0.66516042, 0.66485998, 0.66485998, 0.66485998,\n",
       "        0.66485998, 0.66485998, 0.66485998, 0.66485998, 0.66485998,\n",
       "        0.66485998, 0.66485998, 0.66485998, 0.71041369, 0.67346939,\n",
       "        0.66591012, 0.66485998, 0.66485998, 0.66485998, 0.66485998,\n",
       "        0.66485998, 0.66485998, 0.66485998, 0.66485998, 0.66485998]),\n",
       " 'split1_test_precision': array([0.        , 0.71767994, 0.66762178, 0.66110338, 0.66101695,\n",
       "        0.66014235, 0.65957447, 0.65958389, 0.65958389, 0.65958389,\n",
       "        0.65958389, 0.65958389, 0.65958389, 0.65958389, 0.65958389,\n",
       "        0.65958389, 0.65958389, 0.65973451, 0.71767994, 0.66110338,\n",
       "        0.66014235, 0.65958389, 0.65958389, 0.65958389, 0.65958389,\n",
       "        0.65958389, 0.65958389, 0.65958389, 0.65958389, 0.65958389]),\n",
       " 'split2_test_precision': array([0.        , 0.69424965, 0.64883268, 0.64656381, 0.64426523,\n",
       "        0.64340045, 0.6426025 , 0.64231626, 0.64247551, 0.64247551,\n",
       "        0.64247551, 0.64247551, 0.64247551, 0.64247551, 0.64247551,\n",
       "        0.64247551, 0.64247551, 0.64247551, 0.69424965, 0.64656381,\n",
       "        0.64340045, 0.64231626, 0.64247551, 0.64247551, 0.64247551,\n",
       "        0.64247551, 0.64247551, 0.64247551, 0.64247551, 0.64247551]),\n",
       " 'split3_test_precision': array([0.        , 0.68963068, 0.64956855, 0.64760148, 0.64493077,\n",
       "        0.64498886, 0.64447403, 0.64447403, 0.64418811, 0.64418811,\n",
       "        0.64418811, 0.64418811, 0.64418811, 0.64418811, 0.64418811,\n",
       "        0.64418811, 0.64418811, 0.64418811, 0.68963068, 0.64760148,\n",
       "        0.64498886, 0.64447403, 0.64418811, 0.64418811, 0.64418811,\n",
       "        0.64418811, 0.64418811, 0.64418811, 0.64418811, 0.64418811]),\n",
       " 'split4_test_precision': array([0.        , 0.72455516, 0.69261084, 0.68812115, 0.68058076,\n",
       "        0.68146806, 0.68140794, 0.68185921, 0.68183867, 0.68183867,\n",
       "        0.68183867, 0.68183867, 0.68183867, 0.68183867, 0.68183867,\n",
       "        0.68183867, 0.68183867, 0.68183867, 0.72455516, 0.68812115,\n",
       "        0.68146806, 0.68185921, 0.68183867, 0.68183867, 0.68183867,\n",
       "        0.68183867, 0.68183867, 0.68183867, 0.68183867, 0.68183867]),\n",
       " 'split5_test_precision': array([0.        , 0.71339348, 0.67145594, 0.66850829, 0.66444148,\n",
       "        0.66415262, 0.66298587, 0.66298587, 0.66298587, 0.66298587,\n",
       "        0.66298587, 0.66298587, 0.66298587, 0.66298587, 0.66298587,\n",
       "        0.66298587, 0.66298587, 0.66298587, 0.71339348, 0.66850829,\n",
       "        0.66415262, 0.66298587, 0.66298587, 0.66298587, 0.66298587,\n",
       "        0.66298587, 0.66298587, 0.66298587, 0.66298587, 0.66298587]),\n",
       " 'split6_test_precision': array([0.        , 0.70253598, 0.67159199, 0.66405168, 0.66119005,\n",
       "        0.66076174, 0.66079295, 0.66079295, 0.66079295, 0.66079295,\n",
       "        0.66079295, 0.66079295, 0.66079295, 0.66079295, 0.66079295,\n",
       "        0.66079295, 0.66079295, 0.66079295, 0.70253598, 0.66405168,\n",
       "        0.66076174, 0.66079295, 0.66079295, 0.66079295, 0.66079295,\n",
       "        0.66079295, 0.66079295, 0.66079295, 0.66079295, 0.66079295]),\n",
       " 'split7_test_precision': array([0.        , 0.69490404, 0.66888361, 0.66302865, 0.65463245,\n",
       "        0.65336226, 0.6536965 , 0.6536965 , 0.65384615, 0.65384615,\n",
       "        0.65384615, 0.65384615, 0.65384615, 0.65384615, 0.65384615,\n",
       "        0.65384615, 0.65384615, 0.65384615, 0.69490404, 0.66302865,\n",
       "        0.65336226, 0.6536965 , 0.65384615, 0.65384615, 0.65384615,\n",
       "        0.65384615, 0.65384615, 0.65384615, 0.65384615, 0.65384615]),\n",
       " 'split8_test_precision': array([0.        , 0.6959226 , 0.66367501, 0.66484018, 0.65755208,\n",
       "        0.65770065, 0.65603448, 0.65618268, 0.65590009, 0.65604821,\n",
       "        0.65604821, 0.65604821, 0.65604821, 0.65604821, 0.65604821,\n",
       "        0.65604821, 0.65604821, 0.65604821, 0.6959226 , 0.66484018,\n",
       "        0.65770065, 0.65618268, 0.65604821, 0.65604821, 0.65604821,\n",
       "        0.65604821, 0.65604821, 0.65604821, 0.65590009, 0.65604821]),\n",
       " 'split9_test_precision': array([0.        , 0.71305595, 0.67472853, 0.66321244, 0.65837104,\n",
       "        0.65854759, 0.65827338, 0.65813118, 0.65798923, 0.65798923,\n",
       "        0.6573991 , 0.6573991 , 0.6573991 , 0.6573991 , 0.65710444,\n",
       "        0.6573991 , 0.65769403, 0.6573991 , 0.71305595, 0.66321244,\n",
       "        0.65854759, 0.65783565, 0.65798923, 0.65769403, 0.65769403,\n",
       "        0.65769403, 0.65769403, 0.65769403, 0.6573991 , 0.65798923]),\n",
       " 'mean_test_precision': array([0.        , 0.70563412, 0.66905387, 0.66405005, 0.6593496 ,\n",
       "        0.65904347, 0.65850025, 0.65848825, 0.65844604, 0.65846086,\n",
       "        0.65840184, 0.65840184, 0.65840184, 0.65840184, 0.65837238,\n",
       "        0.65840184, 0.65843134, 0.65841691, 0.70563412, 0.66405005,\n",
       "        0.65904347, 0.6584587 , 0.65846086, 0.65843134, 0.65843134,\n",
       "        0.65843134, 0.65843134, 0.65843134, 0.65838703, 0.65846086]),\n",
       " 'std_test_precision': array([0.        , 0.01116246, 0.01255221, 0.01129333, 0.0099804 ,\n",
       "        0.01028026, 0.010365  , 0.0104875 , 0.01049744, 0.01049394,\n",
       "        0.01049808, 0.01049808, 0.01049808, 0.01049808, 0.01050127,\n",
       "        0.01049808, 0.01049564, 0.01049988, 0.01116246, 0.01129333,\n",
       "        0.01028026, 0.01048888, 0.01049394, 0.01049564, 0.01049564,\n",
       "        0.01049564, 0.01049564, 0.01049564, 0.0105015 , 0.01049394]),\n",
       " 'rank_test_precision': array([30,  1,  3,  4,  6,  7,  9, 10, 15, 11, 23, 23, 23, 23, 29, 23, 16,\n",
       "        22,  1,  4,  7, 14, 11, 16, 16, 16, 16, 16, 28, 11]),\n",
       " 'split0_test_f1_micro': array([0.60014144, 0.68359264, 0.70353607, 0.70353607, 0.70381895,\n",
       "        0.70353607, 0.70353607, 0.70339463, 0.70339463, 0.70339463,\n",
       "        0.70339463, 0.70339463, 0.70339463, 0.70339463, 0.70339463,\n",
       "        0.70339463, 0.70339463, 0.70339463, 0.68359264, 0.70353607,\n",
       "        0.70353607, 0.70339463, 0.70339463, 0.70339463, 0.70339463,\n",
       "        0.70339463, 0.70339463, 0.70339463, 0.70339463, 0.70339463]),\n",
       " 'split1_test_f1_micro': array([0.60014144, 0.68826025, 0.69943423, 0.69844413, 0.70226308,\n",
       "        0.7019802 , 0.7019802 , 0.70212164, 0.70212164, 0.70212164,\n",
       "        0.70212164, 0.70212164, 0.70212164, 0.70212164, 0.70212164,\n",
       "        0.70212164, 0.70212164, 0.70226308, 0.68826025, 0.69844413,\n",
       "        0.7019802 , 0.70212164, 0.70212164, 0.70212164, 0.70212164,\n",
       "        0.70212164, 0.70212164, 0.70212164, 0.70212164, 0.70212164]),\n",
       " 'split2_test_f1_micro': array([0.60008488, 0.67845523, 0.68666007, 0.68878201, 0.69118687,\n",
       "        0.69076248, 0.69062102, 0.69047956, 0.69062102, 0.69062102,\n",
       "        0.69062102, 0.69062102, 0.69062102, 0.69062102, 0.69062102,\n",
       "        0.69062102, 0.69062102, 0.69062102, 0.67845523, 0.68878201,\n",
       "        0.69076248, 0.69047956, 0.69062102, 0.69062102, 0.69062102,\n",
       "        0.69062102, 0.69062102, 0.69062102, 0.69062102, 0.69062102]),\n",
       " 'split3_test_f1_micro': array([0.60008488, 0.67562597, 0.68835762, 0.69062102, 0.69189419,\n",
       "        0.69217711, 0.69217711, 0.69217711, 0.69203565, 0.69203565,\n",
       "        0.69203565, 0.69203565, 0.69203565, 0.69203565, 0.69203565,\n",
       "        0.69203565, 0.69203565, 0.69203565, 0.67562597, 0.69062102,\n",
       "        0.69217711, 0.69217711, 0.69203565, 0.69203565, 0.69203565,\n",
       "        0.69203565, 0.69203565, 0.69203565, 0.69203565, 0.69203565]),\n",
       " 'split4_test_f1_micro': array([0.60008488, 0.68934786, 0.71070873, 0.71254774, 0.71268921,\n",
       "        0.71339652, 0.71382091, 0.71410383, 0.7142453 , 0.7142453 ,\n",
       "        0.7142453 , 0.7142453 , 0.7142453 , 0.7142453 , 0.7142453 ,\n",
       "        0.7142453 , 0.7142453 , 0.7142453 , 0.68934786, 0.71254774,\n",
       "        0.71339652, 0.71410383, 0.7142453 , 0.7142453 , 0.7142453 ,\n",
       "        0.7142453 , 0.7142453 , 0.7142453 , 0.7142453 , 0.7142453 ]),\n",
       " 'split5_test_f1_micro': array([0.60008488, 0.68708445, 0.70137219, 0.70363559, 0.70462583,\n",
       "        0.70476729, 0.70448437, 0.70448437, 0.70448437, 0.70448437,\n",
       "        0.70448437, 0.70448437, 0.70448437, 0.70448437, 0.70448437,\n",
       "        0.70448437, 0.70448437, 0.70448437, 0.68708445, 0.70363559,\n",
       "        0.70476729, 0.70448437, 0.70448437, 0.70448437, 0.70448437,\n",
       "        0.70448437, 0.70448437, 0.70448437, 0.70448437, 0.70448437]),\n",
       " 'split6_test_f1_micro': array([0.60008488, 0.68368935, 0.70193804, 0.70066487, 0.70278682,\n",
       "        0.70278682, 0.70335267, 0.70335267, 0.70335267, 0.70335267,\n",
       "        0.70335267, 0.70335267, 0.70335267, 0.70335267, 0.70335267,\n",
       "        0.70335267, 0.70335267, 0.70335267, 0.68368935, 0.70066487,\n",
       "        0.70278682, 0.70335267, 0.70335267, 0.70335267, 0.70335267,\n",
       "        0.70335267, 0.70335267, 0.70335267, 0.70335267, 0.70335267]),\n",
       " 'split7_test_f1_micro': array([0.60008488, 0.68340642, 0.70066487, 0.70151365, 0.70066487,\n",
       "        0.70009902, 0.70066487, 0.70066487, 0.70080634, 0.70080634,\n",
       "        0.70080634, 0.70080634, 0.70080634, 0.70080634, 0.70080634,\n",
       "        0.70080634, 0.70080634, 0.70080634, 0.68340642, 0.70151365,\n",
       "        0.70009902, 0.70066487, 0.70080634, 0.70080634, 0.70080634,\n",
       "        0.70080634, 0.70080634, 0.70080634, 0.70080634, 0.70080634]),\n",
       " 'split8_test_f1_micro': array([0.60008488, 0.68029424, 0.69811855, 0.70222096, 0.70278682,\n",
       "        0.70292828, 0.70250389, 0.70264535, 0.70250389, 0.70264535,\n",
       "        0.70264535, 0.70264535, 0.70264535, 0.70264535, 0.70264535,\n",
       "        0.70264535, 0.70264535, 0.70264535, 0.68029424, 0.70222096,\n",
       "        0.70292828, 0.70264535, 0.70264535, 0.70264535, 0.70264535,\n",
       "        0.70264535, 0.70264535, 0.70264535, 0.70250389, 0.70264535]),\n",
       " 'split9_test_f1_micro': array([0.60022634, 0.6842552 , 0.70038195, 0.69826001, 0.69925025,\n",
       "        0.69967464, 0.6998161 , 0.6998161 , 0.6998161 , 0.6998161 ,\n",
       "        0.69953317, 0.69953317, 0.69953317, 0.69953317, 0.69939171,\n",
       "        0.69953317, 0.69967464, 0.69953317, 0.6842552 , 0.69826001,\n",
       "        0.69967464, 0.69967464, 0.6998161 , 0.69967464, 0.69967464,\n",
       "        0.69967464, 0.69967464, 0.69967464, 0.69953317, 0.6998161 ]),\n",
       " 'mean_test_f1_micro': array([0.60011034, 0.68340116, 0.69911723, 0.70002261, 0.70119669,\n",
       "        0.70121084, 0.70129572, 0.70132401, 0.70133816, 0.70135231,\n",
       "        0.70132401, 0.70132401, 0.70132401, 0.70132401, 0.70130987,\n",
       "        0.70132401, 0.70133816, 0.70133816, 0.68340116, 0.70002261,\n",
       "        0.70121084, 0.70130987, 0.70135231, 0.70133816, 0.70133816,\n",
       "        0.70133816, 0.70133816, 0.70133816, 0.70130987, 0.70135231]),\n",
       " 'std_test_f1_micro': array([4.46420938e-05, 4.09827415e-03, 6.65562585e-03, 6.40378702e-03,\n",
       "        5.89671010e-03, 6.05758620e-03, 6.14745192e-03, 6.22955015e-03,\n",
       "        6.25096902e-03, 6.25375057e-03, 6.26127198e-03, 6.26127198e-03,\n",
       "        6.26127198e-03, 6.26127198e-03, 6.26546050e-03, 6.26127198e-03,\n",
       "        6.25736849e-03, 6.26321731e-03, 4.09827415e-03, 6.40378702e-03,\n",
       "        6.05758620e-03, 6.23311791e-03, 6.25375057e-03, 6.25736849e-03,\n",
       "        6.25736849e-03, 6.25736849e-03, 6.25736849e-03, 6.25736849e-03,\n",
       "        6.25842982e-03, 6.25375057e-03]),\n",
       " 'rank_test_f1_micro': array([30, 28, 27, 25, 24, 22, 21, 12,  4,  1, 13, 13, 13, 13, 19, 13,  4,\n",
       "        11, 28, 25, 22, 18,  1,  4,  4,  4,  4,  4, 19,  1])}"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_model.cv_results_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "32aa74f3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([30, 28, 27, 25, 24, 22, 21, 12,  4,  1, 13, 13, 13, 13, 19, 13,  4,\n",
       "       11, 28, 25, 22, 18,  1,  4,  4,  4,  4,  4, 19,  1])"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_model.cv_results_['rank_test_accuracy']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "b234d790",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'classifier': LogisticRegression(max_iter=10000),\n",
       " 'classifier__C': 0.0001,\n",
       " 'classifier__penalty': 'l1',\n",
       " 'classifier__solver': 'saga'}"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_model.cv_results_['params'][ np.argmin(best_model.cv_results_['mean_test_accuracy'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "ae51b2e8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>classifier</th>\n",
       "      <th>C</th>\n",
       "      <th>penalty</th>\n",
       "      <th>solver</th>\n",
       "      <th>score_acc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.600110</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.683401</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.699117</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.700023</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0100</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.701197</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0100</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.701211</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.1000</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.701296</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.1000</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.701324</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.701338</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.701352</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>10.0000</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.701324</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>10.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.701324</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>100.0000</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.701324</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>100.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.701324</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>1000.0000</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.701310</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>1000.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.701324</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>10000.0000</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.701338</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>10000.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.701338</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.683401</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.700023</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0100</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.701211</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.1000</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.701310</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.701352</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>10.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.701338</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>100.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.701338</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>1000.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.701338</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>10000.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.701338</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>NaN</td>\n",
       "      <td>none</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.701338</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>NaN</td>\n",
       "      <td>none</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.701310</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.701352</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                            classifier           C penalty solver  score_acc\n",
       "0   LogisticRegression(max_iter=10000)      0.0001      l1   saga   0.600110\n",
       "1   LogisticRegression(max_iter=10000)      0.0001      l2   saga   0.683401\n",
       "2   LogisticRegression(max_iter=10000)      0.0010      l1   saga   0.699117\n",
       "3   LogisticRegression(max_iter=10000)      0.0010      l2   saga   0.700023\n",
       "4   LogisticRegression(max_iter=10000)      0.0100      l1   saga   0.701197\n",
       "5   LogisticRegression(max_iter=10000)      0.0100      l2   saga   0.701211\n",
       "6   LogisticRegression(max_iter=10000)      0.1000      l1   saga   0.701296\n",
       "7   LogisticRegression(max_iter=10000)      0.1000      l2   saga   0.701324\n",
       "8   LogisticRegression(max_iter=10000)      1.0000      l1   saga   0.701338\n",
       "9   LogisticRegression(max_iter=10000)      1.0000      l2   saga   0.701352\n",
       "10  LogisticRegression(max_iter=10000)     10.0000      l1   saga   0.701324\n",
       "11  LogisticRegression(max_iter=10000)     10.0000      l2   saga   0.701324\n",
       "12  LogisticRegression(max_iter=10000)    100.0000      l1   saga   0.701324\n",
       "13  LogisticRegression(max_iter=10000)    100.0000      l2   saga   0.701324\n",
       "14  LogisticRegression(max_iter=10000)   1000.0000      l1   saga   0.701310\n",
       "15  LogisticRegression(max_iter=10000)   1000.0000      l2   saga   0.701324\n",
       "16  LogisticRegression(max_iter=10000)  10000.0000      l1   saga   0.701338\n",
       "17  LogisticRegression(max_iter=10000)  10000.0000      l2   saga   0.701338\n",
       "18  LogisticRegression(max_iter=10000)      0.0001      l2  lbfgs   0.683401\n",
       "19  LogisticRegression(max_iter=10000)      0.0010      l2  lbfgs   0.700023\n",
       "20  LogisticRegression(max_iter=10000)      0.0100      l2  lbfgs   0.701211\n",
       "21  LogisticRegression(max_iter=10000)      0.1000      l2  lbfgs   0.701310\n",
       "22  LogisticRegression(max_iter=10000)      1.0000      l2  lbfgs   0.701352\n",
       "23  LogisticRegression(max_iter=10000)     10.0000      l2  lbfgs   0.701338\n",
       "24  LogisticRegression(max_iter=10000)    100.0000      l2  lbfgs   0.701338\n",
       "25  LogisticRegression(max_iter=10000)   1000.0000      l2  lbfgs   0.701338\n",
       "26  LogisticRegression(max_iter=10000)  10000.0000      l2  lbfgs   0.701338\n",
       "27  LogisticRegression(max_iter=10000)         NaN    none  lbfgs   0.701338\n",
       "28  LogisticRegression(max_iter=10000)         NaN    none   saga   0.701310\n",
       "29  LogisticRegression(max_iter=10000)         NaN     NaN    NaN   0.701352"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results = pd.DataFrame( best_model.cv_results_['params'] )\n",
    "\n",
    "results['score_acc'] = best_model.cv_results_['mean_test_accuracy']\n",
    "cols = results.columns.to_series().str.split('__').apply(lambda x: x[-1])\n",
    "results.columns = cols\n",
    "\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "2ed188a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "acc_list.append(results[0:1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "fb96018d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'classifier': LogisticRegression(max_iter=10000),\n",
       " 'classifier__C': 0.0001,\n",
       " 'classifier__penalty': 'l2',\n",
       " 'classifier__solver': 'saga'}"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_model.cv_results_['params'][ np.argmin(best_model.cv_results_['rank_test_precision'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "c299c812",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>classifier</th>\n",
       "      <th>C</th>\n",
       "      <th>penalty</th>\n",
       "      <th>solver</th>\n",
       "      <th>score_precision</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.705634</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.669054</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.664050</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0100</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.659350</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0100</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.659043</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.1000</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.658500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.1000</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.658488</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.658446</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.658461</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>10.0000</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.658402</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>10.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.658402</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>100.0000</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.658402</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>100.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.658402</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>1000.0000</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.658372</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>1000.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.658402</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>10000.0000</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.658431</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>10000.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.658417</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.705634</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.664050</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0100</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.659043</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.1000</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.658459</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.658461</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>10.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.658431</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>100.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.658431</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>1000.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.658431</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>10000.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.658431</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>NaN</td>\n",
       "      <td>none</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.658431</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>NaN</td>\n",
       "      <td>none</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.658387</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.658461</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                            classifier           C penalty solver  \\\n",
       "0   LogisticRegression(max_iter=10000)      0.0001      l1   saga   \n",
       "1   LogisticRegression(max_iter=10000)      0.0001      l2   saga   \n",
       "2   LogisticRegression(max_iter=10000)      0.0010      l1   saga   \n",
       "3   LogisticRegression(max_iter=10000)      0.0010      l2   saga   \n",
       "4   LogisticRegression(max_iter=10000)      0.0100      l1   saga   \n",
       "5   LogisticRegression(max_iter=10000)      0.0100      l2   saga   \n",
       "6   LogisticRegression(max_iter=10000)      0.1000      l1   saga   \n",
       "7   LogisticRegression(max_iter=10000)      0.1000      l2   saga   \n",
       "8   LogisticRegression(max_iter=10000)      1.0000      l1   saga   \n",
       "9   LogisticRegression(max_iter=10000)      1.0000      l2   saga   \n",
       "10  LogisticRegression(max_iter=10000)     10.0000      l1   saga   \n",
       "11  LogisticRegression(max_iter=10000)     10.0000      l2   saga   \n",
       "12  LogisticRegression(max_iter=10000)    100.0000      l1   saga   \n",
       "13  LogisticRegression(max_iter=10000)    100.0000      l2   saga   \n",
       "14  LogisticRegression(max_iter=10000)   1000.0000      l1   saga   \n",
       "15  LogisticRegression(max_iter=10000)   1000.0000      l2   saga   \n",
       "16  LogisticRegression(max_iter=10000)  10000.0000      l1   saga   \n",
       "17  LogisticRegression(max_iter=10000)  10000.0000      l2   saga   \n",
       "18  LogisticRegression(max_iter=10000)      0.0001      l2  lbfgs   \n",
       "19  LogisticRegression(max_iter=10000)      0.0010      l2  lbfgs   \n",
       "20  LogisticRegression(max_iter=10000)      0.0100      l2  lbfgs   \n",
       "21  LogisticRegression(max_iter=10000)      0.1000      l2  lbfgs   \n",
       "22  LogisticRegression(max_iter=10000)      1.0000      l2  lbfgs   \n",
       "23  LogisticRegression(max_iter=10000)     10.0000      l2  lbfgs   \n",
       "24  LogisticRegression(max_iter=10000)    100.0000      l2  lbfgs   \n",
       "25  LogisticRegression(max_iter=10000)   1000.0000      l2  lbfgs   \n",
       "26  LogisticRegression(max_iter=10000)  10000.0000      l2  lbfgs   \n",
       "27  LogisticRegression(max_iter=10000)         NaN    none  lbfgs   \n",
       "28  LogisticRegression(max_iter=10000)         NaN    none   saga   \n",
       "29  LogisticRegression(max_iter=10000)         NaN     NaN    NaN   \n",
       "\n",
       "    score_precision  \n",
       "0          0.000000  \n",
       "1          0.705634  \n",
       "2          0.669054  \n",
       "3          0.664050  \n",
       "4          0.659350  \n",
       "5          0.659043  \n",
       "6          0.658500  \n",
       "7          0.658488  \n",
       "8          0.658446  \n",
       "9          0.658461  \n",
       "10         0.658402  \n",
       "11         0.658402  \n",
       "12         0.658402  \n",
       "13         0.658402  \n",
       "14         0.658372  \n",
       "15         0.658402  \n",
       "16         0.658431  \n",
       "17         0.658417  \n",
       "18         0.705634  \n",
       "19         0.664050  \n",
       "20         0.659043  \n",
       "21         0.658459  \n",
       "22         0.658461  \n",
       "23         0.658431  \n",
       "24         0.658431  \n",
       "25         0.658431  \n",
       "26         0.658431  \n",
       "27         0.658431  \n",
       "28         0.658387  \n",
       "29         0.658461  "
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results = pd.DataFrame( best_model.cv_results_['params'] )\n",
    "\n",
    "results['score_precision'] = best_model.cv_results_['mean_test_precision']\n",
    "cols = results.columns.to_series().str.split('__').apply(lambda x: x[-1])\n",
    "results.columns = cols\n",
    "\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "950c010d",
   "metadata": {},
   "outputs": [],
   "source": [
    "roc_auc_ovr_List.append(results[1:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "a902208d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'classifier': LogisticRegression(max_iter=10000),\n",
       " 'classifier__C': 1.0,\n",
       " 'classifier__penalty': 'l2',\n",
       " 'classifier__solver': 'saga'}"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_model.cv_results_['params'][ np.argmin(best_model.cv_results_['rank_test_f1_micro'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "a3a9405e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>classifier</th>\n",
       "      <th>C</th>\n",
       "      <th>penalty</th>\n",
       "      <th>solver</th>\n",
       "      <th>score_f1_micro</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.600110</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.683401</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.699117</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.700023</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0100</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.701197</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0100</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.701211</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.1000</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.701296</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.1000</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.701324</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.701338</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.701352</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>10.0000</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.701324</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>10.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.701324</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>100.0000</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.701324</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>100.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.701324</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>1000.0000</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.701310</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>1000.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.701324</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>10000.0000</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.701338</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>10000.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.701338</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.683401</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.700023</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0100</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.701211</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.1000</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.701310</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.701352</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>10.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.701338</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>100.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.701338</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>1000.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.701338</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>10000.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.701338</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>NaN</td>\n",
       "      <td>none</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.701338</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>NaN</td>\n",
       "      <td>none</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.701310</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.701352</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                            classifier           C penalty solver  \\\n",
       "0   LogisticRegression(max_iter=10000)      0.0001      l1   saga   \n",
       "1   LogisticRegression(max_iter=10000)      0.0001      l2   saga   \n",
       "2   LogisticRegression(max_iter=10000)      0.0010      l1   saga   \n",
       "3   LogisticRegression(max_iter=10000)      0.0010      l2   saga   \n",
       "4   LogisticRegression(max_iter=10000)      0.0100      l1   saga   \n",
       "5   LogisticRegression(max_iter=10000)      0.0100      l2   saga   \n",
       "6   LogisticRegression(max_iter=10000)      0.1000      l1   saga   \n",
       "7   LogisticRegression(max_iter=10000)      0.1000      l2   saga   \n",
       "8   LogisticRegression(max_iter=10000)      1.0000      l1   saga   \n",
       "9   LogisticRegression(max_iter=10000)      1.0000      l2   saga   \n",
       "10  LogisticRegression(max_iter=10000)     10.0000      l1   saga   \n",
       "11  LogisticRegression(max_iter=10000)     10.0000      l2   saga   \n",
       "12  LogisticRegression(max_iter=10000)    100.0000      l1   saga   \n",
       "13  LogisticRegression(max_iter=10000)    100.0000      l2   saga   \n",
       "14  LogisticRegression(max_iter=10000)   1000.0000      l1   saga   \n",
       "15  LogisticRegression(max_iter=10000)   1000.0000      l2   saga   \n",
       "16  LogisticRegression(max_iter=10000)  10000.0000      l1   saga   \n",
       "17  LogisticRegression(max_iter=10000)  10000.0000      l2   saga   \n",
       "18  LogisticRegression(max_iter=10000)      0.0001      l2  lbfgs   \n",
       "19  LogisticRegression(max_iter=10000)      0.0010      l2  lbfgs   \n",
       "20  LogisticRegression(max_iter=10000)      0.0100      l2  lbfgs   \n",
       "21  LogisticRegression(max_iter=10000)      0.1000      l2  lbfgs   \n",
       "22  LogisticRegression(max_iter=10000)      1.0000      l2  lbfgs   \n",
       "23  LogisticRegression(max_iter=10000)     10.0000      l2  lbfgs   \n",
       "24  LogisticRegression(max_iter=10000)    100.0000      l2  lbfgs   \n",
       "25  LogisticRegression(max_iter=10000)   1000.0000      l2  lbfgs   \n",
       "26  LogisticRegression(max_iter=10000)  10000.0000      l2  lbfgs   \n",
       "27  LogisticRegression(max_iter=10000)         NaN    none  lbfgs   \n",
       "28  LogisticRegression(max_iter=10000)         NaN    none   saga   \n",
       "29  LogisticRegression(max_iter=10000)         NaN     NaN    NaN   \n",
       "\n",
       "    score_f1_micro  \n",
       "0         0.600110  \n",
       "1         0.683401  \n",
       "2         0.699117  \n",
       "3         0.700023  \n",
       "4         0.701197  \n",
       "5         0.701211  \n",
       "6         0.701296  \n",
       "7         0.701324  \n",
       "8         0.701338  \n",
       "9         0.701352  \n",
       "10        0.701324  \n",
       "11        0.701324  \n",
       "12        0.701324  \n",
       "13        0.701324  \n",
       "14        0.701310  \n",
       "15        0.701324  \n",
       "16        0.701338  \n",
       "17        0.701338  \n",
       "18        0.683401  \n",
       "19        0.700023  \n",
       "20        0.701211  \n",
       "21        0.701310  \n",
       "22        0.701352  \n",
       "23        0.701338  \n",
       "24        0.701338  \n",
       "25        0.701338  \n",
       "26        0.701338  \n",
       "27        0.701338  \n",
       "28        0.701310  \n",
       "29        0.701352  "
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results = pd.DataFrame( best_model.cv_results_['params'] )\n",
    "\n",
    "results['score_f1_micro'] = best_model.cv_results_['mean_test_f1_micro']\n",
    "cols = results.columns.to_series().str.split('__').apply(lambda x: x[-1])\n",
    "results.columns = cols\n",
    "\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "40b8e54c",
   "metadata": {},
   "outputs": [],
   "source": [
    "f1_micro_List.append(results[9:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e446b35",
   "metadata": {},
   "source": [
    "## Trial 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "6b8df2fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, Y_train, Y_test = train_test_split(X, y, test_size=0.2, shuffle=True, random_state = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "8cf50a94",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ENT\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\ENT\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\ENT\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\ENT\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\ENT\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\ENT\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\ENT\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\ENT\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\ENT\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\ENT\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\ENT\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1183: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\ENT\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1183: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\ENT\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1183: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\ENT\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1183: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\ENT\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1183: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\ENT\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1183: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\ENT\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1183: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\ENT\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1183: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\ENT\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1183: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\ENT\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1183: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\ENT\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1183: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\ENT\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1183: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\ENT\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1183: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\ENT\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1183: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\ENT\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1183: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\ENT\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1183: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\ENT\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1183: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\ENT\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1183: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\ENT\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1183: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\ENT\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1183: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Create a pipeline - RF is a stand in, we will populate the classifier part below\n",
    "pipe = Pipeline([('std', StandardScaler()),\n",
    "                 ('classifier', LogisticRegression())])\n",
    "\n",
    "# Create search space of candidate learning algorithms and their hyperparameters\n",
    "# note lbfgs can't do l1, and if you pass penalty='none' it expects no C value\n",
    "search_space = [{'classifier': [LogisticRegression(max_iter=10000)],\n",
    "                 'classifier__solver': ['saga'],\n",
    "                 'classifier__penalty': ['l1', 'l2'],\n",
    "                 'classifier__C': np.logspace(-4, 4, 9)},\n",
    "                {'classifier': [LogisticRegression(max_iter=10000)],\n",
    "                 'classifier__solver': ['lbfgs'],\n",
    "                 'classifier__penalty': ['l2'],\n",
    "                 'classifier__C': np.logspace(-4, 4, 9)},\n",
    "                {'classifier': [LogisticRegression(max_iter=10000)],\n",
    "                 'classifier__solver': ['lbfgs','saga'],\n",
    "                 'classifier__penalty': ['none']},\n",
    "                {'classifier': [LogisticRegression(max_iter=10000)]}\n",
    "                ]\n",
    "\n",
    "# Create grid search\n",
    "clf = GridSearchCV(pipe, search_space, cv=StratifiedKFold(n_splits=10),\n",
    "                   scoring=['accuracy', 'precision', 'f1_micro'], refit=False,\n",
    "                   verbose=0)\n",
    "\n",
    "# Fit grid search\n",
    "best_model = clf.fit(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "a4120924",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'mean_fit_time': array([0.05068355, 0.10720069, 0.11325662, 0.09956424, 0.11455624,\n",
       "        0.09883525, 0.11641784, 0.09921119, 0.11348932, 0.09970057,\n",
       "        0.1185461 , 0.09866321, 0.12117403, 0.10150585, 0.12009635,\n",
       "        0.10695384, 0.1215899 , 0.10464509, 0.02264526, 0.02177875,\n",
       "        0.02112539, 0.02155454, 0.02171004, 0.02151945, 0.02172308,\n",
       "        0.02152593, 0.02165306, 0.02176452, 0.09978416, 0.0219713 ]),\n",
       " 'std_fit_time': array([0.00498382, 0.0059571 , 0.00700139, 0.00751154, 0.00936902,\n",
       "        0.00396015, 0.00624293, 0.00233418, 0.00866296, 0.00765867,\n",
       "        0.00747655, 0.00526311, 0.01195136, 0.00560096, 0.00624616,\n",
       "        0.01026857, 0.00570143, 0.00581748, 0.00073239, 0.00096278,\n",
       "        0.0005982 , 0.00045425, 0.00061283, 0.00067146, 0.00063634,\n",
       "        0.00076445, 0.00059246, 0.00065624, 0.00872789, 0.000807  ]),\n",
       " 'mean_score_time': array([0.00710642, 0.00780687, 0.00770671, 0.00775456, 0.00775728,\n",
       "        0.00730643, 0.00760648, 0.00760658, 0.00770683, 0.00749931,\n",
       "        0.0078429 , 0.00750632, 0.00792375, 0.00774314, 0.00760665,\n",
       "        0.0076426 , 0.00785844, 0.00760612, 0.00710652, 0.00715888,\n",
       "        0.00763829, 0.00737052, 0.00753343, 0.00800722, 0.00731151,\n",
       "        0.00770707, 0.00740681, 0.00753865, 0.00776508, 0.00752029]),\n",
       " 'std_score_time': array([0.00030013, 0.00087252, 0.00045898, 0.00060159, 0.00040328,\n",
       "        0.00045882, 0.00066396, 0.00049018, 0.00045885, 0.00050213,\n",
       "        0.00043215, 0.00050039, 0.0003117 , 0.00049446, 0.00049039,\n",
       "        0.00046872, 0.00031858, 0.00049108, 0.00030026, 0.00031465,\n",
       "        0.00056912, 0.00043431, 0.00045282, 0.00044771, 0.00045581,\n",
       "        0.00045864, 0.00049039, 0.00053943, 0.0003991 , 0.00051515]),\n",
       " 'param_classifier': masked_array(data=[LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000)],\n",
       "              mask=[False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'param_classifier__C': masked_array(data=[0.0001, 0.0001, 0.001, 0.001, 0.01, 0.01, 0.1, 0.1,\n",
       "                    1.0, 1.0, 10.0, 10.0, 100.0, 100.0, 1000.0, 1000.0,\n",
       "                    10000.0, 10000.0, 0.0001, 0.001, 0.01, 0.1, 1.0, 10.0,\n",
       "                    100.0, 1000.0, 10000.0, --, --, --],\n",
       "              mask=[False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False,  True,  True,  True],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'param_classifier__penalty': masked_array(data=['l1', 'l2', 'l1', 'l2', 'l1', 'l2', 'l1', 'l2', 'l1',\n",
       "                    'l2', 'l1', 'l2', 'l1', 'l2', 'l1', 'l2', 'l1', 'l2',\n",
       "                    'l2', 'l2', 'l2', 'l2', 'l2', 'l2', 'l2', 'l2', 'l2',\n",
       "                    'none', 'none', --],\n",
       "              mask=[False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False,  True],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'param_classifier__solver': masked_array(data=['saga', 'saga', 'saga', 'saga', 'saga', 'saga', 'saga',\n",
       "                    'saga', 'saga', 'saga', 'saga', 'saga', 'saga', 'saga',\n",
       "                    'saga', 'saga', 'saga', 'saga', 'lbfgs', 'lbfgs',\n",
       "                    'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs',\n",
       "                    'lbfgs', 'lbfgs', 'saga', --],\n",
       "              mask=[False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False,  True],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'params': [{'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 0.0001,\n",
       "   'classifier__penalty': 'l1',\n",
       "   'classifier__solver': 'saga'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 0.0001,\n",
       "   'classifier__penalty': 'l2',\n",
       "   'classifier__solver': 'saga'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 0.001,\n",
       "   'classifier__penalty': 'l1',\n",
       "   'classifier__solver': 'saga'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 0.001,\n",
       "   'classifier__penalty': 'l2',\n",
       "   'classifier__solver': 'saga'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 0.01,\n",
       "   'classifier__penalty': 'l1',\n",
       "   'classifier__solver': 'saga'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 0.01,\n",
       "   'classifier__penalty': 'l2',\n",
       "   'classifier__solver': 'saga'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 0.1,\n",
       "   'classifier__penalty': 'l1',\n",
       "   'classifier__solver': 'saga'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 0.1,\n",
       "   'classifier__penalty': 'l2',\n",
       "   'classifier__solver': 'saga'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 1.0,\n",
       "   'classifier__penalty': 'l1',\n",
       "   'classifier__solver': 'saga'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 1.0,\n",
       "   'classifier__penalty': 'l2',\n",
       "   'classifier__solver': 'saga'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 10.0,\n",
       "   'classifier__penalty': 'l1',\n",
       "   'classifier__solver': 'saga'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 10.0,\n",
       "   'classifier__penalty': 'l2',\n",
       "   'classifier__solver': 'saga'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 100.0,\n",
       "   'classifier__penalty': 'l1',\n",
       "   'classifier__solver': 'saga'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 100.0,\n",
       "   'classifier__penalty': 'l2',\n",
       "   'classifier__solver': 'saga'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 1000.0,\n",
       "   'classifier__penalty': 'l1',\n",
       "   'classifier__solver': 'saga'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 1000.0,\n",
       "   'classifier__penalty': 'l2',\n",
       "   'classifier__solver': 'saga'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 10000.0,\n",
       "   'classifier__penalty': 'l1',\n",
       "   'classifier__solver': 'saga'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 10000.0,\n",
       "   'classifier__penalty': 'l2',\n",
       "   'classifier__solver': 'saga'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 0.0001,\n",
       "   'classifier__penalty': 'l2',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 0.001,\n",
       "   'classifier__penalty': 'l2',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 0.01,\n",
       "   'classifier__penalty': 'l2',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 0.1,\n",
       "   'classifier__penalty': 'l2',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 1.0,\n",
       "   'classifier__penalty': 'l2',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 10.0,\n",
       "   'classifier__penalty': 'l2',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 100.0,\n",
       "   'classifier__penalty': 'l2',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 1000.0,\n",
       "   'classifier__penalty': 'l2',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 10000.0,\n",
       "   'classifier__penalty': 'l2',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__penalty': 'none',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__penalty': 'none',\n",
       "   'classifier__solver': 'saga'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000)}],\n",
       " 'split0_test_accuracy': array([0.60042433, 0.68217822, 0.7009901 , 0.70438472, 0.70466761,\n",
       "        0.70509194, 0.70523338, 0.70537482, 0.70523338, 0.70523338,\n",
       "        0.70523338, 0.70523338, 0.70523338, 0.70523338, 0.70523338,\n",
       "        0.70523338, 0.70523338, 0.70523338, 0.68217822, 0.70438472,\n",
       "        0.70509194, 0.70537482, 0.70523338, 0.70523338, 0.70523338,\n",
       "        0.70523338, 0.70523338, 0.70523338, 0.70523338, 0.70523338]),\n",
       " 'split1_test_accuracy': array([0.60042433, 0.68401697, 0.69886846, 0.69858557, 0.69830269,\n",
       "        0.69872702, 0.69844413, 0.69844413, 0.69844413, 0.69844413,\n",
       "        0.69858557, 0.69858557, 0.69858557, 0.69858557, 0.69858557,\n",
       "        0.69858557, 0.69858557, 0.69858557, 0.68401697, 0.69858557,\n",
       "        0.69872702, 0.69844413, 0.69858557, 0.69858557, 0.69858557,\n",
       "        0.69858557, 0.69858557, 0.69858557, 0.69858557, 0.69858557]),\n",
       " 'split2_test_accuracy': array([0.6003678 , 0.68482105, 0.70137219, 0.70349413, 0.70306974,\n",
       "        0.7032112 , 0.70349413, 0.70363559, 0.70349413, 0.70349413,\n",
       "        0.70349413, 0.70349413, 0.70349413, 0.70349413, 0.70349413,\n",
       "        0.70349413, 0.70349413, 0.70349413, 0.68482105, 0.70349413,\n",
       "        0.7032112 , 0.70363559, 0.70349413, 0.70349413, 0.70349413,\n",
       "        0.70349413, 0.70349413, 0.70349413, 0.70363559, 0.70349413]),\n",
       " 'split3_test_accuracy': array([0.6003678 , 0.6876503 , 0.70108926, 0.6986844 , 0.69769416,\n",
       "        0.69797708, 0.69826001, 0.69826001, 0.69854293, 0.69854293,\n",
       "        0.69854293, 0.69854293, 0.69854293, 0.69854293, 0.69854293,\n",
       "        0.69854293, 0.69854293, 0.69854293, 0.6876503 , 0.6986844 ,\n",
       "        0.69797708, 0.69826001, 0.69854293, 0.69854293, 0.69854293,\n",
       "        0.69854293, 0.69854293, 0.69854293, 0.69854293, 0.69854293]),\n",
       " 'split4_test_accuracy': array([0.6003678 , 0.67392842, 0.68736738, 0.68793323, 0.68892347,\n",
       "        0.68906493, 0.68963078, 0.68963078, 0.68948932, 0.68948932,\n",
       "        0.68948932, 0.68948932, 0.68948932, 0.68948932, 0.68948932,\n",
       "        0.68948932, 0.68948932, 0.68948932, 0.67392842, 0.68793323,\n",
       "        0.68906493, 0.68963078, 0.68948932, 0.68948932, 0.68948932,\n",
       "        0.68948932, 0.68948932, 0.68948932, 0.68948932, 0.68948932]),\n",
       " 'split5_test_accuracy': array([0.6003678 , 0.68015278, 0.69585514, 0.69741123, 0.69910878,\n",
       "        0.69910878, 0.69854293, 0.69882586, 0.69882586, 0.6986844 ,\n",
       "        0.69882586, 0.69882586, 0.6986844 , 0.69882586, 0.69882586,\n",
       "        0.69882586, 0.69882586, 0.6986844 , 0.68015278, 0.69741123,\n",
       "        0.69910878, 0.69882586, 0.69882586, 0.69882586, 0.69882586,\n",
       "        0.69882586, 0.69882586, 0.69882586, 0.69882586, 0.69882586]),\n",
       " 'split6_test_accuracy': array([0.6003678 , 0.68722592, 0.70066487, 0.70519168, 0.70505022,\n",
       "        0.70490876, 0.70519168, 0.70533314, 0.70533314, 0.70533314,\n",
       "        0.70533314, 0.70533314, 0.70533314, 0.70533314, 0.70533314,\n",
       "        0.70533314, 0.70533314, 0.70533314, 0.68722592, 0.70519168,\n",
       "        0.70490876, 0.70533314, 0.70533314, 0.70533314, 0.70533314,\n",
       "        0.70533314, 0.70533314, 0.70533314, 0.70533314, 0.70533314]),\n",
       " 'split7_test_accuracy': array([0.60050927, 0.68849908, 0.7009478 , 0.70137219, 0.70165511,\n",
       "        0.70151365, 0.70123073, 0.70151365, 0.70151365, 0.70151365,\n",
       "        0.70151365, 0.70151365, 0.70151365, 0.70151365, 0.70151365,\n",
       "        0.70151365, 0.70151365, 0.70151365, 0.68849908, 0.70137219,\n",
       "        0.70151365, 0.70137219, 0.70151365, 0.70151365, 0.70151365,\n",
       "        0.70151365, 0.70151365, 0.70151365, 0.70151365, 0.70151365]),\n",
       " 'split8_test_accuracy': array([0.60050927, 0.67619182, 0.69953317, 0.7020795 , 0.70306974,\n",
       "        0.70335267, 0.70278682, 0.70278682, 0.70278682, 0.70278682,\n",
       "        0.70278682, 0.70278682, 0.70278682, 0.70278682, 0.70278682,\n",
       "        0.70278682, 0.70278682, 0.70278682, 0.67619182, 0.70193804,\n",
       "        0.70335267, 0.70278682, 0.70278682, 0.70278682, 0.70278682,\n",
       "        0.70278682, 0.70278682, 0.70278682, 0.70278682, 0.70278682]),\n",
       " 'split9_test_accuracy': array([0.60050927, 0.68793323, 0.70646485, 0.70604046, 0.70505022,\n",
       "        0.70462583, 0.70490876, 0.70490876, 0.70505022, 0.70505022,\n",
       "        0.70505022, 0.70505022, 0.70505022, 0.70505022, 0.70505022,\n",
       "        0.70505022, 0.70505022, 0.70505022, 0.68793323, 0.70604046,\n",
       "        0.70462583, 0.70490876, 0.70505022, 0.70505022, 0.70505022,\n",
       "        0.70505022, 0.70505022, 0.70505022, 0.70505022, 0.70505022]),\n",
       " 'mean_test_accuracy': array([0.60042155, 0.68325978, 0.69931532, 0.70051771, 0.70065917,\n",
       "        0.70075819, 0.70077233, 0.70087136, 0.70087136, 0.70085721,\n",
       "        0.7008855 , 0.7008855 , 0.70087136, 0.7008855 , 0.7008855 ,\n",
       "        0.7008855 , 0.7008855 , 0.70087136, 0.68325978, 0.70050357,\n",
       "        0.70075819, 0.70085721, 0.7008855 , 0.7008855 , 0.7008855 ,\n",
       "        0.7008855 , 0.7008855 , 0.7008855 , 0.70089965, 0.7008855 ]),\n",
       " 'std_test_accuracy': array([6.12709395e-05, 4.84935832e-03, 4.69848659e-03, 5.04422943e-03,\n",
       "        4.70869955e-03, 4.64327026e-03, 4.57225005e-03, 4.59844564e-03,\n",
       "        4.60875219e-03, 4.61522157e-03, 4.60149264e-03, 4.60149264e-03,\n",
       "        4.60801564e-03, 4.60149264e-03, 4.60149264e-03, 4.60149264e-03,\n",
       "        4.60149264e-03, 4.60801564e-03, 4.84935832e-03, 5.04002625e-03,\n",
       "        4.64327026e-03, 4.59666523e-03, 4.60149264e-03, 4.60149264e-03,\n",
       "        4.60149264e-03, 4.60149264e-03, 4.60149264e-03, 4.60149264e-03,\n",
       "        4.60970067e-03, 4.60149264e-03]),\n",
       " 'rank_test_accuracy': array([30, 28, 27, 25, 24, 22, 21, 16, 15, 19,  2,  2, 17,  2,  2,  2,  2,\n",
       "        17, 28, 26, 22, 20,  2,  2,  2,  2,  2,  2,  1,  2]),\n",
       " 'split0_test_precision': array([0.        , 0.69958564, 0.66936636, 0.67037552, 0.6635597 ,\n",
       "        0.66400709, 0.66357616, 0.66372462, 0.66328779, 0.66328779,\n",
       "        0.66328779, 0.66328779, 0.66328779, 0.66328779, 0.66328779,\n",
       "        0.66328779, 0.66328779, 0.66328779, 0.69958564, 0.67037552,\n",
       "        0.66400709, 0.66372462, 0.66328779, 0.66328779, 0.66328779,\n",
       "        0.66328779, 0.66328779, 0.66328779, 0.66328779, 0.66328779]),\n",
       " 'split1_test_precision': array([0.        , 0.70506593, 0.66909621, 0.66275797, 0.65487914,\n",
       "        0.65520322, 0.65420561, 0.65406847, 0.65406847, 0.65406847,\n",
       "        0.65422222, 0.65422222, 0.65422222, 0.65422222, 0.65422222,\n",
       "        0.65422222, 0.65422222, 0.65422222, 0.70506593, 0.66275797,\n",
       "        0.65520322, 0.65406847, 0.65422222, 0.65422222, 0.65422222,\n",
       "        0.65422222, 0.65422222, 0.65422222, 0.65422222, 0.65422222]),\n",
       " 'split2_test_precision': array([0.        , 0.71125265, 0.67279768, 0.66914153, 0.66076174,\n",
       "        0.66034407, 0.66036076, 0.66051011, 0.66007905, 0.66007905,\n",
       "        0.66007905, 0.66007905, 0.66007905, 0.66007905, 0.66007905,\n",
       "        0.66007905, 0.66007905, 0.66007905, 0.71125265, 0.66914153,\n",
       "        0.66034407, 0.66051011, 0.66007905, 0.66007905, 0.66007905,\n",
       "        0.66007905, 0.66007905, 0.66007905, 0.66022827, 0.66007905]),\n",
       " 'split3_test_precision': array([0.        , 0.70802428, 0.66872038, 0.65947682, 0.65087719,\n",
       "        0.65078671, 0.65056571, 0.65056571, 0.65086957, 0.65086957,\n",
       "        0.65086957, 0.65086957, 0.65086957, 0.65086957, 0.65086957,\n",
       "        0.65086957, 0.65086957, 0.65086957, 0.70802428, 0.65947682,\n",
       "        0.65078671, 0.65056571, 0.65086957, 0.65086957, 0.65086957,\n",
       "        0.65086957, 0.65086957, 0.65086957, 0.65086957, 0.65086957]),\n",
       " 'split4_test_precision': array([0.        , 0.67881706, 0.64635888, 0.64036281, 0.63644289,\n",
       "        0.63624511, 0.63652099, 0.63652099, 0.63624567, 0.63624567,\n",
       "        0.63624567, 0.63624567, 0.63624567, 0.63624567, 0.63624567,\n",
       "        0.63624567, 0.63624567, 0.63624567, 0.67881706, 0.64036281,\n",
       "        0.63624511, 0.63652099, 0.63624567, 0.63624567, 0.63624567,\n",
       "        0.63624567, 0.63624567, 0.63624567, 0.63624567, 0.63624567]),\n",
       " 'split5_test_precision': array([0.        , 0.70200573, 0.6663381 , 0.66427203, 0.6595064 ,\n",
       "        0.65921533, 0.65772727, 0.65832575, 0.65818182, 0.65788278,\n",
       "        0.65818182, 0.65818182, 0.65788278, 0.65818182, 0.65818182,\n",
       "        0.65818182, 0.65818182, 0.65788278, 0.70200573, 0.66427203,\n",
       "        0.65921533, 0.65832575, 0.65818182, 0.65818182, 0.65818182,\n",
       "        0.65818182, 0.65818182, 0.65818182, 0.65818182, 0.65818182]),\n",
       " 'split6_test_precision': array([0.        , 0.71928571, 0.67628046, 0.6760095 , 0.66894977,\n",
       "        0.66803092, 0.66802721, 0.6681777 , 0.66802536, 0.66802536,\n",
       "        0.66802536, 0.66802536, 0.66802536, 0.66802536, 0.66802536,\n",
       "        0.66802536, 0.66802536, 0.66802536, 0.71928571, 0.6760095 ,\n",
       "        0.66803092, 0.6681777 , 0.66802536, 0.66802536, 0.66802536,\n",
       "        0.66802536, 0.66802536, 0.66802536, 0.66802536, 0.66802536]),\n",
       " 'split7_test_precision': array([0.        , 0.72088068, 0.67401961, 0.66682265, 0.66038582,\n",
       "        0.65994624, 0.6587868 , 0.65909091, 0.65894924, 0.65894924,\n",
       "        0.65894924, 0.65894924, 0.65894924, 0.65894924, 0.65894924,\n",
       "        0.65894924, 0.65894924, 0.65894924, 0.72088068, 0.66682265,\n",
       "        0.65994624, 0.65893892, 0.65894924, 0.65894924, 0.65894924,\n",
       "        0.65894924, 0.65894924, 0.65894924, 0.65894924, 0.65894924]),\n",
       " 'split8_test_precision': array([0.        , 0.70280516, 0.68041237, 0.67719645, 0.67074894,\n",
       "        0.67105882, 0.66932084, 0.66932084, 0.66916238, 0.66916238,\n",
       "        0.66916238, 0.66916238, 0.66916238, 0.66916238, 0.66916238,\n",
       "        0.66916238, 0.66916238, 0.66916238, 0.70280516, 0.67686236,\n",
       "        0.67105882, 0.66932084, 0.66916238, 0.66916238, 0.66916238,\n",
       "        0.66916238, 0.66916238, 0.66916238, 0.66916238, 0.66916238]),\n",
       " 'split9_test_precision': array([0.        , 0.71608392, 0.67858846, 0.67047532, 0.66184845,\n",
       "        0.66112084, 0.66099476, 0.66099476, 0.66114261, 0.66114261,\n",
       "        0.66114261, 0.66114261, 0.66114261, 0.66114261, 0.66114261,\n",
       "        0.66114261, 0.66114261, 0.66114261, 0.71608392, 0.67047532,\n",
       "        0.66112084, 0.66099476, 0.66114261, 0.66114261, 0.66114261,\n",
       "        0.66114261, 0.66114261, 0.66114261, 0.66114261, 0.66114261]),\n",
       " 'mean_test_precision': array([0.        , 0.70638067, 0.67019785, 0.66568906, 0.658796  ,\n",
       "        0.65859584, 0.65800861, 0.65812999, 0.6580012 , 0.65797129,\n",
       "        0.65801657, 0.65801657, 0.65798667, 0.65801657, 0.65801657,\n",
       "        0.65801657, 0.65801657, 0.65798667, 0.70638067, 0.66565565,\n",
       "        0.65859584, 0.65811479, 0.65801657, 0.65801657, 0.65801657,\n",
       "        0.65801657, 0.65801657, 0.65801657, 0.65803149, 0.65801657]),\n",
       " 'std_test_precision': array([0.        , 0.0115627 , 0.00905751, 0.00993789, 0.00928757,\n",
       "        0.00925913, 0.00897802, 0.00901659, 0.00898749, 0.00898734,\n",
       "        0.00898088, 0.00898088, 0.00898078, 0.00898088, 0.00898088,\n",
       "        0.00898088, 0.00898088, 0.00898078, 0.0115627 , 0.00989964,\n",
       "        0.00925913, 0.00901508, 0.00898088, 0.00898088, 0.00898088,\n",
       "        0.00898088, 0.00898088, 0.00898088, 0.00898442, 0.00898088]),\n",
       " 'rank_test_precision': array([30,  1,  3,  4,  6,  7, 25,  9, 26, 29, 12, 12, 27, 12, 12, 12, 12,\n",
       "        27,  1,  5,  7, 10, 12, 12, 12, 12, 12, 12, 11, 12]),\n",
       " 'split0_test_f1_micro': array([0.60042433, 0.68217822, 0.7009901 , 0.70438472, 0.70466761,\n",
       "        0.70509194, 0.70523338, 0.70537482, 0.70523338, 0.70523338,\n",
       "        0.70523338, 0.70523338, 0.70523338, 0.70523338, 0.70523338,\n",
       "        0.70523338, 0.70523338, 0.70523338, 0.68217822, 0.70438472,\n",
       "        0.70509194, 0.70537482, 0.70523338, 0.70523338, 0.70523338,\n",
       "        0.70523338, 0.70523338, 0.70523338, 0.70523338, 0.70523338]),\n",
       " 'split1_test_f1_micro': array([0.60042433, 0.68401697, 0.69886846, 0.69858557, 0.69830269,\n",
       "        0.69872702, 0.69844413, 0.69844413, 0.69844413, 0.69844413,\n",
       "        0.69858557, 0.69858557, 0.69858557, 0.69858557, 0.69858557,\n",
       "        0.69858557, 0.69858557, 0.69858557, 0.68401697, 0.69858557,\n",
       "        0.69872702, 0.69844413, 0.69858557, 0.69858557, 0.69858557,\n",
       "        0.69858557, 0.69858557, 0.69858557, 0.69858557, 0.69858557]),\n",
       " 'split2_test_f1_micro': array([0.6003678 , 0.68482105, 0.70137219, 0.70349413, 0.70306974,\n",
       "        0.7032112 , 0.70349413, 0.70363559, 0.70349413, 0.70349413,\n",
       "        0.70349413, 0.70349413, 0.70349413, 0.70349413, 0.70349413,\n",
       "        0.70349413, 0.70349413, 0.70349413, 0.68482105, 0.70349413,\n",
       "        0.7032112 , 0.70363559, 0.70349413, 0.70349413, 0.70349413,\n",
       "        0.70349413, 0.70349413, 0.70349413, 0.70363559, 0.70349413]),\n",
       " 'split3_test_f1_micro': array([0.6003678 , 0.6876503 , 0.70108926, 0.6986844 , 0.69769416,\n",
       "        0.69797708, 0.69826001, 0.69826001, 0.69854293, 0.69854293,\n",
       "        0.69854293, 0.69854293, 0.69854293, 0.69854293, 0.69854293,\n",
       "        0.69854293, 0.69854293, 0.69854293, 0.6876503 , 0.6986844 ,\n",
       "        0.69797708, 0.69826001, 0.69854293, 0.69854293, 0.69854293,\n",
       "        0.69854293, 0.69854293, 0.69854293, 0.69854293, 0.69854293]),\n",
       " 'split4_test_f1_micro': array([0.6003678 , 0.67392842, 0.68736738, 0.68793323, 0.68892347,\n",
       "        0.68906493, 0.68963078, 0.68963078, 0.68948932, 0.68948932,\n",
       "        0.68948932, 0.68948932, 0.68948932, 0.68948932, 0.68948932,\n",
       "        0.68948932, 0.68948932, 0.68948932, 0.67392842, 0.68793323,\n",
       "        0.68906493, 0.68963078, 0.68948932, 0.68948932, 0.68948932,\n",
       "        0.68948932, 0.68948932, 0.68948932, 0.68948932, 0.68948932]),\n",
       " 'split5_test_f1_micro': array([0.6003678 , 0.68015278, 0.69585514, 0.69741123, 0.69910878,\n",
       "        0.69910878, 0.69854293, 0.69882586, 0.69882586, 0.6986844 ,\n",
       "        0.69882586, 0.69882586, 0.6986844 , 0.69882586, 0.69882586,\n",
       "        0.69882586, 0.69882586, 0.6986844 , 0.68015278, 0.69741123,\n",
       "        0.69910878, 0.69882586, 0.69882586, 0.69882586, 0.69882586,\n",
       "        0.69882586, 0.69882586, 0.69882586, 0.69882586, 0.69882586]),\n",
       " 'split6_test_f1_micro': array([0.6003678 , 0.68722592, 0.70066487, 0.70519168, 0.70505022,\n",
       "        0.70490876, 0.70519168, 0.70533314, 0.70533314, 0.70533314,\n",
       "        0.70533314, 0.70533314, 0.70533314, 0.70533314, 0.70533314,\n",
       "        0.70533314, 0.70533314, 0.70533314, 0.68722592, 0.70519168,\n",
       "        0.70490876, 0.70533314, 0.70533314, 0.70533314, 0.70533314,\n",
       "        0.70533314, 0.70533314, 0.70533314, 0.70533314, 0.70533314]),\n",
       " 'split7_test_f1_micro': array([0.60050927, 0.68849908, 0.7009478 , 0.70137219, 0.70165511,\n",
       "        0.70151365, 0.70123073, 0.70151365, 0.70151365, 0.70151365,\n",
       "        0.70151365, 0.70151365, 0.70151365, 0.70151365, 0.70151365,\n",
       "        0.70151365, 0.70151365, 0.70151365, 0.68849908, 0.70137219,\n",
       "        0.70151365, 0.70137219, 0.70151365, 0.70151365, 0.70151365,\n",
       "        0.70151365, 0.70151365, 0.70151365, 0.70151365, 0.70151365]),\n",
       " 'split8_test_f1_micro': array([0.60050927, 0.67619182, 0.69953317, 0.7020795 , 0.70306974,\n",
       "        0.70335267, 0.70278682, 0.70278682, 0.70278682, 0.70278682,\n",
       "        0.70278682, 0.70278682, 0.70278682, 0.70278682, 0.70278682,\n",
       "        0.70278682, 0.70278682, 0.70278682, 0.67619182, 0.70193804,\n",
       "        0.70335267, 0.70278682, 0.70278682, 0.70278682, 0.70278682,\n",
       "        0.70278682, 0.70278682, 0.70278682, 0.70278682, 0.70278682]),\n",
       " 'split9_test_f1_micro': array([0.60050927, 0.68793323, 0.70646485, 0.70604046, 0.70505022,\n",
       "        0.70462583, 0.70490876, 0.70490876, 0.70505022, 0.70505022,\n",
       "        0.70505022, 0.70505022, 0.70505022, 0.70505022, 0.70505022,\n",
       "        0.70505022, 0.70505022, 0.70505022, 0.68793323, 0.70604046,\n",
       "        0.70462583, 0.70490876, 0.70505022, 0.70505022, 0.70505022,\n",
       "        0.70505022, 0.70505022, 0.70505022, 0.70505022, 0.70505022]),\n",
       " 'mean_test_f1_micro': array([0.60042155, 0.68325978, 0.69931532, 0.70051771, 0.70065917,\n",
       "        0.70075819, 0.70077233, 0.70087136, 0.70087136, 0.70085721,\n",
       "        0.7008855 , 0.7008855 , 0.70087136, 0.7008855 , 0.7008855 ,\n",
       "        0.7008855 , 0.7008855 , 0.70087136, 0.68325978, 0.70050357,\n",
       "        0.70075819, 0.70085721, 0.7008855 , 0.7008855 , 0.7008855 ,\n",
       "        0.7008855 , 0.7008855 , 0.7008855 , 0.70089965, 0.7008855 ]),\n",
       " 'std_test_f1_micro': array([6.12709395e-05, 4.84935832e-03, 4.69848659e-03, 5.04422943e-03,\n",
       "        4.70869955e-03, 4.64327026e-03, 4.57225005e-03, 4.59844564e-03,\n",
       "        4.60875219e-03, 4.61522157e-03, 4.60149264e-03, 4.60149264e-03,\n",
       "        4.60801564e-03, 4.60149264e-03, 4.60149264e-03, 4.60149264e-03,\n",
       "        4.60149264e-03, 4.60801564e-03, 4.84935832e-03, 5.04002625e-03,\n",
       "        4.64327026e-03, 4.59666523e-03, 4.60149264e-03, 4.60149264e-03,\n",
       "        4.60149264e-03, 4.60149264e-03, 4.60149264e-03, 4.60149264e-03,\n",
       "        4.60970067e-03, 4.60149264e-03]),\n",
       " 'rank_test_f1_micro': array([30, 28, 27, 25, 24, 22, 21, 16, 15, 19,  2,  2, 17,  2,  2,  2,  2,\n",
       "        17, 28, 26, 22, 20,  2,  2,  2,  2,  2,  2,  1,  2])}"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_model.cv_results_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "6d28a919",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([30, 28, 27, 25, 24, 22, 21, 16, 15, 19,  2,  2, 17,  2,  2,  2,  2,\n",
       "       17, 28, 26, 22, 20,  2,  2,  2,  2,  2,  2,  1,  2])"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_model.cv_results_['rank_test_accuracy']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "0ff3283b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'classifier': LogisticRegression(max_iter=10000),\n",
       " 'classifier__C': 0.0001,\n",
       " 'classifier__penalty': 'l1',\n",
       " 'classifier__solver': 'saga'}"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_model.cv_results_['params'][ np.argmin(best_model.cv_results_['mean_test_accuracy'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "1b20e96a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>classifier</th>\n",
       "      <th>C</th>\n",
       "      <th>penalty</th>\n",
       "      <th>solver</th>\n",
       "      <th>score_acc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.600422</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.683260</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.699315</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.700518</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0100</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.700659</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0100</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.700758</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.1000</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.700772</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.1000</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.700871</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.700871</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.700857</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>10.0000</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.700886</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>10.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.700886</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>100.0000</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.700871</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>100.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.700886</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>1000.0000</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.700886</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>1000.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.700886</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>10000.0000</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.700886</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>10000.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.700871</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.683260</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.700504</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0100</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.700758</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.1000</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.700857</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.700886</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>10.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.700886</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>100.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.700886</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>1000.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.700886</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>10000.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.700886</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>NaN</td>\n",
       "      <td>none</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.700886</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>NaN</td>\n",
       "      <td>none</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.700900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.700886</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                            classifier           C penalty solver  score_acc\n",
       "0   LogisticRegression(max_iter=10000)      0.0001      l1   saga   0.600422\n",
       "1   LogisticRegression(max_iter=10000)      0.0001      l2   saga   0.683260\n",
       "2   LogisticRegression(max_iter=10000)      0.0010      l1   saga   0.699315\n",
       "3   LogisticRegression(max_iter=10000)      0.0010      l2   saga   0.700518\n",
       "4   LogisticRegression(max_iter=10000)      0.0100      l1   saga   0.700659\n",
       "5   LogisticRegression(max_iter=10000)      0.0100      l2   saga   0.700758\n",
       "6   LogisticRegression(max_iter=10000)      0.1000      l1   saga   0.700772\n",
       "7   LogisticRegression(max_iter=10000)      0.1000      l2   saga   0.700871\n",
       "8   LogisticRegression(max_iter=10000)      1.0000      l1   saga   0.700871\n",
       "9   LogisticRegression(max_iter=10000)      1.0000      l2   saga   0.700857\n",
       "10  LogisticRegression(max_iter=10000)     10.0000      l1   saga   0.700886\n",
       "11  LogisticRegression(max_iter=10000)     10.0000      l2   saga   0.700886\n",
       "12  LogisticRegression(max_iter=10000)    100.0000      l1   saga   0.700871\n",
       "13  LogisticRegression(max_iter=10000)    100.0000      l2   saga   0.700886\n",
       "14  LogisticRegression(max_iter=10000)   1000.0000      l1   saga   0.700886\n",
       "15  LogisticRegression(max_iter=10000)   1000.0000      l2   saga   0.700886\n",
       "16  LogisticRegression(max_iter=10000)  10000.0000      l1   saga   0.700886\n",
       "17  LogisticRegression(max_iter=10000)  10000.0000      l2   saga   0.700871\n",
       "18  LogisticRegression(max_iter=10000)      0.0001      l2  lbfgs   0.683260\n",
       "19  LogisticRegression(max_iter=10000)      0.0010      l2  lbfgs   0.700504\n",
       "20  LogisticRegression(max_iter=10000)      0.0100      l2  lbfgs   0.700758\n",
       "21  LogisticRegression(max_iter=10000)      0.1000      l2  lbfgs   0.700857\n",
       "22  LogisticRegression(max_iter=10000)      1.0000      l2  lbfgs   0.700886\n",
       "23  LogisticRegression(max_iter=10000)     10.0000      l2  lbfgs   0.700886\n",
       "24  LogisticRegression(max_iter=10000)    100.0000      l2  lbfgs   0.700886\n",
       "25  LogisticRegression(max_iter=10000)   1000.0000      l2  lbfgs   0.700886\n",
       "26  LogisticRegression(max_iter=10000)  10000.0000      l2  lbfgs   0.700886\n",
       "27  LogisticRegression(max_iter=10000)         NaN    none  lbfgs   0.700886\n",
       "28  LogisticRegression(max_iter=10000)         NaN    none   saga   0.700900\n",
       "29  LogisticRegression(max_iter=10000)         NaN     NaN    NaN   0.700886"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results = pd.DataFrame( best_model.cv_results_['params'] )\n",
    "\n",
    "results['score_acc'] = best_model.cv_results_['mean_test_accuracy']\n",
    "cols = results.columns.to_series().str.split('__').apply(lambda x: x[-1])\n",
    "results.columns = cols\n",
    "\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "a9b8f04c",
   "metadata": {},
   "outputs": [],
   "source": [
    "acc_list.append(results[0:1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "9ff4ec73",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'classifier': LogisticRegression(max_iter=10000),\n",
       " 'classifier__C': 0.0001,\n",
       " 'classifier__penalty': 'l2',\n",
       " 'classifier__solver': 'saga'}"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_model.cv_results_['params'][ np.argmin(best_model.cv_results_['rank_test_precision'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "f5204e89",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>classifier</th>\n",
       "      <th>C</th>\n",
       "      <th>penalty</th>\n",
       "      <th>solver</th>\n",
       "      <th>score_precision</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.706381</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.670198</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.665689</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0100</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.658796</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0100</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.658596</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.1000</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.658009</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.1000</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.658130</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.658001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.657971</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>10.0000</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.658017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>10.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.658017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>100.0000</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.657987</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>100.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.658017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>1000.0000</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.658017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>1000.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.658017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>10000.0000</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.658017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>10000.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.657987</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.706381</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.665656</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0100</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.658596</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.1000</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.658115</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.658017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>10.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.658017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>100.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.658017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>1000.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.658017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>10000.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.658017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>NaN</td>\n",
       "      <td>none</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.658017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>NaN</td>\n",
       "      <td>none</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.658031</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.658017</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                            classifier           C penalty solver  \\\n",
       "0   LogisticRegression(max_iter=10000)      0.0001      l1   saga   \n",
       "1   LogisticRegression(max_iter=10000)      0.0001      l2   saga   \n",
       "2   LogisticRegression(max_iter=10000)      0.0010      l1   saga   \n",
       "3   LogisticRegression(max_iter=10000)      0.0010      l2   saga   \n",
       "4   LogisticRegression(max_iter=10000)      0.0100      l1   saga   \n",
       "5   LogisticRegression(max_iter=10000)      0.0100      l2   saga   \n",
       "6   LogisticRegression(max_iter=10000)      0.1000      l1   saga   \n",
       "7   LogisticRegression(max_iter=10000)      0.1000      l2   saga   \n",
       "8   LogisticRegression(max_iter=10000)      1.0000      l1   saga   \n",
       "9   LogisticRegression(max_iter=10000)      1.0000      l2   saga   \n",
       "10  LogisticRegression(max_iter=10000)     10.0000      l1   saga   \n",
       "11  LogisticRegression(max_iter=10000)     10.0000      l2   saga   \n",
       "12  LogisticRegression(max_iter=10000)    100.0000      l1   saga   \n",
       "13  LogisticRegression(max_iter=10000)    100.0000      l2   saga   \n",
       "14  LogisticRegression(max_iter=10000)   1000.0000      l1   saga   \n",
       "15  LogisticRegression(max_iter=10000)   1000.0000      l2   saga   \n",
       "16  LogisticRegression(max_iter=10000)  10000.0000      l1   saga   \n",
       "17  LogisticRegression(max_iter=10000)  10000.0000      l2   saga   \n",
       "18  LogisticRegression(max_iter=10000)      0.0001      l2  lbfgs   \n",
       "19  LogisticRegression(max_iter=10000)      0.0010      l2  lbfgs   \n",
       "20  LogisticRegression(max_iter=10000)      0.0100      l2  lbfgs   \n",
       "21  LogisticRegression(max_iter=10000)      0.1000      l2  lbfgs   \n",
       "22  LogisticRegression(max_iter=10000)      1.0000      l2  lbfgs   \n",
       "23  LogisticRegression(max_iter=10000)     10.0000      l2  lbfgs   \n",
       "24  LogisticRegression(max_iter=10000)    100.0000      l2  lbfgs   \n",
       "25  LogisticRegression(max_iter=10000)   1000.0000      l2  lbfgs   \n",
       "26  LogisticRegression(max_iter=10000)  10000.0000      l2  lbfgs   \n",
       "27  LogisticRegression(max_iter=10000)         NaN    none  lbfgs   \n",
       "28  LogisticRegression(max_iter=10000)         NaN    none   saga   \n",
       "29  LogisticRegression(max_iter=10000)         NaN     NaN    NaN   \n",
       "\n",
       "    score_precision  \n",
       "0          0.000000  \n",
       "1          0.706381  \n",
       "2          0.670198  \n",
       "3          0.665689  \n",
       "4          0.658796  \n",
       "5          0.658596  \n",
       "6          0.658009  \n",
       "7          0.658130  \n",
       "8          0.658001  \n",
       "9          0.657971  \n",
       "10         0.658017  \n",
       "11         0.658017  \n",
       "12         0.657987  \n",
       "13         0.658017  \n",
       "14         0.658017  \n",
       "15         0.658017  \n",
       "16         0.658017  \n",
       "17         0.657987  \n",
       "18         0.706381  \n",
       "19         0.665656  \n",
       "20         0.658596  \n",
       "21         0.658115  \n",
       "22         0.658017  \n",
       "23         0.658017  \n",
       "24         0.658017  \n",
       "25         0.658017  \n",
       "26         0.658017  \n",
       "27         0.658017  \n",
       "28         0.658031  \n",
       "29         0.658017  "
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results = pd.DataFrame( best_model.cv_results_['params'] )\n",
    "\n",
    "results['score_precision'] = best_model.cv_results_['mean_test_precision']\n",
    "cols = results.columns.to_series().str.split('__').apply(lambda x: x[-1])\n",
    "results.columns = cols\n",
    "\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "43b18550",
   "metadata": {},
   "outputs": [],
   "source": [
    "roc_auc_ovr_List.append(results[1:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "24071971",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'classifier': LogisticRegression(max_iter=10000),\n",
       " 'classifier__penalty': 'none',\n",
       " 'classifier__solver': 'saga'}"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_model.cv_results_['params'][ np.argmin(best_model.cv_results_['rank_test_f1_micro'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "87b8ca97",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>classifier</th>\n",
       "      <th>C</th>\n",
       "      <th>penalty</th>\n",
       "      <th>solver</th>\n",
       "      <th>score_f1_micro</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.600422</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.683260</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.699315</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.700518</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0100</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.700659</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0100</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.700758</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.1000</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.700772</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.1000</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.700871</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.700871</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.700857</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>10.0000</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.700886</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>10.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.700886</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>100.0000</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.700871</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>100.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.700886</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>1000.0000</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.700886</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>1000.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.700886</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>10000.0000</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.700886</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>10000.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.700871</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.683260</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.700504</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0100</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.700758</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.1000</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.700857</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.700886</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>10.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.700886</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>100.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.700886</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>1000.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.700886</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>10000.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.700886</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>NaN</td>\n",
       "      <td>none</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.700886</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>NaN</td>\n",
       "      <td>none</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.700900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.700886</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                            classifier           C penalty solver  \\\n",
       "0   LogisticRegression(max_iter=10000)      0.0001      l1   saga   \n",
       "1   LogisticRegression(max_iter=10000)      0.0001      l2   saga   \n",
       "2   LogisticRegression(max_iter=10000)      0.0010      l1   saga   \n",
       "3   LogisticRegression(max_iter=10000)      0.0010      l2   saga   \n",
       "4   LogisticRegression(max_iter=10000)      0.0100      l1   saga   \n",
       "5   LogisticRegression(max_iter=10000)      0.0100      l2   saga   \n",
       "6   LogisticRegression(max_iter=10000)      0.1000      l1   saga   \n",
       "7   LogisticRegression(max_iter=10000)      0.1000      l2   saga   \n",
       "8   LogisticRegression(max_iter=10000)      1.0000      l1   saga   \n",
       "9   LogisticRegression(max_iter=10000)      1.0000      l2   saga   \n",
       "10  LogisticRegression(max_iter=10000)     10.0000      l1   saga   \n",
       "11  LogisticRegression(max_iter=10000)     10.0000      l2   saga   \n",
       "12  LogisticRegression(max_iter=10000)    100.0000      l1   saga   \n",
       "13  LogisticRegression(max_iter=10000)    100.0000      l2   saga   \n",
       "14  LogisticRegression(max_iter=10000)   1000.0000      l1   saga   \n",
       "15  LogisticRegression(max_iter=10000)   1000.0000      l2   saga   \n",
       "16  LogisticRegression(max_iter=10000)  10000.0000      l1   saga   \n",
       "17  LogisticRegression(max_iter=10000)  10000.0000      l2   saga   \n",
       "18  LogisticRegression(max_iter=10000)      0.0001      l2  lbfgs   \n",
       "19  LogisticRegression(max_iter=10000)      0.0010      l2  lbfgs   \n",
       "20  LogisticRegression(max_iter=10000)      0.0100      l2  lbfgs   \n",
       "21  LogisticRegression(max_iter=10000)      0.1000      l2  lbfgs   \n",
       "22  LogisticRegression(max_iter=10000)      1.0000      l2  lbfgs   \n",
       "23  LogisticRegression(max_iter=10000)     10.0000      l2  lbfgs   \n",
       "24  LogisticRegression(max_iter=10000)    100.0000      l2  lbfgs   \n",
       "25  LogisticRegression(max_iter=10000)   1000.0000      l2  lbfgs   \n",
       "26  LogisticRegression(max_iter=10000)  10000.0000      l2  lbfgs   \n",
       "27  LogisticRegression(max_iter=10000)         NaN    none  lbfgs   \n",
       "28  LogisticRegression(max_iter=10000)         NaN    none   saga   \n",
       "29  LogisticRegression(max_iter=10000)         NaN     NaN    NaN   \n",
       "\n",
       "    score_f1_micro  \n",
       "0         0.600422  \n",
       "1         0.683260  \n",
       "2         0.699315  \n",
       "3         0.700518  \n",
       "4         0.700659  \n",
       "5         0.700758  \n",
       "6         0.700772  \n",
       "7         0.700871  \n",
       "8         0.700871  \n",
       "9         0.700857  \n",
       "10        0.700886  \n",
       "11        0.700886  \n",
       "12        0.700871  \n",
       "13        0.700886  \n",
       "14        0.700886  \n",
       "15        0.700886  \n",
       "16        0.700886  \n",
       "17        0.700871  \n",
       "18        0.683260  \n",
       "19        0.700504  \n",
       "20        0.700758  \n",
       "21        0.700857  \n",
       "22        0.700886  \n",
       "23        0.700886  \n",
       "24        0.700886  \n",
       "25        0.700886  \n",
       "26        0.700886  \n",
       "27        0.700886  \n",
       "28        0.700900  \n",
       "29        0.700886  "
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results = pd.DataFrame( best_model.cv_results_['params'] )\n",
    "\n",
    "results['score_f1_micro'] = best_model.cv_results_['mean_test_f1_micro']\n",
    "cols = results.columns.to_series().str.split('__').apply(lambda x: x[-1])\n",
    "results.columns = cols\n",
    "\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "39fb7b98",
   "metadata": {},
   "outputs": [],
   "source": [
    "f1_micro_List.append(results[28:29])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "cd18e7d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time:  0:08:51.095589\n"
     ]
    }
   ],
   "source": [
    "end = datetime.now()\n",
    "time_taken = end - start\n",
    "print('Time: ',time_taken) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6491ce89",
   "metadata": {},
   "source": [
    "## Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "f82dc269",
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy = [x['score_acc'][0] for x in acc_list]\n",
    "accuracy_c = [x['C'][0] for x in acc_list]\n",
    "accuracy_penalty = [x['penalty'][0] for x in acc_list]\n",
    "roc = [x['score_precision'].reset_index(drop=True)[0] for x in roc_auc_ovr_List]\n",
    "roc_c = [x['C'].reset_index(drop=True)[0] for x in roc_auc_ovr_List]\n",
    "roc_penalty = [x['penalty'].reset_index(drop=True)[0] for x in roc_auc_ovr_List]\n",
    "f1 = [x['score_f1_micro'].reset_index(drop=True)[0] for x in f1_micro_List]\n",
    "f1_c = [x['C'].reset_index(drop=True)[0] for x in f1_micro_List]\n",
    "f1_penalty = [x['penalty'].reset_index(drop=True)[0] for x in f1_micro_List]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "5d58acc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {'Accuracy': accuracy, 'Accuracy C': accuracy_c, 'Accuracy Penalty': accuracy_penalty,\n",
    "        'Precision': roc, 'Precision C': roc_c, 'Precision Penalty': roc_penalty,\n",
    "        'F1_micro':f1, 'F1_micro C': f1_c, 'F1_micro Penalty': f1_penalty}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "27c29018",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Accuracy C</th>\n",
       "      <th>Accuracy Penalty</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Precision C</th>\n",
       "      <th>Precision Penalty</th>\n",
       "      <th>F1_micro</th>\n",
       "      <th>F1_micro C</th>\n",
       "      <th>F1_micro Penalty</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.600535</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>l1</td>\n",
       "      <td>0.704018</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>l2</td>\n",
       "      <td>0.701041</td>\n",
       "      <td>1.00</td>\n",
       "      <td>l1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.599120</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>l1</td>\n",
       "      <td>0.708640</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>l2</td>\n",
       "      <td>0.702781</td>\n",
       "      <td>1.00</td>\n",
       "      <td>l1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.599601</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>l1</td>\n",
       "      <td>0.706928</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>l2</td>\n",
       "      <td>0.701550</td>\n",
       "      <td>0.10</td>\n",
       "      <td>l2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.599332</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>l1</td>\n",
       "      <td>0.704444</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>l2</td>\n",
       "      <td>0.702159</td>\n",
       "      <td>100.00</td>\n",
       "      <td>l1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.600167</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>l1</td>\n",
       "      <td>0.709146</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>l2</td>\n",
       "      <td>0.704068</td>\n",
       "      <td>0.01</td>\n",
       "      <td>l2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.599389</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>l1</td>\n",
       "      <td>0.705512</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>l2</td>\n",
       "      <td>0.701169</td>\n",
       "      <td>1.00</td>\n",
       "      <td>l1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.599785</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>l1</td>\n",
       "      <td>0.705316</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>l2</td>\n",
       "      <td>0.700857</td>\n",
       "      <td>0.10</td>\n",
       "      <td>l2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.599969</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>l1</td>\n",
       "      <td>0.705015</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>l2</td>\n",
       "      <td>0.701692</td>\n",
       "      <td>1.00</td>\n",
       "      <td>l2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.600110</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>l1</td>\n",
       "      <td>0.705634</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>l2</td>\n",
       "      <td>0.701352</td>\n",
       "      <td>1.00</td>\n",
       "      <td>l2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.600422</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>l1</td>\n",
       "      <td>0.706381</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>l2</td>\n",
       "      <td>0.700900</td>\n",
       "      <td>NaN</td>\n",
       "      <td>none</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Accuracy  Accuracy C Accuracy Penalty  Precision  Precision C  \\\n",
       "0  0.600535      0.0001               l1   0.704018       0.0001   \n",
       "1  0.599120      0.0001               l1   0.708640       0.0001   \n",
       "2  0.599601      0.0001               l1   0.706928       0.0001   \n",
       "3  0.599332      0.0001               l1   0.704444       0.0001   \n",
       "4  0.600167      0.0001               l1   0.709146       0.0001   \n",
       "5  0.599389      0.0001               l1   0.705512       0.0001   \n",
       "6  0.599785      0.0001               l1   0.705316       0.0001   \n",
       "7  0.599969      0.0001               l1   0.705015       0.0001   \n",
       "8  0.600110      0.0001               l1   0.705634       0.0001   \n",
       "9  0.600422      0.0001               l1   0.706381       0.0001   \n",
       "\n",
       "  Precision Penalty  F1_micro  F1_micro C F1_micro Penalty  \n",
       "0                l2  0.701041        1.00               l1  \n",
       "1                l2  0.702781        1.00               l1  \n",
       "2                l2  0.701550        0.10               l2  \n",
       "3                l2  0.702159      100.00               l1  \n",
       "4                l2  0.704068        0.01               l2  \n",
       "5                l2  0.701169        1.00               l1  \n",
       "6                l2  0.700857        0.10               l2  \n",
       "7                l2  0.701692        1.00               l2  \n",
       "8                l2  0.701352        1.00               l2  \n",
       "9                l2  0.700900         NaN             none  "
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainingResults  = pd.DataFrame(data = data)\n",
    "pd.options.display.max_colwidth = 100\n",
    "trainingResults"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "21d2830e",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainingResults.to_csv('LR_pca_trainingResults.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f8e62ab",
   "metadata": {},
   "source": [
    "### Testing Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "51a55bba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Accuracy C</th>\n",
       "      <th>Accuracy Penalty</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Precision C</th>\n",
       "      <th>Precision Penalty</th>\n",
       "      <th>F1_micro</th>\n",
       "      <th>F1_micro C</th>\n",
       "      <th>F1_micro Penalty</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.600535</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>l1</td>\n",
       "      <td>0.704018</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>l2</td>\n",
       "      <td>0.701041</td>\n",
       "      <td>1.00</td>\n",
       "      <td>l1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.599120</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>l1</td>\n",
       "      <td>0.708640</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>l2</td>\n",
       "      <td>0.702781</td>\n",
       "      <td>1.00</td>\n",
       "      <td>l1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.599601</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>l1</td>\n",
       "      <td>0.706928</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>l2</td>\n",
       "      <td>0.701550</td>\n",
       "      <td>0.10</td>\n",
       "      <td>l2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.599332</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>l1</td>\n",
       "      <td>0.704444</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>l2</td>\n",
       "      <td>0.702159</td>\n",
       "      <td>100.00</td>\n",
       "      <td>l1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.600167</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>l1</td>\n",
       "      <td>0.709146</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>l2</td>\n",
       "      <td>0.704068</td>\n",
       "      <td>0.01</td>\n",
       "      <td>l2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.599389</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>l1</td>\n",
       "      <td>0.705512</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>l2</td>\n",
       "      <td>0.701169</td>\n",
       "      <td>1.00</td>\n",
       "      <td>l1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.599785</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>l1</td>\n",
       "      <td>0.705316</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>l2</td>\n",
       "      <td>0.700857</td>\n",
       "      <td>0.10</td>\n",
       "      <td>l2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.599969</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>l1</td>\n",
       "      <td>0.705015</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>l2</td>\n",
       "      <td>0.701692</td>\n",
       "      <td>1.00</td>\n",
       "      <td>l2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.600110</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>l1</td>\n",
       "      <td>0.705634</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>l2</td>\n",
       "      <td>0.701352</td>\n",
       "      <td>1.00</td>\n",
       "      <td>l2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.600422</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>l1</td>\n",
       "      <td>0.706381</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>l2</td>\n",
       "      <td>0.700900</td>\n",
       "      <td>NaN</td>\n",
       "      <td>none</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Accuracy  Accuracy C Accuracy Penalty  Precision  Precision C  \\\n",
       "0  0.600535      0.0001               l1   0.704018       0.0001   \n",
       "1  0.599120      0.0001               l1   0.708640       0.0001   \n",
       "2  0.599601      0.0001               l1   0.706928       0.0001   \n",
       "3  0.599332      0.0001               l1   0.704444       0.0001   \n",
       "4  0.600167      0.0001               l1   0.709146       0.0001   \n",
       "5  0.599389      0.0001               l1   0.705512       0.0001   \n",
       "6  0.599785      0.0001               l1   0.705316       0.0001   \n",
       "7  0.599969      0.0001               l1   0.705015       0.0001   \n",
       "8  0.600110      0.0001               l1   0.705634       0.0001   \n",
       "9  0.600422      0.0001               l1   0.706381       0.0001   \n",
       "\n",
       "  Precision Penalty  F1_micro  F1_micro C F1_micro Penalty  \n",
       "0                l2  0.701041        1.00               l1  \n",
       "1                l2  0.702781        1.00               l1  \n",
       "2                l2  0.701550        0.10               l2  \n",
       "3                l2  0.702159      100.00               l1  \n",
       "4                l2  0.704068        0.01               l2  \n",
       "5                l2  0.701169        1.00               l1  \n",
       "6                l2  0.700857        0.10               l2  \n",
       "7                l2  0.701692        1.00               l2  \n",
       "8                l2  0.701352        1.00               l2  \n",
       "9                l2  0.700900         NaN             none  "
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('Preprocessed_pca.csv', index_col = 0)\n",
    "trainingResults = pd.read_csv('LR_pca_trainingResults.csv', index_col = 0)\n",
    "trainingResults"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "9c90a482",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df[['PhysHlth', 'BMI', 'MentHlth', 'Income']]\n",
    "y = df.loc[:, 'Diabetes_binary']\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, y, test_size=0.2, shuffle=True, random_state = 11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "2153ea03",
   "metadata": {},
   "outputs": [],
   "source": [
    "LR = LogisticRegression(penalty = 'l1', C = 0.0001, solver = 'saga').fit(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "1598c291",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score, precision_score\n",
    "acc = LR.score(X_test, Y_test)\n",
    "predicted = LR.predict(X_test)\n",
    "f1 = f1_score(Y_test, predicted)\n",
    "precision = precision_score(Y_test, predicted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "67438dae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.6967124992927064\n",
      "Precision: 0.6890831652095942\n",
      "F1: 0.5342370524852278\n"
     ]
    }
   ],
   "source": [
    "print(\"Accuracy: \" + str(acc) + '\\n'\n",
    "      + \"Precision: \" + str(precision) + '\\n'\n",
    "      + \"F1: \" + str(f1))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
