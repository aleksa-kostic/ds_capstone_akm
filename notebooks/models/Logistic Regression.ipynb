{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c5923a4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.preprocessing import MinMaxScaler, MaxAbsScaler, StandardScaler \n",
    "#from sklearn.preprocessing import RobustScaler, Normalizer, QuantileTransformer, PowerTransformer\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "#from sklearn import svm, datasets\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn import preprocessing\n",
    "from datetime import datetime\n",
    "from sklearn.preprocessing import MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8f9a1a2a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>HighBP</th>\n",
       "      <th>HighChol</th>\n",
       "      <th>CholCheck</th>\n",
       "      <th>BMI</th>\n",
       "      <th>Smoker</th>\n",
       "      <th>Stroke</th>\n",
       "      <th>HeartDiseaseorAttack</th>\n",
       "      <th>PhysActivity</th>\n",
       "      <th>Fruits</th>\n",
       "      <th>Veggies</th>\n",
       "      <th>...</th>\n",
       "      <th>number_inpatient</th>\n",
       "      <th>number_diagnoses</th>\n",
       "      <th>race_AfricanAmerican</th>\n",
       "      <th>race_Asian</th>\n",
       "      <th>race_Caucasian</th>\n",
       "      <th>race_Hispanic</th>\n",
       "      <th>race_Other</th>\n",
       "      <th>readmitted_&lt;30</th>\n",
       "      <th>readmitted_&gt;30</th>\n",
       "      <th>Diabetes_binary</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>40</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>25</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>28</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>27</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>24</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>253675</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>45</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>253676</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>18</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>253677</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>28</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>253678</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>23</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>253679</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>25</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>253680 rows × 43 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        HighBP  HighChol  CholCheck  BMI  Smoker  Stroke  \\\n",
       "0            1         1          1   40       1       0   \n",
       "1            0         0          0   25       1       0   \n",
       "2            1         1          1   28       0       0   \n",
       "3            1         0          1   27       0       0   \n",
       "4            1         1          1   24       0       0   \n",
       "...        ...       ...        ...  ...     ...     ...   \n",
       "253675       1         1          1   45       0       0   \n",
       "253676       1         1          1   18       0       0   \n",
       "253677       0         0          1   28       0       0   \n",
       "253678       1         0          1   23       0       0   \n",
       "253679       1         1          1   25       0       0   \n",
       "\n",
       "        HeartDiseaseorAttack  PhysActivity  Fruits  Veggies  ...  \\\n",
       "0                          0             0       0        1  ...   \n",
       "1                          0             1       0        0  ...   \n",
       "2                          0             0       1        0  ...   \n",
       "3                          0             1       1        1  ...   \n",
       "4                          0             1       1        1  ...   \n",
       "...                      ...           ...     ...      ...  ...   \n",
       "253675                     0             0       1        1  ...   \n",
       "253676                     0             0       0        0  ...   \n",
       "253677                     0             1       1        0  ...   \n",
       "253678                     0             0       1        1  ...   \n",
       "253679                     1             1       1        0  ...   \n",
       "\n",
       "        number_inpatient  number_diagnoses  race_AfricanAmerican  race_Asian  \\\n",
       "0                      0                 9                     0           0   \n",
       "1                      5                 4                     0           0   \n",
       "2                      0                 4                     0           0   \n",
       "3                      1                 9                     0           0   \n",
       "4                      0                 4                     0           0   \n",
       "...                  ...               ...                   ...         ...   \n",
       "253675                 0                 5                     0           0   \n",
       "253676                 0                 7                     1           0   \n",
       "253677                 2                 5                     0           0   \n",
       "253678                 1                 9                     0           0   \n",
       "253679                 0                 9                     1           0   \n",
       "\n",
       "        race_Caucasian  race_Hispanic  race_Other  readmitted_<30  \\\n",
       "0                    1              0           0               0   \n",
       "1                    1              0           0               0   \n",
       "2                    1              0           0               0   \n",
       "3                    1              0           0               0   \n",
       "4                    0              0           1               0   \n",
       "...                ...            ...         ...             ...   \n",
       "253675               1              0           0               0   \n",
       "253676               0              0           0               0   \n",
       "253677               1              0           0               0   \n",
       "253678               1              0           0               0   \n",
       "253679               0              0           0               0   \n",
       "\n",
       "        readmitted_>30  Diabetes_binary  \n",
       "0                    1                0  \n",
       "1                    0                0  \n",
       "2                    0                0  \n",
       "3                    0                0  \n",
       "4                    0                0  \n",
       "...                ...              ...  \n",
       "253675               0                0  \n",
       "253676               0                1  \n",
       "253677               1                0  \n",
       "253678               0                0  \n",
       "253679               0                1  \n",
       "\n",
       "[253680 rows x 43 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('cleaned_augmented_2.csv')\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "185b32db",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.iloc[:,0:-1]\n",
    "scaler = MinMaxScaler()\n",
    "X_sc = scaler.fit_transform(X)\n",
    "X = pd.DataFrame(X_sc, columns=X.columns)\n",
    "y = df.loc[:, 'Diabetes_binary']\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, y, test_size=0.2, shuffle=True, random_state = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9e763e27",
   "metadata": {},
   "outputs": [],
   "source": [
    "acc_list = []\n",
    "roc_auc_ovr_List = []\n",
    "f1_micro_List = []"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62155024",
   "metadata": {},
   "source": [
    "## Trial 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cc362efc",
   "metadata": {},
   "outputs": [],
   "source": [
    "start = datetime.now()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a65d5cf2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Guest-All\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\Guest-All\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\Guest-All\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\Guest-All\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\Guest-All\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\Guest-All\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\Guest-All\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\Guest-All\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\Guest-All\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\Guest-All\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\Guest-All\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1183: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Guest-All\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1183: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Guest-All\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1183: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Guest-All\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1183: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Guest-All\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1183: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Guest-All\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1183: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Guest-All\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1183: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Guest-All\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1183: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Guest-All\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1183: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Guest-All\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1183: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Guest-All\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1183: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Guest-All\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1183: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Guest-All\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1183: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Guest-All\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1183: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Guest-All\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1183: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Guest-All\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1183: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Guest-All\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1183: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Guest-All\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1183: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Guest-All\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1183: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Guest-All\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1183: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Create a pipeline - RF is a stand in, we will populate the classifier part below\n",
    "pipe = Pipeline([('std', StandardScaler()),\n",
    "                 ('classifier', LogisticRegression())])\n",
    "\n",
    "# Create search space of candidate learning algorithms and their hyperparameters\n",
    "# note lbfgs can't do l1, and if you pass penalty='none' it expects no C value\n",
    "search_space = [{'classifier': [LogisticRegression(max_iter=10000)],\n",
    "                 'classifier__solver': ['saga'],\n",
    "                 'classifier__penalty': ['l1', 'l2'],\n",
    "                 'classifier__C': np.logspace(-4, 4, 9)},\n",
    "                {'classifier': [LogisticRegression(max_iter=10000)],\n",
    "                 'classifier__solver': ['lbfgs'],\n",
    "                 'classifier__penalty': ['l2'],\n",
    "                 'classifier__C': np.logspace(-4, 4, 9)},\n",
    "                {'classifier': [LogisticRegression(max_iter=10000)],\n",
    "                 'classifier__solver': ['lbfgs','saga'],\n",
    "                 'classifier__penalty': ['none']},\n",
    "                {'classifier': [LogisticRegression(max_iter=10000)]}\n",
    "                ]\n",
    "\n",
    "# Create grid search\n",
    "clf = GridSearchCV(pipe, search_space, cv=StratifiedKFold(n_splits=10),\n",
    "                   scoring=['accuracy', 'precision', 'f1_micro'], refit=False,\n",
    "                   verbose=0)\n",
    "\n",
    "# Fit grid search\n",
    "best_model = clf.fit(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8390b96b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'mean_fit_time': array([ 4.3806247 ,  3.38458867,  3.70996242,  3.31921885,  3.77723057,\n",
       "         3.4483922 ,  5.84528682,  3.38862891, 34.05756052,  3.35281987,\n",
       "         4.05180383,  3.38630817,  4.09154546,  3.69572284,  4.25456004,\n",
       "         3.27415876,  4.17047069,  3.43583052,  0.65617652,  0.77796314,\n",
       "         0.88159356,  0.87543197,  0.83753331,  0.85844407,  0.85071521,\n",
       "         0.89971638,  0.90057087,  0.90221264,  3.44342806,  0.86133146]),\n",
       " 'std_fit_time': array([0.624228  , 0.19975216, 0.21528794, 0.15484528, 0.38847248,\n",
       "        0.34299458, 3.79087744, 0.20929828, 4.89284511, 0.21705612,\n",
       "        0.1995044 , 0.1687092 , 0.45453564, 0.48707906, 0.38846292,\n",
       "        0.18133617, 0.44029364, 0.2912666 , 0.06753481, 0.05253252,\n",
       "        0.05335588, 0.04681559, 0.04356012, 0.02968904, 0.03619098,\n",
       "        0.03893984, 0.04305104, 0.02873817, 0.27658806, 0.02076749]),\n",
       " 'mean_score_time': array([0.04053006, 0.03519249, 0.03516369, 0.03276143, 0.03383169,\n",
       "        0.03475118, 0.03562136, 0.03502171, 0.03320272, 0.03390031,\n",
       "        0.03730059, 0.03397903, 0.03310261, 0.03866942, 0.03621731,\n",
       "        0.03474677, 0.03107262, 0.03023872, 0.03244841, 0.03334587,\n",
       "        0.0348886 , 0.03269641, 0.03081119, 0.03205459, 0.03334095,\n",
       "        0.0345    , 0.03305867, 0.0328455 , 0.03399646, 0.03445659]),\n",
       " 'std_score_time': array([0.01282212, 0.00522295, 0.0043943 , 0.00444442, 0.00352106,\n",
       "        0.00498537, 0.00755688, 0.00300731, 0.00489045, 0.00447209,\n",
       "        0.00397596, 0.0017152 , 0.0085787 , 0.00798552, 0.00642525,\n",
       "        0.00269369, 0.00453334, 0.00540082, 0.00594264, 0.00498523,\n",
       "        0.00418501, 0.00551086, 0.00349631, 0.00163002, 0.00491121,\n",
       "        0.00387421, 0.00440144, 0.00633858, 0.0023343 , 0.00359221]),\n",
       " 'param_classifier': masked_array(data=[LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000)],\n",
       "              mask=[False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'param_classifier__C': masked_array(data=[0.0001, 0.0001, 0.001, 0.001, 0.01, 0.01, 0.1, 0.1,\n",
       "                    1.0, 1.0, 10.0, 10.0, 100.0, 100.0, 1000.0, 1000.0,\n",
       "                    10000.0, 10000.0, 0.0001, 0.001, 0.01, 0.1, 1.0, 10.0,\n",
       "                    100.0, 1000.0, 10000.0, --, --, --],\n",
       "              mask=[False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False,  True,  True,  True],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'param_classifier__penalty': masked_array(data=['l1', 'l2', 'l1', 'l2', 'l1', 'l2', 'l1', 'l2', 'l1',\n",
       "                    'l2', 'l1', 'l2', 'l1', 'l2', 'l1', 'l2', 'l1', 'l2',\n",
       "                    'l2', 'l2', 'l2', 'l2', 'l2', 'l2', 'l2', 'l2', 'l2',\n",
       "                    'none', 'none', --],\n",
       "              mask=[False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False,  True],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'param_classifier__solver': masked_array(data=['saga', 'saga', 'saga', 'saga', 'saga', 'saga', 'saga',\n",
       "                    'saga', 'saga', 'saga', 'saga', 'saga', 'saga', 'saga',\n",
       "                    'saga', 'saga', 'saga', 'saga', 'lbfgs', 'lbfgs',\n",
       "                    'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs',\n",
       "                    'lbfgs', 'lbfgs', 'saga', --],\n",
       "              mask=[False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False,  True],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'params': [{'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 0.0001,\n",
       "   'classifier__penalty': 'l1',\n",
       "   'classifier__solver': 'saga'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 0.0001,\n",
       "   'classifier__penalty': 'l2',\n",
       "   'classifier__solver': 'saga'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 0.001,\n",
       "   'classifier__penalty': 'l1',\n",
       "   'classifier__solver': 'saga'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 0.001,\n",
       "   'classifier__penalty': 'l2',\n",
       "   'classifier__solver': 'saga'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 0.01,\n",
       "   'classifier__penalty': 'l1',\n",
       "   'classifier__solver': 'saga'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 0.01,\n",
       "   'classifier__penalty': 'l2',\n",
       "   'classifier__solver': 'saga'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 0.1,\n",
       "   'classifier__penalty': 'l1',\n",
       "   'classifier__solver': 'saga'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 0.1,\n",
       "   'classifier__penalty': 'l2',\n",
       "   'classifier__solver': 'saga'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 1.0,\n",
       "   'classifier__penalty': 'l1',\n",
       "   'classifier__solver': 'saga'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 1.0,\n",
       "   'classifier__penalty': 'l2',\n",
       "   'classifier__solver': 'saga'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 10.0,\n",
       "   'classifier__penalty': 'l1',\n",
       "   'classifier__solver': 'saga'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 10.0,\n",
       "   'classifier__penalty': 'l2',\n",
       "   'classifier__solver': 'saga'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 100.0,\n",
       "   'classifier__penalty': 'l1',\n",
       "   'classifier__solver': 'saga'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 100.0,\n",
       "   'classifier__penalty': 'l2',\n",
       "   'classifier__solver': 'saga'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 1000.0,\n",
       "   'classifier__penalty': 'l1',\n",
       "   'classifier__solver': 'saga'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 1000.0,\n",
       "   'classifier__penalty': 'l2',\n",
       "   'classifier__solver': 'saga'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 10000.0,\n",
       "   'classifier__penalty': 'l1',\n",
       "   'classifier__solver': 'saga'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 10000.0,\n",
       "   'classifier__penalty': 'l2',\n",
       "   'classifier__solver': 'saga'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 0.0001,\n",
       "   'classifier__penalty': 'l2',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 0.001,\n",
       "   'classifier__penalty': 'l2',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 0.01,\n",
       "   'classifier__penalty': 'l2',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 0.1,\n",
       "   'classifier__penalty': 'l2',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 1.0,\n",
       "   'classifier__penalty': 'l2',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 10.0,\n",
       "   'classifier__penalty': 'l2',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 100.0,\n",
       "   'classifier__penalty': 'l2',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 1000.0,\n",
       "   'classifier__penalty': 'l2',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 10000.0,\n",
       "   'classifier__penalty': 'l2',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__penalty': 'none',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__penalty': 'none',\n",
       "   'classifier__solver': 'saga'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000)}],\n",
       " 'split0_test_accuracy': array([0.86065533, 0.8691796 , 0.87149544, 0.87257945, 0.87253018,\n",
       "        0.87312146, 0.87307219, 0.87312146, 0.87312146, 0.87312146,\n",
       "        0.87312146, 0.87312146, 0.87312146, 0.87312146, 0.87312146,\n",
       "        0.87312146, 0.87312146, 0.87312146, 0.86922887, 0.87253018,\n",
       "        0.87312146, 0.87312146, 0.87312146, 0.87312146, 0.87312146,\n",
       "        0.87312146, 0.87312146, 0.87312146, 0.87312146, 0.87312146]),\n",
       " 'split1_test_accuracy': array([0.86065533, 0.86903178, 0.87006652, 0.87090416, 0.87115053,\n",
       "        0.87110126, 0.87100271, 0.87105198, 0.87110126, 0.87110126,\n",
       "        0.87110126, 0.87110126, 0.87110126, 0.87110126, 0.87110126,\n",
       "        0.87105198, 0.87110126, 0.87110126, 0.86903178, 0.87085489,\n",
       "        0.87110126, 0.87105198, 0.87110126, 0.87110126, 0.87110126,\n",
       "        0.87110126, 0.87110126, 0.87110126, 0.87110126, 0.87110126]),\n",
       " 'split2_test_accuracy': array([0.86065533, 0.86848978, 0.86957379, 0.86982015, 0.87080562,\n",
       "        0.87055925, 0.87031288, 0.87031288, 0.87036216, 0.87041143,\n",
       "        0.87041143, 0.87031288, 0.87041143, 0.87041143, 0.87041143,\n",
       "        0.87041143, 0.87041143, 0.87041143, 0.86848978, 0.86982015,\n",
       "        0.87055925, 0.87031288, 0.87041143, 0.87041143, 0.87041143,\n",
       "        0.87041143, 0.87041143, 0.87041143, 0.87041143, 0.87041143]),\n",
       " 'split3_test_accuracy': array([0.86060606, 0.86745504, 0.86972161, 0.87006652, 0.87026361,\n",
       "        0.87021434, 0.87036216, 0.87036216, 0.87036216, 0.87036216,\n",
       "        0.87036216, 0.87036216, 0.87036216, 0.87036216, 0.87036216,\n",
       "        0.87036216, 0.87036216, 0.87036216, 0.86745504, 0.87006652,\n",
       "        0.87021434, 0.87041143, 0.87036216, 0.87036216, 0.87036216,\n",
       "        0.87036216, 0.87036216, 0.87036216, 0.87036216, 0.87036216]),\n",
       " 'split4_test_accuracy': array([0.86064847, 0.87129201, 0.87355869, 0.87346014, 0.87326303,\n",
       "        0.8729181 , 0.87311521, 0.87296738, 0.87301666, 0.87301666,\n",
       "        0.87301666, 0.87301666, 0.87301666, 0.87301666, 0.87301666,\n",
       "        0.87301666, 0.87301666, 0.87301666, 0.87129201, 0.87346014,\n",
       "        0.8729181 , 0.87301666, 0.87301666, 0.87301666, 0.87301666,\n",
       "        0.87301666, 0.87301666, 0.87301666, 0.87301666, 0.87301666]),\n",
       " 'split5_test_accuracy': array([0.86064847, 0.86739923, 0.86730068, 0.8686804 , 0.8686804 ,\n",
       "        0.86882823, 0.86877895, 0.86882823, 0.86882823, 0.86877895,\n",
       "        0.86882823, 0.86882823, 0.86882823, 0.86882823, 0.86882823,\n",
       "        0.86882823, 0.86882823, 0.86882823, 0.86739923, 0.8686804 ,\n",
       "        0.86882823, 0.86882823, 0.86877895, 0.86882823, 0.86882823,\n",
       "        0.86882823, 0.86882823, 0.86882823, 0.86882823, 0.86877895]),\n",
       " 'split6_test_accuracy': array([0.86064847, 0.86754706, 0.86872967, 0.86996156, 0.87040505,\n",
       "        0.87010939, 0.87030649, 0.87015867, 0.87015867, 0.87015867,\n",
       "        0.87015867, 0.87015867, 0.87015867, 0.87015867, 0.87015867,\n",
       "        0.87015867, 0.87015867, 0.87015867, 0.86754706, 0.86996156,\n",
       "        0.87010939, 0.87015867, 0.87015867, 0.87015867, 0.87015867,\n",
       "        0.87015867, 0.87015867, 0.87015867, 0.87015867, 0.87015867]),\n",
       " 'split7_test_accuracy': array([0.86064847, 0.87040505, 0.87163694, 0.87395289, 0.8747413 ,\n",
       "        0.87429782, 0.8745442 , 0.87444565, 0.87444565, 0.87439637,\n",
       "        0.87439637, 0.87439637, 0.87439637, 0.87439637, 0.87439637,\n",
       "        0.87439637, 0.87439637, 0.87439637, 0.87040505, 0.87395289,\n",
       "        0.87429782, 0.87444565, 0.87439637, 0.87439637, 0.87439637,\n",
       "        0.87439637, 0.87439637, 0.87439637, 0.87439637, 0.87439637]),\n",
       " 'split8_test_accuracy': array([0.86064847, 0.86828619, 0.87074998, 0.87212969, 0.8725239 ,\n",
       "        0.87242535, 0.87242535, 0.8725239 , 0.87257317, 0.87262245,\n",
       "        0.87262245, 0.87262245, 0.87262245, 0.87262245, 0.87262245,\n",
       "        0.87262245, 0.87262245, 0.87262245, 0.86828619, 0.87212969,\n",
       "        0.87242535, 0.8725239 , 0.87262245, 0.87262245, 0.87262245,\n",
       "        0.87262245, 0.87262245, 0.87262245, 0.87262245, 0.87262245]),\n",
       " 'split9_test_accuracy': array([0.86064847, 0.86818764, 0.86872967, 0.86976446, 0.87010939,\n",
       "        0.86986301, 0.87006012, 0.87001084, 0.87001084, 0.87001084,\n",
       "        0.87001084, 0.87001084, 0.87001084, 0.87001084, 0.87001084,\n",
       "        0.87001084, 0.87001084, 0.87001084, 0.86818764, 0.86976446,\n",
       "        0.86986301, 0.87001084, 0.87001084, 0.87001084, 0.87001084,\n",
       "        0.87001084, 0.87001084, 0.87001084, 0.87001084, 0.87001084]),\n",
       " 'mean_test_accuracy': array([0.86064629, 0.86872734, 0.8701563 , 0.87113194, 0.8714473 ,\n",
       "        0.87134382, 0.87139803, 0.87137831, 0.87139802, 0.87139802,\n",
       "        0.87140295, 0.8713931 , 0.87140295, 0.87140295, 0.87140295,\n",
       "        0.87139802, 0.87140295, 0.87140295, 0.86873226, 0.87112209,\n",
       "        0.87134382, 0.87138817, 0.87139802, 0.87140295, 0.87140295,\n",
       "        0.87140295, 0.87140295, 0.87140295, 0.87140295, 0.87139802]),\n",
       " 'std_test_accuracy': array([1.37558211e-05, 1.22504906e-03, 1.69339036e-03, 1.69194255e-03,\n",
       "        1.70035353e-03, 1.65919045e-03, 1.70156261e-03, 1.68580152e-03,\n",
       "        1.68989482e-03, 1.68919503e-03, 1.68160252e-03, 1.68766206e-03,\n",
       "        1.68160252e-03, 1.68160252e-03, 1.68160252e-03, 1.68255123e-03,\n",
       "        1.68160252e-03, 1.68160252e-03, 1.22695582e-03, 1.68850172e-03,\n",
       "        1.65919045e-03, 1.68759054e-03, 1.68919503e-03, 1.68160252e-03,\n",
       "        1.68160252e-03, 1.68160252e-03, 1.68160252e-03, 1.68160252e-03,\n",
       "        1.68160252e-03, 1.68919503e-03]),\n",
       " 'rank_test_accuracy': array([30, 29, 27, 25,  1, 23, 14, 22, 15, 17,  2, 20,  2,  2,  2, 15,  2,\n",
       "         2, 28, 26, 23, 21, 17,  2,  2,  2,  2,  2,  2, 17]),\n",
       " 'split0_test_precision': array([0.        , 0.6587156 , 0.63513514, 0.62246964, 0.61509074,\n",
       "        0.61833489, 0.61731844, 0.6174559 , 0.6174559 , 0.6174559 ,\n",
       "        0.6174559 , 0.6174559 , 0.6174559 , 0.6174559 , 0.6174559 ,\n",
       "        0.6174559 , 0.6174559 , 0.6174559 , 0.65992647, 0.62184024,\n",
       "        0.61833489, 0.6174559 , 0.6174559 , 0.6174559 , 0.6174559 ,\n",
       "        0.6174559 , 0.6174559 , 0.6174559 , 0.6174559 , 0.6174559 ]),\n",
       " 'split1_test_precision': array([0.        , 0.65124555, 0.62073325, 0.60441767, 0.600377  ,\n",
       "        0.59742647, 0.59562842, 0.59599636, 0.59636364, 0.59636364,\n",
       "        0.59636364, 0.59636364, 0.59636364, 0.59636364, 0.59636364,\n",
       "        0.59582198, 0.59636364, 0.59636364, 0.65124555, 0.60381143,\n",
       "        0.59742647, 0.59599636, 0.59636364, 0.59636364, 0.59636364,\n",
       "        0.59636364, 0.59636364, 0.59636364, 0.59636364, 0.59636364]),\n",
       " 'split2_test_precision': array([0.        , 0.63874346, 0.61104294, 0.59393939, 0.59809524,\n",
       "        0.59366263, 0.59057301, 0.5904059 , 0.59061638, 0.59099265,\n",
       "        0.59099265, 0.59007353, 0.59099265, 0.59099265, 0.59099265,\n",
       "        0.59099265, 0.59099265, 0.59099265, 0.63874346, 0.59393939,\n",
       "        0.59366263, 0.5904059 , 0.59099265, 0.59099265, 0.59099265,\n",
       "        0.59099265, 0.59099265, 0.59099265, 0.59099265, 0.59099265]),\n",
       " 'split3_test_precision': array([0.        , 0.62045061, 0.60743322, 0.59108159, 0.58860759,\n",
       "        0.58545136, 0.58669002, 0.58623693, 0.58608696, 0.58608696,\n",
       "        0.58608696, 0.58608696, 0.58608696, 0.58608696, 0.58608696,\n",
       "        0.58608696, 0.58608696, 0.58608696, 0.62045061, 0.59108159,\n",
       "        0.58545136, 0.58659704, 0.58608696, 0.58608696, 0.58608696,\n",
       "        0.58608696, 0.58608696, 0.58608696, 0.58608696, 0.58608696]),\n",
       " 'split4_test_precision': array([0.        , 0.6862069 , 0.65821256, 0.62670565, 0.61962617,\n",
       "        0.61390668, 0.61510464, 0.61343013, 0.6137806 , 0.6137806 ,\n",
       "        0.6137806 , 0.6137806 , 0.6137806 , 0.6137806 , 0.6137806 ,\n",
       "        0.6137806 , 0.6137806 , 0.6137806 , 0.6862069 , 0.62670565,\n",
       "        0.61390668, 0.61398728, 0.6137806 , 0.6137806 , 0.6137806 ,\n",
       "        0.6137806 , 0.6137806 , 0.6137806 , 0.6137806 , 0.6137806 ]),\n",
       " 'split5_test_precision': array([0.        , 0.62851782, 0.58869908, 0.58516196, 0.57998037,\n",
       "        0.5793499 , 0.57849667, 0.57874763, 0.57874763, 0.57819905,\n",
       "        0.57859848, 0.57859848, 0.57859848, 0.57859848, 0.57859848,\n",
       "        0.57859848, 0.57859848, 0.57859848, 0.62851782, 0.58516196,\n",
       "        0.5793499 , 0.57874763, 0.57819905, 0.57859848, 0.57859848,\n",
       "        0.57859848, 0.57859848, 0.57859848, 0.57859848, 0.57819905]),\n",
       " 'split6_test_precision': array([0.        , 0.63944223, 0.60379747, 0.59732235, 0.59537572,\n",
       "        0.59005629, 0.59124767, 0.58960074, 0.58960074, 0.58960074,\n",
       "        0.58960074, 0.58960074, 0.58960074, 0.58960074, 0.58960074,\n",
       "        0.58960074, 0.58960074, 0.58960074, 0.63944223, 0.59732235,\n",
       "        0.59005629, 0.58960074, 0.58960074, 0.58960074, 0.58960074,\n",
       "        0.58960074, 0.58960074, 0.58960074, 0.58960074, 0.58960074]),\n",
       " 'split7_test_precision': array([0.        , 0.68065693, 0.64613368, 0.63719512, 0.63829787,\n",
       "        0.63029163, 0.63227017, 0.63084112, 0.63084112, 0.6302521 ,\n",
       "        0.6302521 , 0.6302521 , 0.6302521 , 0.6302521 , 0.6302521 ,\n",
       "        0.6302521 , 0.6302521 , 0.6302521 , 0.68065693, 0.63719512,\n",
       "        0.63029163, 0.63084112, 0.6302521 , 0.6302521 , 0.6302521 ,\n",
       "        0.6302521 , 0.6302521 , 0.6302521 , 0.6302521 , 0.6302521 ]),\n",
       " 'split8_test_precision': array([0.        , 0.64595104, 0.63191763, 0.61851475, 0.61733204,\n",
       "        0.61413563, 0.61305582, 0.61378659, 0.61415094, 0.61451461,\n",
       "        0.61451461, 0.61451461, 0.61451461, 0.61451461, 0.61451461,\n",
       "        0.61451461, 0.61451461, 0.61451461, 0.64595104, 0.61851475,\n",
       "        0.61413563, 0.61378659, 0.61451461, 0.61451461, 0.61451461,\n",
       "        0.61451461, 0.61451461, 0.61451461, 0.61451461, 0.61451461]),\n",
       " 'split9_test_precision': array([0.        , 0.64571429, 0.60539846, 0.59585492, 0.59411765,\n",
       "        0.58896289, 0.59000943, 0.5891182 , 0.5891182 , 0.5891182 ,\n",
       "        0.5891182 , 0.5891182 , 0.5891182 , 0.5891182 , 0.5891182 ,\n",
       "        0.5891182 , 0.5891182 , 0.5891182 , 0.64571429, 0.59585492,\n",
       "        0.58896289, 0.5891182 , 0.5891182 , 0.5891182 , 0.5891182 ,\n",
       "        0.5891182 , 0.5891182 , 0.5891182 , 0.5891182 , 0.5891182 ]),\n",
       " 'mean_test_precision': array([0.        , 0.64956444, 0.62085034, 0.60726631, 0.60469004,\n",
       "        0.60115784, 0.60103943, 0.60056195, 0.60067621, 0.60063644,\n",
       "        0.60067639, 0.60058448, 0.60067639, 0.60067639, 0.60067639,\n",
       "        0.60062222, 0.60067639, 0.60067639, 0.64968553, 0.60714274,\n",
       "        0.60115784, 0.60065368, 0.60063644, 0.60067639, 0.60067639,\n",
       "        0.60067639, 0.60067639, 0.60067639, 0.60067639, 0.60063644]),\n",
       " 'std_test_precision': array([0.        , 0.01983429, 0.02049684, 0.01672322, 0.01658435,\n",
       "        0.01593464, 0.01628814, 0.01612105, 0.01616952, 0.01614371,\n",
       "        0.01608855, 0.01614613, 0.01608855, 0.01608855, 0.01608855,\n",
       "        0.01610388, 0.01608855, 0.01608855, 0.0198934 , 0.01667809,\n",
       "        0.01593464, 0.01613462, 0.01614371, 0.01608855, 0.01608855,\n",
       "        0.01608855, 0.01608855, 0.01608855, 0.01608855, 0.01614371]),\n",
       " 'rank_test_precision': array([30,  2,  3,  4,  6,  7,  9, 29, 22, 24, 10, 28, 10, 10, 10, 27, 10,\n",
       "        10,  1,  5,  7, 23, 24, 10, 10, 10, 10, 10, 10, 24]),\n",
       " 'split0_test_f1_micro': array([0.86065533, 0.8691796 , 0.87149544, 0.87257945, 0.87253018,\n",
       "        0.87312146, 0.87307219, 0.87312146, 0.87312146, 0.87312146,\n",
       "        0.87312146, 0.87312146, 0.87312146, 0.87312146, 0.87312146,\n",
       "        0.87312146, 0.87312146, 0.87312146, 0.86922887, 0.87253018,\n",
       "        0.87312146, 0.87312146, 0.87312146, 0.87312146, 0.87312146,\n",
       "        0.87312146, 0.87312146, 0.87312146, 0.87312146, 0.87312146]),\n",
       " 'split1_test_f1_micro': array([0.86065533, 0.86903178, 0.87006652, 0.87090416, 0.87115053,\n",
       "        0.87110126, 0.87100271, 0.87105198, 0.87110126, 0.87110126,\n",
       "        0.87110126, 0.87110126, 0.87110126, 0.87110126, 0.87110126,\n",
       "        0.87105198, 0.87110126, 0.87110126, 0.86903178, 0.87085489,\n",
       "        0.87110126, 0.87105198, 0.87110126, 0.87110126, 0.87110126,\n",
       "        0.87110126, 0.87110126, 0.87110126, 0.87110126, 0.87110126]),\n",
       " 'split2_test_f1_micro': array([0.86065533, 0.86848978, 0.86957379, 0.86982015, 0.87080562,\n",
       "        0.87055925, 0.87031288, 0.87031288, 0.87036216, 0.87041143,\n",
       "        0.87041143, 0.87031288, 0.87041143, 0.87041143, 0.87041143,\n",
       "        0.87041143, 0.87041143, 0.87041143, 0.86848978, 0.86982015,\n",
       "        0.87055925, 0.87031288, 0.87041143, 0.87041143, 0.87041143,\n",
       "        0.87041143, 0.87041143, 0.87041143, 0.87041143, 0.87041143]),\n",
       " 'split3_test_f1_micro': array([0.86060606, 0.86745504, 0.86972161, 0.87006652, 0.87026361,\n",
       "        0.87021434, 0.87036216, 0.87036216, 0.87036216, 0.87036216,\n",
       "        0.87036216, 0.87036216, 0.87036216, 0.87036216, 0.87036216,\n",
       "        0.87036216, 0.87036216, 0.87036216, 0.86745504, 0.87006652,\n",
       "        0.87021434, 0.87041143, 0.87036216, 0.87036216, 0.87036216,\n",
       "        0.87036216, 0.87036216, 0.87036216, 0.87036216, 0.87036216]),\n",
       " 'split4_test_f1_micro': array([0.86064847, 0.87129201, 0.87355869, 0.87346014, 0.87326303,\n",
       "        0.8729181 , 0.87311521, 0.87296738, 0.87301666, 0.87301666,\n",
       "        0.87301666, 0.87301666, 0.87301666, 0.87301666, 0.87301666,\n",
       "        0.87301666, 0.87301666, 0.87301666, 0.87129201, 0.87346014,\n",
       "        0.8729181 , 0.87301666, 0.87301666, 0.87301666, 0.87301666,\n",
       "        0.87301666, 0.87301666, 0.87301666, 0.87301666, 0.87301666]),\n",
       " 'split5_test_f1_micro': array([0.86064847, 0.86739923, 0.86730068, 0.8686804 , 0.8686804 ,\n",
       "        0.86882823, 0.86877895, 0.86882823, 0.86882823, 0.86877895,\n",
       "        0.86882823, 0.86882823, 0.86882823, 0.86882823, 0.86882823,\n",
       "        0.86882823, 0.86882823, 0.86882823, 0.86739923, 0.8686804 ,\n",
       "        0.86882823, 0.86882823, 0.86877895, 0.86882823, 0.86882823,\n",
       "        0.86882823, 0.86882823, 0.86882823, 0.86882823, 0.86877895]),\n",
       " 'split6_test_f1_micro': array([0.86064847, 0.86754706, 0.86872967, 0.86996156, 0.87040505,\n",
       "        0.87010939, 0.87030649, 0.87015867, 0.87015867, 0.87015867,\n",
       "        0.87015867, 0.87015867, 0.87015867, 0.87015867, 0.87015867,\n",
       "        0.87015867, 0.87015867, 0.87015867, 0.86754706, 0.86996156,\n",
       "        0.87010939, 0.87015867, 0.87015867, 0.87015867, 0.87015867,\n",
       "        0.87015867, 0.87015867, 0.87015867, 0.87015867, 0.87015867]),\n",
       " 'split7_test_f1_micro': array([0.86064847, 0.87040505, 0.87163694, 0.87395289, 0.8747413 ,\n",
       "        0.87429782, 0.8745442 , 0.87444565, 0.87444565, 0.87439637,\n",
       "        0.87439637, 0.87439637, 0.87439637, 0.87439637, 0.87439637,\n",
       "        0.87439637, 0.87439637, 0.87439637, 0.87040505, 0.87395289,\n",
       "        0.87429782, 0.87444565, 0.87439637, 0.87439637, 0.87439637,\n",
       "        0.87439637, 0.87439637, 0.87439637, 0.87439637, 0.87439637]),\n",
       " 'split8_test_f1_micro': array([0.86064847, 0.86828619, 0.87074998, 0.87212969, 0.8725239 ,\n",
       "        0.87242535, 0.87242535, 0.8725239 , 0.87257317, 0.87262245,\n",
       "        0.87262245, 0.87262245, 0.87262245, 0.87262245, 0.87262245,\n",
       "        0.87262245, 0.87262245, 0.87262245, 0.86828619, 0.87212969,\n",
       "        0.87242535, 0.8725239 , 0.87262245, 0.87262245, 0.87262245,\n",
       "        0.87262245, 0.87262245, 0.87262245, 0.87262245, 0.87262245]),\n",
       " 'split9_test_f1_micro': array([0.86064847, 0.86818764, 0.86872967, 0.86976446, 0.87010939,\n",
       "        0.86986301, 0.87006012, 0.87001084, 0.87001084, 0.87001084,\n",
       "        0.87001084, 0.87001084, 0.87001084, 0.87001084, 0.87001084,\n",
       "        0.87001084, 0.87001084, 0.87001084, 0.86818764, 0.86976446,\n",
       "        0.86986301, 0.87001084, 0.87001084, 0.87001084, 0.87001084,\n",
       "        0.87001084, 0.87001084, 0.87001084, 0.87001084, 0.87001084]),\n",
       " 'mean_test_f1_micro': array([0.86064629, 0.86872734, 0.8701563 , 0.87113194, 0.8714473 ,\n",
       "        0.87134382, 0.87139803, 0.87137831, 0.87139802, 0.87139802,\n",
       "        0.87140295, 0.8713931 , 0.87140295, 0.87140295, 0.87140295,\n",
       "        0.87139802, 0.87140295, 0.87140295, 0.86873226, 0.87112209,\n",
       "        0.87134382, 0.87138817, 0.87139802, 0.87140295, 0.87140295,\n",
       "        0.87140295, 0.87140295, 0.87140295, 0.87140295, 0.87139802]),\n",
       " 'std_test_f1_micro': array([1.37558211e-05, 1.22504906e-03, 1.69339036e-03, 1.69194255e-03,\n",
       "        1.70035353e-03, 1.65919045e-03, 1.70156261e-03, 1.68580152e-03,\n",
       "        1.68989482e-03, 1.68919503e-03, 1.68160252e-03, 1.68766206e-03,\n",
       "        1.68160252e-03, 1.68160252e-03, 1.68160252e-03, 1.68255123e-03,\n",
       "        1.68160252e-03, 1.68160252e-03, 1.22695582e-03, 1.68850172e-03,\n",
       "        1.65919045e-03, 1.68759054e-03, 1.68919503e-03, 1.68160252e-03,\n",
       "        1.68160252e-03, 1.68160252e-03, 1.68160252e-03, 1.68160252e-03,\n",
       "        1.68160252e-03, 1.68919503e-03]),\n",
       " 'rank_test_f1_micro': array([30, 29, 27, 25,  1, 23, 14, 22, 15, 17,  2, 20,  2,  2,  2, 15,  2,\n",
       "         2, 28, 26, 23, 21, 17,  2,  2,  2,  2,  2,  2, 17])}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_model.cv_results_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a4578782",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([30, 29, 27, 25,  1, 23, 14, 22, 15, 17,  2, 20,  2,  2,  2, 15,  2,\n",
       "        2, 28, 26, 23, 21, 17,  2,  2,  2,  2,  2,  2, 17])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_model.cv_results_['rank_test_accuracy']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "29844e0d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'classifier': LogisticRegression(max_iter=10000),\n",
       " 'classifier__C': 0.0001,\n",
       " 'classifier__penalty': 'l1',\n",
       " 'classifier__solver': 'saga'}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_model.cv_results_['params'][ np.argmin(best_model.cv_results_['mean_test_accuracy'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "da854c46",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>classifier</th>\n",
       "      <th>C</th>\n",
       "      <th>penalty</th>\n",
       "      <th>solver</th>\n",
       "      <th>score_acc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.860646</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.868727</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.870156</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.871132</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0100</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.871447</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0100</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.871344</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.1000</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.871398</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.1000</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.871378</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.871398</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.871398</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>10.0000</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.871403</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>10.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.871393</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>100.0000</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.871403</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>100.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.871403</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>1000.0000</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.871403</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>1000.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.871398</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>10000.0000</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.871403</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>10000.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.871403</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.868732</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.871122</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0100</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.871344</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.1000</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.871388</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.871398</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>10.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.871403</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>100.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.871403</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>1000.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.871403</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>10000.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.871403</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>NaN</td>\n",
       "      <td>none</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.871403</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>NaN</td>\n",
       "      <td>none</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.871403</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.871398</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                            classifier           C penalty solver  score_acc\n",
       "0   LogisticRegression(max_iter=10000)      0.0001      l1   saga   0.860646\n",
       "1   LogisticRegression(max_iter=10000)      0.0001      l2   saga   0.868727\n",
       "2   LogisticRegression(max_iter=10000)      0.0010      l1   saga   0.870156\n",
       "3   LogisticRegression(max_iter=10000)      0.0010      l2   saga   0.871132\n",
       "4   LogisticRegression(max_iter=10000)      0.0100      l1   saga   0.871447\n",
       "5   LogisticRegression(max_iter=10000)      0.0100      l2   saga   0.871344\n",
       "6   LogisticRegression(max_iter=10000)      0.1000      l1   saga   0.871398\n",
       "7   LogisticRegression(max_iter=10000)      0.1000      l2   saga   0.871378\n",
       "8   LogisticRegression(max_iter=10000)      1.0000      l1   saga   0.871398\n",
       "9   LogisticRegression(max_iter=10000)      1.0000      l2   saga   0.871398\n",
       "10  LogisticRegression(max_iter=10000)     10.0000      l1   saga   0.871403\n",
       "11  LogisticRegression(max_iter=10000)     10.0000      l2   saga   0.871393\n",
       "12  LogisticRegression(max_iter=10000)    100.0000      l1   saga   0.871403\n",
       "13  LogisticRegression(max_iter=10000)    100.0000      l2   saga   0.871403\n",
       "14  LogisticRegression(max_iter=10000)   1000.0000      l1   saga   0.871403\n",
       "15  LogisticRegression(max_iter=10000)   1000.0000      l2   saga   0.871398\n",
       "16  LogisticRegression(max_iter=10000)  10000.0000      l1   saga   0.871403\n",
       "17  LogisticRegression(max_iter=10000)  10000.0000      l2   saga   0.871403\n",
       "18  LogisticRegression(max_iter=10000)      0.0001      l2  lbfgs   0.868732\n",
       "19  LogisticRegression(max_iter=10000)      0.0010      l2  lbfgs   0.871122\n",
       "20  LogisticRegression(max_iter=10000)      0.0100      l2  lbfgs   0.871344\n",
       "21  LogisticRegression(max_iter=10000)      0.1000      l2  lbfgs   0.871388\n",
       "22  LogisticRegression(max_iter=10000)      1.0000      l2  lbfgs   0.871398\n",
       "23  LogisticRegression(max_iter=10000)     10.0000      l2  lbfgs   0.871403\n",
       "24  LogisticRegression(max_iter=10000)    100.0000      l2  lbfgs   0.871403\n",
       "25  LogisticRegression(max_iter=10000)   1000.0000      l2  lbfgs   0.871403\n",
       "26  LogisticRegression(max_iter=10000)  10000.0000      l2  lbfgs   0.871403\n",
       "27  LogisticRegression(max_iter=10000)         NaN    none  lbfgs   0.871403\n",
       "28  LogisticRegression(max_iter=10000)         NaN    none   saga   0.871403\n",
       "29  LogisticRegression(max_iter=10000)         NaN     NaN    NaN   0.871398"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results = pd.DataFrame( best_model.cv_results_['params'] )\n",
    "\n",
    "results['score_acc'] = best_model.cv_results_['mean_test_accuracy']\n",
    "cols = results.columns.to_series().str.split('__').apply(lambda x: x[-1])\n",
    "results.columns = cols\n",
    "\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "cec21d10",
   "metadata": {},
   "outputs": [],
   "source": [
    "acc_list.append(results[0:1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8e72c958",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'classifier': LogisticRegression(max_iter=10000),\n",
       " 'classifier__C': 0.0001,\n",
       " 'classifier__penalty': 'l2',\n",
       " 'classifier__solver': 'lbfgs'}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_model.cv_results_['params'][ np.argmin(best_model.cv_results_['rank_test_precision'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "426600e6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>classifier</th>\n",
       "      <th>C</th>\n",
       "      <th>penalty</th>\n",
       "      <th>solver</th>\n",
       "      <th>score_precision</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.712024</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.699787</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.698351</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0100</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.695671</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0100</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.695219</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.1000</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.694857</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.1000</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.694780</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.694794</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.694844</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>10.0000</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.694817</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>10.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.694817</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>100.0000</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.694829</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>100.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.694829</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>1000.0000</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.694829</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>1000.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.694829</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>10000.0000</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.694817</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>10000.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.694817</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.712024</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.698351</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0100</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.695208</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.1000</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.694780</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.694818</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>10.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.694792</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>100.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.694792</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>1000.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.694792</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>10000.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.694792</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>NaN</td>\n",
       "      <td>none</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.694792</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>NaN</td>\n",
       "      <td>none</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.694829</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.694818</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                            classifier           C penalty solver  \\\n",
       "0   LogisticRegression(max_iter=10000)      0.0001      l1   saga   \n",
       "1   LogisticRegression(max_iter=10000)      0.0001      l2   saga   \n",
       "2   LogisticRegression(max_iter=10000)      0.0010      l1   saga   \n",
       "3   LogisticRegression(max_iter=10000)      0.0010      l2   saga   \n",
       "4   LogisticRegression(max_iter=10000)      0.0100      l1   saga   \n",
       "5   LogisticRegression(max_iter=10000)      0.0100      l2   saga   \n",
       "6   LogisticRegression(max_iter=10000)      0.1000      l1   saga   \n",
       "7   LogisticRegression(max_iter=10000)      0.1000      l2   saga   \n",
       "8   LogisticRegression(max_iter=10000)      1.0000      l1   saga   \n",
       "9   LogisticRegression(max_iter=10000)      1.0000      l2   saga   \n",
       "10  LogisticRegression(max_iter=10000)     10.0000      l1   saga   \n",
       "11  LogisticRegression(max_iter=10000)     10.0000      l2   saga   \n",
       "12  LogisticRegression(max_iter=10000)    100.0000      l1   saga   \n",
       "13  LogisticRegression(max_iter=10000)    100.0000      l2   saga   \n",
       "14  LogisticRegression(max_iter=10000)   1000.0000      l1   saga   \n",
       "15  LogisticRegression(max_iter=10000)   1000.0000      l2   saga   \n",
       "16  LogisticRegression(max_iter=10000)  10000.0000      l1   saga   \n",
       "17  LogisticRegression(max_iter=10000)  10000.0000      l2   saga   \n",
       "18  LogisticRegression(max_iter=10000)      0.0001      l2  lbfgs   \n",
       "19  LogisticRegression(max_iter=10000)      0.0010      l2  lbfgs   \n",
       "20  LogisticRegression(max_iter=10000)      0.0100      l2  lbfgs   \n",
       "21  LogisticRegression(max_iter=10000)      0.1000      l2  lbfgs   \n",
       "22  LogisticRegression(max_iter=10000)      1.0000      l2  lbfgs   \n",
       "23  LogisticRegression(max_iter=10000)     10.0000      l2  lbfgs   \n",
       "24  LogisticRegression(max_iter=10000)    100.0000      l2  lbfgs   \n",
       "25  LogisticRegression(max_iter=10000)   1000.0000      l2  lbfgs   \n",
       "26  LogisticRegression(max_iter=10000)  10000.0000      l2  lbfgs   \n",
       "27  LogisticRegression(max_iter=10000)         NaN    none  lbfgs   \n",
       "28  LogisticRegression(max_iter=10000)         NaN    none   saga   \n",
       "29  LogisticRegression(max_iter=10000)         NaN     NaN    NaN   \n",
       "\n",
       "    score_precision  \n",
       "0          0.000000  \n",
       "1          0.712024  \n",
       "2          0.699787  \n",
       "3          0.698351  \n",
       "4          0.695671  \n",
       "5          0.695219  \n",
       "6          0.694857  \n",
       "7          0.694780  \n",
       "8          0.694794  \n",
       "9          0.694844  \n",
       "10         0.694817  \n",
       "11         0.694817  \n",
       "12         0.694829  \n",
       "13         0.694829  \n",
       "14         0.694829  \n",
       "15         0.694829  \n",
       "16         0.694817  \n",
       "17         0.694817  \n",
       "18         0.712024  \n",
       "19         0.698351  \n",
       "20         0.695208  \n",
       "21         0.694780  \n",
       "22         0.694818  \n",
       "23         0.694792  \n",
       "24         0.694792  \n",
       "25         0.694792  \n",
       "26         0.694792  \n",
       "27         0.694792  \n",
       "28         0.694829  \n",
       "29         0.694818  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results = pd.DataFrame( best_model.cv_results_['params'] )\n",
    "\n",
    "results['score_precision'] = best_model.cv_results_['mean_test_precision']\n",
    "cols = results.columns.to_series().str.split('__').apply(lambda x: x[-1])\n",
    "results.columns = cols\n",
    "\n",
    "\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "46a6f899",
   "metadata": {},
   "outputs": [],
   "source": [
    "roc_auc_ovr_List.append(results[18:19])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6ceb54de",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'classifier': LogisticRegression(max_iter=10000),\n",
       " 'classifier__C': 0.01,\n",
       " 'classifier__penalty': 'l1',\n",
       " 'classifier__solver': 'saga'}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_model.cv_results_['params'][ np.argmin(best_model.cv_results_['rank_test_f1_micro'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "886a47b2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>classifier</th>\n",
       "      <th>C</th>\n",
       "      <th>penalty</th>\n",
       "      <th>solver</th>\n",
       "      <th>score_f1_micro</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.860646</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.868727</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.870156</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.871132</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0100</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.871447</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0100</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.871344</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.1000</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.871398</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.1000</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.871378</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.871398</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.871398</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>10.0000</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.871403</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>10.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.871393</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>100.0000</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.871403</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>100.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.871403</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>1000.0000</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.871403</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>1000.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.871398</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>10000.0000</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.871403</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>10000.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.871403</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.868732</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.871122</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0100</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.871344</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.1000</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.871388</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.871398</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>10.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.871403</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>100.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.871403</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>1000.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.871403</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>10000.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.871403</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>NaN</td>\n",
       "      <td>none</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.871403</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>NaN</td>\n",
       "      <td>none</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.871403</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.871398</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                            classifier           C penalty solver  \\\n",
       "0   LogisticRegression(max_iter=10000)      0.0001      l1   saga   \n",
       "1   LogisticRegression(max_iter=10000)      0.0001      l2   saga   \n",
       "2   LogisticRegression(max_iter=10000)      0.0010      l1   saga   \n",
       "3   LogisticRegression(max_iter=10000)      0.0010      l2   saga   \n",
       "4   LogisticRegression(max_iter=10000)      0.0100      l1   saga   \n",
       "5   LogisticRegression(max_iter=10000)      0.0100      l2   saga   \n",
       "6   LogisticRegression(max_iter=10000)      0.1000      l1   saga   \n",
       "7   LogisticRegression(max_iter=10000)      0.1000      l2   saga   \n",
       "8   LogisticRegression(max_iter=10000)      1.0000      l1   saga   \n",
       "9   LogisticRegression(max_iter=10000)      1.0000      l2   saga   \n",
       "10  LogisticRegression(max_iter=10000)     10.0000      l1   saga   \n",
       "11  LogisticRegression(max_iter=10000)     10.0000      l2   saga   \n",
       "12  LogisticRegression(max_iter=10000)    100.0000      l1   saga   \n",
       "13  LogisticRegression(max_iter=10000)    100.0000      l2   saga   \n",
       "14  LogisticRegression(max_iter=10000)   1000.0000      l1   saga   \n",
       "15  LogisticRegression(max_iter=10000)   1000.0000      l2   saga   \n",
       "16  LogisticRegression(max_iter=10000)  10000.0000      l1   saga   \n",
       "17  LogisticRegression(max_iter=10000)  10000.0000      l2   saga   \n",
       "18  LogisticRegression(max_iter=10000)      0.0001      l2  lbfgs   \n",
       "19  LogisticRegression(max_iter=10000)      0.0010      l2  lbfgs   \n",
       "20  LogisticRegression(max_iter=10000)      0.0100      l2  lbfgs   \n",
       "21  LogisticRegression(max_iter=10000)      0.1000      l2  lbfgs   \n",
       "22  LogisticRegression(max_iter=10000)      1.0000      l2  lbfgs   \n",
       "23  LogisticRegression(max_iter=10000)     10.0000      l2  lbfgs   \n",
       "24  LogisticRegression(max_iter=10000)    100.0000      l2  lbfgs   \n",
       "25  LogisticRegression(max_iter=10000)   1000.0000      l2  lbfgs   \n",
       "26  LogisticRegression(max_iter=10000)  10000.0000      l2  lbfgs   \n",
       "27  LogisticRegression(max_iter=10000)         NaN    none  lbfgs   \n",
       "28  LogisticRegression(max_iter=10000)         NaN    none   saga   \n",
       "29  LogisticRegression(max_iter=10000)         NaN     NaN    NaN   \n",
       "\n",
       "    score_f1_micro  \n",
       "0         0.860646  \n",
       "1         0.868727  \n",
       "2         0.870156  \n",
       "3         0.871132  \n",
       "4         0.871447  \n",
       "5         0.871344  \n",
       "6         0.871398  \n",
       "7         0.871378  \n",
       "8         0.871398  \n",
       "9         0.871398  \n",
       "10        0.871403  \n",
       "11        0.871393  \n",
       "12        0.871403  \n",
       "13        0.871403  \n",
       "14        0.871403  \n",
       "15        0.871398  \n",
       "16        0.871403  \n",
       "17        0.871403  \n",
       "18        0.868732  \n",
       "19        0.871122  \n",
       "20        0.871344  \n",
       "21        0.871388  \n",
       "22        0.871398  \n",
       "23        0.871403  \n",
       "24        0.871403  \n",
       "25        0.871403  \n",
       "26        0.871403  \n",
       "27        0.871403  \n",
       "28        0.871403  \n",
       "29        0.871398  "
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results = pd.DataFrame( best_model.cv_results_['params'] )\n",
    "\n",
    "\n",
    "results['score_f1_micro'] = best_model.cv_results_['mean_test_f1_micro']\n",
    "cols = results.columns.to_series().str.split('__').apply(lambda x: x[-1])\n",
    "results.columns = cols\n",
    "\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "943aea19",
   "metadata": {},
   "outputs": [],
   "source": [
    "f1_micro_List.append(results[4:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02458d54",
   "metadata": {},
   "source": [
    "## Trial 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "7bf26ccd",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, Y_train, Y_test = train_test_split(X, y, test_size=0.2, shuffle=True, random_state = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "4628ee31",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Guest-All\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\Guest-All\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\Guest-All\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\Guest-All\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\Guest-All\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\Guest-All\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\Guest-All\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\Guest-All\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\Guest-All\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\Guest-All\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\Guest-All\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1183: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Guest-All\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1183: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Guest-All\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1183: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Guest-All\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1183: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Guest-All\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1183: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Guest-All\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1183: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Guest-All\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1183: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Guest-All\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1183: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Guest-All\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1183: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Guest-All\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1183: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Guest-All\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1183: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Guest-All\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1183: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Guest-All\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1183: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Guest-All\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1183: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Guest-All\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1183: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Guest-All\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1183: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Guest-All\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1183: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Guest-All\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1183: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Guest-All\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1183: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Guest-All\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1183: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Create a pipeline - RF is a stand in, we will populate the classifier part below\n",
    "pipe = Pipeline([('std', StandardScaler()),\n",
    "                 ('classifier', LogisticRegression())])\n",
    "\n",
    "# Create search space of candidate learning algorithms and their hyperparameters\n",
    "# note lbfgs can't do l1, and if you pass penalty='none' it expects no C value\n",
    "search_space = [{'classifier': [LogisticRegression(max_iter=10000)],\n",
    "                 'classifier__solver': ['saga'],\n",
    "                 'classifier__penalty': ['l1', 'l2'],\n",
    "                 'classifier__C': np.logspace(-4, 4, 9)},\n",
    "                {'classifier': [LogisticRegression(max_iter=10000)],\n",
    "                 'classifier__solver': ['lbfgs'],\n",
    "                 'classifier__penalty': ['l2'],\n",
    "                 'classifier__C': np.logspace(-4, 4, 9)},\n",
    "                {'classifier': [LogisticRegression(max_iter=10000)],\n",
    "                 'classifier__solver': ['lbfgs','saga'],\n",
    "                 'classifier__penalty': ['none']},\n",
    "                {'classifier': [LogisticRegression(max_iter=10000)]}\n",
    "                ]\n",
    "\n",
    "# Create grid search\n",
    "clf = GridSearchCV(pipe, search_space, cv=StratifiedKFold(n_splits=10),\n",
    "                   scoring=['accuracy', 'precision', 'f1_micro'], refit=False,\n",
    "                   verbose=0)\n",
    "\n",
    "# Fit grid search\n",
    "best_model = clf.fit(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "2bf2b4d0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'mean_fit_time': array([ 3.94026377,  3.59192417,  3.83788204,  3.56049833,  4.23880861,\n",
       "         3.47576578,  4.48458536,  3.53805666, 29.34059157,  3.2486536 ,\n",
       "         4.29721823,  3.1292469 ,  4.1443958 ,  3.15488994,  4.03799834,\n",
       "         3.36253641,  4.07495077,  3.30090339,  0.60226915,  0.73706663,\n",
       "         0.78173201,  0.97019107,  0.97807169,  0.97526503,  0.95188808,\n",
       "         0.93307862,  0.81913836,  0.79770076,  3.19881263,  0.79571426]),\n",
       " 'std_fit_time': array([0.26827714, 0.30940718, 0.29274109, 0.32732482, 0.46560334,\n",
       "        0.34944967, 0.32014679, 0.36872093, 5.45043547, 0.30093704,\n",
       "        0.37750744, 0.18407931, 0.28661883, 0.27272725, 0.36262261,\n",
       "        0.30428092, 0.26755065, 0.17872438, 0.03168996, 0.05928897,\n",
       "        0.10505277, 0.19150329, 0.12292587, 0.10034138, 0.1418659 ,\n",
       "        0.15483996, 0.1052491 , 0.02105577, 0.17027599, 0.02218333]),\n",
       " 'mean_score_time': array([0.03570385, 0.03520765, 0.03429677, 0.0354193 , 0.04046116,\n",
       "        0.04029939, 0.03635092, 0.0353076 , 0.03481705, 0.03327215,\n",
       "        0.03373914, 0.03078434, 0.03527141, 0.03254287, 0.03568187,\n",
       "        0.03539526, 0.03199332, 0.0320436 , 0.03499901, 0.03357713,\n",
       "        0.03057818, 0.03474979, 0.03819704, 0.0364681 , 0.03475745,\n",
       "        0.03541999, 0.02989573, 0.03158705, 0.03304403, 0.02941043]),\n",
       " 'std_score_time': array([0.00405768, 0.00461503, 0.00686986, 0.006174  , 0.00867887,\n",
       "        0.00659158, 0.00628471, 0.00295489, 0.00412322, 0.00353891,\n",
       "        0.00439519, 0.0052184 , 0.00390693, 0.00459804, 0.00435611,\n",
       "        0.00381519, 0.00443433, 0.00490058, 0.0029991 , 0.00456522,\n",
       "        0.00556002, 0.0086914 , 0.00660303, 0.00675208, 0.00897266,\n",
       "        0.00906685, 0.00608974, 0.00275032, 0.00345863, 0.00493668]),\n",
       " 'param_classifier': masked_array(data=[LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000)],\n",
       "              mask=[False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'param_classifier__C': masked_array(data=[0.0001, 0.0001, 0.001, 0.001, 0.01, 0.01, 0.1, 0.1,\n",
       "                    1.0, 1.0, 10.0, 10.0, 100.0, 100.0, 1000.0, 1000.0,\n",
       "                    10000.0, 10000.0, 0.0001, 0.001, 0.01, 0.1, 1.0, 10.0,\n",
       "                    100.0, 1000.0, 10000.0, --, --, --],\n",
       "              mask=[False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False,  True,  True,  True],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'param_classifier__penalty': masked_array(data=['l1', 'l2', 'l1', 'l2', 'l1', 'l2', 'l1', 'l2', 'l1',\n",
       "                    'l2', 'l1', 'l2', 'l1', 'l2', 'l1', 'l2', 'l1', 'l2',\n",
       "                    'l2', 'l2', 'l2', 'l2', 'l2', 'l2', 'l2', 'l2', 'l2',\n",
       "                    'none', 'none', --],\n",
       "              mask=[False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False,  True],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'param_classifier__solver': masked_array(data=['saga', 'saga', 'saga', 'saga', 'saga', 'saga', 'saga',\n",
       "                    'saga', 'saga', 'saga', 'saga', 'saga', 'saga', 'saga',\n",
       "                    'saga', 'saga', 'saga', 'saga', 'lbfgs', 'lbfgs',\n",
       "                    'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs',\n",
       "                    'lbfgs', 'lbfgs', 'saga', --],\n",
       "              mask=[False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False,  True],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'params': [{'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 0.0001,\n",
       "   'classifier__penalty': 'l1',\n",
       "   'classifier__solver': 'saga'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 0.0001,\n",
       "   'classifier__penalty': 'l2',\n",
       "   'classifier__solver': 'saga'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 0.001,\n",
       "   'classifier__penalty': 'l1',\n",
       "   'classifier__solver': 'saga'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 0.001,\n",
       "   'classifier__penalty': 'l2',\n",
       "   'classifier__solver': 'saga'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 0.01,\n",
       "   'classifier__penalty': 'l1',\n",
       "   'classifier__solver': 'saga'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 0.01,\n",
       "   'classifier__penalty': 'l2',\n",
       "   'classifier__solver': 'saga'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 0.1,\n",
       "   'classifier__penalty': 'l1',\n",
       "   'classifier__solver': 'saga'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 0.1,\n",
       "   'classifier__penalty': 'l2',\n",
       "   'classifier__solver': 'saga'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 1.0,\n",
       "   'classifier__penalty': 'l1',\n",
       "   'classifier__solver': 'saga'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 1.0,\n",
       "   'classifier__penalty': 'l2',\n",
       "   'classifier__solver': 'saga'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 10.0,\n",
       "   'classifier__penalty': 'l1',\n",
       "   'classifier__solver': 'saga'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 10.0,\n",
       "   'classifier__penalty': 'l2',\n",
       "   'classifier__solver': 'saga'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 100.0,\n",
       "   'classifier__penalty': 'l1',\n",
       "   'classifier__solver': 'saga'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 100.0,\n",
       "   'classifier__penalty': 'l2',\n",
       "   'classifier__solver': 'saga'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 1000.0,\n",
       "   'classifier__penalty': 'l1',\n",
       "   'classifier__solver': 'saga'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 1000.0,\n",
       "   'classifier__penalty': 'l2',\n",
       "   'classifier__solver': 'saga'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 10000.0,\n",
       "   'classifier__penalty': 'l1',\n",
       "   'classifier__solver': 'saga'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 10000.0,\n",
       "   'classifier__penalty': 'l2',\n",
       "   'classifier__solver': 'saga'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 0.0001,\n",
       "   'classifier__penalty': 'l2',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 0.001,\n",
       "   'classifier__penalty': 'l2',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 0.01,\n",
       "   'classifier__penalty': 'l2',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 0.1,\n",
       "   'classifier__penalty': 'l2',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 1.0,\n",
       "   'classifier__penalty': 'l2',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 10.0,\n",
       "   'classifier__penalty': 'l2',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 100.0,\n",
       "   'classifier__penalty': 'l2',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 1000.0,\n",
       "   'classifier__penalty': 'l2',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 10000.0,\n",
       "   'classifier__penalty': 'l2',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__penalty': 'none',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__penalty': 'none',\n",
       "   'classifier__solver': 'saga'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000)}],\n",
       " 'split0_test_accuracy': array([0.86100025, 0.86804632, 0.86883469, 0.8699187 , 0.87001725,\n",
       "        0.87016507, 0.87011579, 0.86996797, 0.87001725, 0.87001725,\n",
       "        0.87001725, 0.87001725, 0.87001725, 0.87001725, 0.87001725,\n",
       "        0.87001725, 0.87001725, 0.86996797, 0.86804632, 0.8699187 ,\n",
       "        0.87016507, 0.86996797, 0.87001725, 0.87001725, 0.87001725,\n",
       "        0.87001725, 0.87001725, 0.87001725, 0.87001725, 0.87001725]),\n",
       " 'split1_test_accuracy': array([0.86100025, 0.86691303, 0.86824341, 0.86853905, 0.8686376 ,\n",
       "        0.86883469, 0.86873614, 0.86868687, 0.8686376 , 0.86868687,\n",
       "        0.86868687, 0.86868687, 0.86868687, 0.86868687, 0.86868687,\n",
       "        0.86868687, 0.86868687, 0.86868687, 0.86691303, 0.86853905,\n",
       "        0.86883469, 0.86873614, 0.86868687, 0.86868687, 0.86868687,\n",
       "        0.86868687, 0.86868687, 0.86868687, 0.86868687, 0.86868687]),\n",
       " 'split2_test_accuracy': array([0.86095097, 0.87021434, 0.87174181, 0.87307219, 0.87376201,\n",
       "        0.8734171 , 0.87346637, 0.87331855, 0.87331855, 0.87326928,\n",
       "        0.87326928, 0.87326928, 0.87326928, 0.87326928, 0.87326928,\n",
       "        0.87326928, 0.87326928, 0.87326928, 0.87021434, 0.87307219,\n",
       "        0.8734171 , 0.87336782, 0.87331855, 0.87326928, 0.87326928,\n",
       "        0.87326928, 0.87326928, 0.87326928, 0.87326928, 0.87331855]),\n",
       " 'split3_test_accuracy': array([0.86095097, 0.86839123, 0.87026361, 0.87223454, 0.872678  ,\n",
       "        0.87248091, 0.87223454, 0.87223454, 0.87223454, 0.87223454,\n",
       "        0.87223454, 0.87228381, 0.87228381, 0.87223454, 0.87223454,\n",
       "        0.87223454, 0.87223454, 0.87223454, 0.86839123, 0.87223454,\n",
       "        0.87248091, 0.87223454, 0.87223454, 0.87223454, 0.87223454,\n",
       "        0.87223454, 0.87223454, 0.87223454, 0.87223454, 0.87223454]),\n",
       " 'split4_test_accuracy': array([0.8609934 , 0.86843402, 0.86951808, 0.87217897, 0.87193259,\n",
       "        0.87237607, 0.87247462, 0.87257317, 0.87262245, 0.87257317,\n",
       "        0.87257317, 0.87257317, 0.87257317, 0.87257317, 0.87257317,\n",
       "        0.87257317, 0.87257317, 0.87257317, 0.86843402, 0.87217897,\n",
       "        0.87237607, 0.87257317, 0.87257317, 0.87257317, 0.87257317,\n",
       "        0.87257317, 0.87257317, 0.87257317, 0.87262245, 0.87257317]),\n",
       " 'split5_test_accuracy': array([0.8609934 , 0.8688775 , 0.86912388, 0.86991229, 0.86897605,\n",
       "        0.86946881, 0.86941953, 0.86941953, 0.86941953, 0.86946881,\n",
       "        0.86946881, 0.86946881, 0.86946881, 0.86946881, 0.86946881,\n",
       "        0.86946881, 0.86946881, 0.86946881, 0.8688775 , 0.86991229,\n",
       "        0.86946881, 0.86941953, 0.86946881, 0.86946881, 0.86946881,\n",
       "        0.86946881, 0.86946881, 0.86946881, 0.86946881, 0.86946881]),\n",
       " 'split6_test_accuracy': array([0.8609934 , 0.86956736, 0.87208042, 0.87390362, 0.87380507,\n",
       "        0.87341086, 0.87316448, 0.87321376, 0.87321376, 0.87326303,\n",
       "        0.87326303, 0.87326303, 0.87326303, 0.87326303, 0.87326303,\n",
       "        0.87326303, 0.87326303, 0.87326303, 0.86956736, 0.87395289,\n",
       "        0.87341086, 0.87321376, 0.87326303, 0.87326303, 0.87326303,\n",
       "        0.87326303, 0.87326303, 0.87326303, 0.87326303, 0.87326303]),\n",
       " 'split7_test_accuracy': array([0.8609934 , 0.86971519, 0.87129201, 0.87153839, 0.87212969,\n",
       "        0.87237607, 0.87227752, 0.8723268 , 0.8723268 , 0.8723268 ,\n",
       "        0.8723268 , 0.8723268 , 0.8723268 , 0.8723268 , 0.8723268 ,\n",
       "        0.8723268 , 0.8723268 , 0.8723268 , 0.86966591, 0.87153839,\n",
       "        0.8723268 , 0.8723268 , 0.8723268 , 0.8723268 , 0.8723268 ,\n",
       "        0.8723268 , 0.8723268 , 0.8723268 , 0.8723268 , 0.8723268 ]),\n",
       " 'split8_test_accuracy': array([0.8609934 , 0.86818764, 0.86991229, 0.87134128, 0.87143983,\n",
       "        0.87114418, 0.87143983, 0.87139056, 0.87134128, 0.87143983,\n",
       "        0.87139056, 0.87134128, 0.87134128, 0.87134128, 0.87134128,\n",
       "        0.87134128, 0.87143983, 0.87139056, 0.86818764, 0.87134128,\n",
       "        0.87114418, 0.87139056, 0.87139056, 0.87139056, 0.87139056,\n",
       "        0.87139056, 0.87139056, 0.87139056, 0.87134128, 0.87139056]),\n",
       " 'split9_test_accuracy': array([0.8609934 , 0.86981374, 0.87035577, 0.87193259, 0.87193259,\n",
       "        0.87222824, 0.8723268 , 0.87247462, 0.87247462, 0.87247462,\n",
       "        0.87247462, 0.87247462, 0.87247462, 0.87247462, 0.87247462,\n",
       "        0.87247462, 0.87247462, 0.87247462, 0.86981374, 0.87188332,\n",
       "        0.87222824, 0.87247462, 0.87247462, 0.87247462, 0.87247462,\n",
       "        0.87247462, 0.87247462, 0.87247462, 0.87247462, 0.87247462]),\n",
       " 'mean_test_accuracy': array([0.86098628, 0.86881604, 0.8701366 , 0.87145716, 0.87153107,\n",
       "        0.8715902 , 0.87156556, 0.87156064, 0.87156064, 0.87157542,\n",
       "        0.87157049, 0.87157049, 0.87157049, 0.87156557, 0.87156557,\n",
       "        0.87156557, 0.87157542, 0.87156557, 0.86881111, 0.87145716,\n",
       "        0.87158527, 0.87157049, 0.87157542, 0.87157049, 0.87157049,\n",
       "        0.87157049, 0.87157049, 0.87157049, 0.87157049, 0.87157542]),\n",
       " 'std_test_accuracy': array([1.78526808e-05, 9.62422052e-04, 1.20382225e-03, 1.52426064e-03,\n",
       "        1.71302907e-03, 1.53006851e-03, 1.52507725e-03, 1.55348632e-03,\n",
       "        1.56159233e-03, 1.54104223e-03, 1.54154660e-03, 1.54439915e-03,\n",
       "        1.54439915e-03, 1.54219250e-03, 1.54219250e-03, 1.54219250e-03,\n",
       "        1.54104223e-03, 1.54657399e-03, 9.57921443e-04, 1.53077785e-03,\n",
       "        1.52760706e-03, 1.55006834e-03, 1.54703760e-03, 1.54154660e-03,\n",
       "        1.54154660e-03, 1.54154660e-03, 1.54154660e-03, 1.54154660e-03,\n",
       "        1.54547933e-03, 1.54703760e-03]),\n",
       " 'rank_test_accuracy': array([30, 28, 27, 25, 24,  1, 21, 22, 22,  3,  7, 14, 14, 18, 18, 18,  3,\n",
       "        17, 29, 25,  2, 16,  5,  7,  7,  7,  7,  7,  7,  5]),\n",
       " 'split0_test_precision': array([0.        , 0.6346516 , 0.60474308, 0.59536354, 0.59104478,\n",
       "        0.59117647, 0.5898931 , 0.58817829, 0.58857696, 0.58857696,\n",
       "        0.58857696, 0.58857696, 0.58857696, 0.58857696, 0.58857696,\n",
       "        0.58857696, 0.58857696, 0.58817829, 0.6346516 , 0.59536354,\n",
       "        0.59117647, 0.58817829, 0.58857696, 0.58857696, 0.58857696,\n",
       "        0.58857696, 0.58857696, 0.58857696, 0.58857696, 0.58857696]),\n",
       " 'split1_test_precision': array([0.        , 0.61904762, 0.59459459, 0.57894737, 0.57531584,\n",
       "        0.57535545, 0.57370892, 0.57303371, 0.57249766, 0.5728972 ,\n",
       "        0.5728972 , 0.5728972 , 0.5728972 , 0.5728972 , 0.5728972 ,\n",
       "        0.5728972 , 0.5728972 , 0.5728972 , 0.61904762, 0.57894737,\n",
       "        0.57535545, 0.57357076, 0.5728972 , 0.5728972 , 0.5728972 ,\n",
       "        0.5728972 , 0.5728972 , 0.5728972 , 0.5728972 , 0.5728972 ]),\n",
       " 'split2_test_precision': array([0.        , 0.66491228, 0.63402693, 0.62011719, 0.62014787,\n",
       "        0.61531449, 0.61524501, 0.61357466, 0.61357466, 0.61301989,\n",
       "        0.61301989, 0.61301989, 0.61301989, 0.61301989, 0.61301989,\n",
       "        0.61301989, 0.61301989, 0.61301989, 0.66491228, 0.62011719,\n",
       "        0.61531449, 0.61413043, 0.61357466, 0.61301989, 0.61301989,\n",
       "        0.61301989, 0.61301989, 0.61301989, 0.61301989, 0.61357466]),\n",
       " 'split3_test_precision': array([0.        , 0.63904236, 0.62099872, 0.61840745, 0.61553398,\n",
       "        0.61016949, 0.60651163, 0.60611677, 0.60611677, 0.60611677,\n",
       "        0.60611677, 0.60648148, 0.60648148, 0.60611677, 0.60611677,\n",
       "        0.60611677, 0.60611677, 0.60611677, 0.63904236, 0.61840745,\n",
       "        0.61016949, 0.60611677, 0.60611677, 0.60611677, 0.60611677,\n",
       "        0.60611677, 0.60611677, 0.60611677, 0.60611677, 0.60611677]),\n",
       " 'split4_test_precision': array([0.        , 0.64660194, 0.61641992, 0.62036055, 0.61122244,\n",
       "        0.61356932, 0.6134372 , 0.61441091, 0.61478599, 0.61418853,\n",
       "        0.61418853, 0.61418853, 0.61418853, 0.61418853, 0.61418853,\n",
       "        0.61418853, 0.61418853, 0.61418853, 0.64660194, 0.62036055,\n",
       "        0.61356932, 0.61441091, 0.61418853, 0.61418853, 0.61418853,\n",
       "        0.61418853, 0.61418853, 0.61418853, 0.61478599, 0.61418853]),\n",
       " 'split5_test_precision': array([0.        , 0.64234875, 0.60097919, 0.5893386 , 0.5758427 ,\n",
       "        0.57846715, 0.57765668, 0.57765668, 0.57765668, 0.57803993,\n",
       "        0.57803993, 0.57803993, 0.57803993, 0.57803993, 0.57803993,\n",
       "        0.57803993, 0.57803993, 0.57803993, 0.64234875, 0.5893386 ,\n",
       "        0.57846715, 0.57765668, 0.57803993, 0.57803993, 0.57803993,\n",
       "        0.57803993, 0.57803993, 0.57803993, 0.57803993, 0.57803993]),\n",
       " 'split6_test_precision': array([0.        , 0.65875912, 0.64186633, 0.63286004, 0.62452107,\n",
       "        0.61819887, 0.61509786, 0.61545624, 0.61545624, 0.61581395,\n",
       "        0.61581395, 0.61581395, 0.61581395, 0.61581395, 0.61581395,\n",
       "        0.61581395, 0.61581395, 0.61581395, 0.65875912, 0.63350254,\n",
       "        0.61819887, 0.61545624, 0.61581395, 0.61581395, 0.61581395,\n",
       "        0.61581395, 0.61581395, 0.61581395, 0.61581395, 0.61581395]),\n",
       " 'split7_test_precision': array([0.        , 0.65284974, 0.63078849, 0.60963115, 0.61056751,\n",
       "        0.61031519, 0.60853081, 0.60890152, 0.60890152, 0.60890152,\n",
       "        0.60890152, 0.60890152, 0.60890152, 0.60890152, 0.60890152,\n",
       "        0.60890152, 0.60890152, 0.60890152, 0.65172414, 0.60963115,\n",
       "        0.60973282, 0.60890152, 0.60890152, 0.60890152, 0.60890152,\n",
       "        0.60890152, 0.60890152, 0.60890152, 0.60890152, 0.60890152]),\n",
       " 'split8_test_precision': array([0.        , 0.62989324, 0.61298377, 0.60273973, 0.59851301,\n",
       "        0.59346642, 0.59532374, 0.59478886, 0.59425494, 0.5951526 ,\n",
       "        0.59461883, 0.59408602, 0.59408602, 0.59408602, 0.59408602,\n",
       "        0.59408602, 0.5951526 , 0.59461883, 0.62989324, 0.60273973,\n",
       "        0.59346642, 0.59478886, 0.59461883, 0.59461883, 0.59461883,\n",
       "        0.59461883, 0.59461883, 0.59461883, 0.59408602, 0.59461883]),\n",
       " 'split9_test_precision': array([0.        , 0.66422018, 0.61934673, 0.61326531, 0.60714286,\n",
       "        0.60714286, 0.6076779 , 0.60877684, 0.60877684, 0.60877684,\n",
       "        0.60877684, 0.60877684, 0.60877684, 0.60877684, 0.60877684,\n",
       "        0.60877684, 0.60877684, 0.60877684, 0.66422018, 0.61287028,\n",
       "        0.60714286, 0.60877684, 0.60877684, 0.60877684, 0.60877684,\n",
       "        0.60877684, 0.60877684, 0.60877684, 0.60877684, 0.60877684]),\n",
       " 'mean_test_precision': array([0.        , 0.64523268, 0.61767478, 0.60810309, 0.60298521,\n",
       "        0.60131757, 0.60030828, 0.60008945, 0.60005983, 0.60014842,\n",
       "        0.60009504, 0.60007823, 0.60007823, 0.60004176, 0.60004176,\n",
       "        0.60004176, 0.60014842, 0.60005518, 0.64512012, 0.60812784,\n",
       "        0.60125934, 0.60019873, 0.60015052, 0.60009504, 0.60009504,\n",
       "        0.60009504, 0.60009504, 0.60009504, 0.60010151, 0.60015052]),\n",
       " 'std_test_precision': array([0.        , 0.01438827, 0.01428525, 0.0156073 , 0.01650253,\n",
       "        0.01478141, 0.01458956, 0.01489214, 0.01501511, 0.01478231,\n",
       "        0.01480121, 0.01483711, 0.01483711, 0.01482177, 0.01482177,\n",
       "        0.01482177, 0.01478231, 0.01483268, 0.01433254, 0.01569769,\n",
       "        0.01474695, 0.01484643, 0.0148505 , 0.01480121, 0.01480121,\n",
       "        0.01480121, 0.01480121, 0.01480121, 0.01487976, 0.0148505 ]),\n",
       " 'rank_test_precision': array([30,  1,  3,  5,  6,  7,  9, 22, 25, 13, 16, 23, 23, 27, 27, 27, 13,\n",
       "        26,  2,  4,  8, 10, 11, 16, 16, 16, 16, 16, 15, 11]),\n",
       " 'split0_test_f1_micro': array([0.86100025, 0.86804632, 0.86883469, 0.8699187 , 0.87001725,\n",
       "        0.87016507, 0.87011579, 0.86996797, 0.87001725, 0.87001725,\n",
       "        0.87001725, 0.87001725, 0.87001725, 0.87001725, 0.87001725,\n",
       "        0.87001725, 0.87001725, 0.86996797, 0.86804632, 0.8699187 ,\n",
       "        0.87016507, 0.86996797, 0.87001725, 0.87001725, 0.87001725,\n",
       "        0.87001725, 0.87001725, 0.87001725, 0.87001725, 0.87001725]),\n",
       " 'split1_test_f1_micro': array([0.86100025, 0.86691303, 0.86824341, 0.86853905, 0.8686376 ,\n",
       "        0.86883469, 0.86873614, 0.86868687, 0.8686376 , 0.86868687,\n",
       "        0.86868687, 0.86868687, 0.86868687, 0.86868687, 0.86868687,\n",
       "        0.86868687, 0.86868687, 0.86868687, 0.86691303, 0.86853905,\n",
       "        0.86883469, 0.86873614, 0.86868687, 0.86868687, 0.86868687,\n",
       "        0.86868687, 0.86868687, 0.86868687, 0.86868687, 0.86868687]),\n",
       " 'split2_test_f1_micro': array([0.86095097, 0.87021434, 0.87174181, 0.87307219, 0.87376201,\n",
       "        0.8734171 , 0.87346637, 0.87331855, 0.87331855, 0.87326928,\n",
       "        0.87326928, 0.87326928, 0.87326928, 0.87326928, 0.87326928,\n",
       "        0.87326928, 0.87326928, 0.87326928, 0.87021434, 0.87307219,\n",
       "        0.8734171 , 0.87336782, 0.87331855, 0.87326928, 0.87326928,\n",
       "        0.87326928, 0.87326928, 0.87326928, 0.87326928, 0.87331855]),\n",
       " 'split3_test_f1_micro': array([0.86095097, 0.86839123, 0.87026361, 0.87223454, 0.872678  ,\n",
       "        0.87248091, 0.87223454, 0.87223454, 0.87223454, 0.87223454,\n",
       "        0.87223454, 0.87228381, 0.87228381, 0.87223454, 0.87223454,\n",
       "        0.87223454, 0.87223454, 0.87223454, 0.86839123, 0.87223454,\n",
       "        0.87248091, 0.87223454, 0.87223454, 0.87223454, 0.87223454,\n",
       "        0.87223454, 0.87223454, 0.87223454, 0.87223454, 0.87223454]),\n",
       " 'split4_test_f1_micro': array([0.8609934 , 0.86843402, 0.86951808, 0.87217897, 0.87193259,\n",
       "        0.87237607, 0.87247462, 0.87257317, 0.87262245, 0.87257317,\n",
       "        0.87257317, 0.87257317, 0.87257317, 0.87257317, 0.87257317,\n",
       "        0.87257317, 0.87257317, 0.87257317, 0.86843402, 0.87217897,\n",
       "        0.87237607, 0.87257317, 0.87257317, 0.87257317, 0.87257317,\n",
       "        0.87257317, 0.87257317, 0.87257317, 0.87262245, 0.87257317]),\n",
       " 'split5_test_f1_micro': array([0.8609934 , 0.8688775 , 0.86912388, 0.86991229, 0.86897605,\n",
       "        0.86946881, 0.86941953, 0.86941953, 0.86941953, 0.86946881,\n",
       "        0.86946881, 0.86946881, 0.86946881, 0.86946881, 0.86946881,\n",
       "        0.86946881, 0.86946881, 0.86946881, 0.8688775 , 0.86991229,\n",
       "        0.86946881, 0.86941953, 0.86946881, 0.86946881, 0.86946881,\n",
       "        0.86946881, 0.86946881, 0.86946881, 0.86946881, 0.86946881]),\n",
       " 'split6_test_f1_micro': array([0.8609934 , 0.86956736, 0.87208042, 0.87390362, 0.87380507,\n",
       "        0.87341086, 0.87316448, 0.87321376, 0.87321376, 0.87326303,\n",
       "        0.87326303, 0.87326303, 0.87326303, 0.87326303, 0.87326303,\n",
       "        0.87326303, 0.87326303, 0.87326303, 0.86956736, 0.87395289,\n",
       "        0.87341086, 0.87321376, 0.87326303, 0.87326303, 0.87326303,\n",
       "        0.87326303, 0.87326303, 0.87326303, 0.87326303, 0.87326303]),\n",
       " 'split7_test_f1_micro': array([0.8609934 , 0.86971519, 0.87129201, 0.87153839, 0.87212969,\n",
       "        0.87237607, 0.87227752, 0.8723268 , 0.8723268 , 0.8723268 ,\n",
       "        0.8723268 , 0.8723268 , 0.8723268 , 0.8723268 , 0.8723268 ,\n",
       "        0.8723268 , 0.8723268 , 0.8723268 , 0.86966591, 0.87153839,\n",
       "        0.8723268 , 0.8723268 , 0.8723268 , 0.8723268 , 0.8723268 ,\n",
       "        0.8723268 , 0.8723268 , 0.8723268 , 0.8723268 , 0.8723268 ]),\n",
       " 'split8_test_f1_micro': array([0.8609934 , 0.86818764, 0.86991229, 0.87134128, 0.87143983,\n",
       "        0.87114418, 0.87143983, 0.87139056, 0.87134128, 0.87143983,\n",
       "        0.87139056, 0.87134128, 0.87134128, 0.87134128, 0.87134128,\n",
       "        0.87134128, 0.87143983, 0.87139056, 0.86818764, 0.87134128,\n",
       "        0.87114418, 0.87139056, 0.87139056, 0.87139056, 0.87139056,\n",
       "        0.87139056, 0.87139056, 0.87139056, 0.87134128, 0.87139056]),\n",
       " 'split9_test_f1_micro': array([0.8609934 , 0.86981374, 0.87035577, 0.87193259, 0.87193259,\n",
       "        0.87222824, 0.8723268 , 0.87247462, 0.87247462, 0.87247462,\n",
       "        0.87247462, 0.87247462, 0.87247462, 0.87247462, 0.87247462,\n",
       "        0.87247462, 0.87247462, 0.87247462, 0.86981374, 0.87188332,\n",
       "        0.87222824, 0.87247462, 0.87247462, 0.87247462, 0.87247462,\n",
       "        0.87247462, 0.87247462, 0.87247462, 0.87247462, 0.87247462]),\n",
       " 'mean_test_f1_micro': array([0.86098628, 0.86881604, 0.8701366 , 0.87145716, 0.87153107,\n",
       "        0.8715902 , 0.87156556, 0.87156064, 0.87156064, 0.87157542,\n",
       "        0.87157049, 0.87157049, 0.87157049, 0.87156557, 0.87156557,\n",
       "        0.87156557, 0.87157542, 0.87156557, 0.86881111, 0.87145716,\n",
       "        0.87158527, 0.87157049, 0.87157542, 0.87157049, 0.87157049,\n",
       "        0.87157049, 0.87157049, 0.87157049, 0.87157049, 0.87157542]),\n",
       " 'std_test_f1_micro': array([1.78526808e-05, 9.62422052e-04, 1.20382225e-03, 1.52426064e-03,\n",
       "        1.71302907e-03, 1.53006851e-03, 1.52507725e-03, 1.55348632e-03,\n",
       "        1.56159233e-03, 1.54104223e-03, 1.54154660e-03, 1.54439915e-03,\n",
       "        1.54439915e-03, 1.54219250e-03, 1.54219250e-03, 1.54219250e-03,\n",
       "        1.54104223e-03, 1.54657399e-03, 9.57921443e-04, 1.53077785e-03,\n",
       "        1.52760706e-03, 1.55006834e-03, 1.54703760e-03, 1.54154660e-03,\n",
       "        1.54154660e-03, 1.54154660e-03, 1.54154660e-03, 1.54154660e-03,\n",
       "        1.54547933e-03, 1.54703760e-03]),\n",
       " 'rank_test_f1_micro': array([30, 28, 27, 25, 24,  1, 21, 22, 22,  3,  8, 14, 14, 18, 18, 18,  3,\n",
       "        17, 29, 25,  2, 16,  5,  8,  8,  8,  8,  8,  7,  5])}"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_model.cv_results_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "6e3313e2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([30, 28, 27, 25, 24,  1, 21, 22, 22,  3,  7, 14, 14, 18, 18, 18,  3,\n",
       "       17, 29, 25,  2, 16,  5,  7,  7,  7,  7,  7,  7,  5])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_model.cv_results_['rank_test_accuracy']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "3d08f33e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'classifier': LogisticRegression(max_iter=10000),\n",
       " 'classifier__C': 0.0001,\n",
       " 'classifier__penalty': 'l1',\n",
       " 'classifier__solver': 'saga'}"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_model.cv_results_['params'][ np.argmin(best_model.cv_results_['mean_test_accuracy'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "c8da90d4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>classifier</th>\n",
       "      <th>C</th>\n",
       "      <th>penalty</th>\n",
       "      <th>solver</th>\n",
       "      <th>score_acc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.860986</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.868816</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.870137</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.871457</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0100</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.871531</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0100</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.871590</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.1000</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.871566</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.1000</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.871561</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.871561</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.871575</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>10.0000</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.871570</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>10.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.871570</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>100.0000</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.871570</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>100.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.871566</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>1000.0000</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.871566</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>1000.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.871566</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>10000.0000</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.871575</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>10000.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.871566</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.868811</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.871457</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0100</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.871585</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.1000</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.871570</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.871575</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>10.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.871570</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>100.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.871570</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>1000.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.871570</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>10000.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.871570</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>NaN</td>\n",
       "      <td>none</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.871570</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>NaN</td>\n",
       "      <td>none</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.871570</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.871575</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                            classifier           C penalty solver  score_acc\n",
       "0   LogisticRegression(max_iter=10000)      0.0001      l1   saga   0.860986\n",
       "1   LogisticRegression(max_iter=10000)      0.0001      l2   saga   0.868816\n",
       "2   LogisticRegression(max_iter=10000)      0.0010      l1   saga   0.870137\n",
       "3   LogisticRegression(max_iter=10000)      0.0010      l2   saga   0.871457\n",
       "4   LogisticRegression(max_iter=10000)      0.0100      l1   saga   0.871531\n",
       "5   LogisticRegression(max_iter=10000)      0.0100      l2   saga   0.871590\n",
       "6   LogisticRegression(max_iter=10000)      0.1000      l1   saga   0.871566\n",
       "7   LogisticRegression(max_iter=10000)      0.1000      l2   saga   0.871561\n",
       "8   LogisticRegression(max_iter=10000)      1.0000      l1   saga   0.871561\n",
       "9   LogisticRegression(max_iter=10000)      1.0000      l2   saga   0.871575\n",
       "10  LogisticRegression(max_iter=10000)     10.0000      l1   saga   0.871570\n",
       "11  LogisticRegression(max_iter=10000)     10.0000      l2   saga   0.871570\n",
       "12  LogisticRegression(max_iter=10000)    100.0000      l1   saga   0.871570\n",
       "13  LogisticRegression(max_iter=10000)    100.0000      l2   saga   0.871566\n",
       "14  LogisticRegression(max_iter=10000)   1000.0000      l1   saga   0.871566\n",
       "15  LogisticRegression(max_iter=10000)   1000.0000      l2   saga   0.871566\n",
       "16  LogisticRegression(max_iter=10000)  10000.0000      l1   saga   0.871575\n",
       "17  LogisticRegression(max_iter=10000)  10000.0000      l2   saga   0.871566\n",
       "18  LogisticRegression(max_iter=10000)      0.0001      l2  lbfgs   0.868811\n",
       "19  LogisticRegression(max_iter=10000)      0.0010      l2  lbfgs   0.871457\n",
       "20  LogisticRegression(max_iter=10000)      0.0100      l2  lbfgs   0.871585\n",
       "21  LogisticRegression(max_iter=10000)      0.1000      l2  lbfgs   0.871570\n",
       "22  LogisticRegression(max_iter=10000)      1.0000      l2  lbfgs   0.871575\n",
       "23  LogisticRegression(max_iter=10000)     10.0000      l2  lbfgs   0.871570\n",
       "24  LogisticRegression(max_iter=10000)    100.0000      l2  lbfgs   0.871570\n",
       "25  LogisticRegression(max_iter=10000)   1000.0000      l2  lbfgs   0.871570\n",
       "26  LogisticRegression(max_iter=10000)  10000.0000      l2  lbfgs   0.871570\n",
       "27  LogisticRegression(max_iter=10000)         NaN    none  lbfgs   0.871570\n",
       "28  LogisticRegression(max_iter=10000)         NaN    none   saga   0.871570\n",
       "29  LogisticRegression(max_iter=10000)         NaN     NaN    NaN   0.871575"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results = pd.DataFrame( best_model.cv_results_['params'] )\n",
    "\n",
    "results['score_acc'] = best_model.cv_results_['mean_test_accuracy']\n",
    "cols = results.columns.to_series().str.split('__').apply(lambda x: x[-1])\n",
    "results.columns = cols\n",
    "\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "eb7e7292",
   "metadata": {},
   "outputs": [],
   "source": [
    "acc_list.append(results[0:1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "ce1a13cb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'classifier': LogisticRegression(max_iter=10000),\n",
       " 'classifier__C': 0.0001,\n",
       " 'classifier__penalty': 'l2',\n",
       " 'classifier__solver': 'saga'}"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_model.cv_results_['params'][ np.argmin(best_model.cv_results_['rank_test_precision'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "e6654f1a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>classifier</th>\n",
       "      <th>C</th>\n",
       "      <th>penalty</th>\n",
       "      <th>solver</th>\n",
       "      <th>score_precision</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.645233</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.617675</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.608103</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0100</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.602985</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0100</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.601318</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.1000</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.600308</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.1000</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.600089</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.600060</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.600148</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>10.0000</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.600095</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>10.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.600078</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>100.0000</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.600078</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>100.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.600042</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>1000.0000</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.600042</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>1000.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.600042</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>10000.0000</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.600148</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>10000.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.600055</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.645120</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.608128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0100</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.601259</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.1000</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.600199</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.600151</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>10.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.600095</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>100.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.600095</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>1000.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.600095</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>10000.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.600095</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>NaN</td>\n",
       "      <td>none</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.600095</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>NaN</td>\n",
       "      <td>none</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.600102</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.600151</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                            classifier           C penalty solver  \\\n",
       "0   LogisticRegression(max_iter=10000)      0.0001      l1   saga   \n",
       "1   LogisticRegression(max_iter=10000)      0.0001      l2   saga   \n",
       "2   LogisticRegression(max_iter=10000)      0.0010      l1   saga   \n",
       "3   LogisticRegression(max_iter=10000)      0.0010      l2   saga   \n",
       "4   LogisticRegression(max_iter=10000)      0.0100      l1   saga   \n",
       "5   LogisticRegression(max_iter=10000)      0.0100      l2   saga   \n",
       "6   LogisticRegression(max_iter=10000)      0.1000      l1   saga   \n",
       "7   LogisticRegression(max_iter=10000)      0.1000      l2   saga   \n",
       "8   LogisticRegression(max_iter=10000)      1.0000      l1   saga   \n",
       "9   LogisticRegression(max_iter=10000)      1.0000      l2   saga   \n",
       "10  LogisticRegression(max_iter=10000)     10.0000      l1   saga   \n",
       "11  LogisticRegression(max_iter=10000)     10.0000      l2   saga   \n",
       "12  LogisticRegression(max_iter=10000)    100.0000      l1   saga   \n",
       "13  LogisticRegression(max_iter=10000)    100.0000      l2   saga   \n",
       "14  LogisticRegression(max_iter=10000)   1000.0000      l1   saga   \n",
       "15  LogisticRegression(max_iter=10000)   1000.0000      l2   saga   \n",
       "16  LogisticRegression(max_iter=10000)  10000.0000      l1   saga   \n",
       "17  LogisticRegression(max_iter=10000)  10000.0000      l2   saga   \n",
       "18  LogisticRegression(max_iter=10000)      0.0001      l2  lbfgs   \n",
       "19  LogisticRegression(max_iter=10000)      0.0010      l2  lbfgs   \n",
       "20  LogisticRegression(max_iter=10000)      0.0100      l2  lbfgs   \n",
       "21  LogisticRegression(max_iter=10000)      0.1000      l2  lbfgs   \n",
       "22  LogisticRegression(max_iter=10000)      1.0000      l2  lbfgs   \n",
       "23  LogisticRegression(max_iter=10000)     10.0000      l2  lbfgs   \n",
       "24  LogisticRegression(max_iter=10000)    100.0000      l2  lbfgs   \n",
       "25  LogisticRegression(max_iter=10000)   1000.0000      l2  lbfgs   \n",
       "26  LogisticRegression(max_iter=10000)  10000.0000      l2  lbfgs   \n",
       "27  LogisticRegression(max_iter=10000)         NaN    none  lbfgs   \n",
       "28  LogisticRegression(max_iter=10000)         NaN    none   saga   \n",
       "29  LogisticRegression(max_iter=10000)         NaN     NaN    NaN   \n",
       "\n",
       "    score_precision  \n",
       "0          0.000000  \n",
       "1          0.645233  \n",
       "2          0.617675  \n",
       "3          0.608103  \n",
       "4          0.602985  \n",
       "5          0.601318  \n",
       "6          0.600308  \n",
       "7          0.600089  \n",
       "8          0.600060  \n",
       "9          0.600148  \n",
       "10         0.600095  \n",
       "11         0.600078  \n",
       "12         0.600078  \n",
       "13         0.600042  \n",
       "14         0.600042  \n",
       "15         0.600042  \n",
       "16         0.600148  \n",
       "17         0.600055  \n",
       "18         0.645120  \n",
       "19         0.608128  \n",
       "20         0.601259  \n",
       "21         0.600199  \n",
       "22         0.600151  \n",
       "23         0.600095  \n",
       "24         0.600095  \n",
       "25         0.600095  \n",
       "26         0.600095  \n",
       "27         0.600095  \n",
       "28         0.600102  \n",
       "29         0.600151  "
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results = pd.DataFrame( best_model.cv_results_['params'] )\n",
    "\n",
    "results['score_precision'] = best_model.cv_results_['mean_test_precision']\n",
    "cols = results.columns.to_series().str.split('__').apply(lambda x: x[-1])\n",
    "results.columns = cols\n",
    "\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "71d804bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "roc_auc_ovr_List.append(results[1:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "c8742a4b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'classifier': LogisticRegression(max_iter=10000),\n",
       " 'classifier__C': 0.01,\n",
       " 'classifier__penalty': 'l2',\n",
       " 'classifier__solver': 'saga'}"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_model.cv_results_['params'][ np.argmin(best_model.cv_results_['rank_test_f1_micro'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "4bf5cd96",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>classifier</th>\n",
       "      <th>C</th>\n",
       "      <th>penalty</th>\n",
       "      <th>solver</th>\n",
       "      <th>score_f1_micro</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.860986</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.868816</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.870137</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.871457</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0100</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.871531</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0100</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.871590</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.1000</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.871566</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.1000</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.871561</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.871561</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.871575</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>10.0000</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.871570</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>10.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.871570</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>100.0000</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.871570</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>100.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.871566</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>1000.0000</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.871566</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>1000.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.871566</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>10000.0000</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.871575</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>10000.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.871566</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.868811</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.871457</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0100</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.871585</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.1000</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.871570</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.871575</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>10.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.871570</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>100.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.871570</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>1000.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.871570</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>10000.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.871570</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>NaN</td>\n",
       "      <td>none</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.871570</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>NaN</td>\n",
       "      <td>none</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.871570</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.871575</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                            classifier           C penalty solver  \\\n",
       "0   LogisticRegression(max_iter=10000)      0.0001      l1   saga   \n",
       "1   LogisticRegression(max_iter=10000)      0.0001      l2   saga   \n",
       "2   LogisticRegression(max_iter=10000)      0.0010      l1   saga   \n",
       "3   LogisticRegression(max_iter=10000)      0.0010      l2   saga   \n",
       "4   LogisticRegression(max_iter=10000)      0.0100      l1   saga   \n",
       "5   LogisticRegression(max_iter=10000)      0.0100      l2   saga   \n",
       "6   LogisticRegression(max_iter=10000)      0.1000      l1   saga   \n",
       "7   LogisticRegression(max_iter=10000)      0.1000      l2   saga   \n",
       "8   LogisticRegression(max_iter=10000)      1.0000      l1   saga   \n",
       "9   LogisticRegression(max_iter=10000)      1.0000      l2   saga   \n",
       "10  LogisticRegression(max_iter=10000)     10.0000      l1   saga   \n",
       "11  LogisticRegression(max_iter=10000)     10.0000      l2   saga   \n",
       "12  LogisticRegression(max_iter=10000)    100.0000      l1   saga   \n",
       "13  LogisticRegression(max_iter=10000)    100.0000      l2   saga   \n",
       "14  LogisticRegression(max_iter=10000)   1000.0000      l1   saga   \n",
       "15  LogisticRegression(max_iter=10000)   1000.0000      l2   saga   \n",
       "16  LogisticRegression(max_iter=10000)  10000.0000      l1   saga   \n",
       "17  LogisticRegression(max_iter=10000)  10000.0000      l2   saga   \n",
       "18  LogisticRegression(max_iter=10000)      0.0001      l2  lbfgs   \n",
       "19  LogisticRegression(max_iter=10000)      0.0010      l2  lbfgs   \n",
       "20  LogisticRegression(max_iter=10000)      0.0100      l2  lbfgs   \n",
       "21  LogisticRegression(max_iter=10000)      0.1000      l2  lbfgs   \n",
       "22  LogisticRegression(max_iter=10000)      1.0000      l2  lbfgs   \n",
       "23  LogisticRegression(max_iter=10000)     10.0000      l2  lbfgs   \n",
       "24  LogisticRegression(max_iter=10000)    100.0000      l2  lbfgs   \n",
       "25  LogisticRegression(max_iter=10000)   1000.0000      l2  lbfgs   \n",
       "26  LogisticRegression(max_iter=10000)  10000.0000      l2  lbfgs   \n",
       "27  LogisticRegression(max_iter=10000)         NaN    none  lbfgs   \n",
       "28  LogisticRegression(max_iter=10000)         NaN    none   saga   \n",
       "29  LogisticRegression(max_iter=10000)         NaN     NaN    NaN   \n",
       "\n",
       "    score_f1_micro  \n",
       "0         0.860986  \n",
       "1         0.868816  \n",
       "2         0.870137  \n",
       "3         0.871457  \n",
       "4         0.871531  \n",
       "5         0.871590  \n",
       "6         0.871566  \n",
       "7         0.871561  \n",
       "8         0.871561  \n",
       "9         0.871575  \n",
       "10        0.871570  \n",
       "11        0.871570  \n",
       "12        0.871570  \n",
       "13        0.871566  \n",
       "14        0.871566  \n",
       "15        0.871566  \n",
       "16        0.871575  \n",
       "17        0.871566  \n",
       "18        0.868811  \n",
       "19        0.871457  \n",
       "20        0.871585  \n",
       "21        0.871570  \n",
       "22        0.871575  \n",
       "23        0.871570  \n",
       "24        0.871570  \n",
       "25        0.871570  \n",
       "26        0.871570  \n",
       "27        0.871570  \n",
       "28        0.871570  \n",
       "29        0.871575  "
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results = pd.DataFrame( best_model.cv_results_['params'] )\n",
    "\n",
    "results['score_f1_micro'] = best_model.cv_results_['mean_test_f1_micro']\n",
    "cols = results.columns.to_series().str.split('__').apply(lambda x: x[-1])\n",
    "results.columns = cols\n",
    "\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "8cebbf79",
   "metadata": {},
   "outputs": [],
   "source": [
    "f1_micro_List.append(results[5:6])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b315f477",
   "metadata": {},
   "source": [
    "## Trial 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "8e5bcf1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, Y_train, Y_test = train_test_split(X, y, test_size=0.2, shuffle=True, random_state = 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "e91ba61f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Guest-All\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\Guest-All\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\Guest-All\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\Guest-All\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\Guest-All\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\Guest-All\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\Guest-All\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\Guest-All\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\Guest-All\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\Guest-All\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\Guest-All\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1183: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Guest-All\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1183: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Guest-All\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1183: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Guest-All\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1183: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Guest-All\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1183: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Guest-All\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1183: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Guest-All\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1183: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Guest-All\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1183: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Guest-All\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1183: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Guest-All\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1183: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Guest-All\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1183: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Guest-All\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1183: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Guest-All\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1183: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Guest-All\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1183: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Guest-All\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1183: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Guest-All\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1183: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Guest-All\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1183: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Guest-All\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1183: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Guest-All\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1183: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Guest-All\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1183: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Create a pipeline - RF is a stand in, we will populate the classifier part below\n",
    "pipe = Pipeline([('std', StandardScaler()),\n",
    "                 ('classifier', LogisticRegression())])\n",
    "\n",
    "# Create search space of candidate learning algorithms and their hyperparameters\n",
    "# note lbfgs can't do l1, and if you pass penalty='none' it expects no C value\n",
    "search_space = [{'classifier': [LogisticRegression(max_iter=10000)],\n",
    "                 'classifier__solver': ['saga'],\n",
    "                 'classifier__penalty': ['l1', 'l2'],\n",
    "                 'classifier__C': np.logspace(-4, 4, 9)},\n",
    "                {'classifier': [LogisticRegression(max_iter=10000)],\n",
    "                 'classifier__solver': ['lbfgs'],\n",
    "                 'classifier__penalty': ['l2'],\n",
    "                 'classifier__C': np.logspace(-4, 4, 9)},\n",
    "                {'classifier': [LogisticRegression(max_iter=10000)],\n",
    "                 'classifier__solver': ['lbfgs','saga'],\n",
    "                 'classifier__penalty': ['none']},\n",
    "                {'classifier': [LogisticRegression(max_iter=10000)]}\n",
    "                ]\n",
    "\n",
    "# Create grid search\n",
    "clf = GridSearchCV(pipe, search_space, cv=StratifiedKFold(n_splits=10),\n",
    "                   scoring=['accuracy', 'precision', 'f1_micro'], refit=False,\n",
    "                   verbose=0)\n",
    "\n",
    "# Fit grid search\n",
    "best_model = clf.fit(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "36bb3f82",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'mean_fit_time': array([ 3.83425913,  3.12883043,  3.64660909,  2.94897342,  3.66368117,\n",
       "         3.23431444,  5.07676797,  3.21932609, 37.80682819,  3.36534173,\n",
       "         3.71150627,  3.30982718,  4.00191321,  3.22306211,  3.91506689,\n",
       "         3.03676381,  3.96254191,  3.27362013,  0.54895196,  0.72744513,\n",
       "         0.832376  ,  0.8460145 ,  0.80935409,  0.82585714,  0.82696729,\n",
       "         0.82797039,  0.82416337,  0.76630599,  3.26691189,  0.79947612]),\n",
       " 'std_fit_time': array([0.2265648 , 0.25699164, 0.28061907, 0.21040669, 0.21291128,\n",
       "        0.38442774, 0.30423673, 0.23872425, 2.67728356, 0.43572367,\n",
       "        0.7265313 , 0.63427613, 0.32039875, 0.31571628, 0.29345214,\n",
       "        0.34544884, 0.57141861, 0.39907014, 0.02205815, 0.03769575,\n",
       "        0.01891501, 0.02288818, 0.02070885, 0.02636346, 0.03776218,\n",
       "        0.03510202, 0.03717411, 0.05195164, 0.33159502, 0.02505812]),\n",
       " 'mean_score_time': array([0.03374293, 0.02962801, 0.03065093, 0.03023028, 0.03465598,\n",
       "        0.03465242, 0.03388805, 0.03247743, 0.0335438 , 0.03115904,\n",
       "        0.03235097, 0.03365033, 0.03058932, 0.03220699, 0.03123119,\n",
       "        0.03117726, 0.03081653, 0.03022802, 0.03095987, 0.03065608,\n",
       "        0.03189199, 0.03237891, 0.03334882, 0.03254154, 0.0326046 ,\n",
       "        0.03198276, 0.03334868, 0.02908292, 0.03322155, 0.03316829]),\n",
       " 'std_score_time': array([0.00511414, 0.00301735, 0.0057834 , 0.0052563 , 0.00323097,\n",
       "        0.01053254, 0.00431808, 0.00489559, 0.00210383, 0.00285674,\n",
       "        0.00651991, 0.01095128, 0.00392883, 0.00177364, 0.00229139,\n",
       "        0.00374648, 0.00373395, 0.00405491, 0.00235178, 0.00169162,\n",
       "        0.00086622, 0.00096382, 0.00140446, 0.00144119, 0.00286919,\n",
       "        0.00114298, 0.00112028, 0.00546367, 0.00073984, 0.00293895]),\n",
       " 'param_classifier': masked_array(data=[LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000)],\n",
       "              mask=[False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'param_classifier__C': masked_array(data=[0.0001, 0.0001, 0.001, 0.001, 0.01, 0.01, 0.1, 0.1,\n",
       "                    1.0, 1.0, 10.0, 10.0, 100.0, 100.0, 1000.0, 1000.0,\n",
       "                    10000.0, 10000.0, 0.0001, 0.001, 0.01, 0.1, 1.0, 10.0,\n",
       "                    100.0, 1000.0, 10000.0, --, --, --],\n",
       "              mask=[False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False,  True,  True,  True],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'param_classifier__penalty': masked_array(data=['l1', 'l2', 'l1', 'l2', 'l1', 'l2', 'l1', 'l2', 'l1',\n",
       "                    'l2', 'l1', 'l2', 'l1', 'l2', 'l1', 'l2', 'l1', 'l2',\n",
       "                    'l2', 'l2', 'l2', 'l2', 'l2', 'l2', 'l2', 'l2', 'l2',\n",
       "                    'none', 'none', --],\n",
       "              mask=[False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False,  True],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'param_classifier__solver': masked_array(data=['saga', 'saga', 'saga', 'saga', 'saga', 'saga', 'saga',\n",
       "                    'saga', 'saga', 'saga', 'saga', 'saga', 'saga', 'saga',\n",
       "                    'saga', 'saga', 'saga', 'saga', 'lbfgs', 'lbfgs',\n",
       "                    'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs',\n",
       "                    'lbfgs', 'lbfgs', 'saga', --],\n",
       "              mask=[False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False,  True],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'params': [{'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 0.0001,\n",
       "   'classifier__penalty': 'l1',\n",
       "   'classifier__solver': 'saga'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 0.0001,\n",
       "   'classifier__penalty': 'l2',\n",
       "   'classifier__solver': 'saga'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 0.001,\n",
       "   'classifier__penalty': 'l1',\n",
       "   'classifier__solver': 'saga'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 0.001,\n",
       "   'classifier__penalty': 'l2',\n",
       "   'classifier__solver': 'saga'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 0.01,\n",
       "   'classifier__penalty': 'l1',\n",
       "   'classifier__solver': 'saga'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 0.01,\n",
       "   'classifier__penalty': 'l2',\n",
       "   'classifier__solver': 'saga'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 0.1,\n",
       "   'classifier__penalty': 'l1',\n",
       "   'classifier__solver': 'saga'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 0.1,\n",
       "   'classifier__penalty': 'l2',\n",
       "   'classifier__solver': 'saga'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 1.0,\n",
       "   'classifier__penalty': 'l1',\n",
       "   'classifier__solver': 'saga'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 1.0,\n",
       "   'classifier__penalty': 'l2',\n",
       "   'classifier__solver': 'saga'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 10.0,\n",
       "   'classifier__penalty': 'l1',\n",
       "   'classifier__solver': 'saga'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 10.0,\n",
       "   'classifier__penalty': 'l2',\n",
       "   'classifier__solver': 'saga'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 100.0,\n",
       "   'classifier__penalty': 'l1',\n",
       "   'classifier__solver': 'saga'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 100.0,\n",
       "   'classifier__penalty': 'l2',\n",
       "   'classifier__solver': 'saga'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 1000.0,\n",
       "   'classifier__penalty': 'l1',\n",
       "   'classifier__solver': 'saga'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 1000.0,\n",
       "   'classifier__penalty': 'l2',\n",
       "   'classifier__solver': 'saga'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 10000.0,\n",
       "   'classifier__penalty': 'l1',\n",
       "   'classifier__solver': 'saga'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 10000.0,\n",
       "   'classifier__penalty': 'l2',\n",
       "   'classifier__solver': 'saga'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 0.0001,\n",
       "   'classifier__penalty': 'l2',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 0.001,\n",
       "   'classifier__penalty': 'l2',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 0.01,\n",
       "   'classifier__penalty': 'l2',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 0.1,\n",
       "   'classifier__penalty': 'l2',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 1.0,\n",
       "   'classifier__penalty': 'l2',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 10.0,\n",
       "   'classifier__penalty': 'l2',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 100.0,\n",
       "   'classifier__penalty': 'l2',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 1000.0,\n",
       "   'classifier__penalty': 'l2',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 10000.0,\n",
       "   'classifier__penalty': 'l2',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__penalty': 'none',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__penalty': 'none',\n",
       "   'classifier__solver': 'saga'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000)}],\n",
       " 'split0_test_accuracy': array([0.86050751, 0.86898251, 0.87001725, 0.87228381, 0.87213599,\n",
       "        0.8719389 , 0.87213599, 0.87213599, 0.87213599, 0.87213599,\n",
       "        0.87213599, 0.87213599, 0.87213599, 0.87213599, 0.87213599,\n",
       "        0.87213599, 0.87213599, 0.87213599, 0.86898251, 0.87223454,\n",
       "        0.8719389 , 0.87213599, 0.87213599, 0.87213599, 0.87213599,\n",
       "        0.87213599, 0.87213599, 0.87213599, 0.87213599, 0.87213599]),\n",
       " 'split1_test_accuracy': array([0.86050751, 0.86804632, 0.87011579, 0.87026361, 0.87100271,\n",
       "        0.87105198, 0.87095344, 0.87080562, 0.87070707, 0.87070707,\n",
       "        0.87070707, 0.87070707, 0.87070707, 0.87070707, 0.87070707,\n",
       "        0.87070707, 0.87070707, 0.87070707, 0.86804632, 0.87026361,\n",
       "        0.87105198, 0.87080562, 0.87070707, 0.87070707, 0.87070707,\n",
       "        0.87070707, 0.87070707, 0.87070707, 0.87070707, 0.87070707]),\n",
       " 'split2_test_accuracy': array([0.86050751, 0.86819414, 0.87100271, 0.87124908, 0.8711998 ,\n",
       "        0.87134762, 0.87159399, 0.87149544, 0.87144617, 0.87144617,\n",
       "        0.87144617, 0.87144617, 0.87144617, 0.87144617, 0.87144617,\n",
       "        0.87144617, 0.87144617, 0.87144617, 0.86819414, 0.87124908,\n",
       "        0.87134762, 0.87149544, 0.87144617, 0.87144617, 0.87144617,\n",
       "        0.87144617, 0.87144617, 0.87144617, 0.87144617, 0.87144617]),\n",
       " 'split3_test_accuracy': array([0.86050751, 0.87031288, 0.87425474, 0.87519093, 0.87553585,\n",
       "        0.87578221, 0.87607785, 0.87593003, 0.87593003, 0.87597931,\n",
       "        0.87597931, 0.87597931, 0.87597931, 0.87597931, 0.87597931,\n",
       "        0.87597931, 0.87597931, 0.87597931, 0.87031288, 0.87519093,\n",
       "        0.87578221, 0.87593003, 0.87597931, 0.87597931, 0.87597931,\n",
       "        0.87597931, 0.87597931, 0.87597931, 0.87597931, 0.87597931]),\n",
       " 'split4_test_accuracy': array([0.86050064, 0.86956736, 0.87148911, 0.87222824, 0.87242535,\n",
       "        0.87188332, 0.87193259, 0.87198187, 0.87203114, 0.87203114,\n",
       "        0.87198187, 0.87198187, 0.87198187, 0.87198187, 0.87198187,\n",
       "        0.87198187, 0.87198187, 0.87198187, 0.86956736, 0.87222824,\n",
       "        0.87188332, 0.87193259, 0.87203114, 0.87198187, 0.87198187,\n",
       "        0.87198187, 0.87198187, 0.87198187, 0.87198187, 0.87203114]),\n",
       " 'split5_test_accuracy': array([0.86050064, 0.86690647, 0.86690647, 0.86715285, 0.86670937,\n",
       "        0.86710358, 0.86710358, 0.8670543 , 0.86695575, 0.86695575,\n",
       "        0.86700503, 0.86695575, 0.86700503, 0.86695575, 0.86695575,\n",
       "        0.86695575, 0.86700503, 0.86690647, 0.86690647, 0.86715285,\n",
       "        0.86710358, 0.8670543 , 0.86695575, 0.86695575, 0.86695575,\n",
       "        0.86695575, 0.86695575, 0.86695575, 0.86695575, 0.86695575]),\n",
       " 'split6_test_accuracy': array([0.86054992, 0.86956736, 0.87010939, 0.87267173, 0.87242535,\n",
       "        0.87316448, 0.87311521, 0.87311521, 0.87311521, 0.87306593,\n",
       "        0.87306593, 0.87306593, 0.87311521, 0.87316448, 0.87306593,\n",
       "        0.87306593, 0.87306593, 0.87306593, 0.86956736, 0.87267173,\n",
       "        0.87316448, 0.87311521, 0.87306593, 0.87306593, 0.87306593,\n",
       "        0.87306593, 0.87306593, 0.87306593, 0.87311521, 0.87306593]),\n",
       " 'split7_test_accuracy': array([0.86054992, 0.86784271, 0.87006012, 0.87045432, 0.87025722,\n",
       "        0.86991229, 0.86986301, 0.86976446, 0.86961664, 0.86961664,\n",
       "        0.86961664, 0.86961664, 0.86961664, 0.86961664, 0.86961664,\n",
       "        0.86961664, 0.86961664, 0.86961664, 0.86784271, 0.87045432,\n",
       "        0.86991229, 0.86976446, 0.86961664, 0.86961664, 0.86961664,\n",
       "        0.86961664, 0.86961664, 0.86961664, 0.86961664, 0.86961664]),\n",
       " 'split8_test_accuracy': array([0.86054992, 0.8690746 , 0.87104563, 0.87139056, 0.87148911,\n",
       "        0.87129201, 0.87124273, 0.87134128, 0.87139056, 0.87139056,\n",
       "        0.87139056, 0.87139056, 0.87134128, 0.87139056, 0.87139056,\n",
       "        0.87139056, 0.87139056, 0.87139056, 0.8690746 , 0.87139056,\n",
       "        0.87129201, 0.87134128, 0.87139056, 0.87139056, 0.87139056,\n",
       "        0.87139056, 0.87139056, 0.87139056, 0.87139056, 0.87139056]),\n",
       " 'split9_test_accuracy': array([0.86054992, 0.8668572 , 0.86734996, 0.86828619, 0.86902533,\n",
       "        0.86853257, 0.86853257, 0.86853257, 0.8684833 , 0.8684833 ,\n",
       "        0.8684833 , 0.8684833 , 0.8684833 , 0.8684833 , 0.8684833 ,\n",
       "        0.8684833 , 0.8684833 , 0.8684833 , 0.8668572 , 0.86828619,\n",
       "        0.86853257, 0.86853257, 0.8684833 , 0.8684833 , 0.8684833 ,\n",
       "        0.8684833 , 0.8684833 , 0.8684833 , 0.8684833 , 0.8684833 ]),\n",
       " 'mean_test_accuracy': array([0.8605231 , 0.86853516, 0.87023512, 0.87111713, 0.87122061,\n",
       "        0.8712009 , 0.8712551 , 0.87121568, 0.87118119, 0.87118119,\n",
       "        0.87118119, 0.87117626, 0.87118119, 0.87118611, 0.87117626,\n",
       "        0.87117626, 0.87118119, 0.87117133, 0.86853516, 0.87111221,\n",
       "        0.8712009 , 0.87121075, 0.87118119, 0.87117626, 0.87117626,\n",
       "        0.87117626, 0.87117626, 0.87117626, 0.87118119, 0.87118119]),\n",
       " 'std_test_accuracy': array([2.20385225e-05, 1.09932775e-03, 1.96184022e-03, 2.15865813e-03,\n",
       "        2.20410152e-03, 2.26880953e-03, 2.33976718e-03, 2.32705188e-03,\n",
       "        2.36316806e-03, 2.36913213e-03, 2.35865513e-03, 2.36740979e-03,\n",
       "        2.36225541e-03, 2.37544715e-03, 2.36740979e-03, 2.36740979e-03,\n",
       "        2.35865513e-03, 2.37622417e-03, 1.09932775e-03, 2.15604411e-03,\n",
       "        2.26880953e-03, 2.32547588e-03, 2.36913213e-03, 2.36740979e-03,\n",
       "        2.36740979e-03, 2.36740979e-03, 2.36740979e-03, 2.36740979e-03,\n",
       "        2.37138580e-03, 2.36913213e-03]),\n",
       " 'rank_test_accuracy': array([30, 28, 27, 25,  2,  5,  1,  3,  8, 11, 11, 16,  9,  7, 16, 16, 11,\n",
       "        24, 28, 26,  5,  4, 11, 16, 16, 16, 16, 16,  9, 11]),\n",
       " 'split0_test_precision': array([0.        , 0.66226415, 0.62324393, 0.62281603, 0.61281071,\n",
       "        0.60841121, 0.60925926, 0.6090573 , 0.60865562, 0.60845588,\n",
       "        0.60845588, 0.60845588, 0.60845588, 0.60845588, 0.60845588,\n",
       "        0.60845588, 0.60845588, 0.60845588, 0.66226415, 0.62217659,\n",
       "        0.60841121, 0.6090573 , 0.60845588, 0.60845588, 0.60845588,\n",
       "        0.60845588, 0.60845588, 0.60845588, 0.60845588, 0.60845588]),\n",
       " 'split1_test_precision': array([0.        , 0.63833635, 0.62022195, 0.59979839, 0.6017192 ,\n",
       "        0.59981343, 0.59778598, 0.59666975, 0.59556787, 0.59556787,\n",
       "        0.59556787, 0.59556787, 0.59556787, 0.59556787, 0.59556787,\n",
       "        0.59556787, 0.59556787, 0.59556787, 0.63833635, 0.59979839,\n",
       "        0.59981343, 0.59666975, 0.59556787, 0.59556787, 0.59556787,\n",
       "        0.59556787, 0.59556787, 0.59556787, 0.59556787, 0.59556787]),\n",
       " 'split2_test_precision': array([0.        , 0.63636364, 0.62663496, 0.60603113, 0.59890611,\n",
       "        0.59856631, 0.6       , 0.59893523, 0.59840426, 0.59840426,\n",
       "        0.59840426, 0.59840426, 0.59840426, 0.59840426, 0.59840426,\n",
       "        0.59840426, 0.59840426, 0.59840426, 0.63636364, 0.60603113,\n",
       "        0.59856631, 0.59893523, 0.59840426, 0.59840426, 0.59840426,\n",
       "        0.59840426, 0.59840426, 0.59840426, 0.59840426, 0.59840426]),\n",
       " 'split3_test_precision': array([0.        , 0.69245648, 0.68283093, 0.65952891, 0.65357503,\n",
       "        0.65225933, 0.6536965 , 0.65179437, 0.65179437, 0.65213178,\n",
       "        0.65213178, 0.65213178, 0.65213178, 0.65213178, 0.65213178,\n",
       "        0.65213178, 0.65213178, 0.65213178, 0.69245648, 0.65952891,\n",
       "        0.65225933, 0.65179437, 0.65213178, 0.65213178, 0.65213178,\n",
       "        0.65213178, 0.65213178, 0.65213178, 0.65213178, 0.65213178]),\n",
       " 'split4_test_precision': array([0.        , 0.66254417, 0.64025157, 0.62044534, 0.61612284,\n",
       "        0.60865475, 0.60841121, 0.60857409, 0.60893855, 0.60893855,\n",
       "        0.60837209, 0.60837209, 0.60837209, 0.60837209, 0.60837209,\n",
       "        0.60837209, 0.60837209, 0.60837209, 0.66254417, 0.62044534,\n",
       "        0.60865475, 0.60820896, 0.60893855, 0.60837209, 0.60837209,\n",
       "        0.60837209, 0.60837209, 0.60837209, 0.60837209, 0.60893855]),\n",
       " 'split5_test_precision': array([0.        , 0.61904762, 0.5826972 , 0.56908905, 0.56081081,\n",
       "        0.56320755, 0.56285178, 0.56197577, 0.56093023, 0.56093023,\n",
       "        0.56145251, 0.56093023, 0.56145251, 0.56093023, 0.56093023,\n",
       "        0.56093023, 0.56145251, 0.56052142, 0.61904762, 0.56908905,\n",
       "        0.56320755, 0.56197577, 0.56093023, 0.56093023, 0.56093023,\n",
       "        0.56093023, 0.56093023, 0.56093023, 0.56093023, 0.56093023]),\n",
       " 'split6_test_precision': array([0.        , 0.66024518, 0.61829268, 0.62275449, 0.61209302,\n",
       "        0.61594203, 0.61517615, 0.61476148, 0.61476148, 0.61420863,\n",
       "        0.61420863, 0.61420863, 0.61476148, 0.61531532, 0.61420863,\n",
       "        0.61420863, 0.61420863, 0.61420863, 0.66024518, 0.62275449,\n",
       "        0.61594203, 0.61476148, 0.61420863, 0.61420863, 0.61420863,\n",
       "        0.61420863, 0.61420863, 0.61420863, 0.61476148, 0.61420863]),\n",
       " 'split7_test_precision': array([0.        , 0.63214286, 0.61840491, 0.59901478, 0.59128823,\n",
       "        0.58667883, 0.58598726, 0.58476881, 0.58318264, 0.58318264,\n",
       "        0.58318264, 0.58318264, 0.58318264, 0.58318264, 0.58318264,\n",
       "        0.58318264, 0.58318264, 0.58318264, 0.63214286, 0.59901478,\n",
       "        0.58667883, 0.58476881, 0.58318264, 0.58318264, 0.58318264,\n",
       "        0.58318264, 0.58318264, 0.58318264, 0.58318264, 0.58318264]),\n",
       " 'split8_test_precision': array([0.        , 0.65418895, 0.631644  , 0.60978044, 0.60432331,\n",
       "        0.6       , 0.59872611, 0.59945504, 0.59981851, 0.59981851,\n",
       "        0.59981851, 0.59981851, 0.59927471, 0.59981851, 0.59981851,\n",
       "        0.59981851, 0.59981851, 0.59981851, 0.65418895, 0.60978044,\n",
       "        0.6       , 0.59945504, 0.59981851, 0.59981851, 0.59981851,\n",
       "        0.59981851, 0.59981851, 0.59981851, 0.59981851, 0.59981851]),\n",
       " 'split9_test_precision': array([0.        , 0.61267606, 0.58273381, 0.57703631, 0.58037383,\n",
       "        0.57377049, 0.57323689, 0.57323689, 0.57258792, 0.57258792,\n",
       "        0.57258792, 0.57258792, 0.57258792, 0.57258792, 0.57258792,\n",
       "        0.57258792, 0.57258792, 0.57258792, 0.61267606, 0.57703631,\n",
       "        0.57377049, 0.57323689, 0.57258792, 0.57258792, 0.57258792,\n",
       "        0.57258792, 0.57258792, 0.57258792, 0.57258792, 0.57258792]),\n",
       " 'mean_test_precision': array([0.        , 0.64702654, 0.6226956 , 0.60862949, 0.60320231,\n",
       "        0.60073039, 0.60051311, 0.59992287, 0.59946414, 0.59942263,\n",
       "        0.59941821, 0.59936598, 0.59941911, 0.59947665, 0.59936598,\n",
       "        0.59936598, 0.59941821, 0.5993251 , 0.64702654, 0.60856554,\n",
       "        0.60073039, 0.59988636, 0.59942263, 0.59936598, 0.59936598,\n",
       "        0.59936598, 0.59936598, 0.59936598, 0.59942127, 0.59942263]),\n",
       " 'std_test_precision': array([0.        , 0.02266365, 0.02694642, 0.0242306 , 0.02314682,\n",
       "        0.02312298, 0.02359367, 0.02337235, 0.02373904, 0.02377097,\n",
       "        0.02366473, 0.02374889, 0.02369961, 0.02382027, 0.02374889,\n",
       "        0.02374889, 0.02366473, 0.02381528, 0.02266365, 0.0241939 ,\n",
       "        0.02312298, 0.02335908, 0.02377097, 0.02374889, 0.02374889,\n",
       "        0.02374889, 0.02374889, 0.02374889, 0.023784  , 0.02377097]),\n",
       " 'rank_test_precision': array([30,  1,  3,  4,  6,  7,  9, 10, 13, 14, 19, 21, 18, 12, 21, 21, 19,\n",
       "        29,  1,  5,  7, 11, 14, 21, 21, 21, 21, 21, 17, 14]),\n",
       " 'split0_test_f1_micro': array([0.86050751, 0.86898251, 0.87001725, 0.87228381, 0.87213599,\n",
       "        0.8719389 , 0.87213599, 0.87213599, 0.87213599, 0.87213599,\n",
       "        0.87213599, 0.87213599, 0.87213599, 0.87213599, 0.87213599,\n",
       "        0.87213599, 0.87213599, 0.87213599, 0.86898251, 0.87223454,\n",
       "        0.8719389 , 0.87213599, 0.87213599, 0.87213599, 0.87213599,\n",
       "        0.87213599, 0.87213599, 0.87213599, 0.87213599, 0.87213599]),\n",
       " 'split1_test_f1_micro': array([0.86050751, 0.86804632, 0.87011579, 0.87026361, 0.87100271,\n",
       "        0.87105198, 0.87095344, 0.87080562, 0.87070707, 0.87070707,\n",
       "        0.87070707, 0.87070707, 0.87070707, 0.87070707, 0.87070707,\n",
       "        0.87070707, 0.87070707, 0.87070707, 0.86804632, 0.87026361,\n",
       "        0.87105198, 0.87080562, 0.87070707, 0.87070707, 0.87070707,\n",
       "        0.87070707, 0.87070707, 0.87070707, 0.87070707, 0.87070707]),\n",
       " 'split2_test_f1_micro': array([0.86050751, 0.86819414, 0.87100271, 0.87124908, 0.8711998 ,\n",
       "        0.87134762, 0.87159399, 0.87149544, 0.87144617, 0.87144617,\n",
       "        0.87144617, 0.87144617, 0.87144617, 0.87144617, 0.87144617,\n",
       "        0.87144617, 0.87144617, 0.87144617, 0.86819414, 0.87124908,\n",
       "        0.87134762, 0.87149544, 0.87144617, 0.87144617, 0.87144617,\n",
       "        0.87144617, 0.87144617, 0.87144617, 0.87144617, 0.87144617]),\n",
       " 'split3_test_f1_micro': array([0.86050751, 0.87031288, 0.87425474, 0.87519093, 0.87553585,\n",
       "        0.87578221, 0.87607785, 0.87593003, 0.87593003, 0.87597931,\n",
       "        0.87597931, 0.87597931, 0.87597931, 0.87597931, 0.87597931,\n",
       "        0.87597931, 0.87597931, 0.87597931, 0.87031288, 0.87519093,\n",
       "        0.87578221, 0.87593003, 0.87597931, 0.87597931, 0.87597931,\n",
       "        0.87597931, 0.87597931, 0.87597931, 0.87597931, 0.87597931]),\n",
       " 'split4_test_f1_micro': array([0.86050064, 0.86956736, 0.87148911, 0.87222824, 0.87242535,\n",
       "        0.87188332, 0.87193259, 0.87198187, 0.87203114, 0.87203114,\n",
       "        0.87198187, 0.87198187, 0.87198187, 0.87198187, 0.87198187,\n",
       "        0.87198187, 0.87198187, 0.87198187, 0.86956736, 0.87222824,\n",
       "        0.87188332, 0.87193259, 0.87203114, 0.87198187, 0.87198187,\n",
       "        0.87198187, 0.87198187, 0.87198187, 0.87198187, 0.87203114]),\n",
       " 'split5_test_f1_micro': array([0.86050064, 0.86690647, 0.86690647, 0.86715285, 0.86670937,\n",
       "        0.86710358, 0.86710358, 0.8670543 , 0.86695575, 0.86695575,\n",
       "        0.86700503, 0.86695575, 0.86700503, 0.86695575, 0.86695575,\n",
       "        0.86695575, 0.86700503, 0.86690647, 0.86690647, 0.86715285,\n",
       "        0.86710358, 0.8670543 , 0.86695575, 0.86695575, 0.86695575,\n",
       "        0.86695575, 0.86695575, 0.86695575, 0.86695575, 0.86695575]),\n",
       " 'split6_test_f1_micro': array([0.86054992, 0.86956736, 0.87010939, 0.87267173, 0.87242535,\n",
       "        0.87316448, 0.87311521, 0.87311521, 0.87311521, 0.87306593,\n",
       "        0.87306593, 0.87306593, 0.87311521, 0.87316448, 0.87306593,\n",
       "        0.87306593, 0.87306593, 0.87306593, 0.86956736, 0.87267173,\n",
       "        0.87316448, 0.87311521, 0.87306593, 0.87306593, 0.87306593,\n",
       "        0.87306593, 0.87306593, 0.87306593, 0.87311521, 0.87306593]),\n",
       " 'split7_test_f1_micro': array([0.86054992, 0.86784271, 0.87006012, 0.87045432, 0.87025722,\n",
       "        0.86991229, 0.86986301, 0.86976446, 0.86961664, 0.86961664,\n",
       "        0.86961664, 0.86961664, 0.86961664, 0.86961664, 0.86961664,\n",
       "        0.86961664, 0.86961664, 0.86961664, 0.86784271, 0.87045432,\n",
       "        0.86991229, 0.86976446, 0.86961664, 0.86961664, 0.86961664,\n",
       "        0.86961664, 0.86961664, 0.86961664, 0.86961664, 0.86961664]),\n",
       " 'split8_test_f1_micro': array([0.86054992, 0.8690746 , 0.87104563, 0.87139056, 0.87148911,\n",
       "        0.87129201, 0.87124273, 0.87134128, 0.87139056, 0.87139056,\n",
       "        0.87139056, 0.87139056, 0.87134128, 0.87139056, 0.87139056,\n",
       "        0.87139056, 0.87139056, 0.87139056, 0.8690746 , 0.87139056,\n",
       "        0.87129201, 0.87134128, 0.87139056, 0.87139056, 0.87139056,\n",
       "        0.87139056, 0.87139056, 0.87139056, 0.87139056, 0.87139056]),\n",
       " 'split9_test_f1_micro': array([0.86054992, 0.8668572 , 0.86734996, 0.86828619, 0.86902533,\n",
       "        0.86853257, 0.86853257, 0.86853257, 0.8684833 , 0.8684833 ,\n",
       "        0.8684833 , 0.8684833 , 0.8684833 , 0.8684833 , 0.8684833 ,\n",
       "        0.8684833 , 0.8684833 , 0.8684833 , 0.8668572 , 0.86828619,\n",
       "        0.86853257, 0.86853257, 0.8684833 , 0.8684833 , 0.8684833 ,\n",
       "        0.8684833 , 0.8684833 , 0.8684833 , 0.8684833 , 0.8684833 ]),\n",
       " 'mean_test_f1_micro': array([0.8605231 , 0.86853516, 0.87023512, 0.87111713, 0.87122061,\n",
       "        0.8712009 , 0.8712551 , 0.87121568, 0.87118119, 0.87118119,\n",
       "        0.87118119, 0.87117626, 0.87118119, 0.87118611, 0.87117626,\n",
       "        0.87117626, 0.87118119, 0.87117133, 0.86853516, 0.87111221,\n",
       "        0.8712009 , 0.87121075, 0.87118119, 0.87117626, 0.87117626,\n",
       "        0.87117626, 0.87117626, 0.87117626, 0.87118119, 0.87118119]),\n",
       " 'std_test_f1_micro': array([2.20385225e-05, 1.09932775e-03, 1.96184022e-03, 2.15865813e-03,\n",
       "        2.20410152e-03, 2.26880953e-03, 2.33976718e-03, 2.32705188e-03,\n",
       "        2.36316806e-03, 2.36913213e-03, 2.35865513e-03, 2.36740979e-03,\n",
       "        2.36225541e-03, 2.37544715e-03, 2.36740979e-03, 2.36740979e-03,\n",
       "        2.35865513e-03, 2.37622417e-03, 1.09932775e-03, 2.15604411e-03,\n",
       "        2.26880953e-03, 2.32547588e-03, 2.36913213e-03, 2.36740979e-03,\n",
       "        2.36740979e-03, 2.36740979e-03, 2.36740979e-03, 2.36740979e-03,\n",
       "        2.37138580e-03, 2.36913213e-03]),\n",
       " 'rank_test_f1_micro': array([30, 28, 27, 25,  2,  5,  1,  3,  8, 11, 11, 16,  9,  7, 16, 16, 11,\n",
       "        24, 28, 26,  5,  4, 11, 16, 16, 16, 16, 16,  9, 11])}"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_model.cv_results_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "19851888",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([30, 28, 27, 25,  2,  5,  1,  3,  8, 11, 11, 16,  9,  7, 16, 16, 11,\n",
       "       24, 28, 26,  5,  4, 11, 16, 16, 16, 16, 16,  9, 11])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_model.cv_results_['rank_test_accuracy']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "09bdbbb9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'classifier': LogisticRegression(max_iter=10000),\n",
       " 'classifier__C': 0.0001,\n",
       " 'classifier__penalty': 'l1',\n",
       " 'classifier__solver': 'saga'}"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_model.cv_results_['params'][ np.argmin(best_model.cv_results_['mean_test_accuracy'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "ff293d70",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>classifier</th>\n",
       "      <th>C</th>\n",
       "      <th>penalty</th>\n",
       "      <th>solver</th>\n",
       "      <th>score_acc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.860523</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.868535</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.870235</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.871117</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0100</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.871221</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0100</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.871201</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.1000</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.871255</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.1000</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.871216</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.871181</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.871181</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>10.0000</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.871181</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>10.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.871176</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>100.0000</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.871181</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>100.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.871186</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>1000.0000</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.871176</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>1000.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.871176</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>10000.0000</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.871181</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>10000.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.871171</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.868535</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.871112</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0100</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.871201</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.1000</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.871211</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.871181</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>10.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.871176</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>100.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.871176</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>1000.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.871176</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>10000.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.871176</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>NaN</td>\n",
       "      <td>none</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.871176</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>NaN</td>\n",
       "      <td>none</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.871181</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.871181</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                            classifier           C penalty solver  score_acc\n",
       "0   LogisticRegression(max_iter=10000)      0.0001      l1   saga   0.860523\n",
       "1   LogisticRegression(max_iter=10000)      0.0001      l2   saga   0.868535\n",
       "2   LogisticRegression(max_iter=10000)      0.0010      l1   saga   0.870235\n",
       "3   LogisticRegression(max_iter=10000)      0.0010      l2   saga   0.871117\n",
       "4   LogisticRegression(max_iter=10000)      0.0100      l1   saga   0.871221\n",
       "5   LogisticRegression(max_iter=10000)      0.0100      l2   saga   0.871201\n",
       "6   LogisticRegression(max_iter=10000)      0.1000      l1   saga   0.871255\n",
       "7   LogisticRegression(max_iter=10000)      0.1000      l2   saga   0.871216\n",
       "8   LogisticRegression(max_iter=10000)      1.0000      l1   saga   0.871181\n",
       "9   LogisticRegression(max_iter=10000)      1.0000      l2   saga   0.871181\n",
       "10  LogisticRegression(max_iter=10000)     10.0000      l1   saga   0.871181\n",
       "11  LogisticRegression(max_iter=10000)     10.0000      l2   saga   0.871176\n",
       "12  LogisticRegression(max_iter=10000)    100.0000      l1   saga   0.871181\n",
       "13  LogisticRegression(max_iter=10000)    100.0000      l2   saga   0.871186\n",
       "14  LogisticRegression(max_iter=10000)   1000.0000      l1   saga   0.871176\n",
       "15  LogisticRegression(max_iter=10000)   1000.0000      l2   saga   0.871176\n",
       "16  LogisticRegression(max_iter=10000)  10000.0000      l1   saga   0.871181\n",
       "17  LogisticRegression(max_iter=10000)  10000.0000      l2   saga   0.871171\n",
       "18  LogisticRegression(max_iter=10000)      0.0001      l2  lbfgs   0.868535\n",
       "19  LogisticRegression(max_iter=10000)      0.0010      l2  lbfgs   0.871112\n",
       "20  LogisticRegression(max_iter=10000)      0.0100      l2  lbfgs   0.871201\n",
       "21  LogisticRegression(max_iter=10000)      0.1000      l2  lbfgs   0.871211\n",
       "22  LogisticRegression(max_iter=10000)      1.0000      l2  lbfgs   0.871181\n",
       "23  LogisticRegression(max_iter=10000)     10.0000      l2  lbfgs   0.871176\n",
       "24  LogisticRegression(max_iter=10000)    100.0000      l2  lbfgs   0.871176\n",
       "25  LogisticRegression(max_iter=10000)   1000.0000      l2  lbfgs   0.871176\n",
       "26  LogisticRegression(max_iter=10000)  10000.0000      l2  lbfgs   0.871176\n",
       "27  LogisticRegression(max_iter=10000)         NaN    none  lbfgs   0.871176\n",
       "28  LogisticRegression(max_iter=10000)         NaN    none   saga   0.871181\n",
       "29  LogisticRegression(max_iter=10000)         NaN     NaN    NaN   0.871181"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results = pd.DataFrame( best_model.cv_results_['params'] )\n",
    "\n",
    "results['score_acc'] = best_model.cv_results_['mean_test_accuracy']\n",
    "cols = results.columns.to_series().str.split('__').apply(lambda x: x[-1])\n",
    "results.columns = cols\n",
    "\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "69c1c465",
   "metadata": {},
   "outputs": [],
   "source": [
    "acc_list.append(results[0:1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "05385bec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'classifier': LogisticRegression(max_iter=10000),\n",
       " 'classifier__C': 0.0001,\n",
       " 'classifier__penalty': 'l2',\n",
       " 'classifier__solver': 'saga'}"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_model.cv_results_['params'][ np.argmin(best_model.cv_results_['rank_test_precision'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "2ef6f409",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>classifier</th>\n",
       "      <th>C</th>\n",
       "      <th>penalty</th>\n",
       "      <th>solver</th>\n",
       "      <th>score_precision</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.647027</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.622696</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.608629</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0100</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.603202</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0100</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.600730</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.1000</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.600513</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.1000</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.599923</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.599464</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.599423</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>10.0000</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.599418</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>10.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.599366</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>100.0000</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.599419</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>100.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.599477</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>1000.0000</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.599366</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>1000.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.599366</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>10000.0000</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.599418</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>10000.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.599325</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.647027</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.608566</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0100</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.600730</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.1000</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.599886</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.599423</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>10.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.599366</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>100.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.599366</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>1000.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.599366</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>10000.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.599366</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>NaN</td>\n",
       "      <td>none</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.599366</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>NaN</td>\n",
       "      <td>none</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.599421</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.599423</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                            classifier           C penalty solver  \\\n",
       "0   LogisticRegression(max_iter=10000)      0.0001      l1   saga   \n",
       "1   LogisticRegression(max_iter=10000)      0.0001      l2   saga   \n",
       "2   LogisticRegression(max_iter=10000)      0.0010      l1   saga   \n",
       "3   LogisticRegression(max_iter=10000)      0.0010      l2   saga   \n",
       "4   LogisticRegression(max_iter=10000)      0.0100      l1   saga   \n",
       "5   LogisticRegression(max_iter=10000)      0.0100      l2   saga   \n",
       "6   LogisticRegression(max_iter=10000)      0.1000      l1   saga   \n",
       "7   LogisticRegression(max_iter=10000)      0.1000      l2   saga   \n",
       "8   LogisticRegression(max_iter=10000)      1.0000      l1   saga   \n",
       "9   LogisticRegression(max_iter=10000)      1.0000      l2   saga   \n",
       "10  LogisticRegression(max_iter=10000)     10.0000      l1   saga   \n",
       "11  LogisticRegression(max_iter=10000)     10.0000      l2   saga   \n",
       "12  LogisticRegression(max_iter=10000)    100.0000      l1   saga   \n",
       "13  LogisticRegression(max_iter=10000)    100.0000      l2   saga   \n",
       "14  LogisticRegression(max_iter=10000)   1000.0000      l1   saga   \n",
       "15  LogisticRegression(max_iter=10000)   1000.0000      l2   saga   \n",
       "16  LogisticRegression(max_iter=10000)  10000.0000      l1   saga   \n",
       "17  LogisticRegression(max_iter=10000)  10000.0000      l2   saga   \n",
       "18  LogisticRegression(max_iter=10000)      0.0001      l2  lbfgs   \n",
       "19  LogisticRegression(max_iter=10000)      0.0010      l2  lbfgs   \n",
       "20  LogisticRegression(max_iter=10000)      0.0100      l2  lbfgs   \n",
       "21  LogisticRegression(max_iter=10000)      0.1000      l2  lbfgs   \n",
       "22  LogisticRegression(max_iter=10000)      1.0000      l2  lbfgs   \n",
       "23  LogisticRegression(max_iter=10000)     10.0000      l2  lbfgs   \n",
       "24  LogisticRegression(max_iter=10000)    100.0000      l2  lbfgs   \n",
       "25  LogisticRegression(max_iter=10000)   1000.0000      l2  lbfgs   \n",
       "26  LogisticRegression(max_iter=10000)  10000.0000      l2  lbfgs   \n",
       "27  LogisticRegression(max_iter=10000)         NaN    none  lbfgs   \n",
       "28  LogisticRegression(max_iter=10000)         NaN    none   saga   \n",
       "29  LogisticRegression(max_iter=10000)         NaN     NaN    NaN   \n",
       "\n",
       "    score_precision  \n",
       "0          0.000000  \n",
       "1          0.647027  \n",
       "2          0.622696  \n",
       "3          0.608629  \n",
       "4          0.603202  \n",
       "5          0.600730  \n",
       "6          0.600513  \n",
       "7          0.599923  \n",
       "8          0.599464  \n",
       "9          0.599423  \n",
       "10         0.599418  \n",
       "11         0.599366  \n",
       "12         0.599419  \n",
       "13         0.599477  \n",
       "14         0.599366  \n",
       "15         0.599366  \n",
       "16         0.599418  \n",
       "17         0.599325  \n",
       "18         0.647027  \n",
       "19         0.608566  \n",
       "20         0.600730  \n",
       "21         0.599886  \n",
       "22         0.599423  \n",
       "23         0.599366  \n",
       "24         0.599366  \n",
       "25         0.599366  \n",
       "26         0.599366  \n",
       "27         0.599366  \n",
       "28         0.599421  \n",
       "29         0.599423  "
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results = pd.DataFrame( best_model.cv_results_['params'] )\n",
    "\n",
    "results['score_precision'] = best_model.cv_results_['mean_test_precision']\n",
    "cols = results.columns.to_series().str.split('__').apply(lambda x: x[-1])\n",
    "results.columns = cols\n",
    "\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "be514fd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "roc_auc_ovr_List.append(results[1:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "8cb4c8d2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'classifier': LogisticRegression(max_iter=10000),\n",
       " 'classifier__C': 0.1,\n",
       " 'classifier__penalty': 'l1',\n",
       " 'classifier__solver': 'saga'}"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_model.cv_results_['params'][ np.argmin(best_model.cv_results_['rank_test_f1_micro'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "e1fc8e88",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>classifier</th>\n",
       "      <th>C</th>\n",
       "      <th>penalty</th>\n",
       "      <th>solver</th>\n",
       "      <th>score_f1_micro</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.860523</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.868535</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.870235</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.871117</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0100</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.871221</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0100</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.871201</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.1000</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.871255</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.1000</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.871216</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.871181</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.871181</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>10.0000</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.871181</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>10.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.871176</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>100.0000</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.871181</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>100.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.871186</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>1000.0000</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.871176</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>1000.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.871176</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>10000.0000</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.871181</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>10000.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.871171</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.868535</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.871112</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0100</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.871201</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.1000</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.871211</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.871181</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>10.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.871176</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>100.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.871176</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>1000.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.871176</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>10000.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.871176</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>NaN</td>\n",
       "      <td>none</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.871176</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>NaN</td>\n",
       "      <td>none</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.871181</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.871181</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                            classifier           C penalty solver  \\\n",
       "0   LogisticRegression(max_iter=10000)      0.0001      l1   saga   \n",
       "1   LogisticRegression(max_iter=10000)      0.0001      l2   saga   \n",
       "2   LogisticRegression(max_iter=10000)      0.0010      l1   saga   \n",
       "3   LogisticRegression(max_iter=10000)      0.0010      l2   saga   \n",
       "4   LogisticRegression(max_iter=10000)      0.0100      l1   saga   \n",
       "5   LogisticRegression(max_iter=10000)      0.0100      l2   saga   \n",
       "6   LogisticRegression(max_iter=10000)      0.1000      l1   saga   \n",
       "7   LogisticRegression(max_iter=10000)      0.1000      l2   saga   \n",
       "8   LogisticRegression(max_iter=10000)      1.0000      l1   saga   \n",
       "9   LogisticRegression(max_iter=10000)      1.0000      l2   saga   \n",
       "10  LogisticRegression(max_iter=10000)     10.0000      l1   saga   \n",
       "11  LogisticRegression(max_iter=10000)     10.0000      l2   saga   \n",
       "12  LogisticRegression(max_iter=10000)    100.0000      l1   saga   \n",
       "13  LogisticRegression(max_iter=10000)    100.0000      l2   saga   \n",
       "14  LogisticRegression(max_iter=10000)   1000.0000      l1   saga   \n",
       "15  LogisticRegression(max_iter=10000)   1000.0000      l2   saga   \n",
       "16  LogisticRegression(max_iter=10000)  10000.0000      l1   saga   \n",
       "17  LogisticRegression(max_iter=10000)  10000.0000      l2   saga   \n",
       "18  LogisticRegression(max_iter=10000)      0.0001      l2  lbfgs   \n",
       "19  LogisticRegression(max_iter=10000)      0.0010      l2  lbfgs   \n",
       "20  LogisticRegression(max_iter=10000)      0.0100      l2  lbfgs   \n",
       "21  LogisticRegression(max_iter=10000)      0.1000      l2  lbfgs   \n",
       "22  LogisticRegression(max_iter=10000)      1.0000      l2  lbfgs   \n",
       "23  LogisticRegression(max_iter=10000)     10.0000      l2  lbfgs   \n",
       "24  LogisticRegression(max_iter=10000)    100.0000      l2  lbfgs   \n",
       "25  LogisticRegression(max_iter=10000)   1000.0000      l2  lbfgs   \n",
       "26  LogisticRegression(max_iter=10000)  10000.0000      l2  lbfgs   \n",
       "27  LogisticRegression(max_iter=10000)         NaN    none  lbfgs   \n",
       "28  LogisticRegression(max_iter=10000)         NaN    none   saga   \n",
       "29  LogisticRegression(max_iter=10000)         NaN     NaN    NaN   \n",
       "\n",
       "    score_f1_micro  \n",
       "0         0.860523  \n",
       "1         0.868535  \n",
       "2         0.870235  \n",
       "3         0.871117  \n",
       "4         0.871221  \n",
       "5         0.871201  \n",
       "6         0.871255  \n",
       "7         0.871216  \n",
       "8         0.871181  \n",
       "9         0.871181  \n",
       "10        0.871181  \n",
       "11        0.871176  \n",
       "12        0.871181  \n",
       "13        0.871186  \n",
       "14        0.871176  \n",
       "15        0.871176  \n",
       "16        0.871181  \n",
       "17        0.871171  \n",
       "18        0.868535  \n",
       "19        0.871112  \n",
       "20        0.871201  \n",
       "21        0.871211  \n",
       "22        0.871181  \n",
       "23        0.871176  \n",
       "24        0.871176  \n",
       "25        0.871176  \n",
       "26        0.871176  \n",
       "27        0.871176  \n",
       "28        0.871181  \n",
       "29        0.871181  "
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results = pd.DataFrame( best_model.cv_results_['params'] )\n",
    "\n",
    "results['score_f1_micro'] = best_model.cv_results_['mean_test_f1_micro']\n",
    "cols = results.columns.to_series().str.split('__').apply(lambda x: x[-1])\n",
    "results.columns = cols\n",
    "\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "a8c83f8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "f1_micro_List.append(results[6:7])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cd3a031",
   "metadata": {},
   "source": [
    "## Trial 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "f0c67633",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, Y_train, Y_test = train_test_split(X, y, test_size=0.2, shuffle=True, random_state = 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "382815ad",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Guest-All\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\Guest-All\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\Guest-All\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\Guest-All\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\Guest-All\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\Guest-All\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\Guest-All\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\Guest-All\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\Guest-All\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\Guest-All\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\Guest-All\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1183: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Guest-All\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1183: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Guest-All\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1183: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Guest-All\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1183: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Guest-All\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1183: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Guest-All\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1183: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Guest-All\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1183: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Guest-All\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1183: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Guest-All\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1183: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Guest-All\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1183: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Guest-All\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1183: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Guest-All\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1183: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Guest-All\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1183: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Guest-All\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1183: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Guest-All\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1183: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Guest-All\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1183: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Guest-All\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1183: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Guest-All\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1183: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Guest-All\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1183: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Guest-All\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1183: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Create a pipeline - RF is a stand in, we will populate the classifier part below\n",
    "pipe = Pipeline([('std', StandardScaler()),\n",
    "                 ('classifier', LogisticRegression())])\n",
    "\n",
    "# Create search space of candidate learning algorithms and their hyperparameters\n",
    "# note lbfgs can't do l1, and if you pass penalty='none' it expects no C value\n",
    "search_space = [{'classifier': [LogisticRegression(max_iter=10000)],\n",
    "                 'classifier__solver': ['saga'],\n",
    "                 'classifier__penalty': ['l1', 'l2'],\n",
    "                 'classifier__C': np.logspace(-4, 4, 9)},\n",
    "                {'classifier': [LogisticRegression(max_iter=10000)],\n",
    "                 'classifier__solver': ['lbfgs'],\n",
    "                 'classifier__penalty': ['l2'],\n",
    "                 'classifier__C': np.logspace(-4, 4, 9)},\n",
    "                {'classifier': [LogisticRegression(max_iter=10000)],\n",
    "                 'classifier__solver': ['lbfgs','saga'],\n",
    "                 'classifier__penalty': ['none']},\n",
    "                {'classifier': [LogisticRegression(max_iter=10000)]}\n",
    "                ]\n",
    "\n",
    "# Create grid search\n",
    "clf = GridSearchCV(pipe, search_space, cv=StratifiedKFold(n_splits=10),\n",
    "                   scoring=['accuracy', 'precision', 'f1_micro'], refit=False,\n",
    "                   verbose=0)\n",
    "\n",
    "# Fit grid search\n",
    "best_model = clf.fit(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "befaa8ab",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'mean_fit_time': array([ 3.55223584,  3.3466258 ,  3.53682878,  3.44312558,  3.8852608 ,\n",
       "         3.31105127,  6.57836411,  3.29348133, 27.54237146,  3.04074023,\n",
       "         3.97157073,  3.18827581,  4.07937329,  3.04370422,  4.08045483,\n",
       "         3.40549903,  4.01614037,  3.1975332 ,  0.56378927,  0.69363408,\n",
       "         0.79622443,  0.79962151,  0.75091057,  0.78887646,  0.79482977,\n",
       "         0.80001218,  0.803865  ,  0.79795809,  3.3427321 ,  1.03634036]),\n",
       " 'std_fit_time': array([0.39841963, 0.12036444, 0.26783182, 0.2446778 , 0.30099021,\n",
       "        0.247392  , 3.39143162, 0.16844598, 4.43220908, 0.40044589,\n",
       "        0.24751587, 0.28373086, 0.34523474, 0.31811443, 0.33807551,\n",
       "        0.33130735, 0.27580621, 0.40465446, 0.01005809, 0.01490455,\n",
       "        0.03299577, 0.0236808 , 0.06966027, 0.03324357, 0.02761755,\n",
       "        0.02332827, 0.03478391, 0.02270426, 0.61610691, 0.20458591]),\n",
       " 'mean_score_time': array([0.03174005, 0.03432422, 0.03246238, 0.03379924, 0.03449111,\n",
       "        0.03593814, 0.03830118, 0.03230529, 0.03216019, 0.03119957,\n",
       "        0.03178053, 0.03012426, 0.03384438, 0.03272934, 0.03390262,\n",
       "        0.03309848, 0.03319414, 0.03366649, 0.03424878, 0.03196712,\n",
       "        0.03428481, 0.03442206, 0.02770431, 0.03318982, 0.03359058,\n",
       "        0.03448164, 0.0328424 , 0.03439963, 0.0333045 , 0.04155324]),\n",
       " 'std_score_time': array([0.00404159, 0.00116347, 0.00466295, 0.00132136, 0.0011441 ,\n",
       "        0.00454958, 0.01149347, 0.00247751, 0.00743411, 0.00470215,\n",
       "        0.00324799, 0.00436368, 0.00241986, 0.00389433, 0.00314899,\n",
       "        0.00252919, 0.00286591, 0.00436898, 0.00193505, 0.00344297,\n",
       "        0.00177823, 0.00188056, 0.00667078, 0.00405448, 0.00207691,\n",
       "        0.00124736, 0.00392794, 0.00133125, 0.00560264, 0.00880399]),\n",
       " 'param_classifier': masked_array(data=[LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000)],\n",
       "              mask=[False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'param_classifier__C': masked_array(data=[0.0001, 0.0001, 0.001, 0.001, 0.01, 0.01, 0.1, 0.1,\n",
       "                    1.0, 1.0, 10.0, 10.0, 100.0, 100.0, 1000.0, 1000.0,\n",
       "                    10000.0, 10000.0, 0.0001, 0.001, 0.01, 0.1, 1.0, 10.0,\n",
       "                    100.0, 1000.0, 10000.0, --, --, --],\n",
       "              mask=[False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False,  True,  True,  True],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'param_classifier__penalty': masked_array(data=['l1', 'l2', 'l1', 'l2', 'l1', 'l2', 'l1', 'l2', 'l1',\n",
       "                    'l2', 'l1', 'l2', 'l1', 'l2', 'l1', 'l2', 'l1', 'l2',\n",
       "                    'l2', 'l2', 'l2', 'l2', 'l2', 'l2', 'l2', 'l2', 'l2',\n",
       "                    'none', 'none', --],\n",
       "              mask=[False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False,  True],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'param_classifier__solver': masked_array(data=['saga', 'saga', 'saga', 'saga', 'saga', 'saga', 'saga',\n",
       "                    'saga', 'saga', 'saga', 'saga', 'saga', 'saga', 'saga',\n",
       "                    'saga', 'saga', 'saga', 'saga', 'lbfgs', 'lbfgs',\n",
       "                    'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs',\n",
       "                    'lbfgs', 'lbfgs', 'saga', --],\n",
       "              mask=[False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False,  True],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'params': [{'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 0.0001,\n",
       "   'classifier__penalty': 'l1',\n",
       "   'classifier__solver': 'saga'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 0.0001,\n",
       "   'classifier__penalty': 'l2',\n",
       "   'classifier__solver': 'saga'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 0.001,\n",
       "   'classifier__penalty': 'l1',\n",
       "   'classifier__solver': 'saga'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 0.001,\n",
       "   'classifier__penalty': 'l2',\n",
       "   'classifier__solver': 'saga'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 0.01,\n",
       "   'classifier__penalty': 'l1',\n",
       "   'classifier__solver': 'saga'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 0.01,\n",
       "   'classifier__penalty': 'l2',\n",
       "   'classifier__solver': 'saga'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 0.1,\n",
       "   'classifier__penalty': 'l1',\n",
       "   'classifier__solver': 'saga'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 0.1,\n",
       "   'classifier__penalty': 'l2',\n",
       "   'classifier__solver': 'saga'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 1.0,\n",
       "   'classifier__penalty': 'l1',\n",
       "   'classifier__solver': 'saga'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 1.0,\n",
       "   'classifier__penalty': 'l2',\n",
       "   'classifier__solver': 'saga'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 10.0,\n",
       "   'classifier__penalty': 'l1',\n",
       "   'classifier__solver': 'saga'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 10.0,\n",
       "   'classifier__penalty': 'l2',\n",
       "   'classifier__solver': 'saga'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 100.0,\n",
       "   'classifier__penalty': 'l1',\n",
       "   'classifier__solver': 'saga'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 100.0,\n",
       "   'classifier__penalty': 'l2',\n",
       "   'classifier__solver': 'saga'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 1000.0,\n",
       "   'classifier__penalty': 'l1',\n",
       "   'classifier__solver': 'saga'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 1000.0,\n",
       "   'classifier__penalty': 'l2',\n",
       "   'classifier__solver': 'saga'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 10000.0,\n",
       "   'classifier__penalty': 'l1',\n",
       "   'classifier__solver': 'saga'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 10000.0,\n",
       "   'classifier__penalty': 'l2',\n",
       "   'classifier__solver': 'saga'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 0.0001,\n",
       "   'classifier__penalty': 'l2',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 0.001,\n",
       "   'classifier__penalty': 'l2',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 0.01,\n",
       "   'classifier__penalty': 'l2',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 0.1,\n",
       "   'classifier__penalty': 'l2',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 1.0,\n",
       "   'classifier__penalty': 'l2',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 10.0,\n",
       "   'classifier__penalty': 'l2',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 100.0,\n",
       "   'classifier__penalty': 'l2',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 1000.0,\n",
       "   'classifier__penalty': 'l2',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 10000.0,\n",
       "   'classifier__penalty': 'l2',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__penalty': 'none',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__penalty': 'none',\n",
       "   'classifier__solver': 'saga'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000)}],\n",
       " 'split0_test_accuracy': array([0.86070461, 0.86829268, 0.8704607 , 0.8719389 , 0.8713969 ,\n",
       "        0.87129835, 0.87144617, 0.87144617, 0.87144617, 0.87144617,\n",
       "        0.87144617, 0.87144617, 0.87144617, 0.87144617, 0.87144617,\n",
       "        0.87144617, 0.87144617, 0.87144617, 0.86829268, 0.8719389 ,\n",
       "        0.87129835, 0.87144617, 0.87144617, 0.87144617, 0.87144617,\n",
       "        0.87144617, 0.87144617, 0.87144617, 0.87144617, 0.87144617]),\n",
       " 'split1_test_accuracy': array([0.86070461, 0.86922887, 0.86957379, 0.86952451, 0.87001725,\n",
       "        0.86986943, 0.86986943, 0.86986943, 0.8699187 , 0.8699187 ,\n",
       "        0.8699187 , 0.8699187 , 0.8699187 , 0.8699187 , 0.8699187 ,\n",
       "        0.8699187 , 0.8699187 , 0.8699187 , 0.86922887, 0.86952451,\n",
       "        0.86986943, 0.86986943, 0.8699187 , 0.8699187 , 0.8699187 ,\n",
       "        0.8699187 , 0.8699187 , 0.8699187 , 0.8699187 , 0.8699187 ]),\n",
       " 'split2_test_accuracy': array([0.86070461, 0.86848978, 0.87050998, 0.87080562, 0.87060852,\n",
       "        0.87050998, 0.87050998, 0.87055925, 0.87055925, 0.87055925,\n",
       "        0.87055925, 0.87060852, 0.87060852, 0.87060852, 0.87060852,\n",
       "        0.87060852, 0.87060852, 0.87060852, 0.86848978, 0.87080562,\n",
       "        0.87050998, 0.87055925, 0.87055925, 0.87060852, 0.87060852,\n",
       "        0.87060852, 0.87060852, 0.87060852, 0.87055925, 0.87055925]),\n",
       " 'split3_test_accuracy': array([0.86070461, 0.86996797, 0.87169254, 0.87213599, 0.872678  ,\n",
       "        0.87282582, 0.87262873, 0.87257945, 0.87262873, 0.87257945,\n",
       "        0.87257945, 0.87257945, 0.87257945, 0.87257945, 0.87257945,\n",
       "        0.87257945, 0.87257945, 0.87257945, 0.86996797, 0.87213599,\n",
       "        0.87282582, 0.87257945, 0.87257945, 0.87257945, 0.87257945,\n",
       "        0.87257945, 0.87257945, 0.87257945, 0.87257945, 0.87257945]),\n",
       " 'split4_test_accuracy': array([0.86074702, 0.86720213, 0.86946881, 0.87020794, 0.8705036 ,\n",
       "        0.87055287, 0.87030649, 0.87015867, 0.87006012, 0.87006012,\n",
       "        0.87010939, 0.87010939, 0.87010939, 0.87010939, 0.87015867,\n",
       "        0.87010939, 0.87010939, 0.87010939, 0.86720213, 0.87020794,\n",
       "        0.87055287, 0.87010939, 0.87015867, 0.87015867, 0.87015867,\n",
       "        0.87015867, 0.87015867, 0.87015867, 0.87010939, 0.87015867]),\n",
       " 'split5_test_accuracy': array([0.86074702, 0.86892678, 0.87114418, 0.87336158, 0.87355869,\n",
       "        0.87400217, 0.87395289, 0.87385434, 0.87385434, 0.87385434,\n",
       "        0.87385434, 0.87385434, 0.87385434, 0.87385434, 0.87385434,\n",
       "        0.87385434, 0.87385434, 0.87385434, 0.86892678, 0.87336158,\n",
       "        0.87400217, 0.87385434, 0.87385434, 0.87385434, 0.87385434,\n",
       "        0.87385434, 0.87385434, 0.87385434, 0.87385434, 0.87385434]),\n",
       " 'split6_test_accuracy': array([0.86074702, 0.86897605, 0.86971519, 0.87040505, 0.87074998,\n",
       "        0.87035577, 0.87030649, 0.87035577, 0.87030649, 0.87035577,\n",
       "        0.87035577, 0.87040505, 0.87040505, 0.87040505, 0.87040505,\n",
       "        0.87040505, 0.87035577, 0.87035577, 0.86897605, 0.87040505,\n",
       "        0.87040505, 0.87040505, 0.87035577, 0.87035577, 0.87035577,\n",
       "        0.87035577, 0.87035577, 0.87035577, 0.87035577, 0.87035577]),\n",
       " 'split7_test_accuracy': array([0.86074702, 0.86730068, 0.86976446, 0.87163694, 0.87262245,\n",
       "        0.8723268 , 0.87247462, 0.87242535, 0.8725239 , 0.87247462,\n",
       "        0.87242535, 0.87247462, 0.8725239 , 0.87247462, 0.87247462,\n",
       "        0.8725239 , 0.87247462, 0.87247462, 0.86730068, 0.87163694,\n",
       "        0.8723268 , 0.87242535, 0.87242535, 0.87247462, 0.87247462,\n",
       "        0.87247462, 0.87247462, 0.87247462, 0.87257317, 0.87242535]),\n",
       " 'split8_test_accuracy': array([0.86074702, 0.86882823, 0.86996156, 0.87208042, 0.87242535,\n",
       "        0.87217897, 0.87247462, 0.8723268 , 0.8723268 , 0.8723268 ,\n",
       "        0.8723268 , 0.8723268 , 0.8723268 , 0.8723268 , 0.8723268 ,\n",
       "        0.8723268 , 0.8723268 , 0.8723268 , 0.86882823, 0.87208042,\n",
       "        0.87217897, 0.8723268 , 0.8723268 , 0.8723268 , 0.8723268 ,\n",
       "        0.8723268 , 0.8723268 , 0.8723268 , 0.8723268 , 0.8723268 ]),\n",
       " 'split9_test_accuracy': array([0.86074702, 0.8684833 , 0.86951808, 0.87104563, 0.87168621,\n",
       "        0.87208042, 0.87222824, 0.87217897, 0.87217897, 0.87217897,\n",
       "        0.87217897, 0.87217897, 0.87217897, 0.87217897, 0.87217897,\n",
       "        0.87217897, 0.87217897, 0.87217897, 0.8684833 , 0.87104563,\n",
       "        0.87208042, 0.87217897, 0.87217897, 0.87217897, 0.87217897,\n",
       "        0.87217897, 0.87217897, 0.87217897, 0.87217897, 0.87217897]),\n",
       " 'mean_test_accuracy': array([0.86073005, 0.86856965, 0.87018093, 0.87131426, 0.87162469,\n",
       "        0.87160006, 0.87161977, 0.87157542, 0.87158035, 0.87157542,\n",
       "        0.87157542, 0.8715902 , 0.87159513, 0.8715902 , 0.87159513,\n",
       "        0.87159513, 0.87158527, 0.87158527, 0.86856965, 0.87131426,\n",
       "        0.87160498, 0.87157542, 0.87158035, 0.8715902 , 0.8715902 ,\n",
       "        0.8715902 , 0.8715902 , 0.8715902 , 0.8715902 , 0.87158035]),\n",
       " 'std_test_accuracy': array([2.07774427e-05, 7.95008931e-04, 7.17839510e-04, 1.07390690e-03,\n",
       "        1.10365321e-03, 1.23636138e-03, 1.26760039e-03, 1.23722400e-03,\n",
       "        1.25823245e-03, 1.24565998e-03, 1.23626823e-03, 1.23093924e-03,\n",
       "        1.23456309e-03, 1.23093924e-03, 1.22508627e-03, 1.23456309e-03,\n",
       "        1.23576284e-03, 1.23576284e-03, 7.95008931e-04, 1.07390690e-03,\n",
       "        1.23148098e-03, 1.23820488e-03, 1.23049981e-03, 1.22995257e-03,\n",
       "        1.22995257e-03, 1.22995257e-03, 1.22995257e-03, 1.22995257e-03,\n",
       "        1.24717856e-03, 1.23049981e-03]),\n",
       " 'rank_test_accuracy': array([30, 28, 27, 25,  1,  4,  2, 21, 20, 23, 23,  9,  5,  9,  5,  5, 16,\n",
       "        16, 28, 25,  3, 22, 18,  9,  9,  9,  9,  9,  8, 18]),\n",
       " 'split0_test_precision': array([0.        , 0.6360424 , 0.61956522, 0.61515152, 0.60442733,\n",
       "        0.6       , 0.60073937, 0.60055351, 0.60055351, 0.60055351,\n",
       "        0.60055351, 0.60055351, 0.60055351, 0.60055351, 0.60055351,\n",
       "        0.60055351, 0.60055351, 0.60055351, 0.6360424 , 0.61515152,\n",
       "        0.6       , 0.60055351, 0.60055351, 0.60055351, 0.60055351,\n",
       "        0.60055351, 0.60055351, 0.60055351, 0.60055351, 0.60055351]),\n",
       " 'split1_test_precision': array([0.        , 0.65813528, 0.61538462, 0.59293873, 0.5938431 ,\n",
       "        0.58994197, 0.58908046, 0.58891013, 0.58930277, 0.58930277,\n",
       "        0.58930277, 0.58930277, 0.58930277, 0.58930277, 0.58930277,\n",
       "        0.58930277, 0.58930277, 0.58930277, 0.65813528, 0.59293873,\n",
       "        0.58994197, 0.58891013, 0.58930277, 0.58930277, 0.58930277,\n",
       "        0.58930277, 0.58930277, 0.58930277, 0.58930277, 0.58930277]),\n",
       " 'split2_test_precision': array([0.        , 0.64575646, 0.62299135, 0.60688217, 0.59766764,\n",
       "        0.59576516, 0.59503343, 0.5952381 , 0.5952381 , 0.5952381 ,\n",
       "        0.5952381 , 0.59562322, 0.59562322, 0.59562322, 0.59562322,\n",
       "        0.59562322, 0.59562322, 0.59562322, 0.64575646, 0.60688217,\n",
       "        0.59576516, 0.5952381 , 0.5952381 , 0.59562322, 0.59562322,\n",
       "        0.59562322, 0.59562322, 0.59562322, 0.5952381 , 0.5952381 ]),\n",
       " 'split3_test_precision': array([0.        , 0.66607774, 0.63680982, 0.61153846, 0.61198157,\n",
       "        0.61161525, 0.60900901, 0.60826595, 0.60861759, 0.60807175,\n",
       "        0.60807175, 0.60807175, 0.60807175, 0.60807175, 0.60807175,\n",
       "        0.60807175, 0.60807175, 0.60807175, 0.66607774, 0.61153846,\n",
       "        0.61161525, 0.60826595, 0.60807175, 0.60807175, 0.60807175,\n",
       "        0.60807175, 0.60807175, 0.60807175, 0.60807175, 0.60807175]),\n",
       " 'split4_test_precision': array([0.        , 0.62428843, 0.61690885, 0.5997921 , 0.5963035 ,\n",
       "        0.59485224, 0.59168242, 0.58984008, 0.58856607, 0.58856607,\n",
       "        0.58895131, 0.58895131, 0.58895131, 0.58895131, 0.58933583,\n",
       "        0.58895131, 0.58895131, 0.58895131, 0.62428843, 0.5997921 ,\n",
       "        0.59485224, 0.58928571, 0.58950328, 0.58933583, 0.58933583,\n",
       "        0.58933583, 0.58933583, 0.58933583, 0.58895131, 0.58950328]),\n",
       " 'split5_test_precision': array([0.        , 0.64359862, 0.6310559 , 0.62598425, 0.62218045,\n",
       "        0.62350781, 0.62248629, 0.62112933, 0.62112933, 0.62112933,\n",
       "        0.62112933, 0.62112933, 0.62112933, 0.62112933, 0.62112933,\n",
       "        0.62112933, 0.62112933, 0.62112933, 0.64359862, 0.62598425,\n",
       "        0.62350781, 0.62112933, 0.62112933, 0.62112933, 0.62112933,\n",
       "        0.62112933, 0.62112933, 0.62112933, 0.62112933, 0.62112933]),\n",
       " 'split6_test_precision': array([0.        , 0.65904762, 0.62133333, 0.6018711 , 0.59769009,\n",
       "        0.59241706, 0.5913371 , 0.59172154, 0.59116541, 0.5915493 ,\n",
       "        0.5915493 , 0.59193246, 0.59193246, 0.59193246, 0.59193246,\n",
       "        0.59193246, 0.5915493 , 0.5915493 , 0.65904762, 0.6018711 ,\n",
       "        0.59297913, 0.59227872, 0.5915493 , 0.5915493 , 0.5915493 ,\n",
       "        0.5915493 , 0.5915493 , 0.5915493 , 0.5915493 , 0.5915493 ]),\n",
       " 'split7_test_precision': array([0.        , 0.62068966, 0.62055336, 0.61522419, 0.61825319,\n",
       "        0.61244019, 0.61333333, 0.61296473, 0.61370124, 0.61333333,\n",
       "        0.61296473, 0.61333333, 0.61370124, 0.61333333, 0.61333333,\n",
       "        0.61370124, 0.61333333, 0.61333333, 0.62068966, 0.61522419,\n",
       "        0.61244019, 0.61296473, 0.61296473, 0.61333333, 0.61333333,\n",
       "        0.61333333, 0.61333333, 0.61333333, 0.61406844, 0.61296473]),\n",
       " 'split8_test_precision': array([0.        , 0.64236111, 0.61038961, 0.60973282, 0.60743427,\n",
       "        0.60283688, 0.60475352, 0.60297984, 0.60297984, 0.60297984,\n",
       "        0.60297984, 0.60297984, 0.60297984, 0.60297984, 0.60297984,\n",
       "        0.60297984, 0.60297984, 0.60297984, 0.64236111, 0.60973282,\n",
       "        0.60283688, 0.60297984, 0.60297984, 0.60297984, 0.60297984,\n",
       "        0.60297984, 0.60297984, 0.60297984, 0.60297984, 0.60297984]),\n",
       " 'split9_test_precision': array([0.        , 0.65668663, 0.62125341, 0.60988433, 0.60968379,\n",
       "        0.61100386, 0.61191162, 0.61132438, 0.61111111, 0.61111111,\n",
       "        0.61111111, 0.61111111, 0.61111111, 0.61111111, 0.61111111,\n",
       "        0.61111111, 0.61111111, 0.61111111, 0.65668663, 0.60988433,\n",
       "        0.61100386, 0.61132438, 0.61111111, 0.61111111, 0.61111111,\n",
       "        0.61111111, 0.61111111, 0.61111111, 0.61111111, 0.61111111]),\n",
       " 'mean_test_precision': array([0.        , 0.64526839, 0.62162455, 0.60889997, 0.60594649,\n",
       "        0.60343804, 0.60293666, 0.60229276, 0.6022365 , 0.60218351,\n",
       "        0.60218517, 0.60229886, 0.60233565, 0.60229886, 0.60233731,\n",
       "        0.60233565, 0.60226055, 0.60226055, 0.64526839, 0.60889997,\n",
       "        0.60349425, 0.60229304, 0.60224037, 0.602299  , 0.602299  ,\n",
       "        0.602299  , 0.602299  , 0.602299  , 0.60229554, 0.60224037]),\n",
       " 'std_test_precision': array([0.        , 0.01435165, 0.00717231, 0.00877932, 0.00919834,\n",
       "        0.01027774, 0.01063717, 0.01041699, 0.01066278, 0.01055273,\n",
       "        0.01046505, 0.01043992, 0.01047932, 0.01043992, 0.01039129,\n",
       "        0.01047932, 0.01047853, 0.01047853, 0.01435165, 0.00877932,\n",
       "        0.01021868, 0.01042967, 0.01039633, 0.01043022, 0.01043022,\n",
       "        0.01043022, 0.01043022, 0.01043022, 0.01058331, 0.01039633]),\n",
       " 'rank_test_precision': array([30,  1,  3,  4,  6,  8,  9, 22, 27, 29, 28, 18, 11, 18, 10, 11, 23,\n",
       "        23,  1,  4,  7, 21, 25, 13, 13, 13, 13, 13, 20, 25]),\n",
       " 'split0_test_f1_micro': array([0.86070461, 0.86829268, 0.8704607 , 0.8719389 , 0.8713969 ,\n",
       "        0.87129835, 0.87144617, 0.87144617, 0.87144617, 0.87144617,\n",
       "        0.87144617, 0.87144617, 0.87144617, 0.87144617, 0.87144617,\n",
       "        0.87144617, 0.87144617, 0.87144617, 0.86829268, 0.8719389 ,\n",
       "        0.87129835, 0.87144617, 0.87144617, 0.87144617, 0.87144617,\n",
       "        0.87144617, 0.87144617, 0.87144617, 0.87144617, 0.87144617]),\n",
       " 'split1_test_f1_micro': array([0.86070461, 0.86922887, 0.86957379, 0.86952451, 0.87001725,\n",
       "        0.86986943, 0.86986943, 0.86986943, 0.8699187 , 0.8699187 ,\n",
       "        0.8699187 , 0.8699187 , 0.8699187 , 0.8699187 , 0.8699187 ,\n",
       "        0.8699187 , 0.8699187 , 0.8699187 , 0.86922887, 0.86952451,\n",
       "        0.86986943, 0.86986943, 0.8699187 , 0.8699187 , 0.8699187 ,\n",
       "        0.8699187 , 0.8699187 , 0.8699187 , 0.8699187 , 0.8699187 ]),\n",
       " 'split2_test_f1_micro': array([0.86070461, 0.86848978, 0.87050998, 0.87080562, 0.87060852,\n",
       "        0.87050998, 0.87050998, 0.87055925, 0.87055925, 0.87055925,\n",
       "        0.87055925, 0.87060852, 0.87060852, 0.87060852, 0.87060852,\n",
       "        0.87060852, 0.87060852, 0.87060852, 0.86848978, 0.87080562,\n",
       "        0.87050998, 0.87055925, 0.87055925, 0.87060852, 0.87060852,\n",
       "        0.87060852, 0.87060852, 0.87060852, 0.87055925, 0.87055925]),\n",
       " 'split3_test_f1_micro': array([0.86070461, 0.86996797, 0.87169254, 0.87213599, 0.872678  ,\n",
       "        0.87282582, 0.87262873, 0.87257945, 0.87262873, 0.87257945,\n",
       "        0.87257945, 0.87257945, 0.87257945, 0.87257945, 0.87257945,\n",
       "        0.87257945, 0.87257945, 0.87257945, 0.86996797, 0.87213599,\n",
       "        0.87282582, 0.87257945, 0.87257945, 0.87257945, 0.87257945,\n",
       "        0.87257945, 0.87257945, 0.87257945, 0.87257945, 0.87257945]),\n",
       " 'split4_test_f1_micro': array([0.86074702, 0.86720213, 0.86946881, 0.87020794, 0.8705036 ,\n",
       "        0.87055287, 0.87030649, 0.87015867, 0.87006012, 0.87006012,\n",
       "        0.87010939, 0.87010939, 0.87010939, 0.87010939, 0.87015867,\n",
       "        0.87010939, 0.87010939, 0.87010939, 0.86720213, 0.87020794,\n",
       "        0.87055287, 0.87010939, 0.87015867, 0.87015867, 0.87015867,\n",
       "        0.87015867, 0.87015867, 0.87015867, 0.87010939, 0.87015867]),\n",
       " 'split5_test_f1_micro': array([0.86074702, 0.86892678, 0.87114418, 0.87336158, 0.87355869,\n",
       "        0.87400217, 0.87395289, 0.87385434, 0.87385434, 0.87385434,\n",
       "        0.87385434, 0.87385434, 0.87385434, 0.87385434, 0.87385434,\n",
       "        0.87385434, 0.87385434, 0.87385434, 0.86892678, 0.87336158,\n",
       "        0.87400217, 0.87385434, 0.87385434, 0.87385434, 0.87385434,\n",
       "        0.87385434, 0.87385434, 0.87385434, 0.87385434, 0.87385434]),\n",
       " 'split6_test_f1_micro': array([0.86074702, 0.86897605, 0.86971519, 0.87040505, 0.87074998,\n",
       "        0.87035577, 0.87030649, 0.87035577, 0.87030649, 0.87035577,\n",
       "        0.87035577, 0.87040505, 0.87040505, 0.87040505, 0.87040505,\n",
       "        0.87040505, 0.87035577, 0.87035577, 0.86897605, 0.87040505,\n",
       "        0.87040505, 0.87040505, 0.87035577, 0.87035577, 0.87035577,\n",
       "        0.87035577, 0.87035577, 0.87035577, 0.87035577, 0.87035577]),\n",
       " 'split7_test_f1_micro': array([0.86074702, 0.86730068, 0.86976446, 0.87163694, 0.87262245,\n",
       "        0.8723268 , 0.87247462, 0.87242535, 0.8725239 , 0.87247462,\n",
       "        0.87242535, 0.87247462, 0.8725239 , 0.87247462, 0.87247462,\n",
       "        0.8725239 , 0.87247462, 0.87247462, 0.86730068, 0.87163694,\n",
       "        0.8723268 , 0.87242535, 0.87242535, 0.87247462, 0.87247462,\n",
       "        0.87247462, 0.87247462, 0.87247462, 0.87257317, 0.87242535]),\n",
       " 'split8_test_f1_micro': array([0.86074702, 0.86882823, 0.86996156, 0.87208042, 0.87242535,\n",
       "        0.87217897, 0.87247462, 0.8723268 , 0.8723268 , 0.8723268 ,\n",
       "        0.8723268 , 0.8723268 , 0.8723268 , 0.8723268 , 0.8723268 ,\n",
       "        0.8723268 , 0.8723268 , 0.8723268 , 0.86882823, 0.87208042,\n",
       "        0.87217897, 0.8723268 , 0.8723268 , 0.8723268 , 0.8723268 ,\n",
       "        0.8723268 , 0.8723268 , 0.8723268 , 0.8723268 , 0.8723268 ]),\n",
       " 'split9_test_f1_micro': array([0.86074702, 0.8684833 , 0.86951808, 0.87104563, 0.87168621,\n",
       "        0.87208042, 0.87222824, 0.87217897, 0.87217897, 0.87217897,\n",
       "        0.87217897, 0.87217897, 0.87217897, 0.87217897, 0.87217897,\n",
       "        0.87217897, 0.87217897, 0.87217897, 0.8684833 , 0.87104563,\n",
       "        0.87208042, 0.87217897, 0.87217897, 0.87217897, 0.87217897,\n",
       "        0.87217897, 0.87217897, 0.87217897, 0.87217897, 0.87217897]),\n",
       " 'mean_test_f1_micro': array([0.86073005, 0.86856965, 0.87018093, 0.87131426, 0.87162469,\n",
       "        0.87160006, 0.87161977, 0.87157542, 0.87158035, 0.87157542,\n",
       "        0.87157542, 0.8715902 , 0.87159513, 0.8715902 , 0.87159513,\n",
       "        0.87159513, 0.87158527, 0.87158527, 0.86856965, 0.87131426,\n",
       "        0.87160498, 0.87157542, 0.87158035, 0.8715902 , 0.8715902 ,\n",
       "        0.8715902 , 0.8715902 , 0.8715902 , 0.8715902 , 0.87158035]),\n",
       " 'std_test_f1_micro': array([2.07774427e-05, 7.95008931e-04, 7.17839510e-04, 1.07390690e-03,\n",
       "        1.10365321e-03, 1.23636138e-03, 1.26760039e-03, 1.23722400e-03,\n",
       "        1.25823245e-03, 1.24565998e-03, 1.23626823e-03, 1.23093924e-03,\n",
       "        1.23456309e-03, 1.23093924e-03, 1.22508627e-03, 1.23456309e-03,\n",
       "        1.23576284e-03, 1.23576284e-03, 7.95008931e-04, 1.07390690e-03,\n",
       "        1.23148098e-03, 1.23820488e-03, 1.23049981e-03, 1.22995257e-03,\n",
       "        1.22995257e-03, 1.22995257e-03, 1.22995257e-03, 1.22995257e-03,\n",
       "        1.24717856e-03, 1.23049981e-03]),\n",
       " 'rank_test_f1_micro': array([30, 28, 27, 25,  1,  4,  2, 21, 20, 23, 23,  9,  5,  9,  5,  5, 16,\n",
       "        16, 28, 25,  3, 21, 18,  9,  9,  9,  9,  9,  8, 18])}"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_model.cv_results_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "9aa08645",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([30, 28, 27, 25,  1,  4,  2, 21, 20, 23, 23,  9,  5,  9,  5,  5, 16,\n",
       "       16, 28, 25,  3, 22, 18,  9,  9,  9,  9,  9,  8, 18])"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_model.cv_results_['rank_test_accuracy']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "abdb8b79",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'classifier': LogisticRegression(max_iter=10000),\n",
       " 'classifier__C': 0.0001,\n",
       " 'classifier__penalty': 'l1',\n",
       " 'classifier__solver': 'saga'}"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_model.cv_results_['params'][ np.argmin(best_model.cv_results_['mean_test_accuracy'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "f4602352",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>classifier</th>\n",
       "      <th>C</th>\n",
       "      <th>penalty</th>\n",
       "      <th>solver</th>\n",
       "      <th>score_acc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.860730</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.868570</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.870181</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.871314</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0100</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.871625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0100</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.871600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.1000</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.871620</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.1000</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.871575</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.871580</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.871575</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>10.0000</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.871575</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>10.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.871590</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>100.0000</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.871595</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>100.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.871590</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>1000.0000</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.871595</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>1000.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.871595</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>10000.0000</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.871585</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>10000.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.871585</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.868570</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.871314</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0100</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.871605</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.1000</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.871575</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.871580</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>10.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.871590</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>100.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.871590</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>1000.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.871590</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>10000.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.871590</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>NaN</td>\n",
       "      <td>none</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.871590</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>NaN</td>\n",
       "      <td>none</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.871590</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.871580</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                            classifier           C penalty solver  score_acc\n",
       "0   LogisticRegression(max_iter=10000)      0.0001      l1   saga   0.860730\n",
       "1   LogisticRegression(max_iter=10000)      0.0001      l2   saga   0.868570\n",
       "2   LogisticRegression(max_iter=10000)      0.0010      l1   saga   0.870181\n",
       "3   LogisticRegression(max_iter=10000)      0.0010      l2   saga   0.871314\n",
       "4   LogisticRegression(max_iter=10000)      0.0100      l1   saga   0.871625\n",
       "5   LogisticRegression(max_iter=10000)      0.0100      l2   saga   0.871600\n",
       "6   LogisticRegression(max_iter=10000)      0.1000      l1   saga   0.871620\n",
       "7   LogisticRegression(max_iter=10000)      0.1000      l2   saga   0.871575\n",
       "8   LogisticRegression(max_iter=10000)      1.0000      l1   saga   0.871580\n",
       "9   LogisticRegression(max_iter=10000)      1.0000      l2   saga   0.871575\n",
       "10  LogisticRegression(max_iter=10000)     10.0000      l1   saga   0.871575\n",
       "11  LogisticRegression(max_iter=10000)     10.0000      l2   saga   0.871590\n",
       "12  LogisticRegression(max_iter=10000)    100.0000      l1   saga   0.871595\n",
       "13  LogisticRegression(max_iter=10000)    100.0000      l2   saga   0.871590\n",
       "14  LogisticRegression(max_iter=10000)   1000.0000      l1   saga   0.871595\n",
       "15  LogisticRegression(max_iter=10000)   1000.0000      l2   saga   0.871595\n",
       "16  LogisticRegression(max_iter=10000)  10000.0000      l1   saga   0.871585\n",
       "17  LogisticRegression(max_iter=10000)  10000.0000      l2   saga   0.871585\n",
       "18  LogisticRegression(max_iter=10000)      0.0001      l2  lbfgs   0.868570\n",
       "19  LogisticRegression(max_iter=10000)      0.0010      l2  lbfgs   0.871314\n",
       "20  LogisticRegression(max_iter=10000)      0.0100      l2  lbfgs   0.871605\n",
       "21  LogisticRegression(max_iter=10000)      0.1000      l2  lbfgs   0.871575\n",
       "22  LogisticRegression(max_iter=10000)      1.0000      l2  lbfgs   0.871580\n",
       "23  LogisticRegression(max_iter=10000)     10.0000      l2  lbfgs   0.871590\n",
       "24  LogisticRegression(max_iter=10000)    100.0000      l2  lbfgs   0.871590\n",
       "25  LogisticRegression(max_iter=10000)   1000.0000      l2  lbfgs   0.871590\n",
       "26  LogisticRegression(max_iter=10000)  10000.0000      l2  lbfgs   0.871590\n",
       "27  LogisticRegression(max_iter=10000)         NaN    none  lbfgs   0.871590\n",
       "28  LogisticRegression(max_iter=10000)         NaN    none   saga   0.871590\n",
       "29  LogisticRegression(max_iter=10000)         NaN     NaN    NaN   0.871580"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results = pd.DataFrame( best_model.cv_results_['params'] )\n",
    "\n",
    "results['score_acc'] = best_model.cv_results_['mean_test_accuracy']\n",
    "cols = results.columns.to_series().str.split('__').apply(lambda x: x[-1])\n",
    "results.columns = cols\n",
    "\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "dce721ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "acc_list.append(results[0:1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "ab7ddcc5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'classifier': LogisticRegression(max_iter=10000),\n",
       " 'classifier__C': 0.0001,\n",
       " 'classifier__penalty': 'l2',\n",
       " 'classifier__solver': 'saga'}"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_model.cv_results_['params'][ np.argmin(best_model.cv_results_['rank_test_precision'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "c6c37019",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>classifier</th>\n",
       "      <th>C</th>\n",
       "      <th>penalty</th>\n",
       "      <th>solver</th>\n",
       "      <th>score_precision</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.645268</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.621625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.608900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0100</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.605946</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0100</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.603438</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.1000</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.602937</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.1000</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.602293</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.602236</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.602184</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>10.0000</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.602185</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>10.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.602299</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>100.0000</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.602336</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>100.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.602299</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>1000.0000</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.602337</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>1000.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.602336</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>10000.0000</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.602261</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>10000.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.602261</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.645268</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.608900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0100</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.603494</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.1000</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.602293</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.602240</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>10.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.602299</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>100.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.602299</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>1000.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.602299</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>10000.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.602299</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>NaN</td>\n",
       "      <td>none</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.602299</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>NaN</td>\n",
       "      <td>none</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.602296</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.602240</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                            classifier           C penalty solver  \\\n",
       "0   LogisticRegression(max_iter=10000)      0.0001      l1   saga   \n",
       "1   LogisticRegression(max_iter=10000)      0.0001      l2   saga   \n",
       "2   LogisticRegression(max_iter=10000)      0.0010      l1   saga   \n",
       "3   LogisticRegression(max_iter=10000)      0.0010      l2   saga   \n",
       "4   LogisticRegression(max_iter=10000)      0.0100      l1   saga   \n",
       "5   LogisticRegression(max_iter=10000)      0.0100      l2   saga   \n",
       "6   LogisticRegression(max_iter=10000)      0.1000      l1   saga   \n",
       "7   LogisticRegression(max_iter=10000)      0.1000      l2   saga   \n",
       "8   LogisticRegression(max_iter=10000)      1.0000      l1   saga   \n",
       "9   LogisticRegression(max_iter=10000)      1.0000      l2   saga   \n",
       "10  LogisticRegression(max_iter=10000)     10.0000      l1   saga   \n",
       "11  LogisticRegression(max_iter=10000)     10.0000      l2   saga   \n",
       "12  LogisticRegression(max_iter=10000)    100.0000      l1   saga   \n",
       "13  LogisticRegression(max_iter=10000)    100.0000      l2   saga   \n",
       "14  LogisticRegression(max_iter=10000)   1000.0000      l1   saga   \n",
       "15  LogisticRegression(max_iter=10000)   1000.0000      l2   saga   \n",
       "16  LogisticRegression(max_iter=10000)  10000.0000      l1   saga   \n",
       "17  LogisticRegression(max_iter=10000)  10000.0000      l2   saga   \n",
       "18  LogisticRegression(max_iter=10000)      0.0001      l2  lbfgs   \n",
       "19  LogisticRegression(max_iter=10000)      0.0010      l2  lbfgs   \n",
       "20  LogisticRegression(max_iter=10000)      0.0100      l2  lbfgs   \n",
       "21  LogisticRegression(max_iter=10000)      0.1000      l2  lbfgs   \n",
       "22  LogisticRegression(max_iter=10000)      1.0000      l2  lbfgs   \n",
       "23  LogisticRegression(max_iter=10000)     10.0000      l2  lbfgs   \n",
       "24  LogisticRegression(max_iter=10000)    100.0000      l2  lbfgs   \n",
       "25  LogisticRegression(max_iter=10000)   1000.0000      l2  lbfgs   \n",
       "26  LogisticRegression(max_iter=10000)  10000.0000      l2  lbfgs   \n",
       "27  LogisticRegression(max_iter=10000)         NaN    none  lbfgs   \n",
       "28  LogisticRegression(max_iter=10000)         NaN    none   saga   \n",
       "29  LogisticRegression(max_iter=10000)         NaN     NaN    NaN   \n",
       "\n",
       "    score_precision  \n",
       "0          0.000000  \n",
       "1          0.645268  \n",
       "2          0.621625  \n",
       "3          0.608900  \n",
       "4          0.605946  \n",
       "5          0.603438  \n",
       "6          0.602937  \n",
       "7          0.602293  \n",
       "8          0.602236  \n",
       "9          0.602184  \n",
       "10         0.602185  \n",
       "11         0.602299  \n",
       "12         0.602336  \n",
       "13         0.602299  \n",
       "14         0.602337  \n",
       "15         0.602336  \n",
       "16         0.602261  \n",
       "17         0.602261  \n",
       "18         0.645268  \n",
       "19         0.608900  \n",
       "20         0.603494  \n",
       "21         0.602293  \n",
       "22         0.602240  \n",
       "23         0.602299  \n",
       "24         0.602299  \n",
       "25         0.602299  \n",
       "26         0.602299  \n",
       "27         0.602299  \n",
       "28         0.602296  \n",
       "29         0.602240  "
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results = pd.DataFrame( best_model.cv_results_['params'] )\n",
    "\n",
    "results['score_precision'] = best_model.cv_results_['mean_test_precision']\n",
    "cols = results.columns.to_series().str.split('__').apply(lambda x: x[-1])\n",
    "results.columns = cols\n",
    "\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "30976105",
   "metadata": {},
   "outputs": [],
   "source": [
    "roc_auc_ovr_List.append(results[1:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "ca7bc287",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'classifier': LogisticRegression(max_iter=10000),\n",
       " 'classifier__C': 0.01,\n",
       " 'classifier__penalty': 'l1',\n",
       " 'classifier__solver': 'saga'}"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_model.cv_results_['params'][ np.argmin(best_model.cv_results_['rank_test_f1_micro'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "affe1adf",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>classifier</th>\n",
       "      <th>C</th>\n",
       "      <th>penalty</th>\n",
       "      <th>solver</th>\n",
       "      <th>score_f1_micro</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.860730</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.868570</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.870181</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.871314</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0100</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.871625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0100</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.871600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.1000</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.871620</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.1000</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.871575</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.871580</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.871575</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>10.0000</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.871575</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>10.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.871590</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>100.0000</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.871595</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>100.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.871590</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>1000.0000</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.871595</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>1000.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.871595</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>10000.0000</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.871585</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>10000.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.871585</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.868570</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.871314</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0100</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.871605</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.1000</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.871575</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.871580</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>10.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.871590</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>100.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.871590</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>1000.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.871590</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>10000.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.871590</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>NaN</td>\n",
       "      <td>none</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.871590</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>NaN</td>\n",
       "      <td>none</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.871590</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.871580</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                            classifier           C penalty solver  \\\n",
       "0   LogisticRegression(max_iter=10000)      0.0001      l1   saga   \n",
       "1   LogisticRegression(max_iter=10000)      0.0001      l2   saga   \n",
       "2   LogisticRegression(max_iter=10000)      0.0010      l1   saga   \n",
       "3   LogisticRegression(max_iter=10000)      0.0010      l2   saga   \n",
       "4   LogisticRegression(max_iter=10000)      0.0100      l1   saga   \n",
       "5   LogisticRegression(max_iter=10000)      0.0100      l2   saga   \n",
       "6   LogisticRegression(max_iter=10000)      0.1000      l1   saga   \n",
       "7   LogisticRegression(max_iter=10000)      0.1000      l2   saga   \n",
       "8   LogisticRegression(max_iter=10000)      1.0000      l1   saga   \n",
       "9   LogisticRegression(max_iter=10000)      1.0000      l2   saga   \n",
       "10  LogisticRegression(max_iter=10000)     10.0000      l1   saga   \n",
       "11  LogisticRegression(max_iter=10000)     10.0000      l2   saga   \n",
       "12  LogisticRegression(max_iter=10000)    100.0000      l1   saga   \n",
       "13  LogisticRegression(max_iter=10000)    100.0000      l2   saga   \n",
       "14  LogisticRegression(max_iter=10000)   1000.0000      l1   saga   \n",
       "15  LogisticRegression(max_iter=10000)   1000.0000      l2   saga   \n",
       "16  LogisticRegression(max_iter=10000)  10000.0000      l1   saga   \n",
       "17  LogisticRegression(max_iter=10000)  10000.0000      l2   saga   \n",
       "18  LogisticRegression(max_iter=10000)      0.0001      l2  lbfgs   \n",
       "19  LogisticRegression(max_iter=10000)      0.0010      l2  lbfgs   \n",
       "20  LogisticRegression(max_iter=10000)      0.0100      l2  lbfgs   \n",
       "21  LogisticRegression(max_iter=10000)      0.1000      l2  lbfgs   \n",
       "22  LogisticRegression(max_iter=10000)      1.0000      l2  lbfgs   \n",
       "23  LogisticRegression(max_iter=10000)     10.0000      l2  lbfgs   \n",
       "24  LogisticRegression(max_iter=10000)    100.0000      l2  lbfgs   \n",
       "25  LogisticRegression(max_iter=10000)   1000.0000      l2  lbfgs   \n",
       "26  LogisticRegression(max_iter=10000)  10000.0000      l2  lbfgs   \n",
       "27  LogisticRegression(max_iter=10000)         NaN    none  lbfgs   \n",
       "28  LogisticRegression(max_iter=10000)         NaN    none   saga   \n",
       "29  LogisticRegression(max_iter=10000)         NaN     NaN    NaN   \n",
       "\n",
       "    score_f1_micro  \n",
       "0         0.860730  \n",
       "1         0.868570  \n",
       "2         0.870181  \n",
       "3         0.871314  \n",
       "4         0.871625  \n",
       "5         0.871600  \n",
       "6         0.871620  \n",
       "7         0.871575  \n",
       "8         0.871580  \n",
       "9         0.871575  \n",
       "10        0.871575  \n",
       "11        0.871590  \n",
       "12        0.871595  \n",
       "13        0.871590  \n",
       "14        0.871595  \n",
       "15        0.871595  \n",
       "16        0.871585  \n",
       "17        0.871585  \n",
       "18        0.868570  \n",
       "19        0.871314  \n",
       "20        0.871605  \n",
       "21        0.871575  \n",
       "22        0.871580  \n",
       "23        0.871590  \n",
       "24        0.871590  \n",
       "25        0.871590  \n",
       "26        0.871590  \n",
       "27        0.871590  \n",
       "28        0.871590  \n",
       "29        0.871580  "
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results = pd.DataFrame( best_model.cv_results_['params'] )\n",
    "\n",
    "results['score_f1_micro'] = best_model.cv_results_['mean_test_f1_micro']\n",
    "cols = results.columns.to_series().str.split('__').apply(lambda x: x[-1])\n",
    "results.columns = cols\n",
    "\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "7439a073",
   "metadata": {},
   "outputs": [],
   "source": [
    "f1_micro_List.append(results[4:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efd1babe",
   "metadata": {},
   "source": [
    "## Trial 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "1a0f0acc",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, Y_train, Y_test = train_test_split(X, y, test_size=0.2, shuffle=True, random_state = 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "7eb408a1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Guest-All\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\Guest-All\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\Guest-All\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\Guest-All\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\Guest-All\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\Guest-All\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\Guest-All\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\Guest-All\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\Guest-All\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\Guest-All\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\Guest-All\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1183: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Guest-All\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1183: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Guest-All\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1183: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Guest-All\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1183: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Guest-All\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1183: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Guest-All\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1183: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Guest-All\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1183: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Guest-All\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1183: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Guest-All\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1183: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Guest-All\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1183: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Guest-All\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1183: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Guest-All\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1183: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Guest-All\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1183: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Guest-All\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1183: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Guest-All\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1183: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Guest-All\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1183: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Guest-All\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1183: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Guest-All\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1183: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Guest-All\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1183: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Guest-All\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1183: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Create a pipeline - RF is a stand in, we will populate the classifier part below\n",
    "pipe = Pipeline([('std', StandardScaler()),\n",
    "                 ('classifier', LogisticRegression())])\n",
    "\n",
    "# Create search space of candidate learning algorithms and their hyperparameters\n",
    "# note lbfgs can't do l1, and if you pass penalty='none' it expects no C value\n",
    "search_space = [{'classifier': [LogisticRegression(max_iter=10000)],\n",
    "                 'classifier__solver': ['saga'],\n",
    "                 'classifier__penalty': ['l1', 'l2'],\n",
    "                 'classifier__C': np.logspace(-4, 4, 9)},\n",
    "                {'classifier': [LogisticRegression(max_iter=10000)],\n",
    "                 'classifier__solver': ['lbfgs'],\n",
    "                 'classifier__penalty': ['l2'],\n",
    "                 'classifier__C': np.logspace(-4, 4, 9)},\n",
    "                {'classifier': [LogisticRegression(max_iter=10000)],\n",
    "                 'classifier__solver': ['lbfgs','saga'],\n",
    "                 'classifier__penalty': ['none']},\n",
    "                {'classifier': [LogisticRegression(max_iter=10000)]}\n",
    "                ]\n",
    "\n",
    "# Create grid search\n",
    "clf = GridSearchCV(pipe, search_space, cv=StratifiedKFold(n_splits=10),\n",
    "                   scoring=['accuracy', 'precision', 'f1_micro'], refit=False,\n",
    "                   verbose=0)\n",
    "\n",
    "# Fit grid search\n",
    "best_model = clf.fit(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "bfa6938c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'mean_fit_time': array([ 4.03425508,  3.22669585,  3.69786623,  3.11010406,  3.89753263,\n",
       "         3.53725085,  4.79045019,  3.1439003 , 31.78257127,  3.11349502,\n",
       "         4.15666118,  3.24421229,  4.109043  ,  3.40787208,  4.20525122,\n",
       "         3.26945317,  3.80524766,  3.2251725 ,  0.55379417,  0.70221095,\n",
       "         0.86766813,  1.07786105,  1.03371642,  1.03363779,  0.99528058,\n",
       "         1.00050569,  0.78458393,  0.78568416,  3.24737141,  0.7827508 ]),\n",
       " 'std_fit_time': array([0.70988557, 0.23223846, 0.24466512, 0.14194532, 0.46966466,\n",
       "        0.53947189, 0.93041966, 0.14054258, 3.89469035, 0.12701765,\n",
       "        0.4312792 , 0.3912407 , 0.26346308, 0.65355908, 0.59364877,\n",
       "        0.31637882, 0.50746043, 0.30350596, 0.01406839, 0.01865435,\n",
       "        0.2275691 , 0.2562744 , 0.24497255, 0.23290228, 0.22911254,\n",
       "        0.24870473, 0.0175439 , 0.01467573, 0.16127375, 0.02012865]),\n",
       " 'mean_score_time': array([0.03907421, 0.03267682, 0.03337252, 0.03273404, 0.03198926,\n",
       "        0.0386827 , 0.03509638, 0.03140159, 0.03494599, 0.03077161,\n",
       "        0.03345678, 0.03158476, 0.03266559, 0.03477082, 0.03518031,\n",
       "        0.03178034, 0.03029313, 0.03232701, 0.03265033, 0.03116484,\n",
       "        0.03275123, 0.0394104 , 0.03721206, 0.04042504, 0.03669388,\n",
       "        0.03514743, 0.03217084, 0.0303781 , 0.03226378, 0.03292797]),\n",
       " 'std_score_time': array([0.01072066, 0.00167803, 0.00181382, 0.00182311, 0.00506403,\n",
       "        0.0110613 , 0.00522067, 0.00453935, 0.00187044, 0.00333218,\n",
       "        0.00414212, 0.00461768, 0.00234791, 0.00986615, 0.00974367,\n",
       "        0.00202772, 0.00468666, 0.00252941, 0.00268929, 0.00299316,\n",
       "        0.01164327, 0.01173457, 0.01144627, 0.01153074, 0.01262277,\n",
       "        0.00687395, 0.00211038, 0.00254545, 0.00524378, 0.00423278]),\n",
       " 'param_classifier': masked_array(data=[LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000)],\n",
       "              mask=[False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'param_classifier__C': masked_array(data=[0.0001, 0.0001, 0.001, 0.001, 0.01, 0.01, 0.1, 0.1,\n",
       "                    1.0, 1.0, 10.0, 10.0, 100.0, 100.0, 1000.0, 1000.0,\n",
       "                    10000.0, 10000.0, 0.0001, 0.001, 0.01, 0.1, 1.0, 10.0,\n",
       "                    100.0, 1000.0, 10000.0, --, --, --],\n",
       "              mask=[False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False,  True,  True,  True],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'param_classifier__penalty': masked_array(data=['l1', 'l2', 'l1', 'l2', 'l1', 'l2', 'l1', 'l2', 'l1',\n",
       "                    'l2', 'l1', 'l2', 'l1', 'l2', 'l1', 'l2', 'l1', 'l2',\n",
       "                    'l2', 'l2', 'l2', 'l2', 'l2', 'l2', 'l2', 'l2', 'l2',\n",
       "                    'none', 'none', --],\n",
       "              mask=[False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False,  True],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'param_classifier__solver': masked_array(data=['saga', 'saga', 'saga', 'saga', 'saga', 'saga', 'saga',\n",
       "                    'saga', 'saga', 'saga', 'saga', 'saga', 'saga', 'saga',\n",
       "                    'saga', 'saga', 'saga', 'saga', 'lbfgs', 'lbfgs',\n",
       "                    'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs',\n",
       "                    'lbfgs', 'lbfgs', 'saga', --],\n",
       "              mask=[False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False,  True],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'params': [{'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 0.0001,\n",
       "   'classifier__penalty': 'l1',\n",
       "   'classifier__solver': 'saga'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 0.0001,\n",
       "   'classifier__penalty': 'l2',\n",
       "   'classifier__solver': 'saga'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 0.001,\n",
       "   'classifier__penalty': 'l1',\n",
       "   'classifier__solver': 'saga'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 0.001,\n",
       "   'classifier__penalty': 'l2',\n",
       "   'classifier__solver': 'saga'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 0.01,\n",
       "   'classifier__penalty': 'l1',\n",
       "   'classifier__solver': 'saga'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 0.01,\n",
       "   'classifier__penalty': 'l2',\n",
       "   'classifier__solver': 'saga'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 0.1,\n",
       "   'classifier__penalty': 'l1',\n",
       "   'classifier__solver': 'saga'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 0.1,\n",
       "   'classifier__penalty': 'l2',\n",
       "   'classifier__solver': 'saga'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 1.0,\n",
       "   'classifier__penalty': 'l1',\n",
       "   'classifier__solver': 'saga'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 1.0,\n",
       "   'classifier__penalty': 'l2',\n",
       "   'classifier__solver': 'saga'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 10.0,\n",
       "   'classifier__penalty': 'l1',\n",
       "   'classifier__solver': 'saga'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 10.0,\n",
       "   'classifier__penalty': 'l2',\n",
       "   'classifier__solver': 'saga'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 100.0,\n",
       "   'classifier__penalty': 'l1',\n",
       "   'classifier__solver': 'saga'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 100.0,\n",
       "   'classifier__penalty': 'l2',\n",
       "   'classifier__solver': 'saga'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 1000.0,\n",
       "   'classifier__penalty': 'l1',\n",
       "   'classifier__solver': 'saga'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 1000.0,\n",
       "   'classifier__penalty': 'l2',\n",
       "   'classifier__solver': 'saga'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 10000.0,\n",
       "   'classifier__penalty': 'l1',\n",
       "   'classifier__solver': 'saga'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 10000.0,\n",
       "   'classifier__penalty': 'l2',\n",
       "   'classifier__solver': 'saga'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 0.0001,\n",
       "   'classifier__penalty': 'l2',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 0.001,\n",
       "   'classifier__penalty': 'l2',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 0.01,\n",
       "   'classifier__penalty': 'l2',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 0.1,\n",
       "   'classifier__penalty': 'l2',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 1.0,\n",
       "   'classifier__penalty': 'l2',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 10.0,\n",
       "   'classifier__penalty': 'l2',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 100.0,\n",
       "   'classifier__penalty': 'l2',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 1000.0,\n",
       "   'classifier__penalty': 'l2',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 10000.0,\n",
       "   'classifier__penalty': 'l2',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__penalty': 'none',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__penalty': 'none',\n",
       "   'classifier__solver': 'saga'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000)}],\n",
       " 'split0_test_accuracy': array([0.86035969, 0.86760286, 0.86967233, 0.86986943, 0.87080562,\n",
       "        0.87021434, 0.87026361, 0.87036216, 0.87036216, 0.8704607 ,\n",
       "        0.8704607 , 0.8704607 , 0.8704607 , 0.8704607 , 0.8704607 ,\n",
       "        0.8704607 , 0.8704607 , 0.8704607 , 0.86760286, 0.86982015,\n",
       "        0.87021434, 0.87036216, 0.87041143, 0.8704607 , 0.8704607 ,\n",
       "        0.8704607 , 0.8704607 , 0.8704607 , 0.8704607 , 0.87041143]),\n",
       " 'split1_test_accuracy': array([0.86035969, 0.86829268, 0.87011579, 0.87115053, 0.87149544,\n",
       "        0.87159399, 0.87164326, 0.87188963, 0.87188963, 0.87184035,\n",
       "        0.87184035, 0.87184035, 0.87184035, 0.87184035, 0.87184035,\n",
       "        0.87184035, 0.87184035, 0.87184035, 0.86829268, 0.87115053,\n",
       "        0.87159399, 0.87188963, 0.87184035, 0.87184035, 0.87184035,\n",
       "        0.87184035, 0.87184035, 0.87184035, 0.87184035, 0.87184035]),\n",
       " 'split2_test_accuracy': array([0.86035969, 0.86691303, 0.86794777, 0.86878542, 0.86927815,\n",
       "        0.86908105, 0.86878542, 0.86878542, 0.86878542, 0.86878542,\n",
       "        0.86878542, 0.86878542, 0.86878542, 0.86878542, 0.86878542,\n",
       "        0.86878542, 0.86878542, 0.86883469, 0.86691303, 0.86878542,\n",
       "        0.86908105, 0.86878542, 0.86878542, 0.86878542, 0.86878542,\n",
       "        0.86878542, 0.86878542, 0.86878542, 0.86878542, 0.86878542]),\n",
       " 'split3_test_accuracy': array([0.86035969, 0.8684405 , 0.86809559, 0.86942597, 0.86962306,\n",
       "        0.86967233, 0.86962306, 0.86957379, 0.86957379, 0.86957379,\n",
       "        0.86957379, 0.86957379, 0.86957379, 0.86957379, 0.86957379,\n",
       "        0.86957379, 0.86957379, 0.86957379, 0.86839123, 0.86942597,\n",
       "        0.86967233, 0.86957379, 0.86957379, 0.86957379, 0.86957379,\n",
       "        0.86957379, 0.86957379, 0.86957379, 0.86957379, 0.86957379]),\n",
       " 'split4_test_accuracy': array([0.86035281, 0.8686804 , 0.87129201, 0.8725239 , 0.87355869,\n",
       "        0.87301666, 0.87331231, 0.87331231, 0.87326303, 0.87326303,\n",
       "        0.87326303, 0.87326303, 0.87326303, 0.87321376, 0.87326303,\n",
       "        0.87326303, 0.87326303, 0.87326303, 0.8686804 , 0.8725239 ,\n",
       "        0.87301666, 0.87331231, 0.87326303, 0.87326303, 0.87326303,\n",
       "        0.87326303, 0.87326303, 0.87326303, 0.87326303, 0.87326303]),\n",
       " 'split5_test_accuracy': array([0.86035281, 0.86863112, 0.86882823, 0.86912388, 0.86956736,\n",
       "        0.86946881, 0.86937026, 0.86937026, 0.86937026, 0.86937026,\n",
       "        0.86937026, 0.86937026, 0.86937026, 0.86937026, 0.86937026,\n",
       "        0.86937026, 0.86937026, 0.86937026, 0.86863112, 0.86912388,\n",
       "        0.86946881, 0.86937026, 0.86937026, 0.86937026, 0.86937026,\n",
       "        0.86937026, 0.86937026, 0.86937026, 0.86937026, 0.86937026]),\n",
       " 'split6_test_accuracy': array([0.86035281, 0.86902533, 0.8710949 , 0.8725239 , 0.87370651,\n",
       "        0.87311521, 0.87316448, 0.87301666, 0.87306593, 0.87311521,\n",
       "        0.87311521, 0.87311521, 0.87311521, 0.87311521, 0.87311521,\n",
       "        0.87311521, 0.87311521, 0.87311521, 0.86902533, 0.8725239 ,\n",
       "        0.87311521, 0.87301666, 0.87306593, 0.87311521, 0.87311521,\n",
       "        0.87311521, 0.87311521, 0.87311521, 0.87311521, 0.87306593]),\n",
       " 'split7_test_accuracy': array([0.86035281, 0.86858185, 0.87094708, 0.87212969, 0.87198187,\n",
       "        0.87212969, 0.87183404, 0.87183404, 0.87178476, 0.87178476,\n",
       "        0.87178476, 0.87178476, 0.87183404, 0.87178476, 0.87183404,\n",
       "        0.87183404, 0.87183404, 0.87183404, 0.86858185, 0.87212969,\n",
       "        0.87212969, 0.87178476, 0.87178476, 0.87183404, 0.87183404,\n",
       "        0.87183404, 0.87183404, 0.87183404, 0.87183404, 0.87178476]),\n",
       " 'split8_test_accuracy': array([0.86040209, 0.86759633, 0.86912388, 0.86917315, 0.86941953,\n",
       "        0.86912388, 0.86927171, 0.86932098, 0.86922243, 0.86922243,\n",
       "        0.86922243, 0.86922243, 0.86922243, 0.86922243, 0.86922243,\n",
       "        0.86922243, 0.86922243, 0.86922243, 0.86759633, 0.86917315,\n",
       "        0.86912388, 0.86932098, 0.86922243, 0.86922243, 0.86922243,\n",
       "        0.86922243, 0.86922243, 0.86922243, 0.86922243, 0.86922243]),\n",
       " 'split9_test_accuracy': array([0.86040209, 0.86882823, 0.87193259, 0.87247462, 0.87212969,\n",
       "        0.87153839, 0.87168621, 0.87168621, 0.87158766, 0.87158766,\n",
       "        0.87158766, 0.87158766, 0.87158766, 0.87158766, 0.87158766,\n",
       "        0.87158766, 0.87158766, 0.87158766, 0.86882823, 0.87247462,\n",
       "        0.87153839, 0.87168621, 0.87158766, 0.87158766, 0.87158766,\n",
       "        0.87158766, 0.87158766, 0.87158766, 0.87158766, 0.87158766]),\n",
       " 'mean_test_accuracy': array([0.86036542, 0.86825923, 0.86990502, 0.87071805, 0.87115659,\n",
       "        0.87089543, 0.87089544, 0.87091514, 0.87089051, 0.87090036,\n",
       "        0.87090036, 0.87090036, 0.87090529, 0.87089543, 0.87090529,\n",
       "        0.87090529, 0.87090529, 0.87091022, 0.86825431, 0.87071312,\n",
       "        0.87089543, 0.87091022, 0.87089051, 0.87090529, 0.87090529,\n",
       "        0.87090529, 0.87090529, 0.87090529, 0.87090529, 0.87089051]),\n",
       " 'std_test_accuracy': array([1.85905344e-05, 6.36153349e-04, 1.32323801e-03, 1.51130095e-03,\n",
       "        1.59879584e-03, 1.49255615e-03, 1.56487681e-03, 1.55306444e-03,\n",
       "        1.55509707e-03, 1.55591332e-03, 1.55591332e-03, 1.55591332e-03,\n",
       "        1.55878179e-03, 1.54848324e-03, 1.55878179e-03, 1.55878179e-03,\n",
       "        1.55878179e-03, 1.55213678e-03, 6.34919872e-04, 1.51413736e-03,\n",
       "        1.49255615e-03, 1.55021671e-03, 1.55040633e-03, 1.55878179e-03,\n",
       "        1.55878179e-03, 1.55878179e-03, 1.55878179e-03, 1.55878179e-03,\n",
       "        1.55878179e-03, 1.55040633e-03]),\n",
       " 'rank_test_accuracy': array([30, 28, 27, 25,  1, 19, 18,  2, 22, 15, 15, 15,  5, 21,  5,  5,  5,\n",
       "         4, 29, 26, 19,  3, 22,  5,  5,  5,  5,  5,  5, 22]),\n",
       " 'split0_test_precision': array([0.        , 0.6389414 , 0.61886792, 0.59679037, 0.6       ,\n",
       "        0.59242144, 0.59211732, 0.59286368, 0.59286368, 0.59360731,\n",
       "        0.59360731, 0.59360731, 0.59360731, 0.59360731, 0.59360731,\n",
       "        0.59360731, 0.59360731, 0.59360731, 0.6389414 , 0.59638554,\n",
       "        0.59242144, 0.59286368, 0.59323583, 0.59360731, 0.59360731,\n",
       "        0.59360731, 0.59360731, 0.59360731, 0.59360731, 0.59323583]),\n",
       " 'split1_test_precision': array([0.        , 0.64349376, 0.6234414 , 0.60830861, 0.60640301,\n",
       "        0.6043956 , 0.60437557, 0.60636364, 0.60636364, 0.6058129 ,\n",
       "        0.6058129 , 0.6058129 , 0.6058129 , 0.6058129 , 0.6058129 ,\n",
       "        0.6058129 , 0.6058129 , 0.6058129 , 0.64349376, 0.60830861,\n",
       "        0.6043956 , 0.60636364, 0.6058129 , 0.6058129 , 0.6058129 ,\n",
       "        0.6058129 , 0.6058129 , 0.6058129 , 0.6058129 , 0.6058129 ]),\n",
       " 'split2_test_precision': array([0.        , 0.62383613, 0.59459459, 0.58524427, 0.58529689,\n",
       "        0.58141674, 0.57793984, 0.57793984, 0.57793984, 0.57793984,\n",
       "        0.57793984, 0.57793984, 0.57793984, 0.57793984, 0.57793984,\n",
       "        0.57793984, 0.57793984, 0.57832423, 0.62383613, 0.58524427,\n",
       "        0.58141674, 0.57793984, 0.57793984, 0.57793984, 0.57793984,\n",
       "        0.57793984, 0.57793984, 0.57793984, 0.57793984, 0.57793984]),\n",
       " 'split3_test_precision': array([0.        , 0.65355805, 0.60181582, 0.59406953, 0.58969466,\n",
       "        0.58741906, 0.58623853, 0.58554437, 0.58554437, 0.58554437,\n",
       "        0.58554437, 0.58554437, 0.58554437, 0.58554437, 0.58554437,\n",
       "        0.58554437, 0.58554437, 0.58554437, 0.65233645, 0.59406953,\n",
       "        0.58741906, 0.58554437, 0.58554437, 0.58554437, 0.58554437,\n",
       "        0.58554437, 0.58554437, 0.58554437, 0.58554437, 0.58554437]),\n",
       " 'split4_test_precision': array([0.        , 0.65735568, 0.63875   , 0.62718847, 0.63137255,\n",
       "        0.62391514, 0.62607862, 0.62559694, 0.625     , 0.625     ,\n",
       "        0.625     , 0.625     , 0.625     , 0.62440419, 0.625     ,\n",
       "        0.625     , 0.625     , 0.625     , 0.65735568, 0.62718847,\n",
       "        0.62391514, 0.62559694, 0.625     , 0.625     , 0.625     ,\n",
       "        0.625     , 0.625     , 0.625     , 0.625     , 0.625     ]),\n",
       " 'split5_test_precision': array([0.        , 0.64788732, 0.60386473, 0.58864542, 0.58779343,\n",
       "        0.58462946, 0.58325751, 0.58325751, 0.58325751, 0.58325751,\n",
       "        0.58325751, 0.58325751, 0.58325751, 0.58325751, 0.58325751,\n",
       "        0.58325751, 0.58325751, 0.58325751, 0.64788732, 0.58864542,\n",
       "        0.58462946, 0.58325751, 0.58325751, 0.58325751, 0.58325751,\n",
       "        0.58325751, 0.58325751, 0.58325751, 0.58325751, 0.58325751]),\n",
       " 'split6_test_precision': array([0.        , 0.66236162, 0.63591022, 0.62487361, 0.62892483,\n",
       "        0.61979648, 0.61926606, 0.61821527, 0.61856618, 0.61891644,\n",
       "        0.61891644, 0.61891644, 0.61891644, 0.61891644, 0.61891644,\n",
       "        0.61891644, 0.61891644, 0.61891644, 0.66236162, 0.62487361,\n",
       "        0.61979648, 0.61821527, 0.61856618, 0.61891644, 0.61891644,\n",
       "        0.61891644, 0.61891644, 0.61891644, 0.61891644, 0.61856618]),\n",
       " 'split7_test_precision': array([0.        , 0.64224872, 0.63454318, 0.62107396, 0.61238095,\n",
       "        0.61095636, 0.60737327, 0.60658737, 0.60622711, 0.60603291,\n",
       "        0.60603291, 0.60603291, 0.60639269, 0.60603291, 0.60639269,\n",
       "        0.60639269, 0.60639269, 0.60639269, 0.64224872, 0.62107396,\n",
       "        0.61095636, 0.60622711, 0.60603291, 0.60639269, 0.60639269,\n",
       "        0.60639269, 0.60639269, 0.60639269, 0.60639269, 0.60603291]),\n",
       " 'split8_test_precision': array([0.        , 0.63518519, 0.61629435, 0.59368421, 0.58909445,\n",
       "        0.58452722, 0.58538899, 0.58578199, 0.5846736 , 0.5846736 ,\n",
       "        0.5846736 , 0.5846736 , 0.5846736 , 0.5846736 , 0.5846736 ,\n",
       "        0.5846736 , 0.5846736 , 0.5846736 , 0.63518519, 0.59368421,\n",
       "        0.58452722, 0.58578199, 0.5846736 , 0.5846736 , 0.5846736 ,\n",
       "        0.5846736 , 0.5846736 , 0.5846736 , 0.5846736 , 0.5846736 ]),\n",
       " 'split9_test_precision': array([0.        , 0.66410749, 0.65116279, 0.62336354, 0.61290323,\n",
       "        0.60560748, 0.60611677, 0.60533579, 0.60422406, 0.60422406,\n",
       "        0.60422406, 0.60422406, 0.60422406, 0.60422406, 0.60422406,\n",
       "        0.60422406, 0.60422406, 0.60422406, 0.66410749, 0.62336354,\n",
       "        0.60560748, 0.60533579, 0.60422406, 0.60422406, 0.60422406,\n",
       "        0.60422406, 0.60422406, 0.60422406, 0.60422406, 0.60422406]),\n",
       " 'mean_test_precision': array([0.        , 0.64689754, 0.6219245 , 0.6063242 , 0.6043864 ,\n",
       "        0.5995085 , 0.59881525, 0.59874864, 0.598466  , 0.59850089,\n",
       "        0.59850089, 0.59850089, 0.59853687, 0.59844131, 0.59853687,\n",
       "        0.59853687, 0.59853687, 0.59857531, 0.64677538, 0.60628372,\n",
       "        0.5995085 , 0.59871261, 0.59842872, 0.59853687, 0.59853687,\n",
       "        0.59853687, 0.59853687, 0.59853687, 0.59853687, 0.59842872]),\n",
       " 'std_test_precision': array([0.        , 0.01206458, 0.01735968, 0.0156492 , 0.01606104,\n",
       "        0.01472732, 0.01542765, 0.01520759, 0.01518236, 0.01516601,\n",
       "        0.01516601, 0.01516601, 0.01518425, 0.01506261, 0.01518425,\n",
       "        0.01518425, 0.01518425, 0.01513246, 0.01200254, 0.01567431,\n",
       "        0.01472732, 0.01518939, 0.0151315 , 0.01518425, 0.01518425,\n",
       "        0.01518425, 0.01518425, 0.01518425, 0.01518425, 0.0151315 ]),\n",
       " 'rank_test_precision': array([30,  1,  3,  4,  6,  7,  9, 10, 26, 23, 23, 23, 13, 27, 13, 13, 13,\n",
       "        12,  2,  5,  7, 11, 28, 13, 13, 13, 13, 13, 13, 28]),\n",
       " 'split0_test_f1_micro': array([0.86035969, 0.86760286, 0.86967233, 0.86986943, 0.87080562,\n",
       "        0.87021434, 0.87026361, 0.87036216, 0.87036216, 0.8704607 ,\n",
       "        0.8704607 , 0.8704607 , 0.8704607 , 0.8704607 , 0.8704607 ,\n",
       "        0.8704607 , 0.8704607 , 0.8704607 , 0.86760286, 0.86982015,\n",
       "        0.87021434, 0.87036216, 0.87041143, 0.8704607 , 0.8704607 ,\n",
       "        0.8704607 , 0.8704607 , 0.8704607 , 0.8704607 , 0.87041143]),\n",
       " 'split1_test_f1_micro': array([0.86035969, 0.86829268, 0.87011579, 0.87115053, 0.87149544,\n",
       "        0.87159399, 0.87164326, 0.87188963, 0.87188963, 0.87184035,\n",
       "        0.87184035, 0.87184035, 0.87184035, 0.87184035, 0.87184035,\n",
       "        0.87184035, 0.87184035, 0.87184035, 0.86829268, 0.87115053,\n",
       "        0.87159399, 0.87188963, 0.87184035, 0.87184035, 0.87184035,\n",
       "        0.87184035, 0.87184035, 0.87184035, 0.87184035, 0.87184035]),\n",
       " 'split2_test_f1_micro': array([0.86035969, 0.86691303, 0.86794777, 0.86878542, 0.86927815,\n",
       "        0.86908105, 0.86878542, 0.86878542, 0.86878542, 0.86878542,\n",
       "        0.86878542, 0.86878542, 0.86878542, 0.86878542, 0.86878542,\n",
       "        0.86878542, 0.86878542, 0.86883469, 0.86691303, 0.86878542,\n",
       "        0.86908105, 0.86878542, 0.86878542, 0.86878542, 0.86878542,\n",
       "        0.86878542, 0.86878542, 0.86878542, 0.86878542, 0.86878542]),\n",
       " 'split3_test_f1_micro': array([0.86035969, 0.8684405 , 0.86809559, 0.86942597, 0.86962306,\n",
       "        0.86967233, 0.86962306, 0.86957379, 0.86957379, 0.86957379,\n",
       "        0.86957379, 0.86957379, 0.86957379, 0.86957379, 0.86957379,\n",
       "        0.86957379, 0.86957379, 0.86957379, 0.86839123, 0.86942597,\n",
       "        0.86967233, 0.86957379, 0.86957379, 0.86957379, 0.86957379,\n",
       "        0.86957379, 0.86957379, 0.86957379, 0.86957379, 0.86957379]),\n",
       " 'split4_test_f1_micro': array([0.86035281, 0.8686804 , 0.87129201, 0.8725239 , 0.87355869,\n",
       "        0.87301666, 0.87331231, 0.87331231, 0.87326303, 0.87326303,\n",
       "        0.87326303, 0.87326303, 0.87326303, 0.87321376, 0.87326303,\n",
       "        0.87326303, 0.87326303, 0.87326303, 0.8686804 , 0.8725239 ,\n",
       "        0.87301666, 0.87331231, 0.87326303, 0.87326303, 0.87326303,\n",
       "        0.87326303, 0.87326303, 0.87326303, 0.87326303, 0.87326303]),\n",
       " 'split5_test_f1_micro': array([0.86035281, 0.86863112, 0.86882823, 0.86912388, 0.86956736,\n",
       "        0.86946881, 0.86937026, 0.86937026, 0.86937026, 0.86937026,\n",
       "        0.86937026, 0.86937026, 0.86937026, 0.86937026, 0.86937026,\n",
       "        0.86937026, 0.86937026, 0.86937026, 0.86863112, 0.86912388,\n",
       "        0.86946881, 0.86937026, 0.86937026, 0.86937026, 0.86937026,\n",
       "        0.86937026, 0.86937026, 0.86937026, 0.86937026, 0.86937026]),\n",
       " 'split6_test_f1_micro': array([0.86035281, 0.86902533, 0.8710949 , 0.8725239 , 0.87370651,\n",
       "        0.87311521, 0.87316448, 0.87301666, 0.87306593, 0.87311521,\n",
       "        0.87311521, 0.87311521, 0.87311521, 0.87311521, 0.87311521,\n",
       "        0.87311521, 0.87311521, 0.87311521, 0.86902533, 0.8725239 ,\n",
       "        0.87311521, 0.87301666, 0.87306593, 0.87311521, 0.87311521,\n",
       "        0.87311521, 0.87311521, 0.87311521, 0.87311521, 0.87306593]),\n",
       " 'split7_test_f1_micro': array([0.86035281, 0.86858185, 0.87094708, 0.87212969, 0.87198187,\n",
       "        0.87212969, 0.87183404, 0.87183404, 0.87178476, 0.87178476,\n",
       "        0.87178476, 0.87178476, 0.87183404, 0.87178476, 0.87183404,\n",
       "        0.87183404, 0.87183404, 0.87183404, 0.86858185, 0.87212969,\n",
       "        0.87212969, 0.87178476, 0.87178476, 0.87183404, 0.87183404,\n",
       "        0.87183404, 0.87183404, 0.87183404, 0.87183404, 0.87178476]),\n",
       " 'split8_test_f1_micro': array([0.86040209, 0.86759633, 0.86912388, 0.86917315, 0.86941953,\n",
       "        0.86912388, 0.86927171, 0.86932098, 0.86922243, 0.86922243,\n",
       "        0.86922243, 0.86922243, 0.86922243, 0.86922243, 0.86922243,\n",
       "        0.86922243, 0.86922243, 0.86922243, 0.86759633, 0.86917315,\n",
       "        0.86912388, 0.86932098, 0.86922243, 0.86922243, 0.86922243,\n",
       "        0.86922243, 0.86922243, 0.86922243, 0.86922243, 0.86922243]),\n",
       " 'split9_test_f1_micro': array([0.86040209, 0.86882823, 0.87193259, 0.87247462, 0.87212969,\n",
       "        0.87153839, 0.87168621, 0.87168621, 0.87158766, 0.87158766,\n",
       "        0.87158766, 0.87158766, 0.87158766, 0.87158766, 0.87158766,\n",
       "        0.87158766, 0.87158766, 0.87158766, 0.86882823, 0.87247462,\n",
       "        0.87153839, 0.87168621, 0.87158766, 0.87158766, 0.87158766,\n",
       "        0.87158766, 0.87158766, 0.87158766, 0.87158766, 0.87158766]),\n",
       " 'mean_test_f1_micro': array([0.86036542, 0.86825923, 0.86990502, 0.87071805, 0.87115659,\n",
       "        0.87089543, 0.87089544, 0.87091514, 0.87089051, 0.87090036,\n",
       "        0.87090036, 0.87090036, 0.87090529, 0.87089543, 0.87090529,\n",
       "        0.87090529, 0.87090529, 0.87091022, 0.86825431, 0.87071312,\n",
       "        0.87089543, 0.87091022, 0.87089051, 0.87090529, 0.87090529,\n",
       "        0.87090529, 0.87090529, 0.87090529, 0.87090529, 0.87089051]),\n",
       " 'std_test_f1_micro': array([1.85905344e-05, 6.36153349e-04, 1.32323801e-03, 1.51130095e-03,\n",
       "        1.59879584e-03, 1.49255615e-03, 1.56487681e-03, 1.55306444e-03,\n",
       "        1.55509707e-03, 1.55591332e-03, 1.55591332e-03, 1.55591332e-03,\n",
       "        1.55878179e-03, 1.54848324e-03, 1.55878179e-03, 1.55878179e-03,\n",
       "        1.55878179e-03, 1.55213678e-03, 6.34919872e-04, 1.51413736e-03,\n",
       "        1.49255615e-03, 1.55021671e-03, 1.55040633e-03, 1.55878179e-03,\n",
       "        1.55878179e-03, 1.55878179e-03, 1.55878179e-03, 1.55878179e-03,\n",
       "        1.55878179e-03, 1.55040633e-03]),\n",
       " 'rank_test_f1_micro': array([30, 28, 27, 25,  1, 19, 18,  2, 22, 15, 15, 15,  5, 21,  5,  5,  5,\n",
       "         4, 29, 26, 19,  3, 22,  5,  5,  5,  5,  5,  5, 22])}"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_model.cv_results_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "d443f4d3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([30, 28, 27, 25,  1, 19, 18,  2, 22, 15, 15, 15,  5, 21,  5,  5,  5,\n",
       "        4, 29, 26, 19,  3, 22,  5,  5,  5,  5,  5,  5, 22])"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_model.cv_results_['rank_test_accuracy']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "4f10e207",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'classifier': LogisticRegression(max_iter=10000),\n",
       " 'classifier__C': 0.0001,\n",
       " 'classifier__penalty': 'l1',\n",
       " 'classifier__solver': 'saga'}"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_model.cv_results_['params'][ np.argmin(best_model.cv_results_['mean_test_accuracy'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "51791136",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>classifier</th>\n",
       "      <th>C</th>\n",
       "      <th>penalty</th>\n",
       "      <th>solver</th>\n",
       "      <th>score_acc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.860365</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.868259</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.869905</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.870718</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0100</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.871157</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0100</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.870895</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.1000</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.870895</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.1000</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.870915</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.870891</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.870900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>10.0000</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.870900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>10.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.870900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>100.0000</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.870905</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>100.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.870895</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>1000.0000</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.870905</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>1000.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.870905</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>10000.0000</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.870905</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>10000.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.870910</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.868254</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.870713</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0100</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.870895</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.1000</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.870910</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.870891</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>10.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.870905</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>100.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.870905</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>1000.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.870905</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>10000.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.870905</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>NaN</td>\n",
       "      <td>none</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.870905</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>NaN</td>\n",
       "      <td>none</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.870905</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.870891</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                            classifier           C penalty solver  score_acc\n",
       "0   LogisticRegression(max_iter=10000)      0.0001      l1   saga   0.860365\n",
       "1   LogisticRegression(max_iter=10000)      0.0001      l2   saga   0.868259\n",
       "2   LogisticRegression(max_iter=10000)      0.0010      l1   saga   0.869905\n",
       "3   LogisticRegression(max_iter=10000)      0.0010      l2   saga   0.870718\n",
       "4   LogisticRegression(max_iter=10000)      0.0100      l1   saga   0.871157\n",
       "5   LogisticRegression(max_iter=10000)      0.0100      l2   saga   0.870895\n",
       "6   LogisticRegression(max_iter=10000)      0.1000      l1   saga   0.870895\n",
       "7   LogisticRegression(max_iter=10000)      0.1000      l2   saga   0.870915\n",
       "8   LogisticRegression(max_iter=10000)      1.0000      l1   saga   0.870891\n",
       "9   LogisticRegression(max_iter=10000)      1.0000      l2   saga   0.870900\n",
       "10  LogisticRegression(max_iter=10000)     10.0000      l1   saga   0.870900\n",
       "11  LogisticRegression(max_iter=10000)     10.0000      l2   saga   0.870900\n",
       "12  LogisticRegression(max_iter=10000)    100.0000      l1   saga   0.870905\n",
       "13  LogisticRegression(max_iter=10000)    100.0000      l2   saga   0.870895\n",
       "14  LogisticRegression(max_iter=10000)   1000.0000      l1   saga   0.870905\n",
       "15  LogisticRegression(max_iter=10000)   1000.0000      l2   saga   0.870905\n",
       "16  LogisticRegression(max_iter=10000)  10000.0000      l1   saga   0.870905\n",
       "17  LogisticRegression(max_iter=10000)  10000.0000      l2   saga   0.870910\n",
       "18  LogisticRegression(max_iter=10000)      0.0001      l2  lbfgs   0.868254\n",
       "19  LogisticRegression(max_iter=10000)      0.0010      l2  lbfgs   0.870713\n",
       "20  LogisticRegression(max_iter=10000)      0.0100      l2  lbfgs   0.870895\n",
       "21  LogisticRegression(max_iter=10000)      0.1000      l2  lbfgs   0.870910\n",
       "22  LogisticRegression(max_iter=10000)      1.0000      l2  lbfgs   0.870891\n",
       "23  LogisticRegression(max_iter=10000)     10.0000      l2  lbfgs   0.870905\n",
       "24  LogisticRegression(max_iter=10000)    100.0000      l2  lbfgs   0.870905\n",
       "25  LogisticRegression(max_iter=10000)   1000.0000      l2  lbfgs   0.870905\n",
       "26  LogisticRegression(max_iter=10000)  10000.0000      l2  lbfgs   0.870905\n",
       "27  LogisticRegression(max_iter=10000)         NaN    none  lbfgs   0.870905\n",
       "28  LogisticRegression(max_iter=10000)         NaN    none   saga   0.870905\n",
       "29  LogisticRegression(max_iter=10000)         NaN     NaN    NaN   0.870891"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results = pd.DataFrame( best_model.cv_results_['params'] )\n",
    "\n",
    "results['score_acc'] = best_model.cv_results_['mean_test_accuracy']\n",
    "cols = results.columns.to_series().str.split('__').apply(lambda x: x[-1])\n",
    "results.columns = cols\n",
    "\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "153b94ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "acc_list.append(results[0:1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "a7cbb487",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'classifier': LogisticRegression(max_iter=10000),\n",
       " 'classifier__C': 0.0001,\n",
       " 'classifier__penalty': 'l2',\n",
       " 'classifier__solver': 'saga'}"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_model.cv_results_['params'][ np.argmin(best_model.cv_results_['rank_test_precision'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "155a06f9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>classifier</th>\n",
       "      <th>C</th>\n",
       "      <th>penalty</th>\n",
       "      <th>solver</th>\n",
       "      <th>score_precision</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.646898</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.621925</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.606324</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0100</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.604386</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0100</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.599508</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.1000</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.598815</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.1000</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.598749</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.598466</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.598501</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>10.0000</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.598501</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>10.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.598501</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>100.0000</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.598537</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>100.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.598441</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>1000.0000</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.598537</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>1000.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.598537</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>10000.0000</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.598537</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>10000.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.598575</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.646775</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.606284</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0100</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.599508</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.1000</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.598713</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.598429</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>10.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.598537</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>100.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.598537</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>1000.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.598537</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>10000.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.598537</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>NaN</td>\n",
       "      <td>none</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.598537</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>NaN</td>\n",
       "      <td>none</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.598537</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.598429</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                            classifier           C penalty solver  \\\n",
       "0   LogisticRegression(max_iter=10000)      0.0001      l1   saga   \n",
       "1   LogisticRegression(max_iter=10000)      0.0001      l2   saga   \n",
       "2   LogisticRegression(max_iter=10000)      0.0010      l1   saga   \n",
       "3   LogisticRegression(max_iter=10000)      0.0010      l2   saga   \n",
       "4   LogisticRegression(max_iter=10000)      0.0100      l1   saga   \n",
       "5   LogisticRegression(max_iter=10000)      0.0100      l2   saga   \n",
       "6   LogisticRegression(max_iter=10000)      0.1000      l1   saga   \n",
       "7   LogisticRegression(max_iter=10000)      0.1000      l2   saga   \n",
       "8   LogisticRegression(max_iter=10000)      1.0000      l1   saga   \n",
       "9   LogisticRegression(max_iter=10000)      1.0000      l2   saga   \n",
       "10  LogisticRegression(max_iter=10000)     10.0000      l1   saga   \n",
       "11  LogisticRegression(max_iter=10000)     10.0000      l2   saga   \n",
       "12  LogisticRegression(max_iter=10000)    100.0000      l1   saga   \n",
       "13  LogisticRegression(max_iter=10000)    100.0000      l2   saga   \n",
       "14  LogisticRegression(max_iter=10000)   1000.0000      l1   saga   \n",
       "15  LogisticRegression(max_iter=10000)   1000.0000      l2   saga   \n",
       "16  LogisticRegression(max_iter=10000)  10000.0000      l1   saga   \n",
       "17  LogisticRegression(max_iter=10000)  10000.0000      l2   saga   \n",
       "18  LogisticRegression(max_iter=10000)      0.0001      l2  lbfgs   \n",
       "19  LogisticRegression(max_iter=10000)      0.0010      l2  lbfgs   \n",
       "20  LogisticRegression(max_iter=10000)      0.0100      l2  lbfgs   \n",
       "21  LogisticRegression(max_iter=10000)      0.1000      l2  lbfgs   \n",
       "22  LogisticRegression(max_iter=10000)      1.0000      l2  lbfgs   \n",
       "23  LogisticRegression(max_iter=10000)     10.0000      l2  lbfgs   \n",
       "24  LogisticRegression(max_iter=10000)    100.0000      l2  lbfgs   \n",
       "25  LogisticRegression(max_iter=10000)   1000.0000      l2  lbfgs   \n",
       "26  LogisticRegression(max_iter=10000)  10000.0000      l2  lbfgs   \n",
       "27  LogisticRegression(max_iter=10000)         NaN    none  lbfgs   \n",
       "28  LogisticRegression(max_iter=10000)         NaN    none   saga   \n",
       "29  LogisticRegression(max_iter=10000)         NaN     NaN    NaN   \n",
       "\n",
       "    score_precision  \n",
       "0          0.000000  \n",
       "1          0.646898  \n",
       "2          0.621925  \n",
       "3          0.606324  \n",
       "4          0.604386  \n",
       "5          0.599508  \n",
       "6          0.598815  \n",
       "7          0.598749  \n",
       "8          0.598466  \n",
       "9          0.598501  \n",
       "10         0.598501  \n",
       "11         0.598501  \n",
       "12         0.598537  \n",
       "13         0.598441  \n",
       "14         0.598537  \n",
       "15         0.598537  \n",
       "16         0.598537  \n",
       "17         0.598575  \n",
       "18         0.646775  \n",
       "19         0.606284  \n",
       "20         0.599508  \n",
       "21         0.598713  \n",
       "22         0.598429  \n",
       "23         0.598537  \n",
       "24         0.598537  \n",
       "25         0.598537  \n",
       "26         0.598537  \n",
       "27         0.598537  \n",
       "28         0.598537  \n",
       "29         0.598429  "
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results = pd.DataFrame( best_model.cv_results_['params'] )\n",
    "\n",
    "results['score_precision'] = best_model.cv_results_['mean_test_precision']\n",
    "cols = results.columns.to_series().str.split('__').apply(lambda x: x[-1])\n",
    "results.columns = cols\n",
    "\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "a4de3306",
   "metadata": {},
   "outputs": [],
   "source": [
    "roc_auc_ovr_List.append(results[1:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "58513be7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'classifier': LogisticRegression(max_iter=10000),\n",
       " 'classifier__C': 0.01,\n",
       " 'classifier__penalty': 'l1',\n",
       " 'classifier__solver': 'saga'}"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_model.cv_results_['params'][ np.argmin(best_model.cv_results_['rank_test_f1_micro'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "6f4588d3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>classifier</th>\n",
       "      <th>C</th>\n",
       "      <th>penalty</th>\n",
       "      <th>solver</th>\n",
       "      <th>score_f1_micro</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.860365</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.868259</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.869905</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.870718</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0100</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.871157</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0100</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.870895</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.1000</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.870895</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.1000</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.870915</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.870891</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.870900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>10.0000</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.870900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>10.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.870900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>100.0000</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.870905</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>100.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.870895</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>1000.0000</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.870905</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>1000.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.870905</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>10000.0000</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.870905</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>10000.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.870910</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.868254</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.870713</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0100</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.870895</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.1000</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.870910</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.870891</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>10.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.870905</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>100.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.870905</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>1000.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.870905</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>10000.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.870905</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>NaN</td>\n",
       "      <td>none</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.870905</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>NaN</td>\n",
       "      <td>none</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.870905</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.870891</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                            classifier           C penalty solver  \\\n",
       "0   LogisticRegression(max_iter=10000)      0.0001      l1   saga   \n",
       "1   LogisticRegression(max_iter=10000)      0.0001      l2   saga   \n",
       "2   LogisticRegression(max_iter=10000)      0.0010      l1   saga   \n",
       "3   LogisticRegression(max_iter=10000)      0.0010      l2   saga   \n",
       "4   LogisticRegression(max_iter=10000)      0.0100      l1   saga   \n",
       "5   LogisticRegression(max_iter=10000)      0.0100      l2   saga   \n",
       "6   LogisticRegression(max_iter=10000)      0.1000      l1   saga   \n",
       "7   LogisticRegression(max_iter=10000)      0.1000      l2   saga   \n",
       "8   LogisticRegression(max_iter=10000)      1.0000      l1   saga   \n",
       "9   LogisticRegression(max_iter=10000)      1.0000      l2   saga   \n",
       "10  LogisticRegression(max_iter=10000)     10.0000      l1   saga   \n",
       "11  LogisticRegression(max_iter=10000)     10.0000      l2   saga   \n",
       "12  LogisticRegression(max_iter=10000)    100.0000      l1   saga   \n",
       "13  LogisticRegression(max_iter=10000)    100.0000      l2   saga   \n",
       "14  LogisticRegression(max_iter=10000)   1000.0000      l1   saga   \n",
       "15  LogisticRegression(max_iter=10000)   1000.0000      l2   saga   \n",
       "16  LogisticRegression(max_iter=10000)  10000.0000      l1   saga   \n",
       "17  LogisticRegression(max_iter=10000)  10000.0000      l2   saga   \n",
       "18  LogisticRegression(max_iter=10000)      0.0001      l2  lbfgs   \n",
       "19  LogisticRegression(max_iter=10000)      0.0010      l2  lbfgs   \n",
       "20  LogisticRegression(max_iter=10000)      0.0100      l2  lbfgs   \n",
       "21  LogisticRegression(max_iter=10000)      0.1000      l2  lbfgs   \n",
       "22  LogisticRegression(max_iter=10000)      1.0000      l2  lbfgs   \n",
       "23  LogisticRegression(max_iter=10000)     10.0000      l2  lbfgs   \n",
       "24  LogisticRegression(max_iter=10000)    100.0000      l2  lbfgs   \n",
       "25  LogisticRegression(max_iter=10000)   1000.0000      l2  lbfgs   \n",
       "26  LogisticRegression(max_iter=10000)  10000.0000      l2  lbfgs   \n",
       "27  LogisticRegression(max_iter=10000)         NaN    none  lbfgs   \n",
       "28  LogisticRegression(max_iter=10000)         NaN    none   saga   \n",
       "29  LogisticRegression(max_iter=10000)         NaN     NaN    NaN   \n",
       "\n",
       "    score_f1_micro  \n",
       "0         0.860365  \n",
       "1         0.868259  \n",
       "2         0.869905  \n",
       "3         0.870718  \n",
       "4         0.871157  \n",
       "5         0.870895  \n",
       "6         0.870895  \n",
       "7         0.870915  \n",
       "8         0.870891  \n",
       "9         0.870900  \n",
       "10        0.870900  \n",
       "11        0.870900  \n",
       "12        0.870905  \n",
       "13        0.870895  \n",
       "14        0.870905  \n",
       "15        0.870905  \n",
       "16        0.870905  \n",
       "17        0.870910  \n",
       "18        0.868254  \n",
       "19        0.870713  \n",
       "20        0.870895  \n",
       "21        0.870910  \n",
       "22        0.870891  \n",
       "23        0.870905  \n",
       "24        0.870905  \n",
       "25        0.870905  \n",
       "26        0.870905  \n",
       "27        0.870905  \n",
       "28        0.870905  \n",
       "29        0.870891  "
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results = pd.DataFrame( best_model.cv_results_['params'] )\n",
    "\n",
    "results['score_f1_micro'] = best_model.cv_results_['mean_test_f1_micro']\n",
    "cols = results.columns.to_series().str.split('__').apply(lambda x: x[-1])\n",
    "results.columns = cols\n",
    "\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "4f243c59",
   "metadata": {},
   "outputs": [],
   "source": [
    "f1_micro_List.append(results[4:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94d1e17e",
   "metadata": {},
   "source": [
    "## Trial 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "82374d4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, Y_train, Y_test = train_test_split(X, y, test_size=0.2, shuffle=True, random_state = 6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "64a330b7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Guest-All\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\Guest-All\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\Guest-All\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\Guest-All\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\Guest-All\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\Guest-All\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\Guest-All\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\Guest-All\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\Guest-All\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\Guest-All\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\Guest-All\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1183: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Guest-All\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1183: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Guest-All\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1183: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Guest-All\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1183: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Guest-All\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1183: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Guest-All\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1183: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Guest-All\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1183: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Guest-All\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1183: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Guest-All\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1183: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Guest-All\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1183: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Guest-All\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1183: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Guest-All\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1183: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Guest-All\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1183: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Guest-All\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1183: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Guest-All\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1183: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Guest-All\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1183: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Guest-All\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1183: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Guest-All\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1183: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Guest-All\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1183: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Guest-All\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1183: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Create a pipeline - RF is a stand in, we will populate the classifier part below\n",
    "pipe = Pipeline([('std', StandardScaler()),\n",
    "                 ('classifier', LogisticRegression())])\n",
    "\n",
    "# Create search space of candidate learning algorithms and their hyperparameters\n",
    "# note lbfgs can't do l1, and if you pass penalty='none' it expects no C value\n",
    "search_space = [{'classifier': [LogisticRegression(max_iter=10000)],\n",
    "                 'classifier__solver': ['saga'],\n",
    "                 'classifier__penalty': ['l1', 'l2'],\n",
    "                 'classifier__C': np.logspace(-4, 4, 9)},\n",
    "                {'classifier': [LogisticRegression(max_iter=10000)],\n",
    "                 'classifier__solver': ['lbfgs'],\n",
    "                 'classifier__penalty': ['l2'],\n",
    "                 'classifier__C': np.logspace(-4, 4, 9)},\n",
    "                {'classifier': [LogisticRegression(max_iter=10000)],\n",
    "                 'classifier__solver': ['lbfgs','saga'],\n",
    "                 'classifier__penalty': ['none']},\n",
    "                {'classifier': [LogisticRegression(max_iter=10000)]}\n",
    "                ]\n",
    "\n",
    "# Create grid search\n",
    "clf = GridSearchCV(pipe, search_space, cv=StratifiedKFold(n_splits=10),\n",
    "                   scoring=['accuracy', 'precision', 'f1_micro'], refit=False,\n",
    "                   verbose=0)\n",
    "\n",
    "# Fit grid search\n",
    "best_model = clf.fit(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "b0094308",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'mean_fit_time': array([ 3.67989964,  3.27875111,  3.55268047,  2.95264535,  3.86669946,\n",
       "         3.34934487,  5.50261407,  3.09169595, 25.48187156,  3.31307075,\n",
       "         4.02801998,  3.49474475,  4.2413981 ,  3.49847949,  4.02874799,\n",
       "         3.09594798,  4.06451027,  3.27255731,  0.55583339,  0.69760544,\n",
       "         0.80819983,  0.81386056,  0.77267804,  0.80236039,  0.81025002,\n",
       "         0.8160243 ,  0.80712585,  0.78305824,  3.32110829,  0.80755968]),\n",
       " 'std_fit_time': array([0.43124783, 0.27007019, 0.17057934, 0.36289452, 0.34824934,\n",
       "        0.31127547, 3.04326894, 0.43773101, 4.17106867, 0.22477836,\n",
       "        0.26034424, 0.22546544, 0.43195382, 0.57127854, 0.24909874,\n",
       "        0.389996  , 0.27200302, 0.39953608, 0.01460972, 0.02637907,\n",
       "        0.0165787 , 0.01053   , 0.06818121, 0.019456  , 0.0190502 ,\n",
       "        0.02510649, 0.01908092, 0.03701242, 0.25991028, 0.01693304]),\n",
       " 'mean_score_time': array([0.03194473, 0.03191979, 0.03116677, 0.03028927, 0.03234177,\n",
       "        0.03498728, 0.03194454, 0.03165567, 0.0353621 , 0.03245721,\n",
       "        0.03172359, 0.03254344, 0.03219268, 0.03724005, 0.0316838 ,\n",
       "        0.03120987, 0.03168194, 0.03236392, 0.0321965 , 0.03136086,\n",
       "        0.03128257, 0.03178382, 0.02959177, 0.03180838, 0.03182464,\n",
       "        0.0314307 , 0.03206203, 0.02913077, 0.03267088, 0.03409438]),\n",
       " 'std_score_time': array([0.00310788, 0.00260786, 0.00300135, 0.00423326, 0.0046978 ,\n",
       "        0.00137729, 0.00454512, 0.00493182, 0.00528317, 0.00285132,\n",
       "        0.00352431, 0.00214249, 0.00782359, 0.01262597, 0.00240473,\n",
       "        0.00446764, 0.00257933, 0.00301535, 0.00089441, 0.00239947,\n",
       "        0.00285748, 0.00313229, 0.0048464 , 0.00318068, 0.00247112,\n",
       "        0.00350846, 0.00344778, 0.00378862, 0.00187095, 0.00126561]),\n",
       " 'param_classifier': masked_array(data=[LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000)],\n",
       "              mask=[False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'param_classifier__C': masked_array(data=[0.0001, 0.0001, 0.001, 0.001, 0.01, 0.01, 0.1, 0.1,\n",
       "                    1.0, 1.0, 10.0, 10.0, 100.0, 100.0, 1000.0, 1000.0,\n",
       "                    10000.0, 10000.0, 0.0001, 0.001, 0.01, 0.1, 1.0, 10.0,\n",
       "                    100.0, 1000.0, 10000.0, --, --, --],\n",
       "              mask=[False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False,  True,  True,  True],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'param_classifier__penalty': masked_array(data=['l1', 'l2', 'l1', 'l2', 'l1', 'l2', 'l1', 'l2', 'l1',\n",
       "                    'l2', 'l1', 'l2', 'l1', 'l2', 'l1', 'l2', 'l1', 'l2',\n",
       "                    'l2', 'l2', 'l2', 'l2', 'l2', 'l2', 'l2', 'l2', 'l2',\n",
       "                    'none', 'none', --],\n",
       "              mask=[False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False,  True],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'param_classifier__solver': masked_array(data=['saga', 'saga', 'saga', 'saga', 'saga', 'saga', 'saga',\n",
       "                    'saga', 'saga', 'saga', 'saga', 'saga', 'saga', 'saga',\n",
       "                    'saga', 'saga', 'saga', 'saga', 'lbfgs', 'lbfgs',\n",
       "                    'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs',\n",
       "                    'lbfgs', 'lbfgs', 'saga', --],\n",
       "              mask=[False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False,  True],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'params': [{'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 0.0001,\n",
       "   'classifier__penalty': 'l1',\n",
       "   'classifier__solver': 'saga'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 0.0001,\n",
       "   'classifier__penalty': 'l2',\n",
       "   'classifier__solver': 'saga'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 0.001,\n",
       "   'classifier__penalty': 'l1',\n",
       "   'classifier__solver': 'saga'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 0.001,\n",
       "   'classifier__penalty': 'l2',\n",
       "   'classifier__solver': 'saga'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 0.01,\n",
       "   'classifier__penalty': 'l1',\n",
       "   'classifier__solver': 'saga'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 0.01,\n",
       "   'classifier__penalty': 'l2',\n",
       "   'classifier__solver': 'saga'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 0.1,\n",
       "   'classifier__penalty': 'l1',\n",
       "   'classifier__solver': 'saga'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 0.1,\n",
       "   'classifier__penalty': 'l2',\n",
       "   'classifier__solver': 'saga'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 1.0,\n",
       "   'classifier__penalty': 'l1',\n",
       "   'classifier__solver': 'saga'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 1.0,\n",
       "   'classifier__penalty': 'l2',\n",
       "   'classifier__solver': 'saga'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 10.0,\n",
       "   'classifier__penalty': 'l1',\n",
       "   'classifier__solver': 'saga'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 10.0,\n",
       "   'classifier__penalty': 'l2',\n",
       "   'classifier__solver': 'saga'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 100.0,\n",
       "   'classifier__penalty': 'l1',\n",
       "   'classifier__solver': 'saga'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 100.0,\n",
       "   'classifier__penalty': 'l2',\n",
       "   'classifier__solver': 'saga'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 1000.0,\n",
       "   'classifier__penalty': 'l1',\n",
       "   'classifier__solver': 'saga'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 1000.0,\n",
       "   'classifier__penalty': 'l2',\n",
       "   'classifier__solver': 'saga'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 10000.0,\n",
       "   'classifier__penalty': 'l1',\n",
       "   'classifier__solver': 'saga'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 10000.0,\n",
       "   'classifier__penalty': 'l2',\n",
       "   'classifier__solver': 'saga'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 0.0001,\n",
       "   'classifier__penalty': 'l2',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 0.001,\n",
       "   'classifier__penalty': 'l2',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 0.01,\n",
       "   'classifier__penalty': 'l2',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 0.1,\n",
       "   'classifier__penalty': 'l2',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 1.0,\n",
       "   'classifier__penalty': 'l2',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 10.0,\n",
       "   'classifier__penalty': 'l2',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 100.0,\n",
       "   'classifier__penalty': 'l2',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 1000.0,\n",
       "   'classifier__penalty': 'l2',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 10000.0,\n",
       "   'classifier__penalty': 'l2',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__penalty': 'none',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__penalty': 'none',\n",
       "   'classifier__solver': 'saga'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000)}],\n",
       " 'split0_test_accuracy': array([0.86050751, 0.8686376 , 0.86932742, 0.87110126, 0.87134762,\n",
       "        0.87129835, 0.87129835, 0.87134762, 0.8713969 , 0.8713969 ,\n",
       "        0.87134762, 0.87134762, 0.87134762, 0.8713969 , 0.87134762,\n",
       "        0.87134762, 0.87134762, 0.87134762, 0.8686376 , 0.87110126,\n",
       "        0.87129835, 0.87134762, 0.8713969 , 0.87134762, 0.87134762,\n",
       "        0.87134762, 0.87134762, 0.87134762, 0.87134762, 0.8713969 ]),\n",
       " 'split1_test_accuracy': array([0.86050751, 0.86848978, 0.86937669, 0.87179108, 0.87124908,\n",
       "        0.87129835, 0.87095344, 0.87090416, 0.87090416, 0.87090416,\n",
       "        0.87090416, 0.87090416, 0.87090416, 0.87090416, 0.87090416,\n",
       "        0.87090416, 0.87090416, 0.87090416, 0.86848978, 0.87179108,\n",
       "        0.87129835, 0.87090416, 0.87090416, 0.87090416, 0.87090416,\n",
       "        0.87090416, 0.87090416, 0.87090416, 0.87090416, 0.87090416]),\n",
       " 'split2_test_accuracy': array([0.86050751, 0.86814486, 0.86932742, 0.87006652, 0.86996797,\n",
       "        0.8699187 , 0.86996797, 0.87016507, 0.87016507, 0.87021434,\n",
       "        0.87021434, 0.87021434, 0.87021434, 0.87021434, 0.87021434,\n",
       "        0.87016507, 0.87021434, 0.87021434, 0.86814486, 0.87006652,\n",
       "        0.8699187 , 0.87016507, 0.87021434, 0.87021434, 0.87021434,\n",
       "        0.87021434, 0.87021434, 0.87021434, 0.87021434, 0.87021434]),\n",
       " 'split3_test_accuracy': array([0.86050751, 0.86735649, 0.86942597, 0.86952451, 0.86977088,\n",
       "        0.86967233, 0.86942597, 0.86957379, 0.86957379, 0.86957379,\n",
       "        0.86957379, 0.86957379, 0.86957379, 0.86957379, 0.86957379,\n",
       "        0.86957379, 0.86957379, 0.86957379, 0.86735649, 0.86952451,\n",
       "        0.86967233, 0.86957379, 0.86957379, 0.86957379, 0.86957379,\n",
       "        0.86957379, 0.86957379, 0.86957379, 0.86957379, 0.86957379]),\n",
       " 'split4_test_accuracy': array([0.86054992, 0.86877895, 0.87055287, 0.87114418, 0.87143983,\n",
       "        0.87134128, 0.87143983, 0.87153839, 0.87158766, 0.87158766,\n",
       "        0.87153839, 0.87153839, 0.87153839, 0.87153839, 0.87153839,\n",
       "        0.87153839, 0.87153839, 0.87148911, 0.86877895, 0.87114418,\n",
       "        0.87139056, 0.87153839, 0.87158766, 0.87158766, 0.87158766,\n",
       "        0.87158766, 0.87158766, 0.87158766, 0.87153839, 0.87158766]),\n",
       " 'split5_test_accuracy': array([0.86054992, 0.86981374, 0.87119346, 0.87262245, 0.87316448,\n",
       "        0.87301666, 0.87306593, 0.87301666, 0.8729181 , 0.8729181 ,\n",
       "        0.8729181 , 0.8729181 , 0.8729181 , 0.8729181 , 0.8729181 ,\n",
       "        0.8729181 , 0.8729181 , 0.8729181 , 0.86981374, 0.87262245,\n",
       "        0.87296738, 0.87301666, 0.8729181 , 0.8729181 , 0.8729181 ,\n",
       "        0.8729181 , 0.8729181 , 0.8729181 , 0.8729181 , 0.8729181 ]),\n",
       " 'split6_test_accuracy': array([0.86054992, 0.86937026, 0.86951808, 0.87001084, 0.87015867,\n",
       "        0.87055287, 0.87055287, 0.8705036 , 0.87045432, 0.87045432,\n",
       "        0.87045432, 0.87045432, 0.87045432, 0.87045432, 0.87040505,\n",
       "        0.87045432, 0.87045432, 0.87040505, 0.86932098, 0.87001084,\n",
       "        0.87055287, 0.8705036 , 0.87045432, 0.87045432, 0.87045432,\n",
       "        0.87045432, 0.87045432, 0.87045432, 0.8705036 , 0.87045432]),\n",
       " 'split7_test_accuracy': array([0.86054992, 0.86823692, 0.86932098, 0.86976446, 0.87025722,\n",
       "        0.87055287, 0.8707007 , 0.87055287, 0.87065142, 0.87065142,\n",
       "        0.87060215, 0.8707007 , 0.87074998, 0.8707007 , 0.87074998,\n",
       "        0.87074998, 0.87074998, 0.8707007 , 0.86823692, 0.86976446,\n",
       "        0.87055287, 0.87055287, 0.87065142, 0.8707007 , 0.8707007 ,\n",
       "        0.8707007 , 0.8707007 , 0.8707007 , 0.8707007 , 0.87065142]),\n",
       " 'split8_test_accuracy': array([0.86054992, 0.86828619, 0.87119346, 0.872721  , 0.87321376,\n",
       "        0.87277028, 0.87296738, 0.87301666, 0.87306593, 0.87301666,\n",
       "        0.87301666, 0.87306593, 0.87301666, 0.87301666, 0.87301666,\n",
       "        0.87301666, 0.87301666, 0.87301666, 0.86828619, 0.872721  ,\n",
       "        0.87277028, 0.87301666, 0.87301666, 0.87301666, 0.87301666,\n",
       "        0.87301666, 0.87301666, 0.87301666, 0.87301666, 0.87301666]),\n",
       " 'split9_test_accuracy': array([0.86054992, 0.86843402, 0.8686804 , 0.87040505, 0.87035577,\n",
       "        0.87045432, 0.8705036 , 0.87055287, 0.87060215, 0.87055287,\n",
       "        0.87055287, 0.87055287, 0.87055287, 0.87055287, 0.87055287,\n",
       "        0.87055287, 0.87055287, 0.87055287, 0.86843402, 0.87040505,\n",
       "        0.87045432, 0.87055287, 0.87055287, 0.87055287, 0.87055287,\n",
       "        0.87055287, 0.87055287, 0.87055287, 0.87055287, 0.87055287]),\n",
       " 'mean_test_accuracy': array([0.86053296, 0.86855488, 0.86979168, 0.87091514, 0.87109253,\n",
       "        0.8710876 , 0.8710876 , 0.87111717, 0.87113195, 0.87112702,\n",
       "        0.87111224, 0.87112702, 0.87112702, 0.87112702, 0.8711221 ,\n",
       "        0.8711221 , 0.87112702, 0.87111224, 0.86854995, 0.87091514,\n",
       "        0.8710876 , 0.87111717, 0.87112702, 0.87112702, 0.87112702,\n",
       "        0.87112702, 0.87112702, 0.87112702, 0.87112702, 0.87112702]),\n",
       " 'std_test_accuracy': array([2.07726848e-05, 6.40546595e-04, 8.23271507e-04, 1.09938289e-03,\n",
       "        1.18856386e-03, 1.05109747e-03, 1.11468034e-03, 1.08599542e-03,\n",
       "        1.07674963e-03, 1.06617410e-03, 1.06523472e-03, 1.06978257e-03,\n",
       "        1.05906124e-03, 1.06206689e-03, 1.06228941e-03, 1.06340180e-03,\n",
       "        1.05906124e-03, 1.06226738e-03, 6.34415338e-04, 1.09938289e-03,\n",
       "        1.04344645e-03, 1.08599542e-03, 1.06617410e-03, 1.06295165e-03,\n",
       "        1.06295165e-03, 1.06295165e-03, 1.06295165e-03, 1.06295165e-03,\n",
       "        1.05791428e-03, 1.06617410e-03]),\n",
       " 'rank_test_accuracy': array([30, 28, 27, 25, 21, 23, 22, 17,  1, 11, 19,  2,  2, 11, 16, 15,  2,\n",
       "        19, 29, 25, 23, 17, 11,  2,  2,  2,  2,  2,  2, 11]),\n",
       " 'split0_test_precision': array([0.        , 0.66940452, 0.62482566, 0.61748634, 0.61293634,\n",
       "        0.61005025, 0.60917248, 0.60912698, 0.60951437, 0.60951437,\n",
       "        0.60891089, 0.60891089, 0.60891089, 0.60951437, 0.60891089,\n",
       "        0.60891089, 0.60891089, 0.60891089, 0.66940452, 0.61748634,\n",
       "        0.61005025, 0.60912698, 0.60951437, 0.60891089, 0.60891089,\n",
       "        0.60891089, 0.60891089, 0.60891089, 0.60891089, 0.60951437]),\n",
       " 'split1_test_precision': array([0.        , 0.64412811, 0.61029412, 0.61530715, 0.60480769,\n",
       "        0.6028169 , 0.59851301, 0.59795729, 0.59777572, 0.59777572,\n",
       "        0.59777572, 0.59777572, 0.59777572, 0.59777572, 0.59777572,\n",
       "        0.59777572, 0.59777572, 0.59777572, 0.64412811, 0.61530715,\n",
       "        0.6028169 , 0.59795729, 0.59777572, 0.59777572, 0.59777572,\n",
       "        0.59777572, 0.59777572, 0.59777572, 0.59777572, 0.59777572]),\n",
       " 'split2_test_precision': array([0.        , 0.63864043, 0.61145704, 0.59566075, 0.58921933,\n",
       "        0.58673933, 0.58679928, 0.5881295 , 0.5881295 , 0.58849955,\n",
       "        0.58849955, 0.58849955, 0.58849955, 0.58849955, 0.58849955,\n",
       "        0.5881295 , 0.58849955, 0.58849955, 0.63864043, 0.59566075,\n",
       "        0.58673933, 0.5881295 , 0.58849955, 0.58849955, 0.58849955,\n",
       "        0.58849955, 0.58849955, 0.58849955, 0.58849955, 0.58849955]),\n",
       " 'split3_test_precision': array([0.        , 0.62388592, 0.61050061, 0.59233098, 0.58834586,\n",
       "        0.58611111, 0.58356417, 0.58487085, 0.58487085, 0.58487085,\n",
       "        0.58487085, 0.58487085, 0.58487085, 0.58487085, 0.58487085,\n",
       "        0.58487085, 0.58487085, 0.58487085, 0.62388592, 0.59233098,\n",
       "        0.58611111, 0.58487085, 0.58487085, 0.58487085, 0.58487085,\n",
       "        0.58487085, 0.58487085, 0.58487085, 0.58487085, 0.58487085]),\n",
       " 'split4_test_precision': array([0.        , 0.65725047, 0.62864385, 0.6123302 , 0.60929773,\n",
       "        0.60478469, 0.60493827, 0.6056872 , 0.60606061, 0.60606061,\n",
       "        0.60548723, 0.60548723, 0.60548723, 0.60548723, 0.60548723,\n",
       "        0.60548723, 0.60548723, 0.60491493, 0.65725047, 0.6123302 ,\n",
       "        0.60516252, 0.6056872 , 0.60606061, 0.60606061, 0.60606061,\n",
       "        0.60606061, 0.60606061, 0.60606061, 0.60548723, 0.60606061]),\n",
       " 'split5_test_precision': array([0.        , 0.67216117, 0.63466334, 0.62116716, 0.62075472,\n",
       "        0.61680517, 0.61630037, 0.61552511, 0.61440292, 0.61440292,\n",
       "        0.61440292, 0.61440292, 0.61440292, 0.61440292, 0.61440292,\n",
       "        0.61440292, 0.61440292, 0.61440292, 0.67216117, 0.62116716,\n",
       "        0.61623616, 0.61552511, 0.61440292, 0.61440292, 0.61440292,\n",
       "        0.61440292, 0.61440292, 0.61440292, 0.61440292, 0.61440292]),\n",
       " 'split6_test_precision': array([0.        , 0.65840708, 0.6107056 , 0.59677419, 0.59224219,\n",
       "        0.59389454, 0.59354839, 0.59283088, 0.5922865 , 0.5922865 ,\n",
       "        0.5922865 , 0.5922865 , 0.5922865 , 0.5922865 , 0.59174312,\n",
       "        0.5922865 , 0.5922865 , 0.59174312, 0.65780142, 0.59677419,\n",
       "        0.59389454, 0.59283088, 0.5922865 , 0.5922865 , 0.5922865 ,\n",
       "        0.5922865 , 0.5922865 , 0.5922865 , 0.59283088, 0.5922865 ]),\n",
       " 'split7_test_precision': array([0.        , 0.64181818, 0.60906863, 0.59340659, 0.59095106,\n",
       "        0.59103139, 0.59163701, 0.58990257, 0.59062776, 0.59062776,\n",
       "        0.59026549, 0.5909894 , 0.5913504 , 0.5909894 , 0.5913504 ,\n",
       "        0.5913504 , 0.5913504 , 0.5909894 , 0.64181818, 0.59340659,\n",
       "        0.59103139, 0.58990257, 0.59062776, 0.5909894 , 0.5909894 ,\n",
       "        0.5909894 , 0.5909894 , 0.5909894 , 0.5909894 , 0.59062776]),\n",
       " 'split8_test_precision': array([0.        , 0.65067179, 0.640625  , 0.62718847, 0.62439497,\n",
       "        0.61588785, 0.61775701, 0.61767442, 0.61824953, 0.61767442,\n",
       "        0.61767442, 0.61824953, 0.61767442, 0.61767442, 0.61767442,\n",
       "        0.61767442, 0.61767442, 0.61767442, 0.65067179, 0.62718847,\n",
       "        0.61588785, 0.61767442, 0.61767442, 0.61767442, 0.61767442,\n",
       "        0.61767442, 0.61767442, 0.61767442, 0.61767442, 0.61767442]),\n",
       " 'split9_test_precision': array([0.        , 0.63986014, 0.60429836, 0.60040161, 0.59521531,\n",
       "        0.5940131 , 0.59369202, 0.59354839, 0.59409594, 0.59354839,\n",
       "        0.59354839, 0.59354839, 0.59354839, 0.59354839, 0.59354839,\n",
       "        0.59354839, 0.59354839, 0.59354839, 0.63986014, 0.60040161,\n",
       "        0.5940131 , 0.59354839, 0.59354839, 0.59354839, 0.59354839,\n",
       "        0.59354839, 0.59354839, 0.59354839, 0.59354839, 0.59354839]),\n",
       " 'mean_test_precision': array([0.        , 0.64962278, 0.61850822, 0.60720534, 0.60281652,\n",
       "        0.60021343, 0.5995922 , 0.59952532, 0.59960137, 0.59952611,\n",
       "        0.59937219, 0.5995021 , 0.59948069, 0.59950493, 0.59942635,\n",
       "        0.59944368, 0.59948069, 0.59933302, 0.64956221, 0.60720534,\n",
       "        0.60019432, 0.59952532, 0.59952611, 0.59950192, 0.59950192,\n",
       "        0.59950192, 0.59950192, 0.59950192, 0.59949902, 0.59952611]),\n",
       " 'std_test_precision': array([0.        , 0.01416034, 0.01194153, 0.01221429, 0.01280803,\n",
       "        0.01090193, 0.01133945, 0.01111646, 0.01105937, 0.01095398,\n",
       "        0.01089671, 0.01093588, 0.01081056, 0.0108923 , 0.01084788,\n",
       "        0.01084865, 0.01081056, 0.01084449, 0.01412389, 0.01221429,\n",
       "        0.01083308, 0.01111646, 0.01095398, 0.0108715 , 0.0108715 ,\n",
       "        0.0108715 , 0.0108715 , 0.0108715 , 0.01080344, 0.01095398]),\n",
       " 'rank_test_precision': array([30,  1,  3,  4,  6,  7, 10, 14,  9, 11, 28, 17, 24, 16, 27, 26, 24,\n",
       "        29,  2,  4,  8, 14, 11, 18, 18, 18, 18, 18, 23, 11]),\n",
       " 'split0_test_f1_micro': array([0.86050751, 0.8686376 , 0.86932742, 0.87110126, 0.87134762,\n",
       "        0.87129835, 0.87129835, 0.87134762, 0.8713969 , 0.8713969 ,\n",
       "        0.87134762, 0.87134762, 0.87134762, 0.8713969 , 0.87134762,\n",
       "        0.87134762, 0.87134762, 0.87134762, 0.8686376 , 0.87110126,\n",
       "        0.87129835, 0.87134762, 0.8713969 , 0.87134762, 0.87134762,\n",
       "        0.87134762, 0.87134762, 0.87134762, 0.87134762, 0.8713969 ]),\n",
       " 'split1_test_f1_micro': array([0.86050751, 0.86848978, 0.86937669, 0.87179108, 0.87124908,\n",
       "        0.87129835, 0.87095344, 0.87090416, 0.87090416, 0.87090416,\n",
       "        0.87090416, 0.87090416, 0.87090416, 0.87090416, 0.87090416,\n",
       "        0.87090416, 0.87090416, 0.87090416, 0.86848978, 0.87179108,\n",
       "        0.87129835, 0.87090416, 0.87090416, 0.87090416, 0.87090416,\n",
       "        0.87090416, 0.87090416, 0.87090416, 0.87090416, 0.87090416]),\n",
       " 'split2_test_f1_micro': array([0.86050751, 0.86814486, 0.86932742, 0.87006652, 0.86996797,\n",
       "        0.8699187 , 0.86996797, 0.87016507, 0.87016507, 0.87021434,\n",
       "        0.87021434, 0.87021434, 0.87021434, 0.87021434, 0.87021434,\n",
       "        0.87016507, 0.87021434, 0.87021434, 0.86814486, 0.87006652,\n",
       "        0.8699187 , 0.87016507, 0.87021434, 0.87021434, 0.87021434,\n",
       "        0.87021434, 0.87021434, 0.87021434, 0.87021434, 0.87021434]),\n",
       " 'split3_test_f1_micro': array([0.86050751, 0.86735649, 0.86942597, 0.86952451, 0.86977088,\n",
       "        0.86967233, 0.86942597, 0.86957379, 0.86957379, 0.86957379,\n",
       "        0.86957379, 0.86957379, 0.86957379, 0.86957379, 0.86957379,\n",
       "        0.86957379, 0.86957379, 0.86957379, 0.86735649, 0.86952451,\n",
       "        0.86967233, 0.86957379, 0.86957379, 0.86957379, 0.86957379,\n",
       "        0.86957379, 0.86957379, 0.86957379, 0.86957379, 0.86957379]),\n",
       " 'split4_test_f1_micro': array([0.86054992, 0.86877895, 0.87055287, 0.87114418, 0.87143983,\n",
       "        0.87134128, 0.87143983, 0.87153839, 0.87158766, 0.87158766,\n",
       "        0.87153839, 0.87153839, 0.87153839, 0.87153839, 0.87153839,\n",
       "        0.87153839, 0.87153839, 0.87148911, 0.86877895, 0.87114418,\n",
       "        0.87139056, 0.87153839, 0.87158766, 0.87158766, 0.87158766,\n",
       "        0.87158766, 0.87158766, 0.87158766, 0.87153839, 0.87158766]),\n",
       " 'split5_test_f1_micro': array([0.86054992, 0.86981374, 0.87119346, 0.87262245, 0.87316448,\n",
       "        0.87301666, 0.87306593, 0.87301666, 0.8729181 , 0.8729181 ,\n",
       "        0.8729181 , 0.8729181 , 0.8729181 , 0.8729181 , 0.8729181 ,\n",
       "        0.8729181 , 0.8729181 , 0.8729181 , 0.86981374, 0.87262245,\n",
       "        0.87296738, 0.87301666, 0.8729181 , 0.8729181 , 0.8729181 ,\n",
       "        0.8729181 , 0.8729181 , 0.8729181 , 0.8729181 , 0.8729181 ]),\n",
       " 'split6_test_f1_micro': array([0.86054992, 0.86937026, 0.86951808, 0.87001084, 0.87015867,\n",
       "        0.87055287, 0.87055287, 0.8705036 , 0.87045432, 0.87045432,\n",
       "        0.87045432, 0.87045432, 0.87045432, 0.87045432, 0.87040505,\n",
       "        0.87045432, 0.87045432, 0.87040505, 0.86932098, 0.87001084,\n",
       "        0.87055287, 0.8705036 , 0.87045432, 0.87045432, 0.87045432,\n",
       "        0.87045432, 0.87045432, 0.87045432, 0.8705036 , 0.87045432]),\n",
       " 'split7_test_f1_micro': array([0.86054992, 0.86823692, 0.86932098, 0.86976446, 0.87025722,\n",
       "        0.87055287, 0.8707007 , 0.87055287, 0.87065142, 0.87065142,\n",
       "        0.87060215, 0.8707007 , 0.87074998, 0.8707007 , 0.87074998,\n",
       "        0.87074998, 0.87074998, 0.8707007 , 0.86823692, 0.86976446,\n",
       "        0.87055287, 0.87055287, 0.87065142, 0.8707007 , 0.8707007 ,\n",
       "        0.8707007 , 0.8707007 , 0.8707007 , 0.8707007 , 0.87065142]),\n",
       " 'split8_test_f1_micro': array([0.86054992, 0.86828619, 0.87119346, 0.872721  , 0.87321376,\n",
       "        0.87277028, 0.87296738, 0.87301666, 0.87306593, 0.87301666,\n",
       "        0.87301666, 0.87306593, 0.87301666, 0.87301666, 0.87301666,\n",
       "        0.87301666, 0.87301666, 0.87301666, 0.86828619, 0.872721  ,\n",
       "        0.87277028, 0.87301666, 0.87301666, 0.87301666, 0.87301666,\n",
       "        0.87301666, 0.87301666, 0.87301666, 0.87301666, 0.87301666]),\n",
       " 'split9_test_f1_micro': array([0.86054992, 0.86843402, 0.8686804 , 0.87040505, 0.87035577,\n",
       "        0.87045432, 0.8705036 , 0.87055287, 0.87060215, 0.87055287,\n",
       "        0.87055287, 0.87055287, 0.87055287, 0.87055287, 0.87055287,\n",
       "        0.87055287, 0.87055287, 0.87055287, 0.86843402, 0.87040505,\n",
       "        0.87045432, 0.87055287, 0.87055287, 0.87055287, 0.87055287,\n",
       "        0.87055287, 0.87055287, 0.87055287, 0.87055287, 0.87055287]),\n",
       " 'mean_test_f1_micro': array([0.86053296, 0.86855488, 0.86979168, 0.87091514, 0.87109253,\n",
       "        0.8710876 , 0.8710876 , 0.87111717, 0.87113195, 0.87112702,\n",
       "        0.87111224, 0.87112702, 0.87112702, 0.87112702, 0.8711221 ,\n",
       "        0.8711221 , 0.87112702, 0.87111224, 0.86854995, 0.87091514,\n",
       "        0.8710876 , 0.87111717, 0.87112702, 0.87112702, 0.87112702,\n",
       "        0.87112702, 0.87112702, 0.87112702, 0.87112702, 0.87112702]),\n",
       " 'std_test_f1_micro': array([2.07726848e-05, 6.40546595e-04, 8.23271507e-04, 1.09938289e-03,\n",
       "        1.18856386e-03, 1.05109747e-03, 1.11468034e-03, 1.08599542e-03,\n",
       "        1.07674963e-03, 1.06617410e-03, 1.06523472e-03, 1.06978257e-03,\n",
       "        1.05906124e-03, 1.06206689e-03, 1.06228941e-03, 1.06340180e-03,\n",
       "        1.05906124e-03, 1.06226738e-03, 6.34415338e-04, 1.09938289e-03,\n",
       "        1.04344645e-03, 1.08599542e-03, 1.06617410e-03, 1.06295165e-03,\n",
       "        1.06295165e-03, 1.06295165e-03, 1.06295165e-03, 1.06295165e-03,\n",
       "        1.05791428e-03, 1.06617410e-03]),\n",
       " 'rank_test_f1_micro': array([30, 28, 27, 25, 21, 23, 22, 17,  1, 11, 19,  2,  2, 11, 16, 15,  2,\n",
       "        19, 29, 25, 23, 17, 11,  2,  2,  2,  2,  2,  2, 11])}"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_model.cv_results_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "50a53953",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([30, 28, 27, 25, 21, 23, 22, 17,  1, 11, 19,  2,  2, 11, 16, 15,  2,\n",
       "       19, 29, 25, 23, 17, 11,  2,  2,  2,  2,  2,  2, 11])"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_model.cv_results_['rank_test_accuracy']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "ed852058",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'classifier': LogisticRegression(max_iter=10000),\n",
       " 'classifier__C': 0.0001,\n",
       " 'classifier__penalty': 'l1',\n",
       " 'classifier__solver': 'saga'}"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_model.cv_results_['params'][ np.argmin(best_model.cv_results_['mean_test_accuracy'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "cb83803b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>classifier</th>\n",
       "      <th>C</th>\n",
       "      <th>penalty</th>\n",
       "      <th>solver</th>\n",
       "      <th>score_acc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.860533</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.868555</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.869792</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.870915</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0100</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.871093</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0100</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.871088</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.1000</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.871088</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.1000</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.871117</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.871132</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.871127</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>10.0000</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.871112</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>10.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.871127</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>100.0000</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.871127</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>100.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.871127</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>1000.0000</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.871122</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>1000.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.871122</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>10000.0000</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.871127</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>10000.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.871112</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.868550</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.870915</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0100</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.871088</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.1000</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.871117</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.871127</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>10.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.871127</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>100.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.871127</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>1000.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.871127</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>10000.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.871127</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>NaN</td>\n",
       "      <td>none</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.871127</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>NaN</td>\n",
       "      <td>none</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.871127</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.871127</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                            classifier           C penalty solver  score_acc\n",
       "0   LogisticRegression(max_iter=10000)      0.0001      l1   saga   0.860533\n",
       "1   LogisticRegression(max_iter=10000)      0.0001      l2   saga   0.868555\n",
       "2   LogisticRegression(max_iter=10000)      0.0010      l1   saga   0.869792\n",
       "3   LogisticRegression(max_iter=10000)      0.0010      l2   saga   0.870915\n",
       "4   LogisticRegression(max_iter=10000)      0.0100      l1   saga   0.871093\n",
       "5   LogisticRegression(max_iter=10000)      0.0100      l2   saga   0.871088\n",
       "6   LogisticRegression(max_iter=10000)      0.1000      l1   saga   0.871088\n",
       "7   LogisticRegression(max_iter=10000)      0.1000      l2   saga   0.871117\n",
       "8   LogisticRegression(max_iter=10000)      1.0000      l1   saga   0.871132\n",
       "9   LogisticRegression(max_iter=10000)      1.0000      l2   saga   0.871127\n",
       "10  LogisticRegression(max_iter=10000)     10.0000      l1   saga   0.871112\n",
       "11  LogisticRegression(max_iter=10000)     10.0000      l2   saga   0.871127\n",
       "12  LogisticRegression(max_iter=10000)    100.0000      l1   saga   0.871127\n",
       "13  LogisticRegression(max_iter=10000)    100.0000      l2   saga   0.871127\n",
       "14  LogisticRegression(max_iter=10000)   1000.0000      l1   saga   0.871122\n",
       "15  LogisticRegression(max_iter=10000)   1000.0000      l2   saga   0.871122\n",
       "16  LogisticRegression(max_iter=10000)  10000.0000      l1   saga   0.871127\n",
       "17  LogisticRegression(max_iter=10000)  10000.0000      l2   saga   0.871112\n",
       "18  LogisticRegression(max_iter=10000)      0.0001      l2  lbfgs   0.868550\n",
       "19  LogisticRegression(max_iter=10000)      0.0010      l2  lbfgs   0.870915\n",
       "20  LogisticRegression(max_iter=10000)      0.0100      l2  lbfgs   0.871088\n",
       "21  LogisticRegression(max_iter=10000)      0.1000      l2  lbfgs   0.871117\n",
       "22  LogisticRegression(max_iter=10000)      1.0000      l2  lbfgs   0.871127\n",
       "23  LogisticRegression(max_iter=10000)     10.0000      l2  lbfgs   0.871127\n",
       "24  LogisticRegression(max_iter=10000)    100.0000      l2  lbfgs   0.871127\n",
       "25  LogisticRegression(max_iter=10000)   1000.0000      l2  lbfgs   0.871127\n",
       "26  LogisticRegression(max_iter=10000)  10000.0000      l2  lbfgs   0.871127\n",
       "27  LogisticRegression(max_iter=10000)         NaN    none  lbfgs   0.871127\n",
       "28  LogisticRegression(max_iter=10000)         NaN    none   saga   0.871127\n",
       "29  LogisticRegression(max_iter=10000)         NaN     NaN    NaN   0.871127"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results = pd.DataFrame( best_model.cv_results_['params'] )\n",
    "\n",
    "results['score_acc'] = best_model.cv_results_['mean_test_accuracy']\n",
    "cols = results.columns.to_series().str.split('__').apply(lambda x: x[-1])\n",
    "results.columns = cols\n",
    "\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "e5605c9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "acc_list.append(results[0:1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "687c473b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'classifier': LogisticRegression(max_iter=10000),\n",
       " 'classifier__C': 0.0001,\n",
       " 'classifier__penalty': 'l2',\n",
       " 'classifier__solver': 'saga'}"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_model.cv_results_['params'][ np.argmin(best_model.cv_results_['rank_test_precision'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "f9394193",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>classifier</th>\n",
       "      <th>C</th>\n",
       "      <th>penalty</th>\n",
       "      <th>solver</th>\n",
       "      <th>score_precision</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.649623</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.618508</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.607205</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0100</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.602817</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0100</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.600213</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.1000</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.599592</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.1000</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.599525</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.599601</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.599526</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>10.0000</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.599372</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>10.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.599502</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>100.0000</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.599481</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>100.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.599505</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>1000.0000</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.599426</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>1000.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.599444</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>10000.0000</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.599481</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>10000.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.599333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.649562</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.607205</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0100</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.600194</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.1000</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.599525</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.599526</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>10.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.599502</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>100.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.599502</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>1000.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.599502</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>10000.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.599502</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>NaN</td>\n",
       "      <td>none</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.599502</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>NaN</td>\n",
       "      <td>none</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.599499</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.599526</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                            classifier           C penalty solver  \\\n",
       "0   LogisticRegression(max_iter=10000)      0.0001      l1   saga   \n",
       "1   LogisticRegression(max_iter=10000)      0.0001      l2   saga   \n",
       "2   LogisticRegression(max_iter=10000)      0.0010      l1   saga   \n",
       "3   LogisticRegression(max_iter=10000)      0.0010      l2   saga   \n",
       "4   LogisticRegression(max_iter=10000)      0.0100      l1   saga   \n",
       "5   LogisticRegression(max_iter=10000)      0.0100      l2   saga   \n",
       "6   LogisticRegression(max_iter=10000)      0.1000      l1   saga   \n",
       "7   LogisticRegression(max_iter=10000)      0.1000      l2   saga   \n",
       "8   LogisticRegression(max_iter=10000)      1.0000      l1   saga   \n",
       "9   LogisticRegression(max_iter=10000)      1.0000      l2   saga   \n",
       "10  LogisticRegression(max_iter=10000)     10.0000      l1   saga   \n",
       "11  LogisticRegression(max_iter=10000)     10.0000      l2   saga   \n",
       "12  LogisticRegression(max_iter=10000)    100.0000      l1   saga   \n",
       "13  LogisticRegression(max_iter=10000)    100.0000      l2   saga   \n",
       "14  LogisticRegression(max_iter=10000)   1000.0000      l1   saga   \n",
       "15  LogisticRegression(max_iter=10000)   1000.0000      l2   saga   \n",
       "16  LogisticRegression(max_iter=10000)  10000.0000      l1   saga   \n",
       "17  LogisticRegression(max_iter=10000)  10000.0000      l2   saga   \n",
       "18  LogisticRegression(max_iter=10000)      0.0001      l2  lbfgs   \n",
       "19  LogisticRegression(max_iter=10000)      0.0010      l2  lbfgs   \n",
       "20  LogisticRegression(max_iter=10000)      0.0100      l2  lbfgs   \n",
       "21  LogisticRegression(max_iter=10000)      0.1000      l2  lbfgs   \n",
       "22  LogisticRegression(max_iter=10000)      1.0000      l2  lbfgs   \n",
       "23  LogisticRegression(max_iter=10000)     10.0000      l2  lbfgs   \n",
       "24  LogisticRegression(max_iter=10000)    100.0000      l2  lbfgs   \n",
       "25  LogisticRegression(max_iter=10000)   1000.0000      l2  lbfgs   \n",
       "26  LogisticRegression(max_iter=10000)  10000.0000      l2  lbfgs   \n",
       "27  LogisticRegression(max_iter=10000)         NaN    none  lbfgs   \n",
       "28  LogisticRegression(max_iter=10000)         NaN    none   saga   \n",
       "29  LogisticRegression(max_iter=10000)         NaN     NaN    NaN   \n",
       "\n",
       "    score_precision  \n",
       "0          0.000000  \n",
       "1          0.649623  \n",
       "2          0.618508  \n",
       "3          0.607205  \n",
       "4          0.602817  \n",
       "5          0.600213  \n",
       "6          0.599592  \n",
       "7          0.599525  \n",
       "8          0.599601  \n",
       "9          0.599526  \n",
       "10         0.599372  \n",
       "11         0.599502  \n",
       "12         0.599481  \n",
       "13         0.599505  \n",
       "14         0.599426  \n",
       "15         0.599444  \n",
       "16         0.599481  \n",
       "17         0.599333  \n",
       "18         0.649562  \n",
       "19         0.607205  \n",
       "20         0.600194  \n",
       "21         0.599525  \n",
       "22         0.599526  \n",
       "23         0.599502  \n",
       "24         0.599502  \n",
       "25         0.599502  \n",
       "26         0.599502  \n",
       "27         0.599502  \n",
       "28         0.599499  \n",
       "29         0.599526  "
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results = pd.DataFrame( best_model.cv_results_['params'] )\n",
    "\n",
    "results['score_precision'] = best_model.cv_results_['mean_test_precision']\n",
    "cols = results.columns.to_series().str.split('__').apply(lambda x: x[-1])\n",
    "results.columns = cols\n",
    "\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "f6cdf507",
   "metadata": {},
   "outputs": [],
   "source": [
    "roc_auc_ovr_List.append(results[1:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "342313d7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'classifier': LogisticRegression(max_iter=10000),\n",
       " 'classifier__C': 1.0,\n",
       " 'classifier__penalty': 'l1',\n",
       " 'classifier__solver': 'saga'}"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_model.cv_results_['params'][ np.argmin(best_model.cv_results_['rank_test_f1_micro'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "2d1e6c10",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>classifier</th>\n",
       "      <th>C</th>\n",
       "      <th>penalty</th>\n",
       "      <th>solver</th>\n",
       "      <th>score_f1_micro</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.860533</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.868555</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.869792</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.870915</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0100</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.871093</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0100</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.871088</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.1000</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.871088</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.1000</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.871117</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.871132</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.871127</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>10.0000</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.871112</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>10.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.871127</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>100.0000</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.871127</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>100.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.871127</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>1000.0000</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.871122</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>1000.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.871122</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>10000.0000</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.871127</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>10000.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.871112</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.868550</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.870915</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0100</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.871088</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.1000</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.871117</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.871127</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>10.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.871127</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>100.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.871127</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>1000.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.871127</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>10000.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.871127</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>NaN</td>\n",
       "      <td>none</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.871127</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>NaN</td>\n",
       "      <td>none</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.871127</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.871127</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                            classifier           C penalty solver  \\\n",
       "0   LogisticRegression(max_iter=10000)      0.0001      l1   saga   \n",
       "1   LogisticRegression(max_iter=10000)      0.0001      l2   saga   \n",
       "2   LogisticRegression(max_iter=10000)      0.0010      l1   saga   \n",
       "3   LogisticRegression(max_iter=10000)      0.0010      l2   saga   \n",
       "4   LogisticRegression(max_iter=10000)      0.0100      l1   saga   \n",
       "5   LogisticRegression(max_iter=10000)      0.0100      l2   saga   \n",
       "6   LogisticRegression(max_iter=10000)      0.1000      l1   saga   \n",
       "7   LogisticRegression(max_iter=10000)      0.1000      l2   saga   \n",
       "8   LogisticRegression(max_iter=10000)      1.0000      l1   saga   \n",
       "9   LogisticRegression(max_iter=10000)      1.0000      l2   saga   \n",
       "10  LogisticRegression(max_iter=10000)     10.0000      l1   saga   \n",
       "11  LogisticRegression(max_iter=10000)     10.0000      l2   saga   \n",
       "12  LogisticRegression(max_iter=10000)    100.0000      l1   saga   \n",
       "13  LogisticRegression(max_iter=10000)    100.0000      l2   saga   \n",
       "14  LogisticRegression(max_iter=10000)   1000.0000      l1   saga   \n",
       "15  LogisticRegression(max_iter=10000)   1000.0000      l2   saga   \n",
       "16  LogisticRegression(max_iter=10000)  10000.0000      l1   saga   \n",
       "17  LogisticRegression(max_iter=10000)  10000.0000      l2   saga   \n",
       "18  LogisticRegression(max_iter=10000)      0.0001      l2  lbfgs   \n",
       "19  LogisticRegression(max_iter=10000)      0.0010      l2  lbfgs   \n",
       "20  LogisticRegression(max_iter=10000)      0.0100      l2  lbfgs   \n",
       "21  LogisticRegression(max_iter=10000)      0.1000      l2  lbfgs   \n",
       "22  LogisticRegression(max_iter=10000)      1.0000      l2  lbfgs   \n",
       "23  LogisticRegression(max_iter=10000)     10.0000      l2  lbfgs   \n",
       "24  LogisticRegression(max_iter=10000)    100.0000      l2  lbfgs   \n",
       "25  LogisticRegression(max_iter=10000)   1000.0000      l2  lbfgs   \n",
       "26  LogisticRegression(max_iter=10000)  10000.0000      l2  lbfgs   \n",
       "27  LogisticRegression(max_iter=10000)         NaN    none  lbfgs   \n",
       "28  LogisticRegression(max_iter=10000)         NaN    none   saga   \n",
       "29  LogisticRegression(max_iter=10000)         NaN     NaN    NaN   \n",
       "\n",
       "    score_f1_micro  \n",
       "0         0.860533  \n",
       "1         0.868555  \n",
       "2         0.869792  \n",
       "3         0.870915  \n",
       "4         0.871093  \n",
       "5         0.871088  \n",
       "6         0.871088  \n",
       "7         0.871117  \n",
       "8         0.871132  \n",
       "9         0.871127  \n",
       "10        0.871112  \n",
       "11        0.871127  \n",
       "12        0.871127  \n",
       "13        0.871127  \n",
       "14        0.871122  \n",
       "15        0.871122  \n",
       "16        0.871127  \n",
       "17        0.871112  \n",
       "18        0.868550  \n",
       "19        0.870915  \n",
       "20        0.871088  \n",
       "21        0.871117  \n",
       "22        0.871127  \n",
       "23        0.871127  \n",
       "24        0.871127  \n",
       "25        0.871127  \n",
       "26        0.871127  \n",
       "27        0.871127  \n",
       "28        0.871127  \n",
       "29        0.871127  "
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results = pd.DataFrame( best_model.cv_results_['params'] )\n",
    "\n",
    "results['score_f1_micro'] = best_model.cv_results_['mean_test_f1_micro']\n",
    "cols = results.columns.to_series().str.split('__').apply(lambda x: x[-1])\n",
    "results.columns = cols\n",
    "\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "cd4d8e65",
   "metadata": {},
   "outputs": [],
   "source": [
    "f1_micro_List.append(results[8:9])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c967a56e",
   "metadata": {},
   "source": [
    "## Trial 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "1542f63b",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, Y_train, Y_test = train_test_split(X, y, test_size=0.2, shuffle=True, random_state = 7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "5380e5ce",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Guest-All\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\Guest-All\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\Guest-All\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\Guest-All\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\Guest-All\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\Guest-All\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\Guest-All\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\Guest-All\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\Guest-All\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\Guest-All\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\Guest-All\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1183: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Guest-All\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1183: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Guest-All\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1183: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Guest-All\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1183: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Guest-All\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1183: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Guest-All\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1183: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Guest-All\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1183: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Guest-All\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1183: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Guest-All\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1183: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Guest-All\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1183: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Guest-All\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1183: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Guest-All\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1183: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Guest-All\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1183: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Guest-All\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1183: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Guest-All\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1183: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Guest-All\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1183: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Guest-All\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1183: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Guest-All\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1183: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Guest-All\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1183: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Guest-All\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1183: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Create a pipeline - RF is a stand in, we will populate the classifier part below\n",
    "pipe = Pipeline([('std', StandardScaler()),\n",
    "                 ('classifier', LogisticRegression())])\n",
    "\n",
    "# Create search space of candidate learning algorithms and their hyperparameters\n",
    "# note lbfgs can't do l1, and if you pass penalty='none' it expects no C value\n",
    "search_space = [{'classifier': [LogisticRegression(max_iter=10000)],\n",
    "                 'classifier__solver': ['saga'],\n",
    "                 'classifier__penalty': ['l1', 'l2'],\n",
    "                 'classifier__C': np.logspace(-4, 4, 9)},\n",
    "                {'classifier': [LogisticRegression(max_iter=10000)],\n",
    "                 'classifier__solver': ['lbfgs'],\n",
    "                 'classifier__penalty': ['l2'],\n",
    "                 'classifier__C': np.logspace(-4, 4, 9)},\n",
    "                {'classifier': [LogisticRegression(max_iter=10000)],\n",
    "                 'classifier__solver': ['lbfgs','saga'],\n",
    "                 'classifier__penalty': ['none']},\n",
    "                {'classifier': [LogisticRegression(max_iter=10000)]}\n",
    "                ]\n",
    "\n",
    "# Create grid search\n",
    "clf = GridSearchCV(pipe, search_space, cv=StratifiedKFold(n_splits=10),\n",
    "                   scoring=['accuracy', 'precision', 'f1_micro'], refit=False,\n",
    "                   verbose=0)\n",
    "\n",
    "# Fit grid search\n",
    "best_model = clf.fit(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "2795f9d4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'mean_fit_time': array([ 3.65122905,  3.13813226,  3.70288281,  3.23497024,  3.84335847,\n",
       "         3.41534805,  7.09110096,  3.8016108 , 25.94321277,  3.18369076,\n",
       "         3.80366139,  3.25497293,  3.98476553,  3.21794021,  4.18425422,\n",
       "         3.0496244 ,  4.28200037,  3.02222776,  0.54379284,  0.72130213,\n",
       "         0.82497725,  0.80664604,  0.82014892,  0.82756445,  0.78576009,\n",
       "         0.84258883,  0.81800382,  0.820227  ,  3.14792671,  0.73430715]),\n",
       " 'std_fit_time': array([0.29974356, 0.15869736, 0.30259376, 0.33021399, 0.28688332,\n",
       "        0.29547621, 6.08328629, 0.41433938, 4.87869132, 0.17572368,\n",
       "        0.39795355, 0.22587859, 0.41172631, 0.30335227, 0.27915499,\n",
       "        0.4791843 , 0.5253305 , 0.39589353, 0.01264058, 0.05262125,\n",
       "        0.02815419, 0.02349476, 0.01887277, 0.02571198, 0.04329607,\n",
       "        0.01050822, 0.0323959 , 0.01319452, 0.26507134, 0.05994588]),\n",
       " 'mean_score_time': array([0.03290174, 0.03449361, 0.0337595 , 0.03486841, 0.03458583,\n",
       "        0.0343286 , 0.03178513, 0.04151747, 0.03886373, 0.0330442 ,\n",
       "        0.03012004, 0.03134167, 0.03088858, 0.03017552, 0.03244078,\n",
       "        0.03166213, 0.03293405, 0.03272252, 0.03473101, 0.0353482 ,\n",
       "        0.03147349, 0.03235009, 0.03215957, 0.03241675, 0.03072743,\n",
       "        0.03256273, 0.03195498, 0.03241365, 0.03471568, 0.02785292]),\n",
       " 'std_score_time': array([0.00541448, 0.0017689 , 0.00381133, 0.00395602, 0.0021143 ,\n",
       "        0.00258536, 0.00547568, 0.01190582, 0.01696417, 0.00108369,\n",
       "        0.0041053 , 0.00461677, 0.00369556, 0.00391142, 0.00213763,\n",
       "        0.00391634, 0.00200564, 0.00453587, 0.00054366, 0.00212006,\n",
       "        0.00324888, 0.00256862, 0.00109398, 0.00148841, 0.00317057,\n",
       "        0.00249319, 0.0028019 , 0.00226586, 0.00600413, 0.00638325]),\n",
       " 'param_classifier': masked_array(data=[LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000)],\n",
       "              mask=[False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'param_classifier__C': masked_array(data=[0.0001, 0.0001, 0.001, 0.001, 0.01, 0.01, 0.1, 0.1,\n",
       "                    1.0, 1.0, 10.0, 10.0, 100.0, 100.0, 1000.0, 1000.0,\n",
       "                    10000.0, 10000.0, 0.0001, 0.001, 0.01, 0.1, 1.0, 10.0,\n",
       "                    100.0, 1000.0, 10000.0, --, --, --],\n",
       "              mask=[False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False,  True,  True,  True],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'param_classifier__penalty': masked_array(data=['l1', 'l2', 'l1', 'l2', 'l1', 'l2', 'l1', 'l2', 'l1',\n",
       "                    'l2', 'l1', 'l2', 'l1', 'l2', 'l1', 'l2', 'l1', 'l2',\n",
       "                    'l2', 'l2', 'l2', 'l2', 'l2', 'l2', 'l2', 'l2', 'l2',\n",
       "                    'none', 'none', --],\n",
       "              mask=[False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False,  True],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'param_classifier__solver': masked_array(data=['saga', 'saga', 'saga', 'saga', 'saga', 'saga', 'saga',\n",
       "                    'saga', 'saga', 'saga', 'saga', 'saga', 'saga', 'saga',\n",
       "                    'saga', 'saga', 'saga', 'saga', 'lbfgs', 'lbfgs',\n",
       "                    'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs',\n",
       "                    'lbfgs', 'lbfgs', 'saga', --],\n",
       "              mask=[False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False,  True],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'params': [{'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 0.0001,\n",
       "   'classifier__penalty': 'l1',\n",
       "   'classifier__solver': 'saga'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 0.0001,\n",
       "   'classifier__penalty': 'l2',\n",
       "   'classifier__solver': 'saga'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 0.001,\n",
       "   'classifier__penalty': 'l1',\n",
       "   'classifier__solver': 'saga'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 0.001,\n",
       "   'classifier__penalty': 'l2',\n",
       "   'classifier__solver': 'saga'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 0.01,\n",
       "   'classifier__penalty': 'l1',\n",
       "   'classifier__solver': 'saga'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 0.01,\n",
       "   'classifier__penalty': 'l2',\n",
       "   'classifier__solver': 'saga'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 0.1,\n",
       "   'classifier__penalty': 'l1',\n",
       "   'classifier__solver': 'saga'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 0.1,\n",
       "   'classifier__penalty': 'l2',\n",
       "   'classifier__solver': 'saga'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 1.0,\n",
       "   'classifier__penalty': 'l1',\n",
       "   'classifier__solver': 'saga'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 1.0,\n",
       "   'classifier__penalty': 'l2',\n",
       "   'classifier__solver': 'saga'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 10.0,\n",
       "   'classifier__penalty': 'l1',\n",
       "   'classifier__solver': 'saga'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 10.0,\n",
       "   'classifier__penalty': 'l2',\n",
       "   'classifier__solver': 'saga'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 100.0,\n",
       "   'classifier__penalty': 'l1',\n",
       "   'classifier__solver': 'saga'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 100.0,\n",
       "   'classifier__penalty': 'l2',\n",
       "   'classifier__solver': 'saga'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 1000.0,\n",
       "   'classifier__penalty': 'l1',\n",
       "   'classifier__solver': 'saga'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 1000.0,\n",
       "   'classifier__penalty': 'l2',\n",
       "   'classifier__solver': 'saga'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 10000.0,\n",
       "   'classifier__penalty': 'l1',\n",
       "   'classifier__solver': 'saga'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 10000.0,\n",
       "   'classifier__penalty': 'l2',\n",
       "   'classifier__solver': 'saga'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 0.0001,\n",
       "   'classifier__penalty': 'l2',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 0.001,\n",
       "   'classifier__penalty': 'l2',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 0.01,\n",
       "   'classifier__penalty': 'l2',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 0.1,\n",
       "   'classifier__penalty': 'l2',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 1.0,\n",
       "   'classifier__penalty': 'l2',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 10.0,\n",
       "   'classifier__penalty': 'l2',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 100.0,\n",
       "   'classifier__penalty': 'l2',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 1000.0,\n",
       "   'classifier__penalty': 'l2',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 10000.0,\n",
       "   'classifier__penalty': 'l2',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__penalty': 'none',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__penalty': 'none',\n",
       "   'classifier__solver': 'saga'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000)}],\n",
       " 'split0_test_accuracy': array([0.86129589, 0.8677014 , 0.86799704, 0.8684405 , 0.86829268,\n",
       "        0.86819414, 0.86804632, 0.8678985 , 0.8678985 , 0.8678985 ,\n",
       "        0.8678985 , 0.8678985 , 0.8678985 , 0.8678985 , 0.8678985 ,\n",
       "        0.8678985 , 0.8678985 , 0.8678985 , 0.8677014 , 0.8684405 ,\n",
       "        0.86819414, 0.8678985 , 0.8678985 , 0.8678985 , 0.8678985 ,\n",
       "        0.8678985 , 0.8678985 , 0.8678985 , 0.8678985 , 0.8678985 ]),\n",
       " 'split1_test_accuracy': array([0.86129589, 0.86730722, 0.86686376, 0.86755358, 0.86706085,\n",
       "        0.86765213, 0.86784922, 0.86779995, 0.86794777, 0.86794777,\n",
       "        0.86794777, 0.86794777, 0.86794777, 0.86794777, 0.86794777,\n",
       "        0.86794777, 0.86794777, 0.86794777, 0.86730722, 0.86750431,\n",
       "        0.86765213, 0.86779995, 0.86794777, 0.86794777, 0.86794777,\n",
       "        0.86794777, 0.86794777, 0.86794777, 0.86794777, 0.86794777]),\n",
       " 'split2_test_accuracy': array([0.86124661, 0.86898251, 0.87021434, 0.87144617, 0.87124908,\n",
       "        0.87129835, 0.87115053, 0.87124908, 0.8711998 , 0.87115053,\n",
       "        0.87115053, 0.87115053, 0.87115053, 0.87115053, 0.87115053,\n",
       "        0.87115053, 0.87115053, 0.87115053, 0.86898251, 0.87144617,\n",
       "        0.87129835, 0.87124908, 0.87115053, 0.87115053, 0.87115053,\n",
       "        0.87115053, 0.87115053, 0.87115053, 0.87115053, 0.87115053]),\n",
       " 'split3_test_accuracy': array([0.86124661, 0.86908105, 0.87075634, 0.87184035, 0.87154472,\n",
       "        0.87144617, 0.87159399, 0.87154472, 0.87154472, 0.87154472,\n",
       "        0.87154472, 0.87154472, 0.87154472, 0.87154472, 0.87154472,\n",
       "        0.87154472, 0.87154472, 0.87154472, 0.86908105, 0.87184035,\n",
       "        0.87144617, 0.87154472, 0.87154472, 0.87154472, 0.87154472,\n",
       "        0.87154472, 0.87154472, 0.87154472, 0.87154472, 0.87154472]),\n",
       " 'split4_test_accuracy': array([0.86128905, 0.87035577, 0.87188332, 0.87360796, 0.87346014,\n",
       "        0.87321376, 0.87311521, 0.87331231, 0.87326303, 0.87331231,\n",
       "        0.87326303, 0.87331231, 0.87326303, 0.87326303, 0.87331231,\n",
       "        0.87326303, 0.87326303, 0.87326303, 0.87035577, 0.87360796,\n",
       "        0.87321376, 0.87331231, 0.87331231, 0.87326303, 0.87326303,\n",
       "        0.87326303, 0.87326303, 0.87326303, 0.87326303, 0.87331231]),\n",
       " 'split5_test_accuracy': array([0.86128905, 0.86971519, 0.8723268 , 0.87326303, 0.87355869,\n",
       "        0.87390362, 0.87375579, 0.87375579, 0.87375579, 0.87375579,\n",
       "        0.87375579, 0.87375579, 0.87375579, 0.87375579, 0.87375579,\n",
       "        0.87375579, 0.87375579, 0.87375579, 0.86971519, 0.87326303,\n",
       "        0.87390362, 0.87375579, 0.87375579, 0.87375579, 0.87375579,\n",
       "        0.87375579, 0.87375579, 0.87375579, 0.87375579, 0.87375579]),\n",
       " 'split6_test_accuracy': array([0.86128905, 0.86843402, 0.87114418, 0.87178476, 0.87168621,\n",
       "        0.87173549, 0.87163694, 0.87163694, 0.87173549, 0.87173549,\n",
       "        0.87173549, 0.87173549, 0.87173549, 0.87173549, 0.87173549,\n",
       "        0.87173549, 0.87173549, 0.87173549, 0.86843402, 0.87178476,\n",
       "        0.87173549, 0.87163694, 0.87173549, 0.87173549, 0.87173549,\n",
       "        0.87173549, 0.87173549, 0.87173549, 0.87173549, 0.87173549]),\n",
       " 'split7_test_accuracy': array([0.86128905, 0.86996156, 0.87139056, 0.87350941, 0.87336158,\n",
       "        0.87326303, 0.87301666, 0.87306593, 0.87311521, 0.87311521,\n",
       "        0.87311521, 0.87316448, 0.87311521, 0.87316448, 0.87316448,\n",
       "        0.87316448, 0.87316448, 0.87316448, 0.86996156, 0.87350941,\n",
       "        0.87326303, 0.87306593, 0.87316448, 0.87316448, 0.87316448,\n",
       "        0.87316448, 0.87316448, 0.87316448, 0.87316448, 0.87316448]),\n",
       " 'split8_test_accuracy': array([0.86128905, 0.86961664, 0.87286883, 0.87203114, 0.872721  ,\n",
       "        0.8729181 , 0.87316448, 0.87301666, 0.87301666, 0.87301666,\n",
       "        0.87301666, 0.87301666, 0.87301666, 0.87301666, 0.87301666,\n",
       "        0.87301666, 0.87301666, 0.87301666, 0.86961664, 0.87203114,\n",
       "        0.8729181 , 0.87301666, 0.87301666, 0.87301666, 0.87301666,\n",
       "        0.87301666, 0.87301666, 0.87301666, 0.87301666, 0.87301666]),\n",
       " 'split9_test_accuracy': array([0.86128905, 0.86912388, 0.87084853, 0.87257317, 0.87286883,\n",
       "        0.87262245, 0.87277028, 0.872721  , 0.872721  , 0.872721  ,\n",
       "        0.872721  , 0.872721  , 0.872721  , 0.872721  , 0.872721  ,\n",
       "        0.872721  , 0.872721  , 0.872721  , 0.86912388, 0.87257317,\n",
       "        0.87262245, 0.872721  , 0.872721  , 0.872721  , 0.872721  ,\n",
       "        0.872721  , 0.872721  , 0.872721  , 0.872721  , 0.872721  ]),\n",
       " 'mean_test_accuracy': array([0.86128193, 0.86902792, 0.87062937, 0.87160501, 0.87158038,\n",
       "        0.87162472, 0.87160994, 0.87160009, 0.8716198 , 0.8716198 ,\n",
       "        0.87161487, 0.87162472, 0.87161487, 0.8716198 , 0.87162472,\n",
       "        0.8716198 , 0.8716198 , 0.8716198 , 0.86902792, 0.87160008,\n",
       "        0.87162472, 0.87160009, 0.87162472, 0.8716198 , 0.8716198 ,\n",
       "        0.8716198 , 0.8716198 , 0.8716198 , 0.8716198 , 0.87162472]),\n",
       " 'std_test_accuracy': array([1.78561653e-05, 9.24179042e-04, 1.77928674e-03, 1.95057998e-03,\n",
       "        2.11986567e-03, 2.01920340e-03, 1.99165172e-03, 2.03128056e-03,\n",
       "        2.00474620e-03, 2.00993187e-03, 2.00583267e-03, 2.01364895e-03,\n",
       "        2.00583267e-03, 2.00956942e-03, 2.01364895e-03, 2.00956942e-03,\n",
       "        2.00956942e-03, 2.00956942e-03, 9.24179042e-04, 1.96084321e-03,\n",
       "        2.01920340e-03, 2.03128056e-03, 2.01364895e-03, 2.00956942e-03,\n",
       "        2.00956942e-03, 2.00956942e-03, 2.00956942e-03, 2.00956942e-03,\n",
       "        2.00956942e-03, 2.01364895e-03]),\n",
       " 'rank_test_accuracy': array([30, 28, 27, 22, 26,  5, 21, 23, 18,  7, 19,  1, 19,  8,  1,  8,  8,\n",
       "         8, 28, 25,  5, 23,  1,  8,  8,  8,  8,  8,  8,  1]),\n",
       " 'split0_test_precision': array([0.        , 0.62310606, 0.58585859, 0.57435897, 0.56906615,\n",
       "        0.56542056, 0.56348471, 0.56203704, 0.56203704, 0.56203704,\n",
       "        0.56203704, 0.56203704, 0.56203704, 0.56203704, 0.56203704,\n",
       "        0.56203704, 0.56203704, 0.56203704, 0.62310606, 0.57435897,\n",
       "        0.56542056, 0.56203704, 0.56203704, 0.56203704, 0.56203704,\n",
       "        0.56203704, 0.56203704, 0.56203704, 0.56203704, 0.56203704]),\n",
       " 'split1_test_precision': array([0.        , 0.62055336, 0.57836338, 0.56663169, 0.55752212,\n",
       "        0.56231884, 0.56363636, 0.56321839, 0.56446991, 0.56446991,\n",
       "        0.56446991, 0.56446991, 0.56446991, 0.56446991, 0.56446991,\n",
       "        0.56446991, 0.56446991, 0.56446991, 0.62055336, 0.56603774,\n",
       "        0.56231884, 0.56321839, 0.56446991, 0.56446991, 0.56446991,\n",
       "        0.56446991, 0.56446991, 0.56446991, 0.56446991, 0.56446991]),\n",
       " 'split2_test_precision': array([0.        , 0.65125241, 0.61636829, 0.60703206, 0.60039565,\n",
       "        0.5984556 , 0.59598854, 0.59675882, 0.59619048, 0.59562322,\n",
       "        0.59562322, 0.59562322, 0.59562322, 0.59562322, 0.59562322,\n",
       "        0.59562322, 0.59562322, 0.59562322, 0.65125241, 0.60703206,\n",
       "        0.5984556 , 0.59675882, 0.59562322, 0.59562322, 0.59562322,\n",
       "        0.59562322, 0.59562322, 0.59562322, 0.59562322, 0.59562322]),\n",
       " 'split3_test_precision': array([0.        , 0.64587156, 0.62047441, 0.61025641, 0.60155491,\n",
       "        0.59885387, 0.59943182, 0.598678  , 0.598678  , 0.598678  ,\n",
       "        0.598678  , 0.598678  , 0.598678  , 0.598678  , 0.598678  ,\n",
       "        0.598678  , 0.598678  , 0.598678  , 0.64587156, 0.61025641,\n",
       "        0.59885387, 0.598678  , 0.598678  , 0.598678  , 0.598678  ,\n",
       "        0.598678  , 0.598678  , 0.598678  , 0.598678  , 0.598678  ]),\n",
       " 'split4_test_precision': array([0.        , 0.66312057, 0.63420724, 0.62278978, 0.61574508,\n",
       "        0.61040146, 0.60909091, 0.61050725, 0.61015413, 0.61050725,\n",
       "        0.60995475, 0.61050725, 0.60995475, 0.60995475, 0.61050725,\n",
       "        0.60995475, 0.60995475, 0.60995475, 0.66312057, 0.62278978,\n",
       "        0.61040146, 0.61050725, 0.61050725, 0.60995475, 0.60995475,\n",
       "        0.60995475, 0.60995475, 0.60995475, 0.60995475, 0.61050725]),\n",
       " 'split5_test_precision': array([0.        , 0.65804067, 0.64545455, 0.62695925, 0.62217861,\n",
       "        0.62260536, 0.62105263, 0.62036156, 0.62036156, 0.62036156,\n",
       "        0.62036156, 0.62036156, 0.62036156, 0.62036156, 0.62036156,\n",
       "        0.62036156, 0.62036156, 0.62036156, 0.65804067, 0.62695925,\n",
       "        0.62260536, 0.62036156, 0.62036156, 0.62036156, 0.62036156,\n",
       "        0.62036156, 0.62036156, 0.62036156, 0.62036156, 0.62036156]),\n",
       " 'split6_test_precision': array([0.        , 0.63302752, 0.62376238, 0.60834181, 0.60153994,\n",
       "        0.59925094, 0.59813084, 0.59776536, 0.59851301, 0.59851301,\n",
       "        0.59851301, 0.59851301, 0.59851301, 0.59851301, 0.59851301,\n",
       "        0.59851301, 0.59851301, 0.59851301, 0.63302752, 0.60834181,\n",
       "        0.59925094, 0.59776536, 0.59851301, 0.59851301, 0.59851301,\n",
       "        0.59851301, 0.59851301, 0.59851301, 0.59851301, 0.59851301]),\n",
       " 'split7_test_precision': array([0.        , 0.66296296, 0.62925599, 0.62449799, 0.61545712,\n",
       "        0.61035422, 0.60720721, 0.60736748, 0.60771993, 0.60771993,\n",
       "        0.60771993, 0.60826595, 0.60771993, 0.60826595, 0.60826595,\n",
       "        0.60826595, 0.60826595, 0.60826595, 0.66296296, 0.62449799,\n",
       "        0.61035422, 0.60736748, 0.60826595, 0.60826595, 0.60826595,\n",
       "        0.60826595, 0.60826595, 0.60826595, 0.60826595, 0.60826595]),\n",
       " 'split8_test_precision': array([0.        , 0.6628131 , 0.6516129 , 0.6154661 , 0.61462451,\n",
       "        0.61346154, 0.615311  , 0.61398467, 0.61376673, 0.61376673,\n",
       "        0.61376673, 0.61376673, 0.61376673, 0.61376673, 0.61376673,\n",
       "        0.61376673, 0.61376673, 0.61376673, 0.6628131 , 0.6154661 ,\n",
       "        0.61346154, 0.61398467, 0.61376673, 0.61376673, 0.61376673,\n",
       "        0.61376673, 0.61376673, 0.61376673, 0.61376673, 0.61376673]),\n",
       " 'split9_test_precision': array([0.        , 0.64171123, 0.62340967, 0.61648016, 0.61374637,\n",
       "        0.60849057, 0.60877684, 0.60780669, 0.60780669, 0.60780669,\n",
       "        0.60780669, 0.60780669, 0.60780669, 0.60780669, 0.60780669,\n",
       "        0.60780669, 0.60780669, 0.60780669, 0.64171123, 0.61648016,\n",
       "        0.60849057, 0.60780669, 0.60780669, 0.60780669, 0.60780669,\n",
       "        0.60780669, 0.60780669, 0.60780669, 0.60780669, 0.60780669]),\n",
       " 'mean_test_precision': array([0.        , 0.64624594, 0.62087674, 0.60728142, 0.60118304,\n",
       "        0.5989613 , 0.59821109, 0.59784853, 0.59796975, 0.59794833,\n",
       "        0.59789308, 0.59800294, 0.59789308, 0.59794769, 0.59800294,\n",
       "        0.59794769, 0.59794769, 0.59794769, 0.64624594, 0.60722203,\n",
       "        0.5989613 , 0.59784853, 0.59800294, 0.59794769, 0.59794769,\n",
       "        0.59794769, 0.59794769, 0.59794769, 0.59794769, 0.59800294]),\n",
       " 'std_test_precision': array([0.        , 0.01547209, 0.02205806, 0.01955011, 0.02033171,\n",
       "        0.01894409, 0.01879587, 0.01897583, 0.01873123, 0.01876074,\n",
       "        0.01872446, 0.01878988, 0.01872446, 0.01875381, 0.01878988,\n",
       "        0.01875381, 0.01875381, 0.01875381, 0.01547209, 0.01967403,\n",
       "        0.01894409, 0.01897583, 0.01878988, 0.01875381, 0.01875381,\n",
       "        0.01875381, 0.01875381, 0.01875381, 0.01875381, 0.01878988]),\n",
       " 'rank_test_precision': array([30,  1,  3,  4,  6,  7,  9, 28, 14, 15, 26, 10, 26, 16, 10, 16, 16,\n",
       "        16,  1,  5,  7, 28, 10, 16, 16, 16, 16, 16, 16, 10]),\n",
       " 'split0_test_f1_micro': array([0.86129589, 0.8677014 , 0.86799704, 0.8684405 , 0.86829268,\n",
       "        0.86819414, 0.86804632, 0.8678985 , 0.8678985 , 0.8678985 ,\n",
       "        0.8678985 , 0.8678985 , 0.8678985 , 0.8678985 , 0.8678985 ,\n",
       "        0.8678985 , 0.8678985 , 0.8678985 , 0.8677014 , 0.8684405 ,\n",
       "        0.86819414, 0.8678985 , 0.8678985 , 0.8678985 , 0.8678985 ,\n",
       "        0.8678985 , 0.8678985 , 0.8678985 , 0.8678985 , 0.8678985 ]),\n",
       " 'split1_test_f1_micro': array([0.86129589, 0.86730722, 0.86686376, 0.86755358, 0.86706085,\n",
       "        0.86765213, 0.86784922, 0.86779995, 0.86794777, 0.86794777,\n",
       "        0.86794777, 0.86794777, 0.86794777, 0.86794777, 0.86794777,\n",
       "        0.86794777, 0.86794777, 0.86794777, 0.86730722, 0.86750431,\n",
       "        0.86765213, 0.86779995, 0.86794777, 0.86794777, 0.86794777,\n",
       "        0.86794777, 0.86794777, 0.86794777, 0.86794777, 0.86794777]),\n",
       " 'split2_test_f1_micro': array([0.86124661, 0.86898251, 0.87021434, 0.87144617, 0.87124908,\n",
       "        0.87129835, 0.87115053, 0.87124908, 0.8711998 , 0.87115053,\n",
       "        0.87115053, 0.87115053, 0.87115053, 0.87115053, 0.87115053,\n",
       "        0.87115053, 0.87115053, 0.87115053, 0.86898251, 0.87144617,\n",
       "        0.87129835, 0.87124908, 0.87115053, 0.87115053, 0.87115053,\n",
       "        0.87115053, 0.87115053, 0.87115053, 0.87115053, 0.87115053]),\n",
       " 'split3_test_f1_micro': array([0.86124661, 0.86908105, 0.87075634, 0.87184035, 0.87154472,\n",
       "        0.87144617, 0.87159399, 0.87154472, 0.87154472, 0.87154472,\n",
       "        0.87154472, 0.87154472, 0.87154472, 0.87154472, 0.87154472,\n",
       "        0.87154472, 0.87154472, 0.87154472, 0.86908105, 0.87184035,\n",
       "        0.87144617, 0.87154472, 0.87154472, 0.87154472, 0.87154472,\n",
       "        0.87154472, 0.87154472, 0.87154472, 0.87154472, 0.87154472]),\n",
       " 'split4_test_f1_micro': array([0.86128905, 0.87035577, 0.87188332, 0.87360796, 0.87346014,\n",
       "        0.87321376, 0.87311521, 0.87331231, 0.87326303, 0.87331231,\n",
       "        0.87326303, 0.87331231, 0.87326303, 0.87326303, 0.87331231,\n",
       "        0.87326303, 0.87326303, 0.87326303, 0.87035577, 0.87360796,\n",
       "        0.87321376, 0.87331231, 0.87331231, 0.87326303, 0.87326303,\n",
       "        0.87326303, 0.87326303, 0.87326303, 0.87326303, 0.87331231]),\n",
       " 'split5_test_f1_micro': array([0.86128905, 0.86971519, 0.8723268 , 0.87326303, 0.87355869,\n",
       "        0.87390362, 0.87375579, 0.87375579, 0.87375579, 0.87375579,\n",
       "        0.87375579, 0.87375579, 0.87375579, 0.87375579, 0.87375579,\n",
       "        0.87375579, 0.87375579, 0.87375579, 0.86971519, 0.87326303,\n",
       "        0.87390362, 0.87375579, 0.87375579, 0.87375579, 0.87375579,\n",
       "        0.87375579, 0.87375579, 0.87375579, 0.87375579, 0.87375579]),\n",
       " 'split6_test_f1_micro': array([0.86128905, 0.86843402, 0.87114418, 0.87178476, 0.87168621,\n",
       "        0.87173549, 0.87163694, 0.87163694, 0.87173549, 0.87173549,\n",
       "        0.87173549, 0.87173549, 0.87173549, 0.87173549, 0.87173549,\n",
       "        0.87173549, 0.87173549, 0.87173549, 0.86843402, 0.87178476,\n",
       "        0.87173549, 0.87163694, 0.87173549, 0.87173549, 0.87173549,\n",
       "        0.87173549, 0.87173549, 0.87173549, 0.87173549, 0.87173549]),\n",
       " 'split7_test_f1_micro': array([0.86128905, 0.86996156, 0.87139056, 0.87350941, 0.87336158,\n",
       "        0.87326303, 0.87301666, 0.87306593, 0.87311521, 0.87311521,\n",
       "        0.87311521, 0.87316448, 0.87311521, 0.87316448, 0.87316448,\n",
       "        0.87316448, 0.87316448, 0.87316448, 0.86996156, 0.87350941,\n",
       "        0.87326303, 0.87306593, 0.87316448, 0.87316448, 0.87316448,\n",
       "        0.87316448, 0.87316448, 0.87316448, 0.87316448, 0.87316448]),\n",
       " 'split8_test_f1_micro': array([0.86128905, 0.86961664, 0.87286883, 0.87203114, 0.872721  ,\n",
       "        0.8729181 , 0.87316448, 0.87301666, 0.87301666, 0.87301666,\n",
       "        0.87301666, 0.87301666, 0.87301666, 0.87301666, 0.87301666,\n",
       "        0.87301666, 0.87301666, 0.87301666, 0.86961664, 0.87203114,\n",
       "        0.8729181 , 0.87301666, 0.87301666, 0.87301666, 0.87301666,\n",
       "        0.87301666, 0.87301666, 0.87301666, 0.87301666, 0.87301666]),\n",
       " 'split9_test_f1_micro': array([0.86128905, 0.86912388, 0.87084853, 0.87257317, 0.87286883,\n",
       "        0.87262245, 0.87277028, 0.872721  , 0.872721  , 0.872721  ,\n",
       "        0.872721  , 0.872721  , 0.872721  , 0.872721  , 0.872721  ,\n",
       "        0.872721  , 0.872721  , 0.872721  , 0.86912388, 0.87257317,\n",
       "        0.87262245, 0.872721  , 0.872721  , 0.872721  , 0.872721  ,\n",
       "        0.872721  , 0.872721  , 0.872721  , 0.872721  , 0.872721  ]),\n",
       " 'mean_test_f1_micro': array([0.86128193, 0.86902792, 0.87062937, 0.87160501, 0.87158038,\n",
       "        0.87162472, 0.87160994, 0.87160009, 0.8716198 , 0.8716198 ,\n",
       "        0.87161487, 0.87162472, 0.87161487, 0.8716198 , 0.87162472,\n",
       "        0.8716198 , 0.8716198 , 0.8716198 , 0.86902792, 0.87160008,\n",
       "        0.87162472, 0.87160009, 0.87162472, 0.8716198 , 0.8716198 ,\n",
       "        0.8716198 , 0.8716198 , 0.8716198 , 0.8716198 , 0.87162472]),\n",
       " 'std_test_f1_micro': array([1.78561653e-05, 9.24179042e-04, 1.77928674e-03, 1.95057998e-03,\n",
       "        2.11986567e-03, 2.01920340e-03, 1.99165172e-03, 2.03128056e-03,\n",
       "        2.00474620e-03, 2.00993187e-03, 2.00583267e-03, 2.01364895e-03,\n",
       "        2.00583267e-03, 2.00956942e-03, 2.01364895e-03, 2.00956942e-03,\n",
       "        2.00956942e-03, 2.00956942e-03, 9.24179042e-04, 1.96084321e-03,\n",
       "        2.01920340e-03, 2.03128056e-03, 2.01364895e-03, 2.00956942e-03,\n",
       "        2.00956942e-03, 2.00956942e-03, 2.00956942e-03, 2.00956942e-03,\n",
       "        2.00956942e-03, 2.01364895e-03]),\n",
       " 'rank_test_f1_micro': array([30, 28, 27, 22, 26,  5, 21, 23, 18,  7, 19,  1, 19,  7,  1,  7,  7,\n",
       "         7, 28, 25,  5, 23,  1,  7,  7,  7,  7,  7,  7,  1])}"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_model.cv_results_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "5a869ea2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([30, 28, 27, 22, 26,  5, 21, 23, 18,  7, 19,  1, 19,  8,  1,  8,  8,\n",
       "        8, 28, 25,  5, 23,  1,  8,  8,  8,  8,  8,  8,  1])"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_model.cv_results_['rank_test_accuracy']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "3bcc2637",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'classifier': LogisticRegression(max_iter=10000),\n",
       " 'classifier__C': 0.0001,\n",
       " 'classifier__penalty': 'l1',\n",
       " 'classifier__solver': 'saga'}"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_model.cv_results_['params'][ np.argmin(best_model.cv_results_['mean_test_accuracy'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "0216cb96",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>classifier</th>\n",
       "      <th>C</th>\n",
       "      <th>penalty</th>\n",
       "      <th>solver</th>\n",
       "      <th>score_acc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.861282</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.869028</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.870629</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.871605</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0100</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.871580</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0100</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.871625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.1000</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.871610</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.1000</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.871600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.871620</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.871620</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>10.0000</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.871615</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>10.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.871625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>100.0000</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.871615</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>100.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.871620</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>1000.0000</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.871625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>1000.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.871620</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>10000.0000</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.871620</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>10000.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.871620</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.869028</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.871600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0100</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.871625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.1000</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.871600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.871625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>10.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.871620</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>100.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.871620</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>1000.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.871620</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>10000.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.871620</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>NaN</td>\n",
       "      <td>none</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.871620</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>NaN</td>\n",
       "      <td>none</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.871620</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.871625</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                            classifier           C penalty solver  score_acc\n",
       "0   LogisticRegression(max_iter=10000)      0.0001      l1   saga   0.861282\n",
       "1   LogisticRegression(max_iter=10000)      0.0001      l2   saga   0.869028\n",
       "2   LogisticRegression(max_iter=10000)      0.0010      l1   saga   0.870629\n",
       "3   LogisticRegression(max_iter=10000)      0.0010      l2   saga   0.871605\n",
       "4   LogisticRegression(max_iter=10000)      0.0100      l1   saga   0.871580\n",
       "5   LogisticRegression(max_iter=10000)      0.0100      l2   saga   0.871625\n",
       "6   LogisticRegression(max_iter=10000)      0.1000      l1   saga   0.871610\n",
       "7   LogisticRegression(max_iter=10000)      0.1000      l2   saga   0.871600\n",
       "8   LogisticRegression(max_iter=10000)      1.0000      l1   saga   0.871620\n",
       "9   LogisticRegression(max_iter=10000)      1.0000      l2   saga   0.871620\n",
       "10  LogisticRegression(max_iter=10000)     10.0000      l1   saga   0.871615\n",
       "11  LogisticRegression(max_iter=10000)     10.0000      l2   saga   0.871625\n",
       "12  LogisticRegression(max_iter=10000)    100.0000      l1   saga   0.871615\n",
       "13  LogisticRegression(max_iter=10000)    100.0000      l2   saga   0.871620\n",
       "14  LogisticRegression(max_iter=10000)   1000.0000      l1   saga   0.871625\n",
       "15  LogisticRegression(max_iter=10000)   1000.0000      l2   saga   0.871620\n",
       "16  LogisticRegression(max_iter=10000)  10000.0000      l1   saga   0.871620\n",
       "17  LogisticRegression(max_iter=10000)  10000.0000      l2   saga   0.871620\n",
       "18  LogisticRegression(max_iter=10000)      0.0001      l2  lbfgs   0.869028\n",
       "19  LogisticRegression(max_iter=10000)      0.0010      l2  lbfgs   0.871600\n",
       "20  LogisticRegression(max_iter=10000)      0.0100      l2  lbfgs   0.871625\n",
       "21  LogisticRegression(max_iter=10000)      0.1000      l2  lbfgs   0.871600\n",
       "22  LogisticRegression(max_iter=10000)      1.0000      l2  lbfgs   0.871625\n",
       "23  LogisticRegression(max_iter=10000)     10.0000      l2  lbfgs   0.871620\n",
       "24  LogisticRegression(max_iter=10000)    100.0000      l2  lbfgs   0.871620\n",
       "25  LogisticRegression(max_iter=10000)   1000.0000      l2  lbfgs   0.871620\n",
       "26  LogisticRegression(max_iter=10000)  10000.0000      l2  lbfgs   0.871620\n",
       "27  LogisticRegression(max_iter=10000)         NaN    none  lbfgs   0.871620\n",
       "28  LogisticRegression(max_iter=10000)         NaN    none   saga   0.871620\n",
       "29  LogisticRegression(max_iter=10000)         NaN     NaN    NaN   0.871625"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results = pd.DataFrame( best_model.cv_results_['params'] )\n",
    "\n",
    "results['score_acc'] = best_model.cv_results_['mean_test_accuracy']\n",
    "cols = results.columns.to_series().str.split('__').apply(lambda x: x[-1])\n",
    "results.columns = cols\n",
    "\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "83604c0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "acc_list.append(results[0:1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "9c89d3be",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'classifier': LogisticRegression(max_iter=10000),\n",
       " 'classifier__C': 0.0001,\n",
       " 'classifier__penalty': 'l2',\n",
       " 'classifier__solver': 'saga'}"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_model.cv_results_['params'][ np.argmin(best_model.cv_results_['rank_test_precision'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "69ab11b2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>classifier</th>\n",
       "      <th>C</th>\n",
       "      <th>penalty</th>\n",
       "      <th>solver</th>\n",
       "      <th>score_precision</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.646246</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.620877</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.607281</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0100</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.601183</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0100</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.598961</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.1000</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.598211</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.1000</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.597849</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.597970</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.597948</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>10.0000</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.597893</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>10.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.598003</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>100.0000</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.597893</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>100.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.597948</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>1000.0000</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.598003</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>1000.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.597948</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>10000.0000</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.597948</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>10000.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.597948</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.646246</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.607222</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0100</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.598961</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.1000</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.597849</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.598003</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>10.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.597948</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>100.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.597948</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>1000.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.597948</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>10000.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.597948</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>NaN</td>\n",
       "      <td>none</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.597948</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>NaN</td>\n",
       "      <td>none</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.597948</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.598003</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                            classifier           C penalty solver  \\\n",
       "0   LogisticRegression(max_iter=10000)      0.0001      l1   saga   \n",
       "1   LogisticRegression(max_iter=10000)      0.0001      l2   saga   \n",
       "2   LogisticRegression(max_iter=10000)      0.0010      l1   saga   \n",
       "3   LogisticRegression(max_iter=10000)      0.0010      l2   saga   \n",
       "4   LogisticRegression(max_iter=10000)      0.0100      l1   saga   \n",
       "5   LogisticRegression(max_iter=10000)      0.0100      l2   saga   \n",
       "6   LogisticRegression(max_iter=10000)      0.1000      l1   saga   \n",
       "7   LogisticRegression(max_iter=10000)      0.1000      l2   saga   \n",
       "8   LogisticRegression(max_iter=10000)      1.0000      l1   saga   \n",
       "9   LogisticRegression(max_iter=10000)      1.0000      l2   saga   \n",
       "10  LogisticRegression(max_iter=10000)     10.0000      l1   saga   \n",
       "11  LogisticRegression(max_iter=10000)     10.0000      l2   saga   \n",
       "12  LogisticRegression(max_iter=10000)    100.0000      l1   saga   \n",
       "13  LogisticRegression(max_iter=10000)    100.0000      l2   saga   \n",
       "14  LogisticRegression(max_iter=10000)   1000.0000      l1   saga   \n",
       "15  LogisticRegression(max_iter=10000)   1000.0000      l2   saga   \n",
       "16  LogisticRegression(max_iter=10000)  10000.0000      l1   saga   \n",
       "17  LogisticRegression(max_iter=10000)  10000.0000      l2   saga   \n",
       "18  LogisticRegression(max_iter=10000)      0.0001      l2  lbfgs   \n",
       "19  LogisticRegression(max_iter=10000)      0.0010      l2  lbfgs   \n",
       "20  LogisticRegression(max_iter=10000)      0.0100      l2  lbfgs   \n",
       "21  LogisticRegression(max_iter=10000)      0.1000      l2  lbfgs   \n",
       "22  LogisticRegression(max_iter=10000)      1.0000      l2  lbfgs   \n",
       "23  LogisticRegression(max_iter=10000)     10.0000      l2  lbfgs   \n",
       "24  LogisticRegression(max_iter=10000)    100.0000      l2  lbfgs   \n",
       "25  LogisticRegression(max_iter=10000)   1000.0000      l2  lbfgs   \n",
       "26  LogisticRegression(max_iter=10000)  10000.0000      l2  lbfgs   \n",
       "27  LogisticRegression(max_iter=10000)         NaN    none  lbfgs   \n",
       "28  LogisticRegression(max_iter=10000)         NaN    none   saga   \n",
       "29  LogisticRegression(max_iter=10000)         NaN     NaN    NaN   \n",
       "\n",
       "    score_precision  \n",
       "0          0.000000  \n",
       "1          0.646246  \n",
       "2          0.620877  \n",
       "3          0.607281  \n",
       "4          0.601183  \n",
       "5          0.598961  \n",
       "6          0.598211  \n",
       "7          0.597849  \n",
       "8          0.597970  \n",
       "9          0.597948  \n",
       "10         0.597893  \n",
       "11         0.598003  \n",
       "12         0.597893  \n",
       "13         0.597948  \n",
       "14         0.598003  \n",
       "15         0.597948  \n",
       "16         0.597948  \n",
       "17         0.597948  \n",
       "18         0.646246  \n",
       "19         0.607222  \n",
       "20         0.598961  \n",
       "21         0.597849  \n",
       "22         0.598003  \n",
       "23         0.597948  \n",
       "24         0.597948  \n",
       "25         0.597948  \n",
       "26         0.597948  \n",
       "27         0.597948  \n",
       "28         0.597948  \n",
       "29         0.598003  "
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results = pd.DataFrame( best_model.cv_results_['params'] )\n",
    "\n",
    "results['score_precision'] = best_model.cv_results_['mean_test_precision']\n",
    "cols = results.columns.to_series().str.split('__').apply(lambda x: x[-1])\n",
    "results.columns = cols\n",
    "\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "c584772f",
   "metadata": {},
   "outputs": [],
   "source": [
    "roc_auc_ovr_List.append(results[1:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "70f619b8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'classifier': LogisticRegression(max_iter=10000),\n",
       " 'classifier__C': 10.0,\n",
       " 'classifier__penalty': 'l2',\n",
       " 'classifier__solver': 'saga'}"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_model.cv_results_['params'][ np.argmin(best_model.cv_results_['rank_test_f1_micro'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "11465a39",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>classifier</th>\n",
       "      <th>C</th>\n",
       "      <th>penalty</th>\n",
       "      <th>solver</th>\n",
       "      <th>score_f1_micro</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.861282</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.869028</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.870629</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.871605</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0100</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.871580</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0100</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.871625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.1000</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.871610</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.1000</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.871600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.871620</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.871620</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>10.0000</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.871615</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>10.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.871625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>100.0000</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.871615</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>100.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.871620</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>1000.0000</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.871625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>1000.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.871620</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>10000.0000</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.871620</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>10000.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.871620</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.869028</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.871600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0100</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.871625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.1000</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.871600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.871625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>10.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.871620</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>100.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.871620</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>1000.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.871620</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>10000.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.871620</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>NaN</td>\n",
       "      <td>none</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.871620</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>NaN</td>\n",
       "      <td>none</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.871620</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.871625</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                            classifier           C penalty solver  \\\n",
       "0   LogisticRegression(max_iter=10000)      0.0001      l1   saga   \n",
       "1   LogisticRegression(max_iter=10000)      0.0001      l2   saga   \n",
       "2   LogisticRegression(max_iter=10000)      0.0010      l1   saga   \n",
       "3   LogisticRegression(max_iter=10000)      0.0010      l2   saga   \n",
       "4   LogisticRegression(max_iter=10000)      0.0100      l1   saga   \n",
       "5   LogisticRegression(max_iter=10000)      0.0100      l2   saga   \n",
       "6   LogisticRegression(max_iter=10000)      0.1000      l1   saga   \n",
       "7   LogisticRegression(max_iter=10000)      0.1000      l2   saga   \n",
       "8   LogisticRegression(max_iter=10000)      1.0000      l1   saga   \n",
       "9   LogisticRegression(max_iter=10000)      1.0000      l2   saga   \n",
       "10  LogisticRegression(max_iter=10000)     10.0000      l1   saga   \n",
       "11  LogisticRegression(max_iter=10000)     10.0000      l2   saga   \n",
       "12  LogisticRegression(max_iter=10000)    100.0000      l1   saga   \n",
       "13  LogisticRegression(max_iter=10000)    100.0000      l2   saga   \n",
       "14  LogisticRegression(max_iter=10000)   1000.0000      l1   saga   \n",
       "15  LogisticRegression(max_iter=10000)   1000.0000      l2   saga   \n",
       "16  LogisticRegression(max_iter=10000)  10000.0000      l1   saga   \n",
       "17  LogisticRegression(max_iter=10000)  10000.0000      l2   saga   \n",
       "18  LogisticRegression(max_iter=10000)      0.0001      l2  lbfgs   \n",
       "19  LogisticRegression(max_iter=10000)      0.0010      l2  lbfgs   \n",
       "20  LogisticRegression(max_iter=10000)      0.0100      l2  lbfgs   \n",
       "21  LogisticRegression(max_iter=10000)      0.1000      l2  lbfgs   \n",
       "22  LogisticRegression(max_iter=10000)      1.0000      l2  lbfgs   \n",
       "23  LogisticRegression(max_iter=10000)     10.0000      l2  lbfgs   \n",
       "24  LogisticRegression(max_iter=10000)    100.0000      l2  lbfgs   \n",
       "25  LogisticRegression(max_iter=10000)   1000.0000      l2  lbfgs   \n",
       "26  LogisticRegression(max_iter=10000)  10000.0000      l2  lbfgs   \n",
       "27  LogisticRegression(max_iter=10000)         NaN    none  lbfgs   \n",
       "28  LogisticRegression(max_iter=10000)         NaN    none   saga   \n",
       "29  LogisticRegression(max_iter=10000)         NaN     NaN    NaN   \n",
       "\n",
       "    score_f1_micro  \n",
       "0         0.861282  \n",
       "1         0.869028  \n",
       "2         0.870629  \n",
       "3         0.871605  \n",
       "4         0.871580  \n",
       "5         0.871625  \n",
       "6         0.871610  \n",
       "7         0.871600  \n",
       "8         0.871620  \n",
       "9         0.871620  \n",
       "10        0.871615  \n",
       "11        0.871625  \n",
       "12        0.871615  \n",
       "13        0.871620  \n",
       "14        0.871625  \n",
       "15        0.871620  \n",
       "16        0.871620  \n",
       "17        0.871620  \n",
       "18        0.869028  \n",
       "19        0.871600  \n",
       "20        0.871625  \n",
       "21        0.871600  \n",
       "22        0.871625  \n",
       "23        0.871620  \n",
       "24        0.871620  \n",
       "25        0.871620  \n",
       "26        0.871620  \n",
       "27        0.871620  \n",
       "28        0.871620  \n",
       "29        0.871625  "
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results = pd.DataFrame( best_model.cv_results_['params'] )\n",
    "\n",
    "results['score_f1_micro'] = best_model.cv_results_['mean_test_f1_micro']\n",
    "cols = results.columns.to_series().str.split('__').apply(lambda x: x[-1])\n",
    "results.columns = cols\n",
    "\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "4a693627",
   "metadata": {},
   "outputs": [],
   "source": [
    "f1_micro_List.append(results[11:12])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a0b1170",
   "metadata": {},
   "source": [
    "## Trial 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "d430cae5",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, Y_train, Y_test = train_test_split(X, y, test_size=0.2, shuffle=True, random_state = 8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "ec6bf17a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Guest-All\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\Guest-All\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\Guest-All\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\Guest-All\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\Guest-All\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\Guest-All\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\Guest-All\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\Guest-All\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\Guest-All\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\Guest-All\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\Guest-All\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1183: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Guest-All\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1183: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Guest-All\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1183: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Guest-All\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1183: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Guest-All\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1183: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Guest-All\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1183: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Guest-All\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1183: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Guest-All\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1183: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Guest-All\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1183: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Guest-All\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1183: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Guest-All\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1183: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Guest-All\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1183: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Guest-All\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1183: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Guest-All\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1183: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Guest-All\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1183: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Guest-All\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1183: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Guest-All\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1183: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Guest-All\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1183: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Guest-All\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1183: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Guest-All\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1183: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Create a pipeline - RF is a stand in, we will populate the classifier part below\n",
    "pipe = Pipeline([('std', StandardScaler()),\n",
    "                 ('classifier', LogisticRegression())])\n",
    "\n",
    "# Create search space of candidate learning algorithms and their hyperparameters\n",
    "# note lbfgs can't do l1, and if you pass penalty='none' it expects no C value\n",
    "search_space = [{'classifier': [LogisticRegression(max_iter=10000)],\n",
    "                 'classifier__solver': ['saga'],\n",
    "                 'classifier__penalty': ['l1', 'l2'],\n",
    "                 'classifier__C': np.logspace(-4, 4, 9)},\n",
    "                {'classifier': [LogisticRegression(max_iter=10000)],\n",
    "                 'classifier__solver': ['lbfgs'],\n",
    "                 'classifier__penalty': ['l2'],\n",
    "                 'classifier__C': np.logspace(-4, 4, 9)},\n",
    "                {'classifier': [LogisticRegression(max_iter=10000)],\n",
    "                 'classifier__solver': ['lbfgs','saga'],\n",
    "                 'classifier__penalty': ['none']},\n",
    "                {'classifier': [LogisticRegression(max_iter=10000)]}\n",
    "                ]\n",
    "\n",
    "# Create grid search\n",
    "clf = GridSearchCV(pipe, search_space, cv=StratifiedKFold(n_splits=10),\n",
    "                   scoring=['accuracy', 'precision', 'f1_micro'], refit=False,\n",
    "                   verbose=0)\n",
    "\n",
    "# Fit grid search\n",
    "best_model = clf.fit(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "b6aa2042",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'mean_fit_time': array([ 3.75080531,  3.36446002,  3.75934651,  3.18616269,  3.68624063,\n",
       "         2.87800508,  5.14114368,  3.51263108, 27.73780339,  3.27908576,\n",
       "         4.19539206,  3.16987226,  4.18491242,  3.18566754,  3.85543263,\n",
       "         3.21814952,  4.06310272,  3.31734016,  0.72311349,  0.71358266,\n",
       "         0.78933129,  0.79110131,  0.78845212,  0.79713032,  0.79536765,\n",
       "         0.78302424,  0.78115354,  0.73345118,  3.31885636,  0.81676023]),\n",
       " 'std_fit_time': array([0.19897   , 0.1908906 , 0.37367901, 0.20733495, 0.34414813,\n",
       "        0.3947086 , 2.13487735, 0.50157699, 3.01278875, 0.41782961,\n",
       "        0.26061286, 0.36336336, 0.24514236, 0.25821956, 0.34534619,\n",
       "        0.24312055, 0.52865924, 0.6649475 , 0.03690029, 0.09360731,\n",
       "        0.02604871, 0.01714707, 0.02288138, 0.02115841, 0.02539391,\n",
       "        0.0247659 , 0.02567092, 0.03481695, 0.22580454, 0.01551785]),\n",
       " 'mean_score_time': array([0.03359044, 0.03230343, 0.03605268, 0.03095515, 0.03086517,\n",
       "        0.03041914, 0.0337477 , 0.03219075, 0.03147416, 0.03237324,\n",
       "        0.03442645, 0.03274035, 0.03340983, 0.02861247, 0.03048997,\n",
       "        0.03368056, 0.03414671, 0.03436358, 0.04225304, 0.0303915 ,\n",
       "        0.0316328 , 0.03276756, 0.03164022, 0.03194184, 0.03130071,\n",
       "        0.03043973, 0.03184118, 0.02862184, 0.03223   , 0.03196616]),\n",
       " 'std_score_time': array([0.00282365, 0.00686748, 0.00797951, 0.00490575, 0.00335502,\n",
       "        0.00391435, 0.00486152, 0.00597831, 0.00276112, 0.00443377,\n",
       "        0.00261252, 0.00304913, 0.00139623, 0.00381295, 0.00512959,\n",
       "        0.00097243, 0.00473018, 0.00773334, 0.00434213, 0.00313959,\n",
       "        0.00205796, 0.00103499, 0.00255235, 0.00066026, 0.002129  ,\n",
       "        0.00306728, 0.00237788, 0.00372737, 0.00123223, 0.0008189 ]),\n",
       " 'param_classifier': masked_array(data=[LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000)],\n",
       "              mask=[False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'param_classifier__C': masked_array(data=[0.0001, 0.0001, 0.001, 0.001, 0.01, 0.01, 0.1, 0.1,\n",
       "                    1.0, 1.0, 10.0, 10.0, 100.0, 100.0, 1000.0, 1000.0,\n",
       "                    10000.0, 10000.0, 0.0001, 0.001, 0.01, 0.1, 1.0, 10.0,\n",
       "                    100.0, 1000.0, 10000.0, --, --, --],\n",
       "              mask=[False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False,  True,  True,  True],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'param_classifier__penalty': masked_array(data=['l1', 'l2', 'l1', 'l2', 'l1', 'l2', 'l1', 'l2', 'l1',\n",
       "                    'l2', 'l1', 'l2', 'l1', 'l2', 'l1', 'l2', 'l1', 'l2',\n",
       "                    'l2', 'l2', 'l2', 'l2', 'l2', 'l2', 'l2', 'l2', 'l2',\n",
       "                    'none', 'none', --],\n",
       "              mask=[False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False,  True],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'param_classifier__solver': masked_array(data=['saga', 'saga', 'saga', 'saga', 'saga', 'saga', 'saga',\n",
       "                    'saga', 'saga', 'saga', 'saga', 'saga', 'saga', 'saga',\n",
       "                    'saga', 'saga', 'saga', 'saga', 'lbfgs', 'lbfgs',\n",
       "                    'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs',\n",
       "                    'lbfgs', 'lbfgs', 'saga', --],\n",
       "              mask=[False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False,  True],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'params': [{'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 0.0001,\n",
       "   'classifier__penalty': 'l1',\n",
       "   'classifier__solver': 'saga'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 0.0001,\n",
       "   'classifier__penalty': 'l2',\n",
       "   'classifier__solver': 'saga'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 0.001,\n",
       "   'classifier__penalty': 'l1',\n",
       "   'classifier__solver': 'saga'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 0.001,\n",
       "   'classifier__penalty': 'l2',\n",
       "   'classifier__solver': 'saga'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 0.01,\n",
       "   'classifier__penalty': 'l1',\n",
       "   'classifier__solver': 'saga'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 0.01,\n",
       "   'classifier__penalty': 'l2',\n",
       "   'classifier__solver': 'saga'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 0.1,\n",
       "   'classifier__penalty': 'l1',\n",
       "   'classifier__solver': 'saga'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 0.1,\n",
       "   'classifier__penalty': 'l2',\n",
       "   'classifier__solver': 'saga'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 1.0,\n",
       "   'classifier__penalty': 'l1',\n",
       "   'classifier__solver': 'saga'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 1.0,\n",
       "   'classifier__penalty': 'l2',\n",
       "   'classifier__solver': 'saga'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 10.0,\n",
       "   'classifier__penalty': 'l1',\n",
       "   'classifier__solver': 'saga'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 10.0,\n",
       "   'classifier__penalty': 'l2',\n",
       "   'classifier__solver': 'saga'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 100.0,\n",
       "   'classifier__penalty': 'l1',\n",
       "   'classifier__solver': 'saga'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 100.0,\n",
       "   'classifier__penalty': 'l2',\n",
       "   'classifier__solver': 'saga'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 1000.0,\n",
       "   'classifier__penalty': 'l1',\n",
       "   'classifier__solver': 'saga'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 1000.0,\n",
       "   'classifier__penalty': 'l2',\n",
       "   'classifier__solver': 'saga'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 10000.0,\n",
       "   'classifier__penalty': 'l1',\n",
       "   'classifier__solver': 'saga'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 10000.0,\n",
       "   'classifier__penalty': 'l2',\n",
       "   'classifier__solver': 'saga'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 0.0001,\n",
       "   'classifier__penalty': 'l2',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 0.001,\n",
       "   'classifier__penalty': 'l2',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 0.01,\n",
       "   'classifier__penalty': 'l2',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 0.1,\n",
       "   'classifier__penalty': 'l2',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 1.0,\n",
       "   'classifier__penalty': 'l2',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 10.0,\n",
       "   'classifier__penalty': 'l2',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 100.0,\n",
       "   'classifier__penalty': 'l2',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 1000.0,\n",
       "   'classifier__penalty': 'l2',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 10000.0,\n",
       "   'classifier__penalty': 'l2',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__penalty': 'none',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__penalty': 'none',\n",
       "   'classifier__solver': 'saga'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000)}],\n",
       " 'split0_test_accuracy': array([0.86031042, 0.86735649, 0.86922887, 0.86888396, 0.86986943,\n",
       "        0.86982015, 0.86972161, 0.86972161, 0.86972161, 0.86972161,\n",
       "        0.86972161, 0.86972161, 0.86972161, 0.86972161, 0.86972161,\n",
       "        0.86972161, 0.86972161, 0.86972161, 0.86735649, 0.86888396,\n",
       "        0.86982015, 0.86972161, 0.86972161, 0.86972161, 0.86972161,\n",
       "        0.86972161, 0.86972161, 0.86972161, 0.86972161, 0.86972161]),\n",
       " 'split1_test_accuracy': array([0.86031042, 0.86651885, 0.86962306, 0.8704607 , 0.87095344,\n",
       "        0.87075634, 0.87070707, 0.87055925, 0.87050998, 0.87050998,\n",
       "        0.87050998, 0.87050998, 0.87050998, 0.87050998, 0.87050998,\n",
       "        0.87050998, 0.87050998, 0.87050998, 0.86651885, 0.8704607 ,\n",
       "        0.87075634, 0.87055925, 0.87050998, 0.87050998, 0.87050998,\n",
       "        0.87050998, 0.87050998, 0.87050998, 0.87050998, 0.87050998]),\n",
       " 'split2_test_accuracy': array([0.86031042, 0.86922887, 0.86947524, 0.87026361, 0.87031288,\n",
       "        0.87016507, 0.86996797, 0.8699187 , 0.86982015, 0.86982015,\n",
       "        0.86982015, 0.86982015, 0.86982015, 0.86982015, 0.86982015,\n",
       "        0.86982015, 0.86982015, 0.86982015, 0.86922887, 0.87026361,\n",
       "        0.87016507, 0.8699187 , 0.86982015, 0.86982015, 0.86982015,\n",
       "        0.86982015, 0.86982015, 0.86982015, 0.86982015, 0.86982015]),\n",
       " 'split3_test_accuracy': array([0.86026115, 0.86809559, 0.86977088, 0.87179108, 0.87198817,\n",
       "        0.87164326, 0.8713969 , 0.87134762, 0.8713969 , 0.8713969 ,\n",
       "        0.8713969 , 0.8713969 , 0.8713969 , 0.8713969 , 0.8713969 ,\n",
       "        0.8713969 , 0.8713969 , 0.8713969 , 0.86809559, 0.87179108,\n",
       "        0.87164326, 0.87134762, 0.8713969 , 0.8713969 , 0.8713969 ,\n",
       "        0.8713969 , 0.8713969 , 0.8713969 , 0.8713969 , 0.8713969 ]),\n",
       " 'split4_test_accuracy': array([0.86030354, 0.8690746 , 0.8688775 , 0.87010939, 0.86986301,\n",
       "        0.86996156, 0.86986301, 0.86991229, 0.86996156, 0.86996156,\n",
       "        0.86996156, 0.86996156, 0.86996156, 0.86996156, 0.86996156,\n",
       "        0.86996156, 0.86996156, 0.86996156, 0.8690746 , 0.87010939,\n",
       "        0.86996156, 0.86991229, 0.86996156, 0.86996156, 0.86996156,\n",
       "        0.86996156, 0.86996156, 0.86996156, 0.86996156, 0.86996156]),\n",
       " 'split5_test_accuracy': array([0.86030354, 0.86597024, 0.86651227, 0.86784271, 0.86853257,\n",
       "        0.8686804 , 0.86853257, 0.86838474, 0.86838474, 0.86838474,\n",
       "        0.86838474, 0.86838474, 0.86838474, 0.86838474, 0.86838474,\n",
       "        0.86843402, 0.86838474, 0.86838474, 0.86597024, 0.86784271,\n",
       "        0.8686804 , 0.86838474, 0.86838474, 0.86838474, 0.86838474,\n",
       "        0.86838474, 0.86838474, 0.86838474, 0.86838474, 0.86838474]),\n",
       " 'split6_test_accuracy': array([0.86030354, 0.86932098, 0.87203114, 0.87370651, 0.87400217,\n",
       "        0.87385434, 0.87375579, 0.87390362, 0.87395289, 0.87395289,\n",
       "        0.87395289, 0.87395289, 0.87395289, 0.87395289, 0.87395289,\n",
       "        0.87395289, 0.87395289, 0.87395289, 0.86932098, 0.87370651,\n",
       "        0.87385434, 0.87390362, 0.87395289, 0.87395289, 0.87395289,\n",
       "        0.87395289, 0.87395289, 0.87395289, 0.87395289, 0.87395289]),\n",
       " 'split7_test_accuracy': array([0.86030354, 0.86813837, 0.87006012, 0.87065142, 0.87006012,\n",
       "        0.87030649, 0.87025722, 0.87030649, 0.87030649, 0.87030649,\n",
       "        0.87035577, 0.87035577, 0.87035577, 0.87035577, 0.87030649,\n",
       "        0.87035577, 0.87035577, 0.87035577, 0.86813837, 0.87065142,\n",
       "        0.87030649, 0.87030649, 0.87035577, 0.87035577, 0.87035577,\n",
       "        0.87035577, 0.87035577, 0.87035577, 0.87030649, 0.87035577]),\n",
       " 'split8_test_accuracy': array([0.86030354, 0.86877895, 0.8705036 , 0.87124273, 0.87139056,\n",
       "        0.87134128, 0.87119346, 0.87094708, 0.87099635, 0.87099635,\n",
       "        0.87099635, 0.87099635, 0.87099635, 0.87099635, 0.87099635,\n",
       "        0.87099635, 0.87099635, 0.87099635, 0.86877895, 0.87124273,\n",
       "        0.87134128, 0.87094708, 0.87099635, 0.87099635, 0.87099635,\n",
       "        0.87099635, 0.87099635, 0.87099635, 0.87099635, 0.87099635]),\n",
       " 'split9_test_accuracy': array([0.86030354, 0.86858185, 0.87163694, 0.87247462, 0.87286883,\n",
       "        0.87385434, 0.87395289, 0.87395289, 0.87395289, 0.87395289,\n",
       "        0.87395289, 0.87395289, 0.87395289, 0.87395289, 0.87395289,\n",
       "        0.87395289, 0.87395289, 0.87395289, 0.86858185, 0.87247462,\n",
       "        0.87385434, 0.87395289, 0.87395289, 0.87395289, 0.87395289,\n",
       "        0.87395289, 0.87395289, 0.87395289, 0.87395289, 0.87395289]),\n",
       " 'mean_test_accuracy': array([0.86030136, 0.86810648, 0.86977196, 0.87074268, 0.87098412,\n",
       "        0.87103832, 0.87093485, 0.87089543, 0.87090036, 0.87090036,\n",
       "        0.87090529, 0.87090529, 0.87090529, 0.87090529, 0.87090036,\n",
       "        0.87091021, 0.87090529, 0.87090529, 0.86810648, 0.87074268,\n",
       "        0.87103832, 0.87089543, 0.87090529, 0.87090529, 0.87090529,\n",
       "        0.87090529, 0.87090529, 0.87090529, 0.87090036, 0.87090529]),\n",
       " 'std_test_accuracy': array([1.37542069e-05, 1.09535211e-03, 1.45021262e-03, 1.60362663e-03,\n",
       "        1.53781832e-03, 1.60930025e-03, 1.64876627e-03, 1.69311090e-03,\n",
       "        1.70770936e-03, 1.70770936e-03, 1.70605897e-03, 1.70605897e-03,\n",
       "        1.70605897e-03, 1.70605897e-03, 1.70770936e-03, 1.69882768e-03,\n",
       "        1.70605897e-03, 1.70605897e-03, 1.09535211e-03, 1.60362663e-03,\n",
       "        1.60930025e-03, 1.69311090e-03, 1.70605897e-03, 1.70605897e-03,\n",
       "        1.70605897e-03, 1.70605897e-03, 1.70605897e-03, 1.70605897e-03,\n",
       "        1.70770936e-03, 1.70605897e-03]),\n",
       " 'rank_test_accuracy': array([30, 28, 27, 25,  3,  1,  4, 23, 19, 19,  6,  6,  6,  6, 19,  5,  6,\n",
       "         6, 28, 25,  1, 23,  6,  6,  6,  6,  6,  6, 19,  6]),\n",
       " 'split0_test_precision': array([0.        , 0.62745098, 0.61528662, 0.587     , 0.59116541,\n",
       "        0.5892692 , 0.58801843, 0.58769513, 0.58769513, 0.58769513,\n",
       "        0.58769513, 0.58769513, 0.58769513, 0.58769513, 0.58769513,\n",
       "        0.58769513, 0.58769513, 0.58769513, 0.62745098, 0.587     ,\n",
       "        0.5892692 , 0.58769513, 0.58769513, 0.58769513, 0.58769513,\n",
       "        0.58769513, 0.58769513, 0.58769513, 0.58769513, 0.58769513]),\n",
       " 'split1_test_precision': array([0.        , 0.6149635 , 0.6173913 , 0.60157791, 0.60150376,\n",
       "        0.59760589, 0.59599636, 0.59454545, 0.59400545, 0.59400545,\n",
       "        0.59400545, 0.59400545, 0.59400545, 0.59400545, 0.59400545,\n",
       "        0.59400545, 0.59400545, 0.59400545, 0.6149635 , 0.60157791,\n",
       "        0.59760589, 0.59454545, 0.59400545, 0.59400545, 0.59400545,\n",
       "        0.59400545, 0.59400545, 0.59400545, 0.59400545, 0.59400545]),\n",
       " 'split2_test_precision': array([0.        , 0.65794066, 0.61177885, 0.59786822, 0.59372114,\n",
       "        0.59041591, 0.5881295 , 0.58760108, 0.58654709, 0.58654709,\n",
       "        0.58654709, 0.58654709, 0.58654709, 0.58654709, 0.58654709,\n",
       "        0.58654709, 0.58654709, 0.58654709, 0.65794066, 0.59786822,\n",
       "        0.59041591, 0.58760108, 0.58654709, 0.58654709, 0.58654709,\n",
       "        0.58654709, 0.58654709, 0.58654709, 0.58654709, 0.58654709]),\n",
       " 'split3_test_precision': array([0.        , 0.65200765, 0.62451613, 0.62341772, 0.617357  ,\n",
       "        0.60989534, 0.60660377, 0.60583255, 0.60620301, 0.60620301,\n",
       "        0.60620301, 0.60620301, 0.60620301, 0.60620301, 0.60620301,\n",
       "        0.60620301, 0.60620301, 0.60620301, 0.65200765, 0.62341772,\n",
       "        0.60989534, 0.60583255, 0.60620301, 0.60620301, 0.60620301,\n",
       "        0.60620301, 0.60620301, 0.60620301, 0.60620301, 0.60620301]),\n",
       " 'split4_test_precision': array([0.        , 0.65187713, 0.60456731, 0.59650824, 0.58866545,\n",
       "        0.58718861, 0.58584071, 0.5862069 , 0.58657244, 0.58657244,\n",
       "        0.58657244, 0.58657244, 0.58657244, 0.58657244, 0.58657244,\n",
       "        0.58657244, 0.58657244, 0.58657244, 0.65187713, 0.59650824,\n",
       "        0.58718861, 0.5862069 , 0.58657244, 0.58657244, 0.58657244,\n",
       "        0.58657244, 0.58657244, 0.58657244, 0.58657244, 0.58657244]),\n",
       " 'split5_test_precision': array([0.        , 0.60473588, 0.57934509, 0.57798165, 0.58021134,\n",
       "        0.57914339, 0.57695853, 0.57509158, 0.57509158, 0.57509158,\n",
       "        0.57509158, 0.57509158, 0.57509158, 0.57509158, 0.57509158,\n",
       "        0.5756187 , 0.57509158, 0.57509158, 0.60473588, 0.57798165,\n",
       "        0.57914339, 0.57509158, 0.57509158, 0.57509158, 0.57509158,\n",
       "        0.57509158, 0.57509158, 0.57509158, 0.57509158, 0.57509158]),\n",
       " 'split6_test_precision': array([0.        , 0.67102804, 0.65101523, 0.63877551, 0.63288719,\n",
       "        0.62766945, 0.62603878, 0.62707182, 0.6274149 , 0.6274149 ,\n",
       "        0.6274149 , 0.6274149 , 0.6274149 , 0.6274149 , 0.6274149 ,\n",
       "        0.6274149 , 0.6274149 , 0.6274149 , 0.67102804, 0.63877551,\n",
       "        0.62766945, 0.62707182, 0.6274149 , 0.6274149 , 0.6274149 ,\n",
       "        0.6274149 , 0.6274149 , 0.6274149 , 0.6274149 , 0.6274149 ]),\n",
       " 'split7_test_precision': array([0.        , 0.64971751, 0.62282878, 0.60584677, 0.59269663,\n",
       "        0.59235669, 0.59165154, 0.5918552 , 0.5918552 , 0.5918552 ,\n",
       "        0.59222423, 0.59222423, 0.59222423, 0.59222423, 0.5918552 ,\n",
       "        0.59222423, 0.59222423, 0.59222423, 0.64971751, 0.60584677,\n",
       "        0.59235669, 0.5918552 , 0.59222423, 0.59222423, 0.59222423,\n",
       "        0.59222423, 0.59222423, 0.59222423, 0.5918552 , 0.59222423]),\n",
       " 'split8_test_precision': array([0.        , 0.65523466, 0.63184713, 0.61212121, 0.60683761,\n",
       "        0.6046729 , 0.60298229, 0.60018553, 0.60055607, 0.60055607,\n",
       "        0.60055607, 0.60055607, 0.60055607, 0.60055607, 0.60055607,\n",
       "        0.60055607, 0.60055607, 0.60055607, 0.65523466, 0.61212121,\n",
       "        0.6046729 , 0.60018553, 0.60055607, 0.60055607, 0.60055607,\n",
       "        0.60055607, 0.60055607, 0.60055607, 0.60055607, 0.60055607]),\n",
       " 'split9_test_precision': array([0.        , 0.65555556, 0.64556962, 0.62718847, 0.62271415,\n",
       "        0.62910798, 0.62907735, 0.62907735, 0.62907735, 0.62907735,\n",
       "        0.62907735, 0.62907735, 0.62907735, 0.62907735, 0.62907735,\n",
       "        0.62907735, 0.62907735, 0.62907735, 0.65555556, 0.62718847,\n",
       "        0.62910798, 0.62907735, 0.62907735, 0.62907735, 0.62907735,\n",
       "        0.62907735, 0.62907735, 0.62907735, 0.62907735, 0.62907735]),\n",
       " 'mean_test_precision': array([0.        , 0.64405116, 0.62041461, 0.60682857, 0.60277597,\n",
       "        0.60073254, 0.59912973, 0.59851626, 0.59850182, 0.59850182,\n",
       "        0.59853872, 0.59853872, 0.59853872, 0.59853872, 0.59850182,\n",
       "        0.59859144, 0.59853872, 0.59853872, 0.64405116, 0.60682857,\n",
       "        0.60073254, 0.59851626, 0.59853872, 0.59853872, 0.59853872,\n",
       "        0.59853872, 0.59853872, 0.59853872, 0.59850182, 0.59853872]),\n",
       " 'std_test_precision': array([0.        , 0.02000525, 0.01941747, 0.01782973, 0.01600499,\n",
       "        0.01611691, 0.01634022, 0.01674015, 0.0168784 , 0.0168784 ,\n",
       "        0.01686422, 0.01686422, 0.01686422, 0.01686422, 0.0168784 ,\n",
       "        0.01679152, 0.01686422, 0.01686422, 0.02000525, 0.01782973,\n",
       "        0.01611691, 0.01674015, 0.01686422, 0.01686422, 0.01686422,\n",
       "        0.01686422, 0.01686422, 0.01686422, 0.0168784 , 0.01686422]),\n",
       " 'rank_test_precision': array([30,  1,  3,  4,  6,  7,  9, 24, 26, 26, 11, 11, 11, 11, 26, 10, 11,\n",
       "        11,  1,  4,  7, 24, 11, 11, 11, 11, 11, 11, 26, 11]),\n",
       " 'split0_test_f1_micro': array([0.86031042, 0.86735649, 0.86922887, 0.86888396, 0.86986943,\n",
       "        0.86982015, 0.86972161, 0.86972161, 0.86972161, 0.86972161,\n",
       "        0.86972161, 0.86972161, 0.86972161, 0.86972161, 0.86972161,\n",
       "        0.86972161, 0.86972161, 0.86972161, 0.86735649, 0.86888396,\n",
       "        0.86982015, 0.86972161, 0.86972161, 0.86972161, 0.86972161,\n",
       "        0.86972161, 0.86972161, 0.86972161, 0.86972161, 0.86972161]),\n",
       " 'split1_test_f1_micro': array([0.86031042, 0.86651885, 0.86962306, 0.8704607 , 0.87095344,\n",
       "        0.87075634, 0.87070707, 0.87055925, 0.87050998, 0.87050998,\n",
       "        0.87050998, 0.87050998, 0.87050998, 0.87050998, 0.87050998,\n",
       "        0.87050998, 0.87050998, 0.87050998, 0.86651885, 0.8704607 ,\n",
       "        0.87075634, 0.87055925, 0.87050998, 0.87050998, 0.87050998,\n",
       "        0.87050998, 0.87050998, 0.87050998, 0.87050998, 0.87050998]),\n",
       " 'split2_test_f1_micro': array([0.86031042, 0.86922887, 0.86947524, 0.87026361, 0.87031288,\n",
       "        0.87016507, 0.86996797, 0.8699187 , 0.86982015, 0.86982015,\n",
       "        0.86982015, 0.86982015, 0.86982015, 0.86982015, 0.86982015,\n",
       "        0.86982015, 0.86982015, 0.86982015, 0.86922887, 0.87026361,\n",
       "        0.87016507, 0.8699187 , 0.86982015, 0.86982015, 0.86982015,\n",
       "        0.86982015, 0.86982015, 0.86982015, 0.86982015, 0.86982015]),\n",
       " 'split3_test_f1_micro': array([0.86026115, 0.86809559, 0.86977088, 0.87179108, 0.87198817,\n",
       "        0.87164326, 0.8713969 , 0.87134762, 0.8713969 , 0.8713969 ,\n",
       "        0.8713969 , 0.8713969 , 0.8713969 , 0.8713969 , 0.8713969 ,\n",
       "        0.8713969 , 0.8713969 , 0.8713969 , 0.86809559, 0.87179108,\n",
       "        0.87164326, 0.87134762, 0.8713969 , 0.8713969 , 0.8713969 ,\n",
       "        0.8713969 , 0.8713969 , 0.8713969 , 0.8713969 , 0.8713969 ]),\n",
       " 'split4_test_f1_micro': array([0.86030354, 0.8690746 , 0.8688775 , 0.87010939, 0.86986301,\n",
       "        0.86996156, 0.86986301, 0.86991229, 0.86996156, 0.86996156,\n",
       "        0.86996156, 0.86996156, 0.86996156, 0.86996156, 0.86996156,\n",
       "        0.86996156, 0.86996156, 0.86996156, 0.8690746 , 0.87010939,\n",
       "        0.86996156, 0.86991229, 0.86996156, 0.86996156, 0.86996156,\n",
       "        0.86996156, 0.86996156, 0.86996156, 0.86996156, 0.86996156]),\n",
       " 'split5_test_f1_micro': array([0.86030354, 0.86597024, 0.86651227, 0.86784271, 0.86853257,\n",
       "        0.8686804 , 0.86853257, 0.86838474, 0.86838474, 0.86838474,\n",
       "        0.86838474, 0.86838474, 0.86838474, 0.86838474, 0.86838474,\n",
       "        0.86843402, 0.86838474, 0.86838474, 0.86597024, 0.86784271,\n",
       "        0.8686804 , 0.86838474, 0.86838474, 0.86838474, 0.86838474,\n",
       "        0.86838474, 0.86838474, 0.86838474, 0.86838474, 0.86838474]),\n",
       " 'split6_test_f1_micro': array([0.86030354, 0.86932098, 0.87203114, 0.87370651, 0.87400217,\n",
       "        0.87385434, 0.87375579, 0.87390362, 0.87395289, 0.87395289,\n",
       "        0.87395289, 0.87395289, 0.87395289, 0.87395289, 0.87395289,\n",
       "        0.87395289, 0.87395289, 0.87395289, 0.86932098, 0.87370651,\n",
       "        0.87385434, 0.87390362, 0.87395289, 0.87395289, 0.87395289,\n",
       "        0.87395289, 0.87395289, 0.87395289, 0.87395289, 0.87395289]),\n",
       " 'split7_test_f1_micro': array([0.86030354, 0.86813837, 0.87006012, 0.87065142, 0.87006012,\n",
       "        0.87030649, 0.87025722, 0.87030649, 0.87030649, 0.87030649,\n",
       "        0.87035577, 0.87035577, 0.87035577, 0.87035577, 0.87030649,\n",
       "        0.87035577, 0.87035577, 0.87035577, 0.86813837, 0.87065142,\n",
       "        0.87030649, 0.87030649, 0.87035577, 0.87035577, 0.87035577,\n",
       "        0.87035577, 0.87035577, 0.87035577, 0.87030649, 0.87035577]),\n",
       " 'split8_test_f1_micro': array([0.86030354, 0.86877895, 0.8705036 , 0.87124273, 0.87139056,\n",
       "        0.87134128, 0.87119346, 0.87094708, 0.87099635, 0.87099635,\n",
       "        0.87099635, 0.87099635, 0.87099635, 0.87099635, 0.87099635,\n",
       "        0.87099635, 0.87099635, 0.87099635, 0.86877895, 0.87124273,\n",
       "        0.87134128, 0.87094708, 0.87099635, 0.87099635, 0.87099635,\n",
       "        0.87099635, 0.87099635, 0.87099635, 0.87099635, 0.87099635]),\n",
       " 'split9_test_f1_micro': array([0.86030354, 0.86858185, 0.87163694, 0.87247462, 0.87286883,\n",
       "        0.87385434, 0.87395289, 0.87395289, 0.87395289, 0.87395289,\n",
       "        0.87395289, 0.87395289, 0.87395289, 0.87395289, 0.87395289,\n",
       "        0.87395289, 0.87395289, 0.87395289, 0.86858185, 0.87247462,\n",
       "        0.87385434, 0.87395289, 0.87395289, 0.87395289, 0.87395289,\n",
       "        0.87395289, 0.87395289, 0.87395289, 0.87395289, 0.87395289]),\n",
       " 'mean_test_f1_micro': array([0.86030136, 0.86810648, 0.86977196, 0.87074268, 0.87098412,\n",
       "        0.87103832, 0.87093485, 0.87089543, 0.87090036, 0.87090036,\n",
       "        0.87090529, 0.87090529, 0.87090529, 0.87090529, 0.87090036,\n",
       "        0.87091021, 0.87090529, 0.87090529, 0.86810648, 0.87074268,\n",
       "        0.87103832, 0.87089543, 0.87090529, 0.87090529, 0.87090529,\n",
       "        0.87090529, 0.87090529, 0.87090529, 0.87090036, 0.87090529]),\n",
       " 'std_test_f1_micro': array([1.37542069e-05, 1.09535211e-03, 1.45021262e-03, 1.60362663e-03,\n",
       "        1.53781832e-03, 1.60930025e-03, 1.64876627e-03, 1.69311090e-03,\n",
       "        1.70770936e-03, 1.70770936e-03, 1.70605897e-03, 1.70605897e-03,\n",
       "        1.70605897e-03, 1.70605897e-03, 1.70770936e-03, 1.69882768e-03,\n",
       "        1.70605897e-03, 1.70605897e-03, 1.09535211e-03, 1.60362663e-03,\n",
       "        1.60930025e-03, 1.69311090e-03, 1.70605897e-03, 1.70605897e-03,\n",
       "        1.70605897e-03, 1.70605897e-03, 1.70605897e-03, 1.70605897e-03,\n",
       "        1.70770936e-03, 1.70605897e-03]),\n",
       " 'rank_test_f1_micro': array([30, 28, 27, 25,  3,  1,  4, 23, 19, 19,  6,  6,  6,  6, 19,  5,  6,\n",
       "         6, 28, 25,  1, 23,  6,  6,  6,  6,  6,  6, 19,  6])}"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_model.cv_results_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "a76a5ef1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([30, 28, 27, 25,  3,  1,  4, 23, 19, 19,  6,  6,  6,  6, 19,  5,  6,\n",
       "        6, 28, 25,  1, 23,  6,  6,  6,  6,  6,  6, 19,  6])"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_model.cv_results_['rank_test_accuracy']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "80177251",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'classifier': LogisticRegression(max_iter=10000),\n",
       " 'classifier__C': 0.0001,\n",
       " 'classifier__penalty': 'l1',\n",
       " 'classifier__solver': 'saga'}"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_model.cv_results_['params'][ np.argmin(best_model.cv_results_['mean_test_accuracy'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "543c8047",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>classifier</th>\n",
       "      <th>C</th>\n",
       "      <th>penalty</th>\n",
       "      <th>solver</th>\n",
       "      <th>score_acc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.860301</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.868106</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.869772</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.870743</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0100</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.870984</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0100</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.871038</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.1000</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.870935</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.1000</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.870895</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.870900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.870900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>10.0000</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.870905</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>10.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.870905</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>100.0000</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.870905</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>100.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.870905</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>1000.0000</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.870900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>1000.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.870910</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>10000.0000</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.870905</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>10000.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.870905</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.868106</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.870743</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0100</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.871038</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.1000</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.870895</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.870905</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>10.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.870905</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>100.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.870905</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>1000.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.870905</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>10000.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.870905</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>NaN</td>\n",
       "      <td>none</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.870905</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>NaN</td>\n",
       "      <td>none</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.870900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.870905</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                            classifier           C penalty solver  score_acc\n",
       "0   LogisticRegression(max_iter=10000)      0.0001      l1   saga   0.860301\n",
       "1   LogisticRegression(max_iter=10000)      0.0001      l2   saga   0.868106\n",
       "2   LogisticRegression(max_iter=10000)      0.0010      l1   saga   0.869772\n",
       "3   LogisticRegression(max_iter=10000)      0.0010      l2   saga   0.870743\n",
       "4   LogisticRegression(max_iter=10000)      0.0100      l1   saga   0.870984\n",
       "5   LogisticRegression(max_iter=10000)      0.0100      l2   saga   0.871038\n",
       "6   LogisticRegression(max_iter=10000)      0.1000      l1   saga   0.870935\n",
       "7   LogisticRegression(max_iter=10000)      0.1000      l2   saga   0.870895\n",
       "8   LogisticRegression(max_iter=10000)      1.0000      l1   saga   0.870900\n",
       "9   LogisticRegression(max_iter=10000)      1.0000      l2   saga   0.870900\n",
       "10  LogisticRegression(max_iter=10000)     10.0000      l1   saga   0.870905\n",
       "11  LogisticRegression(max_iter=10000)     10.0000      l2   saga   0.870905\n",
       "12  LogisticRegression(max_iter=10000)    100.0000      l1   saga   0.870905\n",
       "13  LogisticRegression(max_iter=10000)    100.0000      l2   saga   0.870905\n",
       "14  LogisticRegression(max_iter=10000)   1000.0000      l1   saga   0.870900\n",
       "15  LogisticRegression(max_iter=10000)   1000.0000      l2   saga   0.870910\n",
       "16  LogisticRegression(max_iter=10000)  10000.0000      l1   saga   0.870905\n",
       "17  LogisticRegression(max_iter=10000)  10000.0000      l2   saga   0.870905\n",
       "18  LogisticRegression(max_iter=10000)      0.0001      l2  lbfgs   0.868106\n",
       "19  LogisticRegression(max_iter=10000)      0.0010      l2  lbfgs   0.870743\n",
       "20  LogisticRegression(max_iter=10000)      0.0100      l2  lbfgs   0.871038\n",
       "21  LogisticRegression(max_iter=10000)      0.1000      l2  lbfgs   0.870895\n",
       "22  LogisticRegression(max_iter=10000)      1.0000      l2  lbfgs   0.870905\n",
       "23  LogisticRegression(max_iter=10000)     10.0000      l2  lbfgs   0.870905\n",
       "24  LogisticRegression(max_iter=10000)    100.0000      l2  lbfgs   0.870905\n",
       "25  LogisticRegression(max_iter=10000)   1000.0000      l2  lbfgs   0.870905\n",
       "26  LogisticRegression(max_iter=10000)  10000.0000      l2  lbfgs   0.870905\n",
       "27  LogisticRegression(max_iter=10000)         NaN    none  lbfgs   0.870905\n",
       "28  LogisticRegression(max_iter=10000)         NaN    none   saga   0.870900\n",
       "29  LogisticRegression(max_iter=10000)         NaN     NaN    NaN   0.870905"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results = pd.DataFrame( best_model.cv_results_['params'] )\n",
    "\n",
    "results['score_acc'] = best_model.cv_results_['mean_test_accuracy']\n",
    "cols = results.columns.to_series().str.split('__').apply(lambda x: x[-1])\n",
    "results.columns = cols\n",
    "\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "557091b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "acc_list.append(results[0:1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "b765827e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'classifier': LogisticRegression(max_iter=10000),\n",
       " 'classifier__C': 0.0001,\n",
       " 'classifier__penalty': 'l2',\n",
       " 'classifier__solver': 'saga'}"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_model.cv_results_['params'][ np.argmin(best_model.cv_results_['rank_test_precision'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "3d3fe844",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>classifier</th>\n",
       "      <th>C</th>\n",
       "      <th>penalty</th>\n",
       "      <th>solver</th>\n",
       "      <th>score_precision</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.644051</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.620415</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.606829</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0100</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.602776</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0100</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.600733</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.1000</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.599130</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.1000</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.598516</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.598502</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.598502</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>10.0000</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.598539</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>10.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.598539</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>100.0000</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.598539</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>100.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.598539</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>1000.0000</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.598502</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>1000.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.598591</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>10000.0000</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.598539</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>10000.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.598539</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.644051</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.606829</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0100</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.600733</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.1000</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.598516</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.598539</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>10.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.598539</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>100.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.598539</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>1000.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.598539</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>10000.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.598539</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>NaN</td>\n",
       "      <td>none</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.598539</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>NaN</td>\n",
       "      <td>none</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.598502</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.598539</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                            classifier           C penalty solver  \\\n",
       "0   LogisticRegression(max_iter=10000)      0.0001      l1   saga   \n",
       "1   LogisticRegression(max_iter=10000)      0.0001      l2   saga   \n",
       "2   LogisticRegression(max_iter=10000)      0.0010      l1   saga   \n",
       "3   LogisticRegression(max_iter=10000)      0.0010      l2   saga   \n",
       "4   LogisticRegression(max_iter=10000)      0.0100      l1   saga   \n",
       "5   LogisticRegression(max_iter=10000)      0.0100      l2   saga   \n",
       "6   LogisticRegression(max_iter=10000)      0.1000      l1   saga   \n",
       "7   LogisticRegression(max_iter=10000)      0.1000      l2   saga   \n",
       "8   LogisticRegression(max_iter=10000)      1.0000      l1   saga   \n",
       "9   LogisticRegression(max_iter=10000)      1.0000      l2   saga   \n",
       "10  LogisticRegression(max_iter=10000)     10.0000      l1   saga   \n",
       "11  LogisticRegression(max_iter=10000)     10.0000      l2   saga   \n",
       "12  LogisticRegression(max_iter=10000)    100.0000      l1   saga   \n",
       "13  LogisticRegression(max_iter=10000)    100.0000      l2   saga   \n",
       "14  LogisticRegression(max_iter=10000)   1000.0000      l1   saga   \n",
       "15  LogisticRegression(max_iter=10000)   1000.0000      l2   saga   \n",
       "16  LogisticRegression(max_iter=10000)  10000.0000      l1   saga   \n",
       "17  LogisticRegression(max_iter=10000)  10000.0000      l2   saga   \n",
       "18  LogisticRegression(max_iter=10000)      0.0001      l2  lbfgs   \n",
       "19  LogisticRegression(max_iter=10000)      0.0010      l2  lbfgs   \n",
       "20  LogisticRegression(max_iter=10000)      0.0100      l2  lbfgs   \n",
       "21  LogisticRegression(max_iter=10000)      0.1000      l2  lbfgs   \n",
       "22  LogisticRegression(max_iter=10000)      1.0000      l2  lbfgs   \n",
       "23  LogisticRegression(max_iter=10000)     10.0000      l2  lbfgs   \n",
       "24  LogisticRegression(max_iter=10000)    100.0000      l2  lbfgs   \n",
       "25  LogisticRegression(max_iter=10000)   1000.0000      l2  lbfgs   \n",
       "26  LogisticRegression(max_iter=10000)  10000.0000      l2  lbfgs   \n",
       "27  LogisticRegression(max_iter=10000)         NaN    none  lbfgs   \n",
       "28  LogisticRegression(max_iter=10000)         NaN    none   saga   \n",
       "29  LogisticRegression(max_iter=10000)         NaN     NaN    NaN   \n",
       "\n",
       "    score_precision  \n",
       "0          0.000000  \n",
       "1          0.644051  \n",
       "2          0.620415  \n",
       "3          0.606829  \n",
       "4          0.602776  \n",
       "5          0.600733  \n",
       "6          0.599130  \n",
       "7          0.598516  \n",
       "8          0.598502  \n",
       "9          0.598502  \n",
       "10         0.598539  \n",
       "11         0.598539  \n",
       "12         0.598539  \n",
       "13         0.598539  \n",
       "14         0.598502  \n",
       "15         0.598591  \n",
       "16         0.598539  \n",
       "17         0.598539  \n",
       "18         0.644051  \n",
       "19         0.606829  \n",
       "20         0.600733  \n",
       "21         0.598516  \n",
       "22         0.598539  \n",
       "23         0.598539  \n",
       "24         0.598539  \n",
       "25         0.598539  \n",
       "26         0.598539  \n",
       "27         0.598539  \n",
       "28         0.598502  \n",
       "29         0.598539  "
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results = pd.DataFrame( best_model.cv_results_['params'] )\n",
    "\n",
    "results['score_precision'] = best_model.cv_results_['mean_test_precision']\n",
    "cols = results.columns.to_series().str.split('__').apply(lambda x: x[-1])\n",
    "results.columns = cols\n",
    "\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "b27ecb6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "roc_auc_ovr_List.append(results[1:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "531a42ca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'classifier': LogisticRegression(max_iter=10000),\n",
       " 'classifier__C': 0.01,\n",
       " 'classifier__penalty': 'l2',\n",
       " 'classifier__solver': 'saga'}"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_model.cv_results_['params'][ np.argmin(best_model.cv_results_['rank_test_f1_micro'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "37066f63",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>classifier</th>\n",
       "      <th>C</th>\n",
       "      <th>penalty</th>\n",
       "      <th>solver</th>\n",
       "      <th>score_f1_micro</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.860301</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.868106</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.869772</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.870743</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0100</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.870984</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0100</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.871038</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.1000</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.870935</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.1000</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.870895</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.870900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.870900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>10.0000</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.870905</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>10.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.870905</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>100.0000</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.870905</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>100.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.870905</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>1000.0000</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.870900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>1000.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.870910</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>10000.0000</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.870905</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>10000.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.870905</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.868106</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.870743</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0100</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.871038</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.1000</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.870895</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.870905</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>10.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.870905</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>100.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.870905</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>1000.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.870905</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>10000.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.870905</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>NaN</td>\n",
       "      <td>none</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.870905</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>NaN</td>\n",
       "      <td>none</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.870900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.870905</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                            classifier           C penalty solver  \\\n",
       "0   LogisticRegression(max_iter=10000)      0.0001      l1   saga   \n",
       "1   LogisticRegression(max_iter=10000)      0.0001      l2   saga   \n",
       "2   LogisticRegression(max_iter=10000)      0.0010      l1   saga   \n",
       "3   LogisticRegression(max_iter=10000)      0.0010      l2   saga   \n",
       "4   LogisticRegression(max_iter=10000)      0.0100      l1   saga   \n",
       "5   LogisticRegression(max_iter=10000)      0.0100      l2   saga   \n",
       "6   LogisticRegression(max_iter=10000)      0.1000      l1   saga   \n",
       "7   LogisticRegression(max_iter=10000)      0.1000      l2   saga   \n",
       "8   LogisticRegression(max_iter=10000)      1.0000      l1   saga   \n",
       "9   LogisticRegression(max_iter=10000)      1.0000      l2   saga   \n",
       "10  LogisticRegression(max_iter=10000)     10.0000      l1   saga   \n",
       "11  LogisticRegression(max_iter=10000)     10.0000      l2   saga   \n",
       "12  LogisticRegression(max_iter=10000)    100.0000      l1   saga   \n",
       "13  LogisticRegression(max_iter=10000)    100.0000      l2   saga   \n",
       "14  LogisticRegression(max_iter=10000)   1000.0000      l1   saga   \n",
       "15  LogisticRegression(max_iter=10000)   1000.0000      l2   saga   \n",
       "16  LogisticRegression(max_iter=10000)  10000.0000      l1   saga   \n",
       "17  LogisticRegression(max_iter=10000)  10000.0000      l2   saga   \n",
       "18  LogisticRegression(max_iter=10000)      0.0001      l2  lbfgs   \n",
       "19  LogisticRegression(max_iter=10000)      0.0010      l2  lbfgs   \n",
       "20  LogisticRegression(max_iter=10000)      0.0100      l2  lbfgs   \n",
       "21  LogisticRegression(max_iter=10000)      0.1000      l2  lbfgs   \n",
       "22  LogisticRegression(max_iter=10000)      1.0000      l2  lbfgs   \n",
       "23  LogisticRegression(max_iter=10000)     10.0000      l2  lbfgs   \n",
       "24  LogisticRegression(max_iter=10000)    100.0000      l2  lbfgs   \n",
       "25  LogisticRegression(max_iter=10000)   1000.0000      l2  lbfgs   \n",
       "26  LogisticRegression(max_iter=10000)  10000.0000      l2  lbfgs   \n",
       "27  LogisticRegression(max_iter=10000)         NaN    none  lbfgs   \n",
       "28  LogisticRegression(max_iter=10000)         NaN    none   saga   \n",
       "29  LogisticRegression(max_iter=10000)         NaN     NaN    NaN   \n",
       "\n",
       "    score_f1_micro  \n",
       "0         0.860301  \n",
       "1         0.868106  \n",
       "2         0.869772  \n",
       "3         0.870743  \n",
       "4         0.870984  \n",
       "5         0.871038  \n",
       "6         0.870935  \n",
       "7         0.870895  \n",
       "8         0.870900  \n",
       "9         0.870900  \n",
       "10        0.870905  \n",
       "11        0.870905  \n",
       "12        0.870905  \n",
       "13        0.870905  \n",
       "14        0.870900  \n",
       "15        0.870910  \n",
       "16        0.870905  \n",
       "17        0.870905  \n",
       "18        0.868106  \n",
       "19        0.870743  \n",
       "20        0.871038  \n",
       "21        0.870895  \n",
       "22        0.870905  \n",
       "23        0.870905  \n",
       "24        0.870905  \n",
       "25        0.870905  \n",
       "26        0.870905  \n",
       "27        0.870905  \n",
       "28        0.870900  \n",
       "29        0.870905  "
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results = pd.DataFrame( best_model.cv_results_['params'] )\n",
    "\n",
    "results['score_f1_micro'] = best_model.cv_results_['mean_test_f1_micro']\n",
    "cols = results.columns.to_series().str.split('__').apply(lambda x: x[-1])\n",
    "results.columns = cols\n",
    "\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "56f4f911",
   "metadata": {},
   "outputs": [],
   "source": [
    "f1_micro_List.append(results[5:6])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b7bf1d0",
   "metadata": {},
   "source": [
    "## Trial 9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "b5b093d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, Y_train, Y_test = train_test_split(X, y, test_size=0.2, shuffle=True, random_state = 9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "90fdec31",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Guest-All\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\Guest-All\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\Guest-All\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\Guest-All\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\Guest-All\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\Guest-All\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\Guest-All\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\Guest-All\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\Guest-All\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\Guest-All\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\Guest-All\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1183: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Guest-All\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1183: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Guest-All\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1183: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Guest-All\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1183: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Guest-All\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1183: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Guest-All\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1183: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Guest-All\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1183: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Guest-All\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1183: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Guest-All\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1183: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Guest-All\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1183: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Guest-All\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1183: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Guest-All\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1183: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Guest-All\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1183: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Guest-All\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1183: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Guest-All\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1183: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Guest-All\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1183: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Guest-All\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1183: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Guest-All\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1183: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Guest-All\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1183: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Guest-All\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1183: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Create a pipeline - RF is a stand in, we will populate the classifier part below\n",
    "pipe = Pipeline([('std', StandardScaler()),\n",
    "                 ('classifier', LogisticRegression())])\n",
    "\n",
    "# Create search space of candidate learning algorithms and their hyperparameters\n",
    "# note lbfgs can't do l1, and if you pass penalty='none' it expects no C value\n",
    "search_space = [{'classifier': [LogisticRegression(max_iter=10000)],\n",
    "                 'classifier__solver': ['saga'],\n",
    "                 'classifier__penalty': ['l1', 'l2'],\n",
    "                 'classifier__C': np.logspace(-4, 4, 9)},\n",
    "                {'classifier': [LogisticRegression(max_iter=10000)],\n",
    "                 'classifier__solver': ['lbfgs'],\n",
    "                 'classifier__penalty': ['l2'],\n",
    "                 'classifier__C': np.logspace(-4, 4, 9)},\n",
    "                {'classifier': [LogisticRegression(max_iter=10000)],\n",
    "                 'classifier__solver': ['lbfgs','saga'],\n",
    "                 'classifier__penalty': ['none']},\n",
    "                {'classifier': [LogisticRegression(max_iter=10000)]}\n",
    "                ]\n",
    "\n",
    "# Create grid search\n",
    "clf = GridSearchCV(pipe, search_space, cv=StratifiedKFold(n_splits=10),\n",
    "                   scoring=['accuracy', 'precision', 'f1_micro'], refit=False,\n",
    "                   verbose=0)\n",
    "\n",
    "# Fit grid search\n",
    "best_model = clf.fit(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "079fae53",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'mean_fit_time': array([ 3.81404195,  3.29863496,  3.67973931,  3.18386858,  3.79206805,\n",
       "         3.12478194,  5.31404982,  3.3342998 , 22.93795497,  3.28737216,\n",
       "         3.84688201,  3.21711311,  4.11449006,  3.47707298,  3.99158392,\n",
       "         3.20086236,  4.08531544,  3.58315196,  0.65118871,  0.80257869,\n",
       "         0.78553126,  0.78475246,  0.78589613,  0.81626387,  0.78615971,\n",
       "         0.75021381,  0.78681819,  0.78860142,  3.05998812,  0.82441604]),\n",
       " 'std_fit_time': array([0.42957878, 0.59670179, 0.30380133, 0.33204467, 0.24284309,\n",
       "        0.18985491, 4.24008378, 0.26816328, 3.18096944, 0.41882781,\n",
       "        0.22163342, 0.38832972, 0.70127485, 0.21001656, 0.66482569,\n",
       "        0.4266609 , 0.38626548, 0.70893669, 0.16217133, 0.22771697,\n",
       "        0.02723293, 0.01633426, 0.03721344, 0.01374991, 0.02477595,\n",
       "        0.05236972, 0.02391608, 0.03823321, 0.37355285, 0.01752674]),\n",
       " 'mean_score_time': array([0.03375831, 0.03261621, 0.03203528, 0.02993832, 0.03121328,\n",
       "        0.0299391 , 0.03146269, 0.03187337, 0.03328116, 0.03168507,\n",
       "        0.03299561, 0.03458648, 0.0316864 , 0.03055046, 0.02913902,\n",
       "        0.03188255, 0.03191128, 0.03629041, 0.03491602, 0.03344173,\n",
       "        0.03119581, 0.03194418, 0.03054349, 0.03162553, 0.03119893,\n",
       "        0.02950854, 0.03238473, 0.02925909, 0.02899206, 0.03127501]),\n",
       " 'std_score_time': array([0.00783368, 0.00822578, 0.00123888, 0.00387745, 0.00255559,\n",
       "        0.00279028, 0.00224009, 0.00192485, 0.00134645, 0.00447615,\n",
       "        0.00415074, 0.00615691, 0.00987701, 0.00343614, 0.00465719,\n",
       "        0.00275321, 0.00430912, 0.009413  , 0.01116548, 0.01130101,\n",
       "        0.00234639, 0.00117816, 0.00272787, 0.0011574 , 0.001504  ,\n",
       "        0.00381022, 0.00069738, 0.00282425, 0.00423968, 0.00228516]),\n",
       " 'param_classifier': masked_array(data=[LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000)],\n",
       "              mask=[False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'param_classifier__C': masked_array(data=[0.0001, 0.0001, 0.001, 0.001, 0.01, 0.01, 0.1, 0.1,\n",
       "                    1.0, 1.0, 10.0, 10.0, 100.0, 100.0, 1000.0, 1000.0,\n",
       "                    10000.0, 10000.0, 0.0001, 0.001, 0.01, 0.1, 1.0, 10.0,\n",
       "                    100.0, 1000.0, 10000.0, --, --, --],\n",
       "              mask=[False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False,  True,  True,  True],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'param_classifier__penalty': masked_array(data=['l1', 'l2', 'l1', 'l2', 'l1', 'l2', 'l1', 'l2', 'l1',\n",
       "                    'l2', 'l1', 'l2', 'l1', 'l2', 'l1', 'l2', 'l1', 'l2',\n",
       "                    'l2', 'l2', 'l2', 'l2', 'l2', 'l2', 'l2', 'l2', 'l2',\n",
       "                    'none', 'none', --],\n",
       "              mask=[False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False,  True],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'param_classifier__solver': masked_array(data=['saga', 'saga', 'saga', 'saga', 'saga', 'saga', 'saga',\n",
       "                    'saga', 'saga', 'saga', 'saga', 'saga', 'saga', 'saga',\n",
       "                    'saga', 'saga', 'saga', 'saga', 'lbfgs', 'lbfgs',\n",
       "                    'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs',\n",
       "                    'lbfgs', 'lbfgs', 'saga', --],\n",
       "              mask=[False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False,  True],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'params': [{'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 0.0001,\n",
       "   'classifier__penalty': 'l1',\n",
       "   'classifier__solver': 'saga'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 0.0001,\n",
       "   'classifier__penalty': 'l2',\n",
       "   'classifier__solver': 'saga'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 0.001,\n",
       "   'classifier__penalty': 'l1',\n",
       "   'classifier__solver': 'saga'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 0.001,\n",
       "   'classifier__penalty': 'l2',\n",
       "   'classifier__solver': 'saga'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 0.01,\n",
       "   'classifier__penalty': 'l1',\n",
       "   'classifier__solver': 'saga'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 0.01,\n",
       "   'classifier__penalty': 'l2',\n",
       "   'classifier__solver': 'saga'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 0.1,\n",
       "   'classifier__penalty': 'l1',\n",
       "   'classifier__solver': 'saga'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 0.1,\n",
       "   'classifier__penalty': 'l2',\n",
       "   'classifier__solver': 'saga'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 1.0,\n",
       "   'classifier__penalty': 'l1',\n",
       "   'classifier__solver': 'saga'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 1.0,\n",
       "   'classifier__penalty': 'l2',\n",
       "   'classifier__solver': 'saga'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 10.0,\n",
       "   'classifier__penalty': 'l1',\n",
       "   'classifier__solver': 'saga'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 10.0,\n",
       "   'classifier__penalty': 'l2',\n",
       "   'classifier__solver': 'saga'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 100.0,\n",
       "   'classifier__penalty': 'l1',\n",
       "   'classifier__solver': 'saga'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 100.0,\n",
       "   'classifier__penalty': 'l2',\n",
       "   'classifier__solver': 'saga'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 1000.0,\n",
       "   'classifier__penalty': 'l1',\n",
       "   'classifier__solver': 'saga'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 1000.0,\n",
       "   'classifier__penalty': 'l2',\n",
       "   'classifier__solver': 'saga'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 10000.0,\n",
       "   'classifier__penalty': 'l1',\n",
       "   'classifier__solver': 'saga'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 10000.0,\n",
       "   'classifier__penalty': 'l2',\n",
       "   'classifier__solver': 'saga'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 0.0001,\n",
       "   'classifier__penalty': 'l2',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 0.001,\n",
       "   'classifier__penalty': 'l2',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 0.01,\n",
       "   'classifier__penalty': 'l2',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 0.1,\n",
       "   'classifier__penalty': 'l2',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 1.0,\n",
       "   'classifier__penalty': 'l2',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 10.0,\n",
       "   'classifier__penalty': 'l2',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 100.0,\n",
       "   'classifier__penalty': 'l2',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 1000.0,\n",
       "   'classifier__penalty': 'l2',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 10000.0,\n",
       "   'classifier__penalty': 'l2',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__penalty': 'none',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__penalty': 'none',\n",
       "   'classifier__solver': 'saga'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000)}],\n",
       " 'split0_test_accuracy': array([0.86075388, 0.86942597, 0.87036216, 0.87257945, 0.87218527,\n",
       "        0.87262873, 0.87243163, 0.87248091, 0.87243163, 0.87243163,\n",
       "        0.87243163, 0.87243163, 0.87243163, 0.87243163, 0.87248091,\n",
       "        0.87243163, 0.87253018, 0.87243163, 0.86942597, 0.87257945,\n",
       "        0.87262873, 0.87243163, 0.87238236, 0.87243163, 0.87243163,\n",
       "        0.87243163, 0.87243163, 0.87243163, 0.87243163, 0.87238236]),\n",
       " 'split1_test_accuracy': array([0.86075388, 0.86932742, 0.8711998 , 0.87115053, 0.87115053,\n",
       "        0.87090416, 0.87090416, 0.87085489, 0.87080562, 0.87080562,\n",
       "        0.87080562, 0.87080562, 0.87080562, 0.87080562, 0.87080562,\n",
       "        0.87080562, 0.87080562, 0.87080562, 0.86932742, 0.87115053,\n",
       "        0.87090416, 0.87085489, 0.87080562, 0.87080562, 0.87080562,\n",
       "        0.87080562, 0.87080562, 0.87080562, 0.87080562, 0.87080562]),\n",
       " 'split2_test_accuracy': array([0.86075388, 0.86706085, 0.86967233, 0.86977088, 0.86967233,\n",
       "        0.86962306, 0.86982015, 0.86977088, 0.86977088, 0.86977088,\n",
       "        0.86977088, 0.86977088, 0.86977088, 0.86977088, 0.86977088,\n",
       "        0.86977088, 0.86977088, 0.86977088, 0.86706085, 0.86972161,\n",
       "        0.86967233, 0.86977088, 0.86977088, 0.86977088, 0.86977088,\n",
       "        0.86977088, 0.86977088, 0.86977088, 0.86977088, 0.86977088]),\n",
       " 'split3_test_accuracy': array([0.86075388, 0.8678985 , 0.86794777, 0.86986943, 0.86986943,\n",
       "        0.87006652, 0.87001725, 0.87011579, 0.87006652, 0.87006652,\n",
       "        0.87006652, 0.87006652, 0.87006652, 0.87006652, 0.87006652,\n",
       "        0.87006652, 0.87006652, 0.87006652, 0.8678985 , 0.86982015,\n",
       "        0.87006652, 0.87011579, 0.87006652, 0.87006652, 0.87006652,\n",
       "        0.87006652, 0.87006652, 0.87006652, 0.87006652, 0.87006652]),\n",
       " 'split4_test_accuracy': array([0.86074702, 0.87030649, 0.87129201, 0.87208042, 0.87163694,\n",
       "        0.87139056, 0.87148911, 0.87143983, 0.87143983, 0.87143983,\n",
       "        0.87143983, 0.87143983, 0.87143983, 0.87143983, 0.87143983,\n",
       "        0.87143983, 0.87143983, 0.87143983, 0.87025722, 0.87208042,\n",
       "        0.87139056, 0.87143983, 0.87143983, 0.87143983, 0.87143983,\n",
       "        0.87143983, 0.87143983, 0.87143983, 0.87143983, 0.87143983]),\n",
       " 'split5_test_accuracy': array([0.86074702, 0.86670937, 0.86863112, 0.87035577, 0.86996156,\n",
       "        0.87015867, 0.87040505, 0.87030649, 0.87030649, 0.87030649,\n",
       "        0.87030649, 0.87030649, 0.87030649, 0.87030649, 0.87030649,\n",
       "        0.87030649, 0.87030649, 0.87030649, 0.86670937, 0.87020794,\n",
       "        0.87015867, 0.87030649, 0.87025722, 0.87025722, 0.87025722,\n",
       "        0.87025722, 0.87025722, 0.87025722, 0.87025722, 0.87025722]),\n",
       " 'split6_test_accuracy': array([0.86074702, 0.8688775 , 0.87114418, 0.87163694, 0.87208042,\n",
       "        0.87173549, 0.87158766, 0.87158766, 0.87158766, 0.87163694,\n",
       "        0.87163694, 0.87163694, 0.87163694, 0.87163694, 0.87163694,\n",
       "        0.87163694, 0.87163694, 0.87163694, 0.8688775 , 0.87163694,\n",
       "        0.87173549, 0.87158766, 0.87163694, 0.87163694, 0.87163694,\n",
       "        0.87163694, 0.87163694, 0.87163694, 0.87163694, 0.87163694]),\n",
       " 'split7_test_accuracy': array([0.86074702, 0.86902533, 0.86991229, 0.87124273, 0.87143983,\n",
       "        0.87124273, 0.87124273, 0.87129201, 0.87124273, 0.87124273,\n",
       "        0.87124273, 0.87124273, 0.87124273, 0.87124273, 0.87124273,\n",
       "        0.87124273, 0.87124273, 0.87124273, 0.86902533, 0.87124273,\n",
       "        0.87124273, 0.87124273, 0.87124273, 0.87124273, 0.87124273,\n",
       "        0.87124273, 0.87124273, 0.87124273, 0.87119346, 0.87124273]),\n",
       " 'split8_test_accuracy': array([0.86074702, 0.86937026, 0.8707007 , 0.87065142, 0.87208042,\n",
       "        0.87193259, 0.87198187, 0.87217897, 0.87208042, 0.87208042,\n",
       "        0.87208042, 0.87208042, 0.87208042, 0.87208042, 0.87208042,\n",
       "        0.87208042, 0.87208042, 0.87208042, 0.86932098, 0.87065142,\n",
       "        0.87193259, 0.87217897, 0.87208042, 0.87208042, 0.87208042,\n",
       "        0.87208042, 0.87208042, 0.87208042, 0.87208042, 0.87208042]),\n",
       " 'split9_test_accuracy': array([0.86074702, 0.86971519, 0.87178476, 0.87296738, 0.87336158,\n",
       "        0.87346014, 0.87341086, 0.87365724, 0.87365724, 0.87365724,\n",
       "        0.87365724, 0.87365724, 0.87365724, 0.87365724, 0.87365724,\n",
       "        0.87360796, 0.87365724, 0.87360796, 0.86971519, 0.87296738,\n",
       "        0.87346014, 0.87365724, 0.87365724, 0.87365724, 0.87365724,\n",
       "        0.87365724, 0.87365724, 0.87365724, 0.87365724, 0.87365724]),\n",
       " 'mean_test_accuracy': array([0.86074976, 0.86877169, 0.87026471, 0.87123049, 0.87134383,\n",
       "        0.87131426, 0.87132905, 0.87136847, 0.8713389 , 0.87134383,\n",
       "        0.87134383, 0.87134383, 0.87134383, 0.87134383, 0.87134876,\n",
       "        0.8713389 , 0.87135368, 0.8713389 , 0.86876183, 0.87120586,\n",
       "        0.87131919, 0.87135861, 0.87133398, 0.8713389 , 0.8713389 ,\n",
       "        0.8713389 , 0.8713389 , 0.8713389 , 0.87133398, 0.87133398]),\n",
       " 'std_test_accuracy': array([3.36140674e-06, 1.11171589e-03, 1.17154951e-03, 1.03990465e-03,\n",
       "        1.13469791e-03, 1.13242653e-03, 1.05392873e-03, 1.12716118e-03,\n",
       "        1.12373577e-03, 1.12492318e-03, 1.12492318e-03, 1.12492318e-03,\n",
       "        1.12492318e-03, 1.12492318e-03, 1.12977457e-03, 1.11484158e-03,\n",
       "        1.13479778e-03, 1.11484158e-03, 1.10239557e-03, 1.06636014e-03,\n",
       "        1.12514094e-03, 1.12279635e-03, 1.12487502e-03, 1.12955468e-03,\n",
       "        1.12955468e-03, 1.12955468e-03, 1.12955468e-03, 1.12955468e-03,\n",
       "        1.13007083e-03, 1.12487502e-03]),\n",
       " 'rank_test_accuracy': array([30, 28, 27, 25,  5, 24, 22,  1, 11,  6,  6,  6,  6,  6,  4, 11,  3,\n",
       "        11, 29, 26, 23,  2, 19, 11, 11, 11, 11, 11, 21, 19]),\n",
       " 'split0_test_precision': array([0.        , 0.66988417, 0.62812089, 0.62578616, 0.61623246,\n",
       "        0.61710398, 0.61449275, 0.61442308, 0.61383285, 0.61383285,\n",
       "        0.61361457, 0.61361457, 0.61339713, 0.61361457, 0.61398467,\n",
       "        0.61361457, 0.61457335, 0.61361457, 0.66988417, 0.62578616,\n",
       "        0.61710398, 0.61383285, 0.61324376, 0.61361457, 0.61361457,\n",
       "        0.61361457, 0.61361457, 0.61361457, 0.61361457, 0.61324376]),\n",
       " 'split1_test_precision': array([0.        , 0.66051661, 0.64095745, 0.60910031, 0.60173578,\n",
       "        0.59772296, 0.59716981, 0.59606373, 0.59550562, 0.59550562,\n",
       "        0.59550562, 0.59550562, 0.59550562, 0.59550562, 0.59550562,\n",
       "        0.59550562, 0.59550562, 0.59550562, 0.66051661, 0.60910031,\n",
       "        0.59772296, 0.59606373, 0.59550562, 0.59550562, 0.59550562,\n",
       "        0.59550562, 0.59550562, 0.59550562, 0.59550562, 0.59550562]),\n",
       " 'split2_test_precision': array([0.        , 0.61188811, 0.61326658, 0.5919598 , 0.58561968,\n",
       "        0.58364312, 0.58487085, 0.5843318 , 0.58417663, 0.58417663,\n",
       "        0.58417663, 0.58417663, 0.58417663, 0.58417663, 0.58417663,\n",
       "        0.58417663, 0.58417663, 0.58417663, 0.61188811, 0.5915493 ,\n",
       "        0.58418605, 0.5843318 , 0.58417663, 0.58417663, 0.58417663,\n",
       "        0.58417663, 0.58417663, 0.58417663, 0.58417663, 0.58417663]),\n",
       " 'split3_test_precision': array([0.        , 0.63602251, 0.59580052, 0.59645464, 0.59167493,\n",
       "        0.59130435, 0.58986616, 0.59064885, 0.5900858 , 0.5900858 ,\n",
       "        0.5900858 , 0.5900858 , 0.5900858 , 0.5900858 , 0.5900858 ,\n",
       "        0.5900858 , 0.5900858 , 0.5900858 , 0.63602251, 0.5960334 ,\n",
       "        0.59130435, 0.59064885, 0.5900858 , 0.5900858 , 0.5900858 ,\n",
       "        0.5900858 , 0.5900858 , 0.5900858 , 0.5900858 , 0.5900858 ]),\n",
       " 'split4_test_precision': array([0.        , 0.66724138, 0.63080685, 0.61318898, 0.60395108,\n",
       "        0.59908257, 0.59909091, 0.59854678, 0.59854678, 0.59854678,\n",
       "        0.59854678, 0.59854678, 0.59854678, 0.59854678, 0.59854678,\n",
       "        0.59854678, 0.59854678, 0.59854678, 0.66666667, 0.61318898,\n",
       "        0.59908257, 0.59854678, 0.59854678, 0.59854678, 0.59854678,\n",
       "        0.59854678, 0.59854678, 0.59854678, 0.59854678, 0.59854678]),\n",
       " 'split5_test_precision': array([0.        , 0.6152381 , 0.60152284, 0.6006192 , 0.59285005,\n",
       "        0.59280855, 0.59496124, 0.59362934, 0.59362934, 0.59344894,\n",
       "        0.59344894, 0.59344894, 0.59344894, 0.59344894, 0.59344894,\n",
       "        0.59344894, 0.59344894, 0.59344894, 0.6152381 , 0.59937888,\n",
       "        0.59280855, 0.59362934, 0.59305689, 0.59305689, 0.59305689,\n",
       "        0.59305689, 0.59305689, 0.59305689, 0.59305689, 0.59305689]),\n",
       " 'split6_test_precision': array([0.        , 0.65595463, 0.63303909, 0.61264016, 0.61230469,\n",
       "        0.60608944, 0.60416667, 0.60396975, 0.60396975, 0.60434372,\n",
       "        0.60434372, 0.60434372, 0.60434372, 0.60434372, 0.60434372,\n",
       "        0.60434372, 0.60434372, 0.60434372, 0.65595463, 0.61264016,\n",
       "        0.60608944, 0.60396975, 0.60434372, 0.60434372, 0.60434372,\n",
       "        0.60434372, 0.60434372, 0.60434372, 0.60434372, 0.60434372]),\n",
       " 'split7_test_precision': array([0.        , 0.64685315, 0.61286408, 0.60471976, 0.60055607,\n",
       "        0.59603246, 0.5955157 , 0.59587814, 0.59534467, 0.59534467,\n",
       "        0.59534467, 0.59534467, 0.59534467, 0.59534467, 0.59534467,\n",
       "        0.59534467, 0.59534467, 0.59534467, 0.64685315, 0.60471976,\n",
       "        0.59603246, 0.5955157 , 0.59534467, 0.59534467, 0.59534467,\n",
       "        0.59534467, 0.59534467, 0.59534467, 0.59481216, 0.59534467]),\n",
       " 'split8_test_precision': array([0.        , 0.65996344, 0.62849873, 0.60265577, 0.61036468,\n",
       "        0.60737938, 0.60714286, 0.60841121, 0.60727612, 0.60727612,\n",
       "        0.60727612, 0.60727612, 0.60727612, 0.60727612, 0.60727612,\n",
       "        0.60727612, 0.60727612, 0.60727612, 0.65934066, 0.60265577,\n",
       "        0.60737938, 0.60841121, 0.60727612, 0.60727612, 0.60727612,\n",
       "        0.60727612, 0.60727612, 0.60727612, 0.60727612, 0.60727612]),\n",
       " 'split9_test_precision': array([0.        , 0.6625    , 0.63053613, 0.61698113, 0.61449016,\n",
       "        0.61355634, 0.61242345, 0.6141115 , 0.61391304, 0.61391304,\n",
       "        0.61391304, 0.61391304, 0.61391304, 0.61391304, 0.61391304,\n",
       "        0.61357702, 0.61391304, 0.61337967, 0.6625    , 0.61698113,\n",
       "        0.61355634, 0.6141115 , 0.61391304, 0.61391304, 0.61391304,\n",
       "        0.61391304, 0.61391304, 0.61391304, 0.61391304, 0.61391304]),\n",
       " 'mean_test_precision': array([0.        , 0.64860621, 0.62154132, 0.60741059, 0.60297796,\n",
       "        0.60047231, 0.59997004, 0.60000142, 0.59962806, 0.59964742,\n",
       "        0.59962559, 0.59962559, 0.59960384, 0.59962559, 0.5996626 ,\n",
       "        0.59959199, 0.59972147, 0.59957225, 0.64848646, 0.60720339,\n",
       "        0.60052661, 0.59990615, 0.5995493 , 0.59958638, 0.59958638,\n",
       "        0.59958638, 0.59958638, 0.59958638, 0.59953313, 0.5995493 ]),\n",
       " 'std_test_precision': array([0.        , 0.01985189, 0.01405251, 0.00963111, 0.00994868,\n",
       "        0.00991097, 0.00904807, 0.00949623, 0.00940998, 0.00943958,\n",
       "        0.00940695, 0.00940695, 0.00937479, 0.00940695, 0.00946248,\n",
       "        0.00935632, 0.0095528 , 0.00932696, 0.01976356, 0.00983768,\n",
       "        0.0098197 , 0.00942411, 0.00937875, 0.00943339, 0.00943339,\n",
       "        0.00943339, 0.00943339, 0.00943339, 0.00945866, 0.00937875]),\n",
       " 'rank_test_precision': array([30,  1,  3,  4,  6,  8, 10,  9, 15, 14, 16, 16, 19, 16, 13, 20, 12,\n",
       "        26,  2,  5,  7, 11, 27, 21, 21, 21, 21, 21, 29, 27]),\n",
       " 'split0_test_f1_micro': array([0.86075388, 0.86942597, 0.87036216, 0.87257945, 0.87218527,\n",
       "        0.87262873, 0.87243163, 0.87248091, 0.87243163, 0.87243163,\n",
       "        0.87243163, 0.87243163, 0.87243163, 0.87243163, 0.87248091,\n",
       "        0.87243163, 0.87253018, 0.87243163, 0.86942597, 0.87257945,\n",
       "        0.87262873, 0.87243163, 0.87238236, 0.87243163, 0.87243163,\n",
       "        0.87243163, 0.87243163, 0.87243163, 0.87243163, 0.87238236]),\n",
       " 'split1_test_f1_micro': array([0.86075388, 0.86932742, 0.8711998 , 0.87115053, 0.87115053,\n",
       "        0.87090416, 0.87090416, 0.87085489, 0.87080562, 0.87080562,\n",
       "        0.87080562, 0.87080562, 0.87080562, 0.87080562, 0.87080562,\n",
       "        0.87080562, 0.87080562, 0.87080562, 0.86932742, 0.87115053,\n",
       "        0.87090416, 0.87085489, 0.87080562, 0.87080562, 0.87080562,\n",
       "        0.87080562, 0.87080562, 0.87080562, 0.87080562, 0.87080562]),\n",
       " 'split2_test_f1_micro': array([0.86075388, 0.86706085, 0.86967233, 0.86977088, 0.86967233,\n",
       "        0.86962306, 0.86982015, 0.86977088, 0.86977088, 0.86977088,\n",
       "        0.86977088, 0.86977088, 0.86977088, 0.86977088, 0.86977088,\n",
       "        0.86977088, 0.86977088, 0.86977088, 0.86706085, 0.86972161,\n",
       "        0.86967233, 0.86977088, 0.86977088, 0.86977088, 0.86977088,\n",
       "        0.86977088, 0.86977088, 0.86977088, 0.86977088, 0.86977088]),\n",
       " 'split3_test_f1_micro': array([0.86075388, 0.8678985 , 0.86794777, 0.86986943, 0.86986943,\n",
       "        0.87006652, 0.87001725, 0.87011579, 0.87006652, 0.87006652,\n",
       "        0.87006652, 0.87006652, 0.87006652, 0.87006652, 0.87006652,\n",
       "        0.87006652, 0.87006652, 0.87006652, 0.8678985 , 0.86982015,\n",
       "        0.87006652, 0.87011579, 0.87006652, 0.87006652, 0.87006652,\n",
       "        0.87006652, 0.87006652, 0.87006652, 0.87006652, 0.87006652]),\n",
       " 'split4_test_f1_micro': array([0.86074702, 0.87030649, 0.87129201, 0.87208042, 0.87163694,\n",
       "        0.87139056, 0.87148911, 0.87143983, 0.87143983, 0.87143983,\n",
       "        0.87143983, 0.87143983, 0.87143983, 0.87143983, 0.87143983,\n",
       "        0.87143983, 0.87143983, 0.87143983, 0.87025722, 0.87208042,\n",
       "        0.87139056, 0.87143983, 0.87143983, 0.87143983, 0.87143983,\n",
       "        0.87143983, 0.87143983, 0.87143983, 0.87143983, 0.87143983]),\n",
       " 'split5_test_f1_micro': array([0.86074702, 0.86670937, 0.86863112, 0.87035577, 0.86996156,\n",
       "        0.87015867, 0.87040505, 0.87030649, 0.87030649, 0.87030649,\n",
       "        0.87030649, 0.87030649, 0.87030649, 0.87030649, 0.87030649,\n",
       "        0.87030649, 0.87030649, 0.87030649, 0.86670937, 0.87020794,\n",
       "        0.87015867, 0.87030649, 0.87025722, 0.87025722, 0.87025722,\n",
       "        0.87025722, 0.87025722, 0.87025722, 0.87025722, 0.87025722]),\n",
       " 'split6_test_f1_micro': array([0.86074702, 0.8688775 , 0.87114418, 0.87163694, 0.87208042,\n",
       "        0.87173549, 0.87158766, 0.87158766, 0.87158766, 0.87163694,\n",
       "        0.87163694, 0.87163694, 0.87163694, 0.87163694, 0.87163694,\n",
       "        0.87163694, 0.87163694, 0.87163694, 0.8688775 , 0.87163694,\n",
       "        0.87173549, 0.87158766, 0.87163694, 0.87163694, 0.87163694,\n",
       "        0.87163694, 0.87163694, 0.87163694, 0.87163694, 0.87163694]),\n",
       " 'split7_test_f1_micro': array([0.86074702, 0.86902533, 0.86991229, 0.87124273, 0.87143983,\n",
       "        0.87124273, 0.87124273, 0.87129201, 0.87124273, 0.87124273,\n",
       "        0.87124273, 0.87124273, 0.87124273, 0.87124273, 0.87124273,\n",
       "        0.87124273, 0.87124273, 0.87124273, 0.86902533, 0.87124273,\n",
       "        0.87124273, 0.87124273, 0.87124273, 0.87124273, 0.87124273,\n",
       "        0.87124273, 0.87124273, 0.87124273, 0.87119346, 0.87124273]),\n",
       " 'split8_test_f1_micro': array([0.86074702, 0.86937026, 0.8707007 , 0.87065142, 0.87208042,\n",
       "        0.87193259, 0.87198187, 0.87217897, 0.87208042, 0.87208042,\n",
       "        0.87208042, 0.87208042, 0.87208042, 0.87208042, 0.87208042,\n",
       "        0.87208042, 0.87208042, 0.87208042, 0.86932098, 0.87065142,\n",
       "        0.87193259, 0.87217897, 0.87208042, 0.87208042, 0.87208042,\n",
       "        0.87208042, 0.87208042, 0.87208042, 0.87208042, 0.87208042]),\n",
       " 'split9_test_f1_micro': array([0.86074702, 0.86971519, 0.87178476, 0.87296738, 0.87336158,\n",
       "        0.87346014, 0.87341086, 0.87365724, 0.87365724, 0.87365724,\n",
       "        0.87365724, 0.87365724, 0.87365724, 0.87365724, 0.87365724,\n",
       "        0.87360796, 0.87365724, 0.87360796, 0.86971519, 0.87296738,\n",
       "        0.87346014, 0.87365724, 0.87365724, 0.87365724, 0.87365724,\n",
       "        0.87365724, 0.87365724, 0.87365724, 0.87365724, 0.87365724]),\n",
       " 'mean_test_f1_micro': array([0.86074976, 0.86877169, 0.87026471, 0.87123049, 0.87134383,\n",
       "        0.87131426, 0.87132905, 0.87136847, 0.8713389 , 0.87134383,\n",
       "        0.87134383, 0.87134383, 0.87134383, 0.87134383, 0.87134876,\n",
       "        0.8713389 , 0.87135368, 0.8713389 , 0.86876183, 0.87120586,\n",
       "        0.87131919, 0.87135861, 0.87133398, 0.8713389 , 0.8713389 ,\n",
       "        0.8713389 , 0.8713389 , 0.8713389 , 0.87133398, 0.87133398]),\n",
       " 'std_test_f1_micro': array([3.36140674e-06, 1.11171589e-03, 1.17154951e-03, 1.03990465e-03,\n",
       "        1.13469791e-03, 1.13242653e-03, 1.05392873e-03, 1.12716118e-03,\n",
       "        1.12373577e-03, 1.12492318e-03, 1.12492318e-03, 1.12492318e-03,\n",
       "        1.12492318e-03, 1.12492318e-03, 1.12977457e-03, 1.11484158e-03,\n",
       "        1.13479778e-03, 1.11484158e-03, 1.10239557e-03, 1.06636014e-03,\n",
       "        1.12514094e-03, 1.12279635e-03, 1.12487502e-03, 1.12955468e-03,\n",
       "        1.12955468e-03, 1.12955468e-03, 1.12955468e-03, 1.12955468e-03,\n",
       "        1.13007083e-03, 1.12487502e-03]),\n",
       " 'rank_test_f1_micro': array([30, 28, 27, 25,  5, 24, 22,  1, 11,  6,  6,  6,  6,  6,  4, 11,  3,\n",
       "        11, 29, 26, 23,  2, 19, 11, 11, 11, 11, 11, 21, 19])}"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_model.cv_results_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "32aa74f3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([30, 28, 27, 25,  5, 24, 22,  1, 11,  6,  6,  6,  6,  6,  4, 11,  3,\n",
       "       11, 29, 26, 23,  2, 19, 11, 11, 11, 11, 11, 21, 19])"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_model.cv_results_['rank_test_accuracy']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "b234d790",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'classifier': LogisticRegression(max_iter=10000),\n",
       " 'classifier__C': 0.0001,\n",
       " 'classifier__penalty': 'l1',\n",
       " 'classifier__solver': 'saga'}"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_model.cv_results_['params'][ np.argmin(best_model.cv_results_['mean_test_accuracy'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "ae51b2e8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>classifier</th>\n",
       "      <th>C</th>\n",
       "      <th>penalty</th>\n",
       "      <th>solver</th>\n",
       "      <th>score_acc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.860750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.868772</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.870265</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.871230</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0100</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.871344</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0100</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.871314</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.1000</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.871329</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.1000</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.871368</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.871339</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.871344</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>10.0000</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.871344</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>10.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.871344</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>100.0000</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.871344</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>100.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.871344</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>1000.0000</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.871349</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>1000.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.871339</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>10000.0000</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.871354</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>10000.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.871339</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.868762</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.871206</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0100</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.871319</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.1000</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.871359</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.871334</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>10.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.871339</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>100.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.871339</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>1000.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.871339</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>10000.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.871339</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>NaN</td>\n",
       "      <td>none</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.871339</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>NaN</td>\n",
       "      <td>none</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.871334</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.871334</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                            classifier           C penalty solver  score_acc\n",
       "0   LogisticRegression(max_iter=10000)      0.0001      l1   saga   0.860750\n",
       "1   LogisticRegression(max_iter=10000)      0.0001      l2   saga   0.868772\n",
       "2   LogisticRegression(max_iter=10000)      0.0010      l1   saga   0.870265\n",
       "3   LogisticRegression(max_iter=10000)      0.0010      l2   saga   0.871230\n",
       "4   LogisticRegression(max_iter=10000)      0.0100      l1   saga   0.871344\n",
       "5   LogisticRegression(max_iter=10000)      0.0100      l2   saga   0.871314\n",
       "6   LogisticRegression(max_iter=10000)      0.1000      l1   saga   0.871329\n",
       "7   LogisticRegression(max_iter=10000)      0.1000      l2   saga   0.871368\n",
       "8   LogisticRegression(max_iter=10000)      1.0000      l1   saga   0.871339\n",
       "9   LogisticRegression(max_iter=10000)      1.0000      l2   saga   0.871344\n",
       "10  LogisticRegression(max_iter=10000)     10.0000      l1   saga   0.871344\n",
       "11  LogisticRegression(max_iter=10000)     10.0000      l2   saga   0.871344\n",
       "12  LogisticRegression(max_iter=10000)    100.0000      l1   saga   0.871344\n",
       "13  LogisticRegression(max_iter=10000)    100.0000      l2   saga   0.871344\n",
       "14  LogisticRegression(max_iter=10000)   1000.0000      l1   saga   0.871349\n",
       "15  LogisticRegression(max_iter=10000)   1000.0000      l2   saga   0.871339\n",
       "16  LogisticRegression(max_iter=10000)  10000.0000      l1   saga   0.871354\n",
       "17  LogisticRegression(max_iter=10000)  10000.0000      l2   saga   0.871339\n",
       "18  LogisticRegression(max_iter=10000)      0.0001      l2  lbfgs   0.868762\n",
       "19  LogisticRegression(max_iter=10000)      0.0010      l2  lbfgs   0.871206\n",
       "20  LogisticRegression(max_iter=10000)      0.0100      l2  lbfgs   0.871319\n",
       "21  LogisticRegression(max_iter=10000)      0.1000      l2  lbfgs   0.871359\n",
       "22  LogisticRegression(max_iter=10000)      1.0000      l2  lbfgs   0.871334\n",
       "23  LogisticRegression(max_iter=10000)     10.0000      l2  lbfgs   0.871339\n",
       "24  LogisticRegression(max_iter=10000)    100.0000      l2  lbfgs   0.871339\n",
       "25  LogisticRegression(max_iter=10000)   1000.0000      l2  lbfgs   0.871339\n",
       "26  LogisticRegression(max_iter=10000)  10000.0000      l2  lbfgs   0.871339\n",
       "27  LogisticRegression(max_iter=10000)         NaN    none  lbfgs   0.871339\n",
       "28  LogisticRegression(max_iter=10000)         NaN    none   saga   0.871334\n",
       "29  LogisticRegression(max_iter=10000)         NaN     NaN    NaN   0.871334"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results = pd.DataFrame( best_model.cv_results_['params'] )\n",
    "\n",
    "results['score_acc'] = best_model.cv_results_['mean_test_accuracy']\n",
    "cols = results.columns.to_series().str.split('__').apply(lambda x: x[-1])\n",
    "results.columns = cols\n",
    "\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "2ed188a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "acc_list.append(results[0:1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "fb96018d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'classifier': LogisticRegression(max_iter=10000),\n",
       " 'classifier__C': 0.0001,\n",
       " 'classifier__penalty': 'l2',\n",
       " 'classifier__solver': 'saga'}"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_model.cv_results_['params'][ np.argmin(best_model.cv_results_['rank_test_precision'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "c299c812",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>classifier</th>\n",
       "      <th>C</th>\n",
       "      <th>penalty</th>\n",
       "      <th>solver</th>\n",
       "      <th>score_precision</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.648606</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.621541</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.607411</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0100</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.602978</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0100</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.600472</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.1000</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.599970</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.1000</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.600001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.599628</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.599647</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>10.0000</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.599626</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>10.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.599626</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>100.0000</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.599604</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>100.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.599626</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>1000.0000</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.599663</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>1000.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.599592</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>10000.0000</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.599721</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>10000.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.599572</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.648486</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.607203</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0100</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.600527</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.1000</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.599906</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.599549</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>10.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.599586</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>100.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.599586</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>1000.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.599586</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>10000.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.599586</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>NaN</td>\n",
       "      <td>none</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.599586</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>NaN</td>\n",
       "      <td>none</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.599533</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.599549</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                            classifier           C penalty solver  \\\n",
       "0   LogisticRegression(max_iter=10000)      0.0001      l1   saga   \n",
       "1   LogisticRegression(max_iter=10000)      0.0001      l2   saga   \n",
       "2   LogisticRegression(max_iter=10000)      0.0010      l1   saga   \n",
       "3   LogisticRegression(max_iter=10000)      0.0010      l2   saga   \n",
       "4   LogisticRegression(max_iter=10000)      0.0100      l1   saga   \n",
       "5   LogisticRegression(max_iter=10000)      0.0100      l2   saga   \n",
       "6   LogisticRegression(max_iter=10000)      0.1000      l1   saga   \n",
       "7   LogisticRegression(max_iter=10000)      0.1000      l2   saga   \n",
       "8   LogisticRegression(max_iter=10000)      1.0000      l1   saga   \n",
       "9   LogisticRegression(max_iter=10000)      1.0000      l2   saga   \n",
       "10  LogisticRegression(max_iter=10000)     10.0000      l1   saga   \n",
       "11  LogisticRegression(max_iter=10000)     10.0000      l2   saga   \n",
       "12  LogisticRegression(max_iter=10000)    100.0000      l1   saga   \n",
       "13  LogisticRegression(max_iter=10000)    100.0000      l2   saga   \n",
       "14  LogisticRegression(max_iter=10000)   1000.0000      l1   saga   \n",
       "15  LogisticRegression(max_iter=10000)   1000.0000      l2   saga   \n",
       "16  LogisticRegression(max_iter=10000)  10000.0000      l1   saga   \n",
       "17  LogisticRegression(max_iter=10000)  10000.0000      l2   saga   \n",
       "18  LogisticRegression(max_iter=10000)      0.0001      l2  lbfgs   \n",
       "19  LogisticRegression(max_iter=10000)      0.0010      l2  lbfgs   \n",
       "20  LogisticRegression(max_iter=10000)      0.0100      l2  lbfgs   \n",
       "21  LogisticRegression(max_iter=10000)      0.1000      l2  lbfgs   \n",
       "22  LogisticRegression(max_iter=10000)      1.0000      l2  lbfgs   \n",
       "23  LogisticRegression(max_iter=10000)     10.0000      l2  lbfgs   \n",
       "24  LogisticRegression(max_iter=10000)    100.0000      l2  lbfgs   \n",
       "25  LogisticRegression(max_iter=10000)   1000.0000      l2  lbfgs   \n",
       "26  LogisticRegression(max_iter=10000)  10000.0000      l2  lbfgs   \n",
       "27  LogisticRegression(max_iter=10000)         NaN    none  lbfgs   \n",
       "28  LogisticRegression(max_iter=10000)         NaN    none   saga   \n",
       "29  LogisticRegression(max_iter=10000)         NaN     NaN    NaN   \n",
       "\n",
       "    score_precision  \n",
       "0          0.000000  \n",
       "1          0.648606  \n",
       "2          0.621541  \n",
       "3          0.607411  \n",
       "4          0.602978  \n",
       "5          0.600472  \n",
       "6          0.599970  \n",
       "7          0.600001  \n",
       "8          0.599628  \n",
       "9          0.599647  \n",
       "10         0.599626  \n",
       "11         0.599626  \n",
       "12         0.599604  \n",
       "13         0.599626  \n",
       "14         0.599663  \n",
       "15         0.599592  \n",
       "16         0.599721  \n",
       "17         0.599572  \n",
       "18         0.648486  \n",
       "19         0.607203  \n",
       "20         0.600527  \n",
       "21         0.599906  \n",
       "22         0.599549  \n",
       "23         0.599586  \n",
       "24         0.599586  \n",
       "25         0.599586  \n",
       "26         0.599586  \n",
       "27         0.599586  \n",
       "28         0.599533  \n",
       "29         0.599549  "
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results = pd.DataFrame( best_model.cv_results_['params'] )\n",
    "\n",
    "results['score_precision'] = best_model.cv_results_['mean_test_precision']\n",
    "cols = results.columns.to_series().str.split('__').apply(lambda x: x[-1])\n",
    "results.columns = cols\n",
    "\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "950c010d",
   "metadata": {},
   "outputs": [],
   "source": [
    "roc_auc_ovr_List.append(results[1:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "a902208d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'classifier': LogisticRegression(max_iter=10000),\n",
       " 'classifier__C': 0.1,\n",
       " 'classifier__penalty': 'l2',\n",
       " 'classifier__solver': 'saga'}"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_model.cv_results_['params'][ np.argmin(best_model.cv_results_['rank_test_f1_micro'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "a3a9405e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>classifier</th>\n",
       "      <th>C</th>\n",
       "      <th>penalty</th>\n",
       "      <th>solver</th>\n",
       "      <th>score_f1_micro</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.860750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.868772</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.870265</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.871230</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0100</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.871344</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0100</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.871314</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.1000</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.871329</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.1000</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.871368</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.871339</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.871344</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>10.0000</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.871344</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>10.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.871344</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>100.0000</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.871344</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>100.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.871344</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>1000.0000</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.871349</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>1000.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.871339</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>10000.0000</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.871354</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>10000.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.871339</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.868762</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.871206</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0100</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.871319</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.1000</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.871359</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.871334</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>10.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.871339</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>100.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.871339</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>1000.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.871339</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>10000.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.871339</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>NaN</td>\n",
       "      <td>none</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.871339</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>NaN</td>\n",
       "      <td>none</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.871334</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.871334</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                            classifier           C penalty solver  \\\n",
       "0   LogisticRegression(max_iter=10000)      0.0001      l1   saga   \n",
       "1   LogisticRegression(max_iter=10000)      0.0001      l2   saga   \n",
       "2   LogisticRegression(max_iter=10000)      0.0010      l1   saga   \n",
       "3   LogisticRegression(max_iter=10000)      0.0010      l2   saga   \n",
       "4   LogisticRegression(max_iter=10000)      0.0100      l1   saga   \n",
       "5   LogisticRegression(max_iter=10000)      0.0100      l2   saga   \n",
       "6   LogisticRegression(max_iter=10000)      0.1000      l1   saga   \n",
       "7   LogisticRegression(max_iter=10000)      0.1000      l2   saga   \n",
       "8   LogisticRegression(max_iter=10000)      1.0000      l1   saga   \n",
       "9   LogisticRegression(max_iter=10000)      1.0000      l2   saga   \n",
       "10  LogisticRegression(max_iter=10000)     10.0000      l1   saga   \n",
       "11  LogisticRegression(max_iter=10000)     10.0000      l2   saga   \n",
       "12  LogisticRegression(max_iter=10000)    100.0000      l1   saga   \n",
       "13  LogisticRegression(max_iter=10000)    100.0000      l2   saga   \n",
       "14  LogisticRegression(max_iter=10000)   1000.0000      l1   saga   \n",
       "15  LogisticRegression(max_iter=10000)   1000.0000      l2   saga   \n",
       "16  LogisticRegression(max_iter=10000)  10000.0000      l1   saga   \n",
       "17  LogisticRegression(max_iter=10000)  10000.0000      l2   saga   \n",
       "18  LogisticRegression(max_iter=10000)      0.0001      l2  lbfgs   \n",
       "19  LogisticRegression(max_iter=10000)      0.0010      l2  lbfgs   \n",
       "20  LogisticRegression(max_iter=10000)      0.0100      l2  lbfgs   \n",
       "21  LogisticRegression(max_iter=10000)      0.1000      l2  lbfgs   \n",
       "22  LogisticRegression(max_iter=10000)      1.0000      l2  lbfgs   \n",
       "23  LogisticRegression(max_iter=10000)     10.0000      l2  lbfgs   \n",
       "24  LogisticRegression(max_iter=10000)    100.0000      l2  lbfgs   \n",
       "25  LogisticRegression(max_iter=10000)   1000.0000      l2  lbfgs   \n",
       "26  LogisticRegression(max_iter=10000)  10000.0000      l2  lbfgs   \n",
       "27  LogisticRegression(max_iter=10000)         NaN    none  lbfgs   \n",
       "28  LogisticRegression(max_iter=10000)         NaN    none   saga   \n",
       "29  LogisticRegression(max_iter=10000)         NaN     NaN    NaN   \n",
       "\n",
       "    score_f1_micro  \n",
       "0         0.860750  \n",
       "1         0.868772  \n",
       "2         0.870265  \n",
       "3         0.871230  \n",
       "4         0.871344  \n",
       "5         0.871314  \n",
       "6         0.871329  \n",
       "7         0.871368  \n",
       "8         0.871339  \n",
       "9         0.871344  \n",
       "10        0.871344  \n",
       "11        0.871344  \n",
       "12        0.871344  \n",
       "13        0.871344  \n",
       "14        0.871349  \n",
       "15        0.871339  \n",
       "16        0.871354  \n",
       "17        0.871339  \n",
       "18        0.868762  \n",
       "19        0.871206  \n",
       "20        0.871319  \n",
       "21        0.871359  \n",
       "22        0.871334  \n",
       "23        0.871339  \n",
       "24        0.871339  \n",
       "25        0.871339  \n",
       "26        0.871339  \n",
       "27        0.871339  \n",
       "28        0.871334  \n",
       "29        0.871334  "
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results = pd.DataFrame( best_model.cv_results_['params'] )\n",
    "\n",
    "results['score_f1_micro'] = best_model.cv_results_['mean_test_f1_micro']\n",
    "cols = results.columns.to_series().str.split('__').apply(lambda x: x[-1])\n",
    "results.columns = cols\n",
    "\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "40b8e54c",
   "metadata": {},
   "outputs": [],
   "source": [
    "f1_micro_List.append(results[7:8])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e446b35",
   "metadata": {},
   "source": [
    "## Trial 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "6b8df2fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, Y_train, Y_test = train_test_split(X, y, test_size=0.2, shuffle=True, random_state = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "8cf50a94",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Guest-All\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\Guest-All\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\Guest-All\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\Guest-All\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\Guest-All\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\Guest-All\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\Guest-All\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\Guest-All\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\Guest-All\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\Guest-All\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\Guest-All\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1183: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Guest-All\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1183: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Guest-All\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1183: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Guest-All\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1183: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Guest-All\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1183: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Guest-All\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1183: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Guest-All\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1183: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Guest-All\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1183: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Guest-All\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1183: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Guest-All\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1183: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Guest-All\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1183: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Guest-All\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1183: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Guest-All\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1183: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Guest-All\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1183: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Guest-All\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1183: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Guest-All\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1183: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Guest-All\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1183: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Guest-All\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1183: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Guest-All\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1183: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Guest-All\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1183: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Create a pipeline - RF is a stand in, we will populate the classifier part below\n",
    "pipe = Pipeline([('std', StandardScaler()),\n",
    "                 ('classifier', LogisticRegression())])\n",
    "\n",
    "# Create search space of candidate learning algorithms and their hyperparameters\n",
    "# note lbfgs can't do l1, and if you pass penalty='none' it expects no C value\n",
    "search_space = [{'classifier': [LogisticRegression(max_iter=10000)],\n",
    "                 'classifier__solver': ['saga'],\n",
    "                 'classifier__penalty': ['l1', 'l2'],\n",
    "                 'classifier__C': np.logspace(-4, 4, 9)},\n",
    "                {'classifier': [LogisticRegression(max_iter=10000)],\n",
    "                 'classifier__solver': ['lbfgs'],\n",
    "                 'classifier__penalty': ['l2'],\n",
    "                 'classifier__C': np.logspace(-4, 4, 9)},\n",
    "                {'classifier': [LogisticRegression(max_iter=10000)],\n",
    "                 'classifier__solver': ['lbfgs','saga'],\n",
    "                 'classifier__penalty': ['none']},\n",
    "                {'classifier': [LogisticRegression(max_iter=10000)]}\n",
    "                ]\n",
    "\n",
    "# Create grid search\n",
    "clf = GridSearchCV(pipe, search_space, cv=StratifiedKFold(n_splits=10),\n",
    "                   scoring=['accuracy', 'precision', 'f1_micro'], refit=False,\n",
    "                   verbose=0)\n",
    "\n",
    "# Fit grid search\n",
    "best_model = clf.fit(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "a4120924",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'mean_fit_time': array([ 3.76432095,  3.23556762,  3.63526437,  3.08584995,  3.86892366,\n",
       "         3.27888174,  4.66941769,  3.28319571, 34.78959284,  3.47551346,\n",
       "         4.13314774,  3.09002459,  4.04790206,  3.01634719,  3.87201457,\n",
       "         3.09820516,  3.8712846 ,  3.39026322,  0.5421638 ,  0.68056023,\n",
       "         0.79440646,  0.81485946,  0.7936141 ,  0.79658685,  0.78473294,\n",
       "         0.78687603,  0.79767401,  0.7683157 ,  3.23616278,  0.80910137]),\n",
       " 'std_fit_time': array([0.418395  , 0.26922886, 0.29112241, 0.35162338, 0.26947284,\n",
       "        0.21283686, 0.45579279, 0.21791899, 5.27978142, 0.38222731,\n",
       "        0.29493676, 0.31975845, 0.49960318, 0.35585867, 0.30115152,\n",
       "        0.40955517, 0.39246951, 0.25466022, 0.01757247, 0.06563742,\n",
       "        0.02777561, 0.02780585, 0.01368417, 0.02430184, 0.01780923,\n",
       "        0.0292079 , 0.01963487, 0.05009041, 0.22080929, 0.01233622]),\n",
       " 'mean_score_time': array([0.03180506, 0.03167865, 0.03197293, 0.03070526, 0.03140495,\n",
       "        0.03394287, 0.03226011, 0.03162055, 0.03205814, 0.03319426,\n",
       "        0.0336868 , 0.02993679, 0.0319221 , 0.02967598, 0.03121202,\n",
       "        0.03013144, 0.03060675, 0.03431327, 0.03141308, 0.02879267,\n",
       "        0.03118551, 0.03231533, 0.03297882, 0.03344274, 0.02982912,\n",
       "        0.03018177, 0.03446763, 0.02912529, 0.03306963, 0.03383505]),\n",
       " 'std_score_time': array([0.00446889, 0.00294422, 0.00247975, 0.00443212, 0.00354245,\n",
       "        0.00127224, 0.00349133, 0.00388025, 0.00788342, 0.00688437,\n",
       "        0.00564203, 0.00423566, 0.00240887, 0.00426324, 0.0018616 ,\n",
       "        0.00241859, 0.00447322, 0.00379472, 0.00294036, 0.00521767,\n",
       "        0.00400474, 0.00271282, 0.00065571, 0.00089356, 0.00358412,\n",
       "        0.00505118, 0.00132867, 0.00466697, 0.00277399, 0.00115772]),\n",
       " 'param_classifier': masked_array(data=[LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000)],\n",
       "              mask=[False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'param_classifier__C': masked_array(data=[0.0001, 0.0001, 0.001, 0.001, 0.01, 0.01, 0.1, 0.1,\n",
       "                    1.0, 1.0, 10.0, 10.0, 100.0, 100.0, 1000.0, 1000.0,\n",
       "                    10000.0, 10000.0, 0.0001, 0.001, 0.01, 0.1, 1.0, 10.0,\n",
       "                    100.0, 1000.0, 10000.0, --, --, --],\n",
       "              mask=[False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False,  True,  True,  True],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'param_classifier__penalty': masked_array(data=['l1', 'l2', 'l1', 'l2', 'l1', 'l2', 'l1', 'l2', 'l1',\n",
       "                    'l2', 'l1', 'l2', 'l1', 'l2', 'l1', 'l2', 'l1', 'l2',\n",
       "                    'l2', 'l2', 'l2', 'l2', 'l2', 'l2', 'l2', 'l2', 'l2',\n",
       "                    'none', 'none', --],\n",
       "              mask=[False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False,  True],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'param_classifier__solver': masked_array(data=['saga', 'saga', 'saga', 'saga', 'saga', 'saga', 'saga',\n",
       "                    'saga', 'saga', 'saga', 'saga', 'saga', 'saga', 'saga',\n",
       "                    'saga', 'saga', 'saga', 'saga', 'lbfgs', 'lbfgs',\n",
       "                    'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs',\n",
       "                    'lbfgs', 'lbfgs', 'saga', --],\n",
       "              mask=[False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False,  True],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'params': [{'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 0.0001,\n",
       "   'classifier__penalty': 'l1',\n",
       "   'classifier__solver': 'saga'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 0.0001,\n",
       "   'classifier__penalty': 'l2',\n",
       "   'classifier__solver': 'saga'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 0.001,\n",
       "   'classifier__penalty': 'l1',\n",
       "   'classifier__solver': 'saga'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 0.001,\n",
       "   'classifier__penalty': 'l2',\n",
       "   'classifier__solver': 'saga'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 0.01,\n",
       "   'classifier__penalty': 'l1',\n",
       "   'classifier__solver': 'saga'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 0.01,\n",
       "   'classifier__penalty': 'l2',\n",
       "   'classifier__solver': 'saga'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 0.1,\n",
       "   'classifier__penalty': 'l1',\n",
       "   'classifier__solver': 'saga'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 0.1,\n",
       "   'classifier__penalty': 'l2',\n",
       "   'classifier__solver': 'saga'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 1.0,\n",
       "   'classifier__penalty': 'l1',\n",
       "   'classifier__solver': 'saga'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 1.0,\n",
       "   'classifier__penalty': 'l2',\n",
       "   'classifier__solver': 'saga'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 10.0,\n",
       "   'classifier__penalty': 'l1',\n",
       "   'classifier__solver': 'saga'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 10.0,\n",
       "   'classifier__penalty': 'l2',\n",
       "   'classifier__solver': 'saga'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 100.0,\n",
       "   'classifier__penalty': 'l1',\n",
       "   'classifier__solver': 'saga'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 100.0,\n",
       "   'classifier__penalty': 'l2',\n",
       "   'classifier__solver': 'saga'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 1000.0,\n",
       "   'classifier__penalty': 'l1',\n",
       "   'classifier__solver': 'saga'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 1000.0,\n",
       "   'classifier__penalty': 'l2',\n",
       "   'classifier__solver': 'saga'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 10000.0,\n",
       "   'classifier__penalty': 'l1',\n",
       "   'classifier__solver': 'saga'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 10000.0,\n",
       "   'classifier__penalty': 'l2',\n",
       "   'classifier__solver': 'saga'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 0.0001,\n",
       "   'classifier__penalty': 'l2',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 0.001,\n",
       "   'classifier__penalty': 'l2',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 0.01,\n",
       "   'classifier__penalty': 'l2',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 0.1,\n",
       "   'classifier__penalty': 'l2',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 1.0,\n",
       "   'classifier__penalty': 'l2',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 10.0,\n",
       "   'classifier__penalty': 'l2',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 100.0,\n",
       "   'classifier__penalty': 'l2',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 1000.0,\n",
       "   'classifier__penalty': 'l2',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 10000.0,\n",
       "   'classifier__penalty': 'l2',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__penalty': 'none',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__penalty': 'none',\n",
       "   'classifier__solver': 'saga'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000)}],\n",
       " 'split0_test_accuracy': array([0.86134516, 0.86972161, 0.87179108, 0.87257945, 0.87248091,\n",
       "        0.87243163, 0.87233309, 0.87233309, 0.87228381, 0.87233309,\n",
       "        0.87238236, 0.87238236, 0.87238236, 0.87238236, 0.87233309,\n",
       "        0.87238236, 0.87238236, 0.87238236, 0.86972161, 0.87257945,\n",
       "        0.87238236, 0.87233309, 0.87238236, 0.87238236, 0.87238236,\n",
       "        0.87238236, 0.87238236, 0.87238236, 0.87233309, 0.87238236]),\n",
       " 'split1_test_accuracy': array([0.86134516, 0.86720867, 0.8686376 , 0.86947524, 0.8699187 ,\n",
       "        0.86982015, 0.86986943, 0.86967233, 0.86967233, 0.86967233,\n",
       "        0.86967233, 0.86967233, 0.86962306, 0.86967233, 0.86962306,\n",
       "        0.86962306, 0.86967233, 0.86967233, 0.86720867, 0.86947524,\n",
       "        0.86982015, 0.86967233, 0.86967233, 0.86962306, 0.86962306,\n",
       "        0.86962306, 0.86962306, 0.86962306, 0.86962306, 0.86967233]),\n",
       " 'split2_test_accuracy': array([0.86134516, 0.87016507, 0.87164326, 0.87312146, 0.87376201,\n",
       "        0.87366346, 0.8739591 , 0.87405765, 0.87405765, 0.87400838,\n",
       "        0.87400838, 0.87400838, 0.87400838, 0.87400838, 0.87400838,\n",
       "        0.87400838, 0.87400838, 0.87400838, 0.87016507, 0.87312146,\n",
       "        0.87366346, 0.87405765, 0.87400838, 0.87400838, 0.87400838,\n",
       "        0.87400838, 0.87400838, 0.87400838, 0.87400838, 0.87400838]),\n",
       " 'split3_test_accuracy': array([0.86134516, 0.86853905, 0.87134762, 0.87154472, 0.87174181,\n",
       "        0.87179108, 0.87179108, 0.87179108, 0.87179108, 0.87179108,\n",
       "        0.87179108, 0.87179108, 0.87179108, 0.87179108, 0.87179108,\n",
       "        0.87179108, 0.87179108, 0.87179108, 0.86853905, 0.87154472,\n",
       "        0.87179108, 0.87179108, 0.87179108, 0.87179108, 0.87179108,\n",
       "        0.87179108, 0.87179108, 0.87179108, 0.87179108, 0.87179108]),\n",
       " 'split4_test_accuracy': array([0.8613876 , 0.86818764, 0.86971519, 0.87020794, 0.87001084,\n",
       "        0.86966591, 0.86986301, 0.86981374, 0.86996156, 0.86996156,\n",
       "        0.86991229, 0.86991229, 0.86991229, 0.86991229, 0.86991229,\n",
       "        0.86991229, 0.86991229, 0.86991229, 0.86823692, 0.87020794,\n",
       "        0.86966591, 0.86981374, 0.86991229, 0.86991229, 0.86991229,\n",
       "        0.86991229, 0.86991229, 0.86991229, 0.86991229, 0.86991229]),\n",
       " 'split5_test_accuracy': array([0.8613876 , 0.86912388, 0.87010939, 0.87099635, 0.87139056,\n",
       "        0.87183404, 0.87163694, 0.87163694, 0.87158766, 0.87158766,\n",
       "        0.87163694, 0.87163694, 0.87163694, 0.87163694, 0.87163694,\n",
       "        0.87163694, 0.87163694, 0.87163694, 0.86912388, 0.87099635,\n",
       "        0.87183404, 0.87163694, 0.87163694, 0.87163694, 0.87163694,\n",
       "        0.87163694, 0.87163694, 0.87163694, 0.87163694, 0.87163694]),\n",
       " 'split6_test_accuracy': array([0.86133833, 0.86803981, 0.8705036 , 0.87193259, 0.87242535,\n",
       "        0.87296738, 0.8729181 , 0.87286883, 0.87281955, 0.87281955,\n",
       "        0.87281955, 0.87281955, 0.87281955, 0.87281955, 0.87281955,\n",
       "        0.87281955, 0.87281955, 0.87281955, 0.86803981, 0.87193259,\n",
       "        0.87296738, 0.87286883, 0.87281955, 0.87281955, 0.87281955,\n",
       "        0.87281955, 0.87281955, 0.87281955, 0.87281955, 0.87281955]),\n",
       " 'split7_test_accuracy': array([0.86133833, 0.87114418, 0.87237607, 0.87321376, 0.87341086,\n",
       "        0.87331231, 0.87306593, 0.87321376, 0.87321376, 0.87321376,\n",
       "        0.87326303, 0.87321376, 0.87326303, 0.87326303, 0.87326303,\n",
       "        0.87326303, 0.87326303, 0.87326303, 0.87114418, 0.87321376,\n",
       "        0.87331231, 0.87321376, 0.87321376, 0.87321376, 0.87321376,\n",
       "        0.87321376, 0.87321376, 0.87321376, 0.87321376, 0.87321376]),\n",
       " 'split8_test_accuracy': array([0.86133833, 0.87025722, 0.86902533, 0.87055287, 0.87065142,\n",
       "        0.87074998, 0.87055287, 0.87065142, 0.87060215, 0.87060215,\n",
       "        0.87060215, 0.87060215, 0.87060215, 0.87060215, 0.87060215,\n",
       "        0.87060215, 0.87060215, 0.87060215, 0.87025722, 0.87055287,\n",
       "        0.87074998, 0.87065142, 0.87060215, 0.87060215, 0.87060215,\n",
       "        0.87060215, 0.87060215, 0.87060215, 0.87060215, 0.87060215]),\n",
       " 'split9_test_accuracy': array([0.86133833, 0.86823692, 0.86927171, 0.87119346, 0.8707007 ,\n",
       "        0.87074998, 0.8707007 , 0.87079925, 0.87074998, 0.87074998,\n",
       "        0.87074998, 0.87074998, 0.87074998, 0.87074998, 0.87074998,\n",
       "        0.87074998, 0.87074998, 0.87074998, 0.86823692, 0.87119346,\n",
       "        0.87074998, 0.87079925, 0.87074998, 0.87074998, 0.87074998,\n",
       "        0.87074998, 0.87074998, 0.87074998, 0.87074998, 0.87074998]),\n",
       " 'mean_test_accuracy': array([0.86135091, 0.8690624 , 0.87044208, 0.87148178, 0.87164932,\n",
       "        0.87169859, 0.87166903, 0.87168381, 0.87167395, 0.87167395,\n",
       "        0.87168381, 0.87167888, 0.87167888, 0.87168381, 0.87167395,\n",
       "        0.87167888, 0.87168381, 0.87168381, 0.86906733, 0.87148178,\n",
       "        0.87169366, 0.87168381, 0.87167888, 0.87167395, 0.87167395,\n",
       "        0.87167395, 0.87167395, 0.87167395, 0.87166903, 0.87167888]),\n",
       " 'std_test_accuracy': array([1.85965319e-05, 1.16696674e-03, 1.22877748e-03, 1.18029107e-03,\n",
       "        1.28495490e-03, 1.34532334e-03, 1.33778141e-03, 1.38588823e-03,\n",
       "        1.36747055e-03, 1.36124236e-03, 1.37533659e-03, 1.36974662e-03,\n",
       "        1.38260320e-03, 1.37533659e-03, 1.38017303e-03, 1.38260320e-03,\n",
       "        1.37533659e-03, 1.37533659e-03, 1.16336107e-03, 1.18029107e-03,\n",
       "        1.34271722e-03, 1.38588823e-03, 1.36974662e-03, 1.37702510e-03,\n",
       "        1.37702510e-03, 1.37702510e-03, 1.37702510e-03, 1.37702510e-03,\n",
       "        1.37456740e-03, 1.36974662e-03]),\n",
       " 'rank_test_accuracy': array([30, 29, 27, 25, 24,  1, 23,  7, 15, 15,  3, 11,  9,  3, 14,  9,  3,\n",
       "         3, 28, 25,  2,  7, 11, 15, 15, 15, 15, 15, 22, 11]),\n",
       " 'split0_test_precision': array([0.        , 0.64655172, 0.62740385, 0.61132812, 0.60405157,\n",
       "        0.60144274, 0.59982095, 0.59946476, 0.59893048, 0.59946476,\n",
       "        0.6       , 0.6       , 0.6       , 0.6       , 0.59946476,\n",
       "        0.6       , 0.6       , 0.6       , 0.64655172, 0.61132812,\n",
       "        0.60108303, 0.59946476, 0.6       , 0.6       , 0.6       ,\n",
       "        0.6       , 0.6       , 0.6       , 0.59946476, 0.6       ]),\n",
       " 'split1_test_precision': array([0.        , 0.61376673, 0.59840426, 0.58711721, 0.587     ,\n",
       "        0.58382066, 0.58341369, 0.58180058, 0.58164251, 0.58164251,\n",
       "        0.58164251, 0.58164251, 0.58108108, 0.58164251, 0.58108108,\n",
       "        0.58108108, 0.58164251, 0.58164251, 0.61376673, 0.58711721,\n",
       "        0.58382066, 0.58180058, 0.58164251, 0.58108108, 0.58108108,\n",
       "        0.58108108, 0.58108108, 0.58108108, 0.58108108, 0.58164251]),\n",
       " 'split2_test_precision': array([0.        , 0.66918715, 0.63624511, 0.62281603, 0.62045889,\n",
       "        0.61682243, 0.61808118, 0.61878453, 0.61878453, 0.61821527,\n",
       "        0.61821527, 0.61821527, 0.61821527, 0.61821527, 0.61821527,\n",
       "        0.61821527, 0.61821527, 0.61821527, 0.66918715, 0.62281603,\n",
       "        0.61682243, 0.61878453, 0.61821527, 0.61821527, 0.61821527,\n",
       "        0.61821527, 0.61821527, 0.61821527, 0.61821527, 0.61821527]),\n",
       " 'split3_test_precision': array([0.        , 0.63082437, 0.62515413, 0.60486322, 0.60153994,\n",
       "        0.5988806 , 0.59869646, 0.59851301, 0.59851301, 0.59851301,\n",
       "        0.59851301, 0.59851301, 0.59851301, 0.59851301, 0.59851301,\n",
       "        0.59851301, 0.59851301, 0.59851301, 0.63082437, 0.60486322,\n",
       "        0.5988806 , 0.59851301, 0.59851301, 0.59851301, 0.59851301,\n",
       "        0.59851301, 0.59851301, 0.59851301, 0.59851301, 0.59851301]),\n",
       " 'split4_test_precision': array([0.        , 0.63582677, 0.6122178 , 0.59141982, 0.58341277,\n",
       "        0.57763401, 0.57904412, 0.5785124 , 0.57967033, 0.57967033,\n",
       "        0.57913998, 0.57913998, 0.57913998, 0.57913998, 0.57913998,\n",
       "        0.57913998, 0.57913998, 0.57913998, 0.63708087, 0.59141982,\n",
       "        0.57763401, 0.5785124 , 0.57913998, 0.57913998, 0.57913998,\n",
       "        0.57913998, 0.57913998, 0.57913998, 0.57913998, 0.57913998]),\n",
       " 'split5_test_precision': array([0.        , 0.6500956 , 0.61302682, 0.6023085 , 0.59921799,\n",
       "        0.60095238, 0.59829868, 0.59829868, 0.59773371, 0.59773371,\n",
       "        0.59811321, 0.59811321, 0.59811321, 0.59811321, 0.59811321,\n",
       "        0.59811321, 0.59811321, 0.59811321, 0.6500956 , 0.6023085 ,\n",
       "        0.60095238, 0.59829868, 0.59811321, 0.59811321, 0.59811321,\n",
       "        0.59811321, 0.59811321, 0.59811321, 0.59811321, 0.59811321]),\n",
       " 'split6_test_precision': array([0.        , 0.61805556, 0.61862245, 0.61025641, 0.61040236,\n",
       "        0.61259542, 0.61179829, 0.61142857, 0.61105815, 0.61105815,\n",
       "        0.61105815, 0.61105815, 0.61105815, 0.61105815, 0.61105815,\n",
       "        0.61105815, 0.61105815, 0.61105815, 0.61805556, 0.61025641,\n",
       "        0.61259542, 0.61142857, 0.61105815, 0.61105815, 0.61105815,\n",
       "        0.61105815, 0.61105815, 0.61105815, 0.61105815, 0.61105815]),\n",
       " 'split7_test_precision': array([0.        , 0.67992767, 0.63861386, 0.6246122 , 0.6195122 ,\n",
       "        0.61494797, 0.61205273, 0.61314554, 0.61314554, 0.61314554,\n",
       "        0.61350844, 0.61314554, 0.61350844, 0.61350844, 0.61350844,\n",
       "        0.61350844, 0.61350844, 0.61350844, 0.67992767, 0.6246122 ,\n",
       "        0.61494797, 0.61314554, 0.61314554, 0.61314554, 0.61314554,\n",
       "        0.61314554, 0.61314554, 0.61314554, 0.61314554, 0.61314554]),\n",
       " 'split8_test_precision': array([0.        , 0.660746  , 0.59558824, 0.59285005, 0.58974359,\n",
       "        0.58850788, 0.58633426, 0.58709677, 0.58655617, 0.58655617,\n",
       "        0.58655617, 0.58655617, 0.58655617, 0.58655617, 0.58655617,\n",
       "        0.58655617, 0.58655617, 0.58655617, 0.660746  , 0.59285005,\n",
       "        0.58850788, 0.58709677, 0.58655617, 0.58655617, 0.58655617,\n",
       "        0.58655617, 0.58655617, 0.58655617, 0.58655617, 0.58655617]),\n",
       " 'split9_test_precision': array([0.        , 0.63011152, 0.60634082, 0.60683761, 0.5959596 ,\n",
       "        0.59427443, 0.59332024, 0.59411765, 0.59353575, 0.59353575,\n",
       "        0.59353575, 0.59353575, 0.59353575, 0.59353575, 0.59353575,\n",
       "        0.59353575, 0.59353575, 0.59353575, 0.63011152, 0.60683761,\n",
       "        0.59427443, 0.59411765, 0.59353575, 0.59353575, 0.59353575,\n",
       "        0.59353575, 0.59353575, 0.59353575, 0.59353575, 0.59353575]),\n",
       " 'mean_test_precision': array([0.        , 0.64350931, 0.61716173, 0.60544092, 0.60112989,\n",
       "        0.59898785, 0.59808606, 0.59811625, 0.59795702, 0.59795352,\n",
       "        0.59802825, 0.59799196, 0.59797211, 0.59802825, 0.59791858,\n",
       "        0.59797211, 0.59802825, 0.59802825, 0.64363472, 0.60544092,\n",
       "        0.59895188, 0.59811625, 0.59799196, 0.59793582, 0.59793582,\n",
       "        0.59793582, 0.59793582, 0.59793582, 0.59788229, 0.59799196]),\n",
       " 'std_test_precision': array([0.        , 0.02065752, 0.01403985, 0.01195933, 0.01216465,\n",
       "        0.0125934 , 0.01234916, 0.01272876, 0.01259967, 0.01251181,\n",
       "        0.01264164, 0.01259759, 0.01271532, 0.01264164, 0.01270779,\n",
       "        0.01271532, 0.01264164, 0.01264164, 0.02061426, 0.01195933,\n",
       "        0.01258685, 0.01272876, 0.01259759, 0.01267137, 0.01267137,\n",
       "        0.01267137, 0.01267137, 0.01267137, 0.01266366, 0.01259759]),\n",
       " 'rank_test_precision': array([30,  2,  3,  4,  6,  7, 11,  9, 21, 22, 12, 16, 19, 12, 28, 19, 12,\n",
       "        12,  1,  4,  8,  9, 16, 23, 23, 23, 23, 23, 29, 16]),\n",
       " 'split0_test_f1_micro': array([0.86134516, 0.86972161, 0.87179108, 0.87257945, 0.87248091,\n",
       "        0.87243163, 0.87233309, 0.87233309, 0.87228381, 0.87233309,\n",
       "        0.87238236, 0.87238236, 0.87238236, 0.87238236, 0.87233309,\n",
       "        0.87238236, 0.87238236, 0.87238236, 0.86972161, 0.87257945,\n",
       "        0.87238236, 0.87233309, 0.87238236, 0.87238236, 0.87238236,\n",
       "        0.87238236, 0.87238236, 0.87238236, 0.87233309, 0.87238236]),\n",
       " 'split1_test_f1_micro': array([0.86134516, 0.86720867, 0.8686376 , 0.86947524, 0.8699187 ,\n",
       "        0.86982015, 0.86986943, 0.86967233, 0.86967233, 0.86967233,\n",
       "        0.86967233, 0.86967233, 0.86962306, 0.86967233, 0.86962306,\n",
       "        0.86962306, 0.86967233, 0.86967233, 0.86720867, 0.86947524,\n",
       "        0.86982015, 0.86967233, 0.86967233, 0.86962306, 0.86962306,\n",
       "        0.86962306, 0.86962306, 0.86962306, 0.86962306, 0.86967233]),\n",
       " 'split2_test_f1_micro': array([0.86134516, 0.87016507, 0.87164326, 0.87312146, 0.87376201,\n",
       "        0.87366346, 0.8739591 , 0.87405765, 0.87405765, 0.87400838,\n",
       "        0.87400838, 0.87400838, 0.87400838, 0.87400838, 0.87400838,\n",
       "        0.87400838, 0.87400838, 0.87400838, 0.87016507, 0.87312146,\n",
       "        0.87366346, 0.87405765, 0.87400838, 0.87400838, 0.87400838,\n",
       "        0.87400838, 0.87400838, 0.87400838, 0.87400838, 0.87400838]),\n",
       " 'split3_test_f1_micro': array([0.86134516, 0.86853905, 0.87134762, 0.87154472, 0.87174181,\n",
       "        0.87179108, 0.87179108, 0.87179108, 0.87179108, 0.87179108,\n",
       "        0.87179108, 0.87179108, 0.87179108, 0.87179108, 0.87179108,\n",
       "        0.87179108, 0.87179108, 0.87179108, 0.86853905, 0.87154472,\n",
       "        0.87179108, 0.87179108, 0.87179108, 0.87179108, 0.87179108,\n",
       "        0.87179108, 0.87179108, 0.87179108, 0.87179108, 0.87179108]),\n",
       " 'split4_test_f1_micro': array([0.8613876 , 0.86818764, 0.86971519, 0.87020794, 0.87001084,\n",
       "        0.86966591, 0.86986301, 0.86981374, 0.86996156, 0.86996156,\n",
       "        0.86991229, 0.86991229, 0.86991229, 0.86991229, 0.86991229,\n",
       "        0.86991229, 0.86991229, 0.86991229, 0.86823692, 0.87020794,\n",
       "        0.86966591, 0.86981374, 0.86991229, 0.86991229, 0.86991229,\n",
       "        0.86991229, 0.86991229, 0.86991229, 0.86991229, 0.86991229]),\n",
       " 'split5_test_f1_micro': array([0.8613876 , 0.86912388, 0.87010939, 0.87099635, 0.87139056,\n",
       "        0.87183404, 0.87163694, 0.87163694, 0.87158766, 0.87158766,\n",
       "        0.87163694, 0.87163694, 0.87163694, 0.87163694, 0.87163694,\n",
       "        0.87163694, 0.87163694, 0.87163694, 0.86912388, 0.87099635,\n",
       "        0.87183404, 0.87163694, 0.87163694, 0.87163694, 0.87163694,\n",
       "        0.87163694, 0.87163694, 0.87163694, 0.87163694, 0.87163694]),\n",
       " 'split6_test_f1_micro': array([0.86133833, 0.86803981, 0.8705036 , 0.87193259, 0.87242535,\n",
       "        0.87296738, 0.8729181 , 0.87286883, 0.87281955, 0.87281955,\n",
       "        0.87281955, 0.87281955, 0.87281955, 0.87281955, 0.87281955,\n",
       "        0.87281955, 0.87281955, 0.87281955, 0.86803981, 0.87193259,\n",
       "        0.87296738, 0.87286883, 0.87281955, 0.87281955, 0.87281955,\n",
       "        0.87281955, 0.87281955, 0.87281955, 0.87281955, 0.87281955]),\n",
       " 'split7_test_f1_micro': array([0.86133833, 0.87114418, 0.87237607, 0.87321376, 0.87341086,\n",
       "        0.87331231, 0.87306593, 0.87321376, 0.87321376, 0.87321376,\n",
       "        0.87326303, 0.87321376, 0.87326303, 0.87326303, 0.87326303,\n",
       "        0.87326303, 0.87326303, 0.87326303, 0.87114418, 0.87321376,\n",
       "        0.87331231, 0.87321376, 0.87321376, 0.87321376, 0.87321376,\n",
       "        0.87321376, 0.87321376, 0.87321376, 0.87321376, 0.87321376]),\n",
       " 'split8_test_f1_micro': array([0.86133833, 0.87025722, 0.86902533, 0.87055287, 0.87065142,\n",
       "        0.87074998, 0.87055287, 0.87065142, 0.87060215, 0.87060215,\n",
       "        0.87060215, 0.87060215, 0.87060215, 0.87060215, 0.87060215,\n",
       "        0.87060215, 0.87060215, 0.87060215, 0.87025722, 0.87055287,\n",
       "        0.87074998, 0.87065142, 0.87060215, 0.87060215, 0.87060215,\n",
       "        0.87060215, 0.87060215, 0.87060215, 0.87060215, 0.87060215]),\n",
       " 'split9_test_f1_micro': array([0.86133833, 0.86823692, 0.86927171, 0.87119346, 0.8707007 ,\n",
       "        0.87074998, 0.8707007 , 0.87079925, 0.87074998, 0.87074998,\n",
       "        0.87074998, 0.87074998, 0.87074998, 0.87074998, 0.87074998,\n",
       "        0.87074998, 0.87074998, 0.87074998, 0.86823692, 0.87119346,\n",
       "        0.87074998, 0.87079925, 0.87074998, 0.87074998, 0.87074998,\n",
       "        0.87074998, 0.87074998, 0.87074998, 0.87074998, 0.87074998]),\n",
       " 'mean_test_f1_micro': array([0.86135091, 0.8690624 , 0.87044208, 0.87148178, 0.87164932,\n",
       "        0.87169859, 0.87166903, 0.87168381, 0.87167395, 0.87167395,\n",
       "        0.87168381, 0.87167888, 0.87167888, 0.87168381, 0.87167395,\n",
       "        0.87167888, 0.87168381, 0.87168381, 0.86906733, 0.87148178,\n",
       "        0.87169366, 0.87168381, 0.87167888, 0.87167395, 0.87167395,\n",
       "        0.87167395, 0.87167395, 0.87167395, 0.87166903, 0.87167888]),\n",
       " 'std_test_f1_micro': array([1.85965319e-05, 1.16696674e-03, 1.22877748e-03, 1.18029107e-03,\n",
       "        1.28495490e-03, 1.34532334e-03, 1.33778141e-03, 1.38588823e-03,\n",
       "        1.36747055e-03, 1.36124236e-03, 1.37533659e-03, 1.36974662e-03,\n",
       "        1.38260320e-03, 1.37533659e-03, 1.38017303e-03, 1.38260320e-03,\n",
       "        1.37533659e-03, 1.37533659e-03, 1.16336107e-03, 1.18029107e-03,\n",
       "        1.34271722e-03, 1.38588823e-03, 1.36974662e-03, 1.37702510e-03,\n",
       "        1.37702510e-03, 1.37702510e-03, 1.37702510e-03, 1.37702510e-03,\n",
       "        1.37456740e-03, 1.36974662e-03]),\n",
       " 'rank_test_f1_micro': array([30, 29, 27, 25, 24,  1, 23,  7, 15, 15,  3, 11,  9,  3, 14,  9,  3,\n",
       "         3, 28, 25,  2,  7, 11, 15, 15, 15, 15, 15, 22, 11])}"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_model.cv_results_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "6d28a919",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([30, 29, 27, 25, 24,  1, 23,  7, 15, 15,  3, 11,  9,  3, 14,  9,  3,\n",
       "        3, 28, 25,  2,  7, 11, 15, 15, 15, 15, 15, 22, 11])"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_model.cv_results_['rank_test_accuracy']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "0ff3283b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'classifier': LogisticRegression(max_iter=10000),\n",
       " 'classifier__C': 0.0001,\n",
       " 'classifier__penalty': 'l1',\n",
       " 'classifier__solver': 'saga'}"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_model.cv_results_['params'][ np.argmin(best_model.cv_results_['mean_test_accuracy'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "1b20e96a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>classifier</th>\n",
       "      <th>C</th>\n",
       "      <th>penalty</th>\n",
       "      <th>solver</th>\n",
       "      <th>score_acc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.861351</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.869062</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.870442</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.871482</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0100</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.871649</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0100</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.871699</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.1000</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.871669</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.1000</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.871684</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.871674</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.871674</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>10.0000</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.871684</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>10.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.871679</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>100.0000</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.871679</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>100.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.871684</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>1000.0000</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.871674</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>1000.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.871679</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>10000.0000</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.871684</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>10000.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.871684</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.869067</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.871482</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0100</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.871694</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.1000</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.871684</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.871679</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>10.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.871674</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>100.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.871674</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>1000.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.871674</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>10000.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.871674</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>NaN</td>\n",
       "      <td>none</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.871674</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>NaN</td>\n",
       "      <td>none</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.871669</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.871679</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                            classifier           C penalty solver  score_acc\n",
       "0   LogisticRegression(max_iter=10000)      0.0001      l1   saga   0.861351\n",
       "1   LogisticRegression(max_iter=10000)      0.0001      l2   saga   0.869062\n",
       "2   LogisticRegression(max_iter=10000)      0.0010      l1   saga   0.870442\n",
       "3   LogisticRegression(max_iter=10000)      0.0010      l2   saga   0.871482\n",
       "4   LogisticRegression(max_iter=10000)      0.0100      l1   saga   0.871649\n",
       "5   LogisticRegression(max_iter=10000)      0.0100      l2   saga   0.871699\n",
       "6   LogisticRegression(max_iter=10000)      0.1000      l1   saga   0.871669\n",
       "7   LogisticRegression(max_iter=10000)      0.1000      l2   saga   0.871684\n",
       "8   LogisticRegression(max_iter=10000)      1.0000      l1   saga   0.871674\n",
       "9   LogisticRegression(max_iter=10000)      1.0000      l2   saga   0.871674\n",
       "10  LogisticRegression(max_iter=10000)     10.0000      l1   saga   0.871684\n",
       "11  LogisticRegression(max_iter=10000)     10.0000      l2   saga   0.871679\n",
       "12  LogisticRegression(max_iter=10000)    100.0000      l1   saga   0.871679\n",
       "13  LogisticRegression(max_iter=10000)    100.0000      l2   saga   0.871684\n",
       "14  LogisticRegression(max_iter=10000)   1000.0000      l1   saga   0.871674\n",
       "15  LogisticRegression(max_iter=10000)   1000.0000      l2   saga   0.871679\n",
       "16  LogisticRegression(max_iter=10000)  10000.0000      l1   saga   0.871684\n",
       "17  LogisticRegression(max_iter=10000)  10000.0000      l2   saga   0.871684\n",
       "18  LogisticRegression(max_iter=10000)      0.0001      l2  lbfgs   0.869067\n",
       "19  LogisticRegression(max_iter=10000)      0.0010      l2  lbfgs   0.871482\n",
       "20  LogisticRegression(max_iter=10000)      0.0100      l2  lbfgs   0.871694\n",
       "21  LogisticRegression(max_iter=10000)      0.1000      l2  lbfgs   0.871684\n",
       "22  LogisticRegression(max_iter=10000)      1.0000      l2  lbfgs   0.871679\n",
       "23  LogisticRegression(max_iter=10000)     10.0000      l2  lbfgs   0.871674\n",
       "24  LogisticRegression(max_iter=10000)    100.0000      l2  lbfgs   0.871674\n",
       "25  LogisticRegression(max_iter=10000)   1000.0000      l2  lbfgs   0.871674\n",
       "26  LogisticRegression(max_iter=10000)  10000.0000      l2  lbfgs   0.871674\n",
       "27  LogisticRegression(max_iter=10000)         NaN    none  lbfgs   0.871674\n",
       "28  LogisticRegression(max_iter=10000)         NaN    none   saga   0.871669\n",
       "29  LogisticRegression(max_iter=10000)         NaN     NaN    NaN   0.871679"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results = pd.DataFrame( best_model.cv_results_['params'] )\n",
    "\n",
    "results['score_acc'] = best_model.cv_results_['mean_test_accuracy']\n",
    "cols = results.columns.to_series().str.split('__').apply(lambda x: x[-1])\n",
    "results.columns = cols\n",
    "\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "a9b8f04c",
   "metadata": {},
   "outputs": [],
   "source": [
    "acc_list.append(results[0:1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "9ff4ec73",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'classifier': LogisticRegression(max_iter=10000),\n",
       " 'classifier__C': 0.0001,\n",
       " 'classifier__penalty': 'l2',\n",
       " 'classifier__solver': 'lbfgs'}"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_model.cv_results_['params'][ np.argmin(best_model.cv_results_['rank_test_precision'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "f5204e89",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>classifier</th>\n",
       "      <th>C</th>\n",
       "      <th>penalty</th>\n",
       "      <th>solver</th>\n",
       "      <th>score_precision</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.643509</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.617162</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.605441</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0100</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.601130</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0100</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.598988</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.1000</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.598086</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.1000</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.598116</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.597957</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.597954</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>10.0000</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.598028</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>10.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.597992</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>100.0000</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.597972</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>100.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.598028</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>1000.0000</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.597919</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>1000.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.597972</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>10000.0000</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.598028</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>10000.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.598028</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.643635</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.605441</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0100</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.598952</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.1000</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.598116</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.597992</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>10.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.597936</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>100.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.597936</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>1000.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.597936</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>10000.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.597936</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>NaN</td>\n",
       "      <td>none</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.597936</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>NaN</td>\n",
       "      <td>none</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.597882</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.597992</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                            classifier           C penalty solver  \\\n",
       "0   LogisticRegression(max_iter=10000)      0.0001      l1   saga   \n",
       "1   LogisticRegression(max_iter=10000)      0.0001      l2   saga   \n",
       "2   LogisticRegression(max_iter=10000)      0.0010      l1   saga   \n",
       "3   LogisticRegression(max_iter=10000)      0.0010      l2   saga   \n",
       "4   LogisticRegression(max_iter=10000)      0.0100      l1   saga   \n",
       "5   LogisticRegression(max_iter=10000)      0.0100      l2   saga   \n",
       "6   LogisticRegression(max_iter=10000)      0.1000      l1   saga   \n",
       "7   LogisticRegression(max_iter=10000)      0.1000      l2   saga   \n",
       "8   LogisticRegression(max_iter=10000)      1.0000      l1   saga   \n",
       "9   LogisticRegression(max_iter=10000)      1.0000      l2   saga   \n",
       "10  LogisticRegression(max_iter=10000)     10.0000      l1   saga   \n",
       "11  LogisticRegression(max_iter=10000)     10.0000      l2   saga   \n",
       "12  LogisticRegression(max_iter=10000)    100.0000      l1   saga   \n",
       "13  LogisticRegression(max_iter=10000)    100.0000      l2   saga   \n",
       "14  LogisticRegression(max_iter=10000)   1000.0000      l1   saga   \n",
       "15  LogisticRegression(max_iter=10000)   1000.0000      l2   saga   \n",
       "16  LogisticRegression(max_iter=10000)  10000.0000      l1   saga   \n",
       "17  LogisticRegression(max_iter=10000)  10000.0000      l2   saga   \n",
       "18  LogisticRegression(max_iter=10000)      0.0001      l2  lbfgs   \n",
       "19  LogisticRegression(max_iter=10000)      0.0010      l2  lbfgs   \n",
       "20  LogisticRegression(max_iter=10000)      0.0100      l2  lbfgs   \n",
       "21  LogisticRegression(max_iter=10000)      0.1000      l2  lbfgs   \n",
       "22  LogisticRegression(max_iter=10000)      1.0000      l2  lbfgs   \n",
       "23  LogisticRegression(max_iter=10000)     10.0000      l2  lbfgs   \n",
       "24  LogisticRegression(max_iter=10000)    100.0000      l2  lbfgs   \n",
       "25  LogisticRegression(max_iter=10000)   1000.0000      l2  lbfgs   \n",
       "26  LogisticRegression(max_iter=10000)  10000.0000      l2  lbfgs   \n",
       "27  LogisticRegression(max_iter=10000)         NaN    none  lbfgs   \n",
       "28  LogisticRegression(max_iter=10000)         NaN    none   saga   \n",
       "29  LogisticRegression(max_iter=10000)         NaN     NaN    NaN   \n",
       "\n",
       "    score_precision  \n",
       "0          0.000000  \n",
       "1          0.643509  \n",
       "2          0.617162  \n",
       "3          0.605441  \n",
       "4          0.601130  \n",
       "5          0.598988  \n",
       "6          0.598086  \n",
       "7          0.598116  \n",
       "8          0.597957  \n",
       "9          0.597954  \n",
       "10         0.598028  \n",
       "11         0.597992  \n",
       "12         0.597972  \n",
       "13         0.598028  \n",
       "14         0.597919  \n",
       "15         0.597972  \n",
       "16         0.598028  \n",
       "17         0.598028  \n",
       "18         0.643635  \n",
       "19         0.605441  \n",
       "20         0.598952  \n",
       "21         0.598116  \n",
       "22         0.597992  \n",
       "23         0.597936  \n",
       "24         0.597936  \n",
       "25         0.597936  \n",
       "26         0.597936  \n",
       "27         0.597936  \n",
       "28         0.597882  \n",
       "29         0.597992  "
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results = pd.DataFrame( best_model.cv_results_['params'] )\n",
    "\n",
    "results['score_precision'] = best_model.cv_results_['mean_test_precision']\n",
    "cols = results.columns.to_series().str.split('__').apply(lambda x: x[-1])\n",
    "results.columns = cols\n",
    "\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "43b18550",
   "metadata": {},
   "outputs": [],
   "source": [
    "roc_auc_ovr_List.append(results[18:19])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "24071971",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'classifier': LogisticRegression(max_iter=10000),\n",
       " 'classifier__C': 0.01,\n",
       " 'classifier__penalty': 'l2',\n",
       " 'classifier__solver': 'saga'}"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_model.cv_results_['params'][ np.argmin(best_model.cv_results_['rank_test_f1_micro'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "87b8ca97",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>classifier</th>\n",
       "      <th>C</th>\n",
       "      <th>penalty</th>\n",
       "      <th>solver</th>\n",
       "      <th>score_f1_micro</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.861351</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.869062</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.870442</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.871482</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0100</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.871649</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0100</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.871699</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.1000</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.871669</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.1000</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.871684</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.871674</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.871674</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>10.0000</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.871684</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>10.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.871679</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>100.0000</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.871679</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>100.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.871684</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>1000.0000</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.871674</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>1000.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.871679</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>10000.0000</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.871684</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>10000.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.871684</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.869067</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.871482</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0100</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.871694</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.1000</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.871684</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.871679</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>10.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.871674</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>100.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.871674</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>1000.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.871674</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>10000.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.871674</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>NaN</td>\n",
       "      <td>none</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.871674</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>NaN</td>\n",
       "      <td>none</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.871669</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.871679</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                            classifier           C penalty solver  \\\n",
       "0   LogisticRegression(max_iter=10000)      0.0001      l1   saga   \n",
       "1   LogisticRegression(max_iter=10000)      0.0001      l2   saga   \n",
       "2   LogisticRegression(max_iter=10000)      0.0010      l1   saga   \n",
       "3   LogisticRegression(max_iter=10000)      0.0010      l2   saga   \n",
       "4   LogisticRegression(max_iter=10000)      0.0100      l1   saga   \n",
       "5   LogisticRegression(max_iter=10000)      0.0100      l2   saga   \n",
       "6   LogisticRegression(max_iter=10000)      0.1000      l1   saga   \n",
       "7   LogisticRegression(max_iter=10000)      0.1000      l2   saga   \n",
       "8   LogisticRegression(max_iter=10000)      1.0000      l1   saga   \n",
       "9   LogisticRegression(max_iter=10000)      1.0000      l2   saga   \n",
       "10  LogisticRegression(max_iter=10000)     10.0000      l1   saga   \n",
       "11  LogisticRegression(max_iter=10000)     10.0000      l2   saga   \n",
       "12  LogisticRegression(max_iter=10000)    100.0000      l1   saga   \n",
       "13  LogisticRegression(max_iter=10000)    100.0000      l2   saga   \n",
       "14  LogisticRegression(max_iter=10000)   1000.0000      l1   saga   \n",
       "15  LogisticRegression(max_iter=10000)   1000.0000      l2   saga   \n",
       "16  LogisticRegression(max_iter=10000)  10000.0000      l1   saga   \n",
       "17  LogisticRegression(max_iter=10000)  10000.0000      l2   saga   \n",
       "18  LogisticRegression(max_iter=10000)      0.0001      l2  lbfgs   \n",
       "19  LogisticRegression(max_iter=10000)      0.0010      l2  lbfgs   \n",
       "20  LogisticRegression(max_iter=10000)      0.0100      l2  lbfgs   \n",
       "21  LogisticRegression(max_iter=10000)      0.1000      l2  lbfgs   \n",
       "22  LogisticRegression(max_iter=10000)      1.0000      l2  lbfgs   \n",
       "23  LogisticRegression(max_iter=10000)     10.0000      l2  lbfgs   \n",
       "24  LogisticRegression(max_iter=10000)    100.0000      l2  lbfgs   \n",
       "25  LogisticRegression(max_iter=10000)   1000.0000      l2  lbfgs   \n",
       "26  LogisticRegression(max_iter=10000)  10000.0000      l2  lbfgs   \n",
       "27  LogisticRegression(max_iter=10000)         NaN    none  lbfgs   \n",
       "28  LogisticRegression(max_iter=10000)         NaN    none   saga   \n",
       "29  LogisticRegression(max_iter=10000)         NaN     NaN    NaN   \n",
       "\n",
       "    score_f1_micro  \n",
       "0         0.861351  \n",
       "1         0.869062  \n",
       "2         0.870442  \n",
       "3         0.871482  \n",
       "4         0.871649  \n",
       "5         0.871699  \n",
       "6         0.871669  \n",
       "7         0.871684  \n",
       "8         0.871674  \n",
       "9         0.871674  \n",
       "10        0.871684  \n",
       "11        0.871679  \n",
       "12        0.871679  \n",
       "13        0.871684  \n",
       "14        0.871674  \n",
       "15        0.871679  \n",
       "16        0.871684  \n",
       "17        0.871684  \n",
       "18        0.869067  \n",
       "19        0.871482  \n",
       "20        0.871694  \n",
       "21        0.871684  \n",
       "22        0.871679  \n",
       "23        0.871674  \n",
       "24        0.871674  \n",
       "25        0.871674  \n",
       "26        0.871674  \n",
       "27        0.871674  \n",
       "28        0.871669  \n",
       "29        0.871679  "
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results = pd.DataFrame( best_model.cv_results_['params'] )\n",
    "\n",
    "results['score_f1_micro'] = best_model.cv_results_['mean_test_f1_micro']\n",
    "cols = results.columns.to_series().str.split('__').apply(lambda x: x[-1])\n",
    "results.columns = cols\n",
    "\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "39fb7b98",
   "metadata": {},
   "outputs": [],
   "source": [
    "f1_micro_List.append(results[5:6])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "cd18e7d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time:  3:00:48.846202\n"
     ]
    }
   ],
   "source": [
    "end = datetime.now()\n",
    "time_taken = end - start\n",
    "print('Time: ',time_taken) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6491ce89",
   "metadata": {},
   "source": [
    "## Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "f82dc269",
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy = [x['score_acc'][0] for x in acc_list]\n",
    "accuracy_c = [x['C'][0] for x in acc_list]\n",
    "accuracy_penalty = [x['penalty'][0] for x in acc_list]\n",
    "roc = [x['score_precision'].reset_index(drop=True)[0] for x in roc_auc_ovr_List]\n",
    "roc_c = [x['C'].reset_index(drop=True)[0] for x in roc_auc_ovr_List]\n",
    "roc_penalty = [x['penalty'].reset_index(drop=True)[0] for x in roc_auc_ovr_List]\n",
    "f1 = [x['score_f1_micro'].reset_index(drop=True)[0] for x in f1_micro_List]\n",
    "f1_c = [x['C'].reset_index(drop=True)[0] for x in f1_micro_List]\n",
    "f1_penalty = [x['penalty'].reset_index(drop=True)[0] for x in f1_micro_List]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "5d58acc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {'Accuracy': accuracy, 'Accuracy C': accuracy_c, 'Accuracy Penalty': accuracy_penalty,\n",
    "        'Precision': roc, 'Precision C': roc_c, 'Precision Penalty': roc_penalty,\n",
    "        'F1_micro':f1, 'F1_micro C': f1_c, 'F1_micro Penalty': f1_penalty}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "id": "c174d24d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Accuracy C</th>\n",
       "      <th>Accuracy Penalty</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Precision C</th>\n",
       "      <th>Precision Penalty</th>\n",
       "      <th>F1_micro</th>\n",
       "      <th>F1_micro C</th>\n",
       "      <th>F1_micro Penalty</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.860646</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>l1</td>\n",
       "      <td>0.712024</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>l2</td>\n",
       "      <td>0.871447</td>\n",
       "      <td>0.01</td>\n",
       "      <td>l1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.860986</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>l1</td>\n",
       "      <td>0.645233</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>l2</td>\n",
       "      <td>0.871590</td>\n",
       "      <td>0.01</td>\n",
       "      <td>l2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.860523</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>l1</td>\n",
       "      <td>0.647027</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>l2</td>\n",
       "      <td>0.871255</td>\n",
       "      <td>0.10</td>\n",
       "      <td>l1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.860730</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>l1</td>\n",
       "      <td>0.645268</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>l2</td>\n",
       "      <td>0.871625</td>\n",
       "      <td>1.00</td>\n",
       "      <td>l2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.860365</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>l1</td>\n",
       "      <td>0.646898</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>l2</td>\n",
       "      <td>0.871157</td>\n",
       "      <td>0.10</td>\n",
       "      <td>l1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.860533</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>l1</td>\n",
       "      <td>0.649623</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>l2</td>\n",
       "      <td>0.871132</td>\n",
       "      <td>10000.00</td>\n",
       "      <td>l1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.861282</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>l1</td>\n",
       "      <td>0.646246</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>l2</td>\n",
       "      <td>0.871625</td>\n",
       "      <td>0.10</td>\n",
       "      <td>l1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.860301</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>l1</td>\n",
       "      <td>0.644051</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>l2</td>\n",
       "      <td>0.871038</td>\n",
       "      <td>1.00</td>\n",
       "      <td>l1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.860750</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>l1</td>\n",
       "      <td>0.648606</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>l2</td>\n",
       "      <td>0.871368</td>\n",
       "      <td>0.10</td>\n",
       "      <td>l1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.861351</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>l1</td>\n",
       "      <td>0.643635</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>l2</td>\n",
       "      <td>0.871699</td>\n",
       "      <td>0.10</td>\n",
       "      <td>l1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Accuracy  Accuracy C Accuracy Penalty  Precision  Precision C  \\\n",
       "0  0.860646      0.0001               l1   0.712024       0.0001   \n",
       "1  0.860986      0.0001               l1   0.645233       0.0001   \n",
       "2  0.860523      0.0001               l1   0.647027       0.0001   \n",
       "3  0.860730      0.0001               l1   0.645268       0.0001   \n",
       "4  0.860365      0.0001               l1   0.646898       0.0001   \n",
       "5  0.860533      0.0001               l1   0.649623       0.0001   \n",
       "6  0.861282      0.0001               l1   0.646246       0.0001   \n",
       "7  0.860301      0.0001               l1   0.644051       0.0001   \n",
       "8  0.860750      0.0001               l1   0.648606       0.0001   \n",
       "9  0.861351      0.0001               l1   0.643635       0.0001   \n",
       "\n",
       "  Precision Penalty  F1_micro  F1_micro C F1_micro Penalty  \n",
       "0                l2  0.871447        0.01               l1  \n",
       "1                l2  0.871590        0.01               l2  \n",
       "2                l2  0.871255        0.10               l1  \n",
       "3                l2  0.871625        1.00               l2  \n",
       "4                l2  0.871157        0.10               l1  \n",
       "5                l2  0.871132    10000.00               l1  \n",
       "6                l2  0.871625        0.10               l1  \n",
       "7                l2  0.871038        1.00               l1  \n",
       "8                l2  0.871368        0.10               l1  \n",
       "9                l2  0.871699        0.10               l1  "
      ]
     },
     "execution_count": 192,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainingResults  = pd.DataFrame(data = data)\n",
    "pd.options.display.max_colwidth = 100\n",
    "trainingResults"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "id": "21d2830e",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainingResults.to_csv('LR_trainingResults.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "531a03de",
   "metadata": {},
   "source": [
    "### Testing Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "id": "91fd59ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, Y_train, Y_test = train_test_split(X, y, test_size = 0.2, shuffle=True, random_state = 11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "id": "66eea9d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "LR = LogisticRegression(penalty = 'l1', C = 0.0001, solver = 'saga').fit(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "id": "b3a38413",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Guest-All\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import f1_score, precision_score, confusion_matrix\n",
    "acc = LR.score(X_test, Y_test)\n",
    "predicted = LR.predict(X_test)\n",
    "f1 = f1_score(Y_test, predicted)\n",
    "precision = precision_score(Y_test, predicted)\n",
    "cm = confusion_matrix(Y_test, predicted)\n",
    "tn, fp, fn, tp = confusion_matrix(Y_test, predicted).ravel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "id": "9b255fb7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8575173446862189\n",
      "Precision: 0.0\n",
      "F1: 0.0\n",
      "True Negative 43507\n",
      "False Positive 0\n",
      "False Negative 7229\n",
      "True Positive 0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Accuracy: \" + str(acc) + '\\n'\n",
    "      + \"Precision: \" + str(precision) + '\\n'\n",
    "      + \"F1: \" + str(f1) + '\\n'\n",
    "      + \"True Negative \" + str(tn) + '\\n'\n",
    "      + \"False Positive \" + str(fp) + '\\n'\n",
    "      + \"False Negative \" + str(fn) + '\\n'\n",
    "      + \"True Positive \" + str(tp) + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "id": "85ddd714",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[43507,     0],\n",
       "       [ 7229,     0]], dtype=int64)"
      ]
     },
     "execution_count": 266,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f73e35c9",
   "metadata": {},
   "source": [
    "# Increasing C due to 0's in precision and f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e0d730cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, Y_train, Y_test = train_test_split(X, y, test_size = 0.2, shuffle=True, random_state = 11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f2cb0097",
   "metadata": {},
   "outputs": [],
   "source": [
    "LR = LogisticRegression(penalty = 'l1', C = 0.01, solver = 'saga').fit(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1a504bbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score, precision_score, confusion_matrix\n",
    "acc = LR.score(X_test, Y_test)\n",
    "predicted = LR.predict(X_test)\n",
    "f1 = f1_score(Y_test, predicted)\n",
    "precision = precision_score(Y_test, predicted)\n",
    "cm = confusion_matrix(Y_test, predicted)\n",
    "tn, fp, fn, tp = confusion_matrix(Y_test, predicted).ravel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2a036834",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8665444654683065\n",
      "Precision: 0.5978632478632478\n",
      "F1: 0.29240254990072106\n",
      "True Negative: 42566\n",
      "False Positive: 941\n",
      "False Negative: 5830\n",
      "True Positive: 1399\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Accuracy: \" + str(acc) + '\\n'\n",
    "      + \"Precision: \" + str(precision) + '\\n'\n",
    "      + \"F1: \" + str(f1) + '\\n'\n",
    "      + \"True Negative: \" + str(tn) + '\\n'\n",
    "      + \"False Positive: \" + str(fp) + '\\n'\n",
    "      + \"False Negative: \" + str(fn) + '\\n'\n",
    "      + \"True Positive: \" + str(tp) + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "51c64fd6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[42566,   941],\n",
       "       [ 5830,  1399]], dtype=int64)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cm"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
