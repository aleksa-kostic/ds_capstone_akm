{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c5923a4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import KFold\n",
    "#from sklearn.preprocessing import MinMaxScaler, MaxAbsScaler, StandardScaler \n",
    "#from sklearn.preprocessing import RobustScaler, Normalizer, QuantileTransformer, PowerTransformer\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "#from sklearn import svm, datasets\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "import seaborn as sns\n",
    "from sklearn import preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8f9a1a2a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>HighBP</th>\n",
       "      <th>HighChol</th>\n",
       "      <th>CholCheck</th>\n",
       "      <th>BMI</th>\n",
       "      <th>Smoker</th>\n",
       "      <th>Stroke</th>\n",
       "      <th>HeartDiseaseorAttack</th>\n",
       "      <th>PhysActivity</th>\n",
       "      <th>Fruits</th>\n",
       "      <th>Veggies</th>\n",
       "      <th>...</th>\n",
       "      <th>NoDocbcCost</th>\n",
       "      <th>GenHlth</th>\n",
       "      <th>MentHlth</th>\n",
       "      <th>PhysHlth</th>\n",
       "      <th>DiffWalk</th>\n",
       "      <th>Sex</th>\n",
       "      <th>Age</th>\n",
       "      <th>Education</th>\n",
       "      <th>Income</th>\n",
       "      <th>Diabetes_binary</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>23</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>11</td>\n",
       "      <td>5</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>19</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>26</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>22</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>12</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>22</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>6</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>253659</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>37</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>253668</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>29</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>3</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>253670</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>25</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>15</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>13</td>\n",
       "      <td>6</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>253676</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>18</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>11</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>253679</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>25</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>6</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>88365 rows Ã— 22 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        HighBP  HighChol  CholCheck  BMI  Smoker  Stroke  \\\n",
       "0            0         0          1   23       0       0   \n",
       "1            1         0          1   19       0       0   \n",
       "2            0         0          1   26       1       0   \n",
       "3            0         1          1   22       0       0   \n",
       "4            0         0          1   22       0       0   \n",
       "...        ...       ...        ...  ...     ...     ...   \n",
       "253659       0         1          1   37       0       0   \n",
       "253668       0         1          1   29       1       0   \n",
       "253670       1         1          1   25       0       0   \n",
       "253676       1         1          1   18       0       0   \n",
       "253679       1         1          1   25       0       0   \n",
       "\n",
       "        HeartDiseaseorAttack  PhysActivity  Fruits  Veggies  ...  NoDocbcCost  \\\n",
       "0                          0             1       0        0  ...            0   \n",
       "1                          0             0       1        1  ...            0   \n",
       "2                          0             1       1        1  ...            0   \n",
       "3                          0             1       1        1  ...            0   \n",
       "4                          0             0       1        1  ...            0   \n",
       "...                      ...           ...     ...      ...  ...          ...   \n",
       "253659                     0             0       0        1  ...            0   \n",
       "253668                     1             0       1        1  ...            0   \n",
       "253670                     1             0       1        0  ...            0   \n",
       "253676                     0             0       0        0  ...            0   \n",
       "253679                     1             1       1        0  ...            0   \n",
       "\n",
       "        GenHlth  MentHlth  PhysHlth  DiffWalk  Sex  Age  Education  Income  \\\n",
       "0             1         0         0         0    0   11          5       7   \n",
       "1             3         0         0         0    0    6          6       8   \n",
       "2             2         0         0         0    0    1          4       4   \n",
       "3             1         0         0         0    1   12          4       2   \n",
       "4             1         0         0         0    0    4          6       8   \n",
       "...         ...       ...       ...       ...  ...  ...        ...     ...   \n",
       "253659        4         0         0         0    0    6          4       1   \n",
       "253668        2         0         0         1    1   10          3       6   \n",
       "253670        5        15         0         1    0   13          6       4   \n",
       "253676        4         0         0         1    0   11          2       4   \n",
       "253679        2         0         0         0    0    9          6       2   \n",
       "\n",
       "        Diabetes_binary  \n",
       "0                     0  \n",
       "1                     0  \n",
       "2                     0  \n",
       "3                     0  \n",
       "4                     0  \n",
       "...                 ...  \n",
       "253659                1  \n",
       "253668                1  \n",
       "253670                1  \n",
       "253676                1  \n",
       "253679                1  \n",
       "\n",
       "[88365 rows x 22 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('Preprocessed_data.csv', index_col = 0)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "185b32db",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.iloc[:,0:-1]\n",
    "y = df.loc[:, 'Diabetes_binary']\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, y, train_size=10000, shuffle=True, random_state = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9e763e27",
   "metadata": {},
   "outputs": [],
   "source": [
    "acc_list = []\n",
    "roc_auc_ovr_List = []\n",
    "f1_micro_List = []"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62155024",
   "metadata": {},
   "source": [
    "## Trial 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a65d5cf2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Guest-All\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1469: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\Guest-All\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1469: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\Guest-All\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1469: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\Guest-All\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1469: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\Guest-All\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1469: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\Guest-All\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1469: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\Guest-All\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1469: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\Guest-All\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1469: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\Guest-All\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1469: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\Guest-All\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1469: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\Guest-All\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1182: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Guest-All\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1182: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Guest-All\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1182: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Guest-All\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1182: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Guest-All\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1182: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Guest-All\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1182: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Guest-All\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1182: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Guest-All\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1182: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Guest-All\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1182: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Guest-All\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1182: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Guest-All\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1182: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Guest-All\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1182: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Guest-All\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1182: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Guest-All\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1182: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Guest-All\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1182: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Guest-All\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1182: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Guest-All\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1182: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Guest-All\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1182: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Guest-All\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1182: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Guest-All\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1182: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Create a pipeline - RF is a stand in, we will populate the classifier part below\n",
    "pipe = Pipeline([('std', StandardScaler()),\n",
    "                 ('classifier', LogisticRegression())])\n",
    "\n",
    "# Create search space of candidate learning algorithms and their hyperparameters\n",
    "# note lbfgs can't do l1, and if you pass penalty='none' it expects no C value\n",
    "search_space = [{'classifier': [LogisticRegression(max_iter=10000)],\n",
    "                 'classifier__solver': ['saga'],\n",
    "                 'classifier__penalty': ['l1', 'l2'],\n",
    "                 'classifier__C': np.logspace(-4, 4, 9)},\n",
    "                {'classifier': [LogisticRegression(max_iter=10000)],\n",
    "                 'classifier__solver': ['lbfgs'],\n",
    "                 'classifier__penalty': ['l2'],\n",
    "                 'classifier__C': np.logspace(-4, 4, 9)},\n",
    "                {'classifier': [LogisticRegression(max_iter=10000)],\n",
    "                 'classifier__solver': ['lbfgs','saga'],\n",
    "                 'classifier__penalty': ['none']},\n",
    "                {'classifier': [LogisticRegression(max_iter=10000)]}\n",
    "                ]\n",
    "\n",
    "# Create grid search\n",
    "clf = GridSearchCV(pipe, search_space, cv=StratifiedKFold(n_splits=10),\n",
    "                   scoring=['accuracy', 'precision', 'f1_micro'], refit=False,\n",
    "                   verbose=0)\n",
    "\n",
    "# Fit grid search\n",
    "best_model = clf.fit(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8390b96b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'mean_fit_time': array([0.01412556, 0.04239457, 0.05425239, 0.05092468, 0.07401793,\n",
       "        0.05697544, 0.0591347 , 0.04778697, 0.05953901, 0.06501472,\n",
       "        0.06724946, 0.0554184 , 0.07635636, 0.05205688, 0.07601104,\n",
       "        0.06919959, 0.08491945, 0.06727309, 0.01940565, 0.01570559,\n",
       "        0.02421415, 0.02037351, 0.01966259, 0.0199415 , 0.0190665 ,\n",
       "        0.02110868, 0.02598965, 0.02459764, 0.06443098, 0.01985743]),\n",
       " 'std_fit_time': array([0.00631732, 0.01076439, 0.0061579 , 0.01261764, 0.00710797,\n",
       "        0.01122736, 0.00962797, 0.00702328, 0.01149314, 0.01103928,\n",
       "        0.01307511, 0.01294183, 0.0184085 , 0.00733833, 0.01112508,\n",
       "        0.01433921, 0.01746264, 0.01197843, 0.00623642, 0.00345271,\n",
       "        0.00759256, 0.00620458, 0.00633709, 0.0075049 , 0.00667065,\n",
       "        0.00703841, 0.00677995, 0.00889527, 0.00684424, 0.00598256]),\n",
       " 'mean_score_time': array([0.00701127, 0.00797207, 0.00776415, 0.00399191, 0.00778275,\n",
       "        0.00590842, 0.00686014, 0.00581388, 0.00738184, 0.0041822 ,\n",
       "        0.00521138, 0.00736637, 0.00614152, 0.00630624, 0.0100421 ,\n",
       "        0.00633104, 0.00788782, 0.00833609, 0.00413821, 0.00889788,\n",
       "        0.00718317, 0.00638008, 0.00549827, 0.00676644, 0.01101358,\n",
       "        0.00714028, 0.00235801, 0.00642278, 0.00411882, 0.00721436]),\n",
       " 'std_score_time': array([0.00719024, 0.00701282, 0.00714739, 0.00534057, 0.00778408,\n",
       "        0.00690737, 0.00857749, 0.00725489, 0.00751595, 0.00654212,\n",
       "        0.00804524, 0.00701882, 0.00752766, 0.00772392, 0.0083116 ,\n",
       "        0.00775427, 0.00789019, 0.00764594, 0.00650843, 0.00721154,\n",
       "        0.00740784, 0.00760828, 0.00704883, 0.008359  , 0.00721053,\n",
       "        0.00742402, 0.00487367, 0.00689211, 0.00537977, 0.00725739]),\n",
       " 'param_classifier': masked_array(data=[LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000)],\n",
       "              mask=[False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'param_classifier__C': masked_array(data=[0.0001, 0.0001, 0.001, 0.001, 0.01, 0.01, 0.1, 0.1,\n",
       "                    1.0, 1.0, 10.0, 10.0, 100.0, 100.0, 1000.0, 1000.0,\n",
       "                    10000.0, 10000.0, 0.0001, 0.001, 0.01, 0.1, 1.0, 10.0,\n",
       "                    100.0, 1000.0, 10000.0, --, --, --],\n",
       "              mask=[False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False,  True,  True,  True],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'param_classifier__penalty': masked_array(data=['l1', 'l2', 'l1', 'l2', 'l1', 'l2', 'l1', 'l2', 'l1',\n",
       "                    'l2', 'l1', 'l2', 'l1', 'l2', 'l1', 'l2', 'l1', 'l2',\n",
       "                    'l2', 'l2', 'l2', 'l2', 'l2', 'l2', 'l2', 'l2', 'l2',\n",
       "                    'none', 'none', --],\n",
       "              mask=[False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False,  True],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'param_classifier__solver': masked_array(data=['saga', 'saga', 'saga', 'saga', 'saga', 'saga', 'saga',\n",
       "                    'saga', 'saga', 'saga', 'saga', 'saga', 'saga', 'saga',\n",
       "                    'saga', 'saga', 'saga', 'saga', 'lbfgs', 'lbfgs',\n",
       "                    'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs',\n",
       "                    'lbfgs', 'lbfgs', 'saga', --],\n",
       "              mask=[False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False,  True],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'params': [{'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 0.0001,\n",
       "   'classifier__penalty': 'l1',\n",
       "   'classifier__solver': 'saga'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 0.0001,\n",
       "   'classifier__penalty': 'l2',\n",
       "   'classifier__solver': 'saga'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 0.001,\n",
       "   'classifier__penalty': 'l1',\n",
       "   'classifier__solver': 'saga'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 0.001,\n",
       "   'classifier__penalty': 'l2',\n",
       "   'classifier__solver': 'saga'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 0.01,\n",
       "   'classifier__penalty': 'l1',\n",
       "   'classifier__solver': 'saga'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 0.01,\n",
       "   'classifier__penalty': 'l2',\n",
       "   'classifier__solver': 'saga'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 0.1,\n",
       "   'classifier__penalty': 'l1',\n",
       "   'classifier__solver': 'saga'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 0.1,\n",
       "   'classifier__penalty': 'l2',\n",
       "   'classifier__solver': 'saga'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 1.0,\n",
       "   'classifier__penalty': 'l1',\n",
       "   'classifier__solver': 'saga'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 1.0,\n",
       "   'classifier__penalty': 'l2',\n",
       "   'classifier__solver': 'saga'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 10.0,\n",
       "   'classifier__penalty': 'l1',\n",
       "   'classifier__solver': 'saga'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 10.0,\n",
       "   'classifier__penalty': 'l2',\n",
       "   'classifier__solver': 'saga'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 100.0,\n",
       "   'classifier__penalty': 'l1',\n",
       "   'classifier__solver': 'saga'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 100.0,\n",
       "   'classifier__penalty': 'l2',\n",
       "   'classifier__solver': 'saga'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 1000.0,\n",
       "   'classifier__penalty': 'l1',\n",
       "   'classifier__solver': 'saga'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 1000.0,\n",
       "   'classifier__penalty': 'l2',\n",
       "   'classifier__solver': 'saga'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 10000.0,\n",
       "   'classifier__penalty': 'l1',\n",
       "   'classifier__solver': 'saga'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 10000.0,\n",
       "   'classifier__penalty': 'l2',\n",
       "   'classifier__solver': 'saga'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 0.0001,\n",
       "   'classifier__penalty': 'l2',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 0.001,\n",
       "   'classifier__penalty': 'l2',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 0.01,\n",
       "   'classifier__penalty': 'l2',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 0.1,\n",
       "   'classifier__penalty': 'l2',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 1.0,\n",
       "   'classifier__penalty': 'l2',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 10.0,\n",
       "   'classifier__penalty': 'l2',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 100.0,\n",
       "   'classifier__penalty': 'l2',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 1000.0,\n",
       "   'classifier__penalty': 'l2',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 10000.0,\n",
       "   'classifier__penalty': 'l2',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__penalty': 'none',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__penalty': 'none',\n",
       "   'classifier__solver': 'saga'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000)}],\n",
       " 'split0_test_accuracy': array([0.604, 0.683, 0.689, 0.724, 0.731, 0.732, 0.731, 0.734, 0.735,\n",
       "        0.735, 0.735, 0.735, 0.735, 0.735, 0.735, 0.735, 0.735, 0.735,\n",
       "        0.683, 0.724, 0.732, 0.734, 0.735, 0.735, 0.735, 0.735, 0.735,\n",
       "        0.735, 0.735, 0.735]),\n",
       " 'split1_test_accuracy': array([0.604, 0.695, 0.679, 0.737, 0.739, 0.731, 0.728, 0.727, 0.728,\n",
       "        0.727, 0.727, 0.727, 0.727, 0.727, 0.727, 0.727, 0.727, 0.727,\n",
       "        0.695, 0.737, 0.731, 0.727, 0.727, 0.727, 0.727, 0.727, 0.727,\n",
       "        0.727, 0.727, 0.727]),\n",
       " 'split2_test_accuracy': array([0.604, 0.693, 0.691, 0.73 , 0.73 , 0.728, 0.73 , 0.729, 0.73 ,\n",
       "        0.73 , 0.73 , 0.73 , 0.73 , 0.73 , 0.73 , 0.73 , 0.73 , 0.73 ,\n",
       "        0.693, 0.73 , 0.728, 0.729, 0.73 , 0.73 , 0.73 , 0.73 , 0.73 ,\n",
       "        0.73 , 0.73 , 0.73 ]),\n",
       " 'split3_test_accuracy': array([0.604, 0.702, 0.684, 0.75 , 0.761, 0.757, 0.757, 0.758, 0.759,\n",
       "        0.759, 0.759, 0.759, 0.759, 0.759, 0.759, 0.759, 0.759, 0.759,\n",
       "        0.702, 0.75 , 0.757, 0.758, 0.759, 0.759, 0.759, 0.759, 0.759,\n",
       "        0.759, 0.759, 0.759]),\n",
       " 'split4_test_accuracy': array([0.604, 0.68 , 0.677, 0.734, 0.733, 0.742, 0.745, 0.745, 0.745,\n",
       "        0.745, 0.745, 0.745, 0.745, 0.745, 0.745, 0.745, 0.745, 0.745,\n",
       "        0.68 , 0.734, 0.742, 0.745, 0.745, 0.745, 0.745, 0.745, 0.745,\n",
       "        0.745, 0.745, 0.745]),\n",
       " 'split5_test_accuracy': array([0.604, 0.691, 0.695, 0.739, 0.752, 0.745, 0.746, 0.748, 0.748,\n",
       "        0.747, 0.747, 0.747, 0.747, 0.747, 0.747, 0.747, 0.747, 0.747,\n",
       "        0.691, 0.739, 0.745, 0.748, 0.747, 0.747, 0.747, 0.747, 0.747,\n",
       "        0.747, 0.747, 0.747]),\n",
       " 'split6_test_accuracy': array([0.604, 0.693, 0.684, 0.729, 0.731, 0.727, 0.73 , 0.73 , 0.729,\n",
       "        0.729, 0.729, 0.729, 0.729, 0.729, 0.729, 0.729, 0.729, 0.729,\n",
       "        0.693, 0.729, 0.727, 0.73 , 0.729, 0.729, 0.729, 0.729, 0.729,\n",
       "        0.729, 0.729, 0.729]),\n",
       " 'split7_test_accuracy': array([0.604, 0.695, 0.693, 0.751, 0.755, 0.759, 0.758, 0.755, 0.757,\n",
       "        0.756, 0.757, 0.757, 0.757, 0.757, 0.757, 0.757, 0.757, 0.757,\n",
       "        0.695, 0.751, 0.759, 0.755, 0.756, 0.757, 0.757, 0.757, 0.757,\n",
       "        0.757, 0.757, 0.756]),\n",
       " 'split8_test_accuracy': array([0.604, 0.715, 0.694, 0.735, 0.75 , 0.746, 0.749, 0.751, 0.752,\n",
       "        0.751, 0.752, 0.752, 0.752, 0.752, 0.752, 0.752, 0.752, 0.752,\n",
       "        0.715, 0.735, 0.746, 0.751, 0.751, 0.752, 0.752, 0.752, 0.752,\n",
       "        0.752, 0.752, 0.751]),\n",
       " 'split9_test_accuracy': array([0.603, 0.671, 0.671, 0.735, 0.723, 0.734, 0.731, 0.731, 0.731,\n",
       "        0.731, 0.731, 0.731, 0.731, 0.731, 0.731, 0.731, 0.731, 0.731,\n",
       "        0.671, 0.735, 0.734, 0.731, 0.731, 0.731, 0.731, 0.731, 0.731,\n",
       "        0.731, 0.731, 0.731]),\n",
       " 'mean_test_accuracy': array([0.6039, 0.6918, 0.6857, 0.7364, 0.7405, 0.7401, 0.7405, 0.7408,\n",
       "        0.7414, 0.741 , 0.7412, 0.7412, 0.7412, 0.7412, 0.7412, 0.7412,\n",
       "        0.7412, 0.7412, 0.6918, 0.7364, 0.7401, 0.7408, 0.741 , 0.7412,\n",
       "        0.7412, 0.7412, 0.7412, 0.7412, 0.7412, 0.741 ]),\n",
       " 'std_test_accuracy': array([0.0003    , 0.01146996, 0.0076818 , 0.00815107, 0.0122821 ,\n",
       "        0.010995  , 0.01121829, 0.01122319, 0.01155162, 0.01139298,\n",
       "        0.01161723, 0.01161723, 0.01161723, 0.01161723, 0.01161723,\n",
       "        0.01161723, 0.01161723, 0.01161723, 0.01146996, 0.00815107,\n",
       "        0.010995  , 0.01122319, 0.01139298, 0.01161723, 0.01161723,\n",
       "        0.01161723, 0.01161723, 0.01161723, 0.01161723, 0.01139298]),\n",
       " 'rank_test_accuracy': array([30, 27, 29, 25, 21, 23, 21, 19,  1, 16,  2,  2,  2,  2,  2,  2,  2,\n",
       "         2, 27, 25, 23, 19, 16,  2,  2,  2,  2,  2,  2, 16]),\n",
       " 'split0_test_precision': array([0.        , 0.71122995, 0.72020725, 0.67857143, 0.67208672,\n",
       "        0.66931217, 0.66666667, 0.67015707, 0.67101828, 0.67101828,\n",
       "        0.67101828, 0.67101828, 0.67101828, 0.67101828, 0.67101828,\n",
       "        0.67101828, 0.67101828, 0.67101828, 0.71122995, 0.67857143,\n",
       "        0.66931217, 0.67015707, 0.67101828, 0.67101828, 0.67101828,\n",
       "        0.67101828, 0.67101828, 0.67101828, 0.67101828, 0.67101828]),\n",
       " 'split1_test_precision': array([0.        , 0.78980892, 0.74509804, 0.71382637, 0.696793  ,\n",
       "        0.68091168, 0.67714286, 0.67323944, 0.67514124, 0.67323944,\n",
       "        0.67323944, 0.67323944, 0.67323944, 0.67323944, 0.67323944,\n",
       "        0.67323944, 0.67323944, 0.67323944, 0.78980892, 0.71382637,\n",
       "        0.68091168, 0.67323944, 0.67323944, 0.67323944, 0.67323944,\n",
       "        0.67323944, 0.67323944, 0.67323944, 0.67323944, 0.67323944]),\n",
       " 'split2_test_precision': array([0.        , 0.76331361, 0.74576271, 0.69444444, 0.68103448,\n",
       "        0.67222222, 0.67403315, 0.67217631, 0.67307692, 0.67307692,\n",
       "        0.67307692, 0.67307692, 0.67307692, 0.67307692, 0.67307692,\n",
       "        0.67307692, 0.67307692, 0.67307692, 0.76331361, 0.69444444,\n",
       "        0.67222222, 0.67217631, 0.67307692, 0.67307692, 0.67307692,\n",
       "        0.67307692, 0.67307692, 0.67307692, 0.67307692, 0.67307692]),\n",
       " 'split3_test_precision': array([0.        , 0.75520833, 0.69417476, 0.70738636, 0.70822281,\n",
       "        0.6997389 , 0.69767442, 0.69948187, 0.7002584 , 0.7002584 ,\n",
       "        0.7002584 , 0.7002584 , 0.7002584 , 0.7002584 , 0.7002584 ,\n",
       "        0.7002584 , 0.7002584 , 0.7002584 , 0.75520833, 0.70738636,\n",
       "        0.6997389 , 0.69948187, 0.7002584 , 0.7002584 , 0.7002584 ,\n",
       "        0.7002584 , 0.7002584 , 0.7002584 , 0.7002584 , 0.7002584 ]),\n",
       " 'split4_test_precision': array([0.        , 0.72891566, 0.72392638, 0.69817073, 0.68587896,\n",
       "        0.69273743, 0.69859155, 0.69637883, 0.69637883, 0.69637883,\n",
       "        0.69637883, 0.69637883, 0.69637883, 0.69637883, 0.69637883,\n",
       "        0.69637883, 0.69637883, 0.69637883, 0.72891566, 0.69817073,\n",
       "        0.69273743, 0.69637883, 0.69637883, 0.69637883, 0.69637883,\n",
       "        0.69637883, 0.69637883, 0.69637883, 0.69637883, 0.69637883]),\n",
       " 'split5_test_precision': array([0.        , 0.75144509, 0.75418994, 0.71028037, 0.71764706,\n",
       "        0.70200573, 0.70170455, 0.70338983, 0.70338983, 0.70140845,\n",
       "        0.70140845, 0.70140845, 0.70140845, 0.70140845, 0.70140845,\n",
       "        0.70140845, 0.70140845, 0.70140845, 0.75144509, 0.71028037,\n",
       "        0.70200573, 0.70338983, 0.70140845, 0.70140845, 0.70140845,\n",
       "        0.70140845, 0.70140845, 0.70140845, 0.70140845, 0.70140845]),\n",
       " 'split6_test_precision': array([0.        , 0.75428571, 0.7173913 , 0.69470405, 0.67988669,\n",
       "        0.66942149, 0.67119565, 0.67213115, 0.67029973, 0.67029973,\n",
       "        0.67029973, 0.67029973, 0.67029973, 0.67029973, 0.67029973,\n",
       "        0.67029973, 0.67029973, 0.67029973, 0.75428571, 0.69470405,\n",
       "        0.66942149, 0.67213115, 0.67029973, 0.67029973, 0.67029973,\n",
       "        0.67029973, 0.67029973, 0.67029973, 0.67029973, 0.67029973]),\n",
       " 'split7_test_precision': array([0.        , 0.7826087 , 0.7431694 , 0.73482428, 0.72672673,\n",
       "        0.72861357, 0.7251462 , 0.72011662, 0.72303207, 0.72093023,\n",
       "        0.72303207, 0.72303207, 0.72303207, 0.72303207, 0.72303207,\n",
       "        0.72303207, 0.72303207, 0.72303207, 0.7826087 , 0.73482428,\n",
       "        0.72861357, 0.72011662, 0.72093023, 0.72303207, 0.72303207,\n",
       "        0.72303207, 0.72303207, 0.72303207, 0.72303207, 0.72093023]),\n",
       " 'split8_test_precision': array([0.        , 0.80327869, 0.725     , 0.70153846, 0.70165746,\n",
       "        0.6961326 , 0.69647696, 0.69811321, 0.69892473, 0.69811321,\n",
       "        0.69892473, 0.69892473, 0.69892473, 0.69892473, 0.69892473,\n",
       "        0.69892473, 0.69892473, 0.69892473, 0.80327869, 0.70153846,\n",
       "        0.6961326 , 0.69811321, 0.69811321, 0.69892473, 0.69892473,\n",
       "        0.69892473, 0.69892473, 0.69892473, 0.69892473, 0.69811321]),\n",
       " 'split9_test_precision': array([0.        , 0.7       , 0.68478261, 0.70121951, 0.67045455,\n",
       "        0.68245125, 0.67486339, 0.67486339, 0.67486339, 0.67486339,\n",
       "        0.67486339, 0.67486339, 0.67486339, 0.67486339, 0.67486339,\n",
       "        0.67486339, 0.67486339, 0.67486339, 0.7       , 0.70121951,\n",
       "        0.68245125, 0.67486339, 0.67486339, 0.67486339, 0.67486339,\n",
       "        0.67486339, 0.67486339, 0.67486339, 0.67486339, 0.67486339]),\n",
       " 'mean_test_precision': array([0.        , 0.75400947, 0.72537024, 0.7034966 , 0.69403885,\n",
       "        0.6893547 , 0.68834954, 0.68800477, 0.68863834, 0.68795869,\n",
       "        0.68825002, 0.68825002, 0.68825002, 0.68825002, 0.68825002,\n",
       "        0.68825002, 0.68825002, 0.68825002, 0.75400947, 0.7034966 ,\n",
       "        0.6893547 , 0.68800477, 0.68795869, 0.68825002, 0.68825002,\n",
       "        0.68825002, 0.68825002, 0.68825002, 0.68825002, 0.68795869]),\n",
       " 'std_test_precision': array([0.        , 0.03162031, 0.02159972, 0.01402561, 0.01834872,\n",
       "        0.01754429, 0.0175131 , 0.01668431, 0.01721086, 0.0167384 ,\n",
       "        0.01720771, 0.01720771, 0.01720771, 0.01720771, 0.01720771,\n",
       "        0.01720771, 0.01720771, 0.01720771, 0.03162031, 0.01402561,\n",
       "        0.01754429, 0.01668431, 0.0167384 , 0.01720771, 0.01720771,\n",
       "        0.01720771, 0.01720771, 0.01720771, 0.01720771, 0.0167384 ]),\n",
       " 'rank_test_precision': array([30,  1,  3,  4,  6,  7, 10, 25,  9, 27, 11, 11, 11, 11, 11, 11, 11,\n",
       "        11,  1,  4,  7, 25, 27, 11, 11, 11, 11, 11, 11, 27]),\n",
       " 'split0_test_f1_micro': array([0.604, 0.683, 0.689, 0.724, 0.731, 0.732, 0.731, 0.734, 0.735,\n",
       "        0.735, 0.735, 0.735, 0.735, 0.735, 0.735, 0.735, 0.735, 0.735,\n",
       "        0.683, 0.724, 0.732, 0.734, 0.735, 0.735, 0.735, 0.735, 0.735,\n",
       "        0.735, 0.735, 0.735]),\n",
       " 'split1_test_f1_micro': array([0.604, 0.695, 0.679, 0.737, 0.739, 0.731, 0.728, 0.727, 0.728,\n",
       "        0.727, 0.727, 0.727, 0.727, 0.727, 0.727, 0.727, 0.727, 0.727,\n",
       "        0.695, 0.737, 0.731, 0.727, 0.727, 0.727, 0.727, 0.727, 0.727,\n",
       "        0.727, 0.727, 0.727]),\n",
       " 'split2_test_f1_micro': array([0.604, 0.693, 0.691, 0.73 , 0.73 , 0.728, 0.73 , 0.729, 0.73 ,\n",
       "        0.73 , 0.73 , 0.73 , 0.73 , 0.73 , 0.73 , 0.73 , 0.73 , 0.73 ,\n",
       "        0.693, 0.73 , 0.728, 0.729, 0.73 , 0.73 , 0.73 , 0.73 , 0.73 ,\n",
       "        0.73 , 0.73 , 0.73 ]),\n",
       " 'split3_test_f1_micro': array([0.604, 0.702, 0.684, 0.75 , 0.761, 0.757, 0.757, 0.758, 0.759,\n",
       "        0.759, 0.759, 0.759, 0.759, 0.759, 0.759, 0.759, 0.759, 0.759,\n",
       "        0.702, 0.75 , 0.757, 0.758, 0.759, 0.759, 0.759, 0.759, 0.759,\n",
       "        0.759, 0.759, 0.759]),\n",
       " 'split4_test_f1_micro': array([0.604, 0.68 , 0.677, 0.734, 0.733, 0.742, 0.745, 0.745, 0.745,\n",
       "        0.745, 0.745, 0.745, 0.745, 0.745, 0.745, 0.745, 0.745, 0.745,\n",
       "        0.68 , 0.734, 0.742, 0.745, 0.745, 0.745, 0.745, 0.745, 0.745,\n",
       "        0.745, 0.745, 0.745]),\n",
       " 'split5_test_f1_micro': array([0.604, 0.691, 0.695, 0.739, 0.752, 0.745, 0.746, 0.748, 0.748,\n",
       "        0.747, 0.747, 0.747, 0.747, 0.747, 0.747, 0.747, 0.747, 0.747,\n",
       "        0.691, 0.739, 0.745, 0.748, 0.747, 0.747, 0.747, 0.747, 0.747,\n",
       "        0.747, 0.747, 0.747]),\n",
       " 'split6_test_f1_micro': array([0.604, 0.693, 0.684, 0.729, 0.731, 0.727, 0.73 , 0.73 , 0.729,\n",
       "        0.729, 0.729, 0.729, 0.729, 0.729, 0.729, 0.729, 0.729, 0.729,\n",
       "        0.693, 0.729, 0.727, 0.73 , 0.729, 0.729, 0.729, 0.729, 0.729,\n",
       "        0.729, 0.729, 0.729]),\n",
       " 'split7_test_f1_micro': array([0.604, 0.695, 0.693, 0.751, 0.755, 0.759, 0.758, 0.755, 0.757,\n",
       "        0.756, 0.757, 0.757, 0.757, 0.757, 0.757, 0.757, 0.757, 0.757,\n",
       "        0.695, 0.751, 0.759, 0.755, 0.756, 0.757, 0.757, 0.757, 0.757,\n",
       "        0.757, 0.757, 0.756]),\n",
       " 'split8_test_f1_micro': array([0.604, 0.715, 0.694, 0.735, 0.75 , 0.746, 0.749, 0.751, 0.752,\n",
       "        0.751, 0.752, 0.752, 0.752, 0.752, 0.752, 0.752, 0.752, 0.752,\n",
       "        0.715, 0.735, 0.746, 0.751, 0.751, 0.752, 0.752, 0.752, 0.752,\n",
       "        0.752, 0.752, 0.751]),\n",
       " 'split9_test_f1_micro': array([0.603, 0.671, 0.671, 0.735, 0.723, 0.734, 0.731, 0.731, 0.731,\n",
       "        0.731, 0.731, 0.731, 0.731, 0.731, 0.731, 0.731, 0.731, 0.731,\n",
       "        0.671, 0.735, 0.734, 0.731, 0.731, 0.731, 0.731, 0.731, 0.731,\n",
       "        0.731, 0.731, 0.731]),\n",
       " 'mean_test_f1_micro': array([0.6039, 0.6918, 0.6857, 0.7364, 0.7405, 0.7401, 0.7405, 0.7408,\n",
       "        0.7414, 0.741 , 0.7412, 0.7412, 0.7412, 0.7412, 0.7412, 0.7412,\n",
       "        0.7412, 0.7412, 0.6918, 0.7364, 0.7401, 0.7408, 0.741 , 0.7412,\n",
       "        0.7412, 0.7412, 0.7412, 0.7412, 0.7412, 0.741 ]),\n",
       " 'std_test_f1_micro': array([0.0003    , 0.01146996, 0.0076818 , 0.00815107, 0.0122821 ,\n",
       "        0.010995  , 0.01121829, 0.01122319, 0.01155162, 0.01139298,\n",
       "        0.01161723, 0.01161723, 0.01161723, 0.01161723, 0.01161723,\n",
       "        0.01161723, 0.01161723, 0.01161723, 0.01146996, 0.00815107,\n",
       "        0.010995  , 0.01122319, 0.01139298, 0.01161723, 0.01161723,\n",
       "        0.01161723, 0.01161723, 0.01161723, 0.01161723, 0.01139298]),\n",
       " 'rank_test_f1_micro': array([30, 27, 29, 25, 21, 23, 22, 19,  1, 16,  2,  2,  2,  2,  2,  2,  2,\n",
       "         2, 27, 25, 23, 19, 16,  2,  2,  2,  2,  2,  2, 16])}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_model.cv_results_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a4578782",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([30, 27, 29, 25, 21, 23, 21, 19,  1, 16,  2,  2,  2,  2,  2,  2,  2,\n",
       "        2, 27, 25, 23, 19, 16,  2,  2,  2,  2,  2,  2, 16])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_model.cv_results_['rank_test_accuracy']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "29844e0d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'classifier': LogisticRegression(max_iter=10000),\n",
       " 'classifier__C': 0.0001,\n",
       " 'classifier__penalty': 'l1',\n",
       " 'classifier__solver': 'saga'}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_model.cv_results_['params'][ np.argmin(best_model.cv_results_['mean_test_accuracy'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "da854c46",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>classifier</th>\n",
       "      <th>C</th>\n",
       "      <th>penalty</th>\n",
       "      <th>solver</th>\n",
       "      <th>score_acc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.3961</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.3082</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.3143</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.2636</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0100</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.2595</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0100</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.2599</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.1000</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.2595</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.1000</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.2592</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.2586</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.2590</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>10.0000</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.2588</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>10.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.2588</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>100.0000</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.2588</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>100.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.2588</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>1000.0000</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.2588</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>1000.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.2588</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>10000.0000</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.2588</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>10000.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.2588</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.3082</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.2636</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0100</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.2599</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.1000</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.2592</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.2590</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>10.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.2588</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>100.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.2588</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>1000.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.2588</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>10000.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.2588</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>NaN</td>\n",
       "      <td>none</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.2588</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>NaN</td>\n",
       "      <td>none</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.2588</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.2590</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                            classifier           C penalty solver  score_acc\n",
       "0   LogisticRegression(max_iter=10000)      0.0001      l1   saga     0.3961\n",
       "1   LogisticRegression(max_iter=10000)      0.0001      l2   saga     0.3082\n",
       "2   LogisticRegression(max_iter=10000)      0.0010      l1   saga     0.3143\n",
       "3   LogisticRegression(max_iter=10000)      0.0010      l2   saga     0.2636\n",
       "4   LogisticRegression(max_iter=10000)      0.0100      l1   saga     0.2595\n",
       "5   LogisticRegression(max_iter=10000)      0.0100      l2   saga     0.2599\n",
       "6   LogisticRegression(max_iter=10000)      0.1000      l1   saga     0.2595\n",
       "7   LogisticRegression(max_iter=10000)      0.1000      l2   saga     0.2592\n",
       "8   LogisticRegression(max_iter=10000)      1.0000      l1   saga     0.2586\n",
       "9   LogisticRegression(max_iter=10000)      1.0000      l2   saga     0.2590\n",
       "10  LogisticRegression(max_iter=10000)     10.0000      l1   saga     0.2588\n",
       "11  LogisticRegression(max_iter=10000)     10.0000      l2   saga     0.2588\n",
       "12  LogisticRegression(max_iter=10000)    100.0000      l1   saga     0.2588\n",
       "13  LogisticRegression(max_iter=10000)    100.0000      l2   saga     0.2588\n",
       "14  LogisticRegression(max_iter=10000)   1000.0000      l1   saga     0.2588\n",
       "15  LogisticRegression(max_iter=10000)   1000.0000      l2   saga     0.2588\n",
       "16  LogisticRegression(max_iter=10000)  10000.0000      l1   saga     0.2588\n",
       "17  LogisticRegression(max_iter=10000)  10000.0000      l2   saga     0.2588\n",
       "18  LogisticRegression(max_iter=10000)      0.0001      l2  lbfgs     0.3082\n",
       "19  LogisticRegression(max_iter=10000)      0.0010      l2  lbfgs     0.2636\n",
       "20  LogisticRegression(max_iter=10000)      0.0100      l2  lbfgs     0.2599\n",
       "21  LogisticRegression(max_iter=10000)      0.1000      l2  lbfgs     0.2592\n",
       "22  LogisticRegression(max_iter=10000)      1.0000      l2  lbfgs     0.2590\n",
       "23  LogisticRegression(max_iter=10000)     10.0000      l2  lbfgs     0.2588\n",
       "24  LogisticRegression(max_iter=10000)    100.0000      l2  lbfgs     0.2588\n",
       "25  LogisticRegression(max_iter=10000)   1000.0000      l2  lbfgs     0.2588\n",
       "26  LogisticRegression(max_iter=10000)  10000.0000      l2  lbfgs     0.2588\n",
       "27  LogisticRegression(max_iter=10000)         NaN    none  lbfgs     0.2588\n",
       "28  LogisticRegression(max_iter=10000)         NaN    none   saga     0.2588\n",
       "29  LogisticRegression(max_iter=10000)         NaN     NaN    NaN     0.2590"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results = pd.DataFrame( best_model.cv_results_['params'] )\n",
    "\n",
    "# next grab the score resulting from those parameters, add it to the data\n",
    "# score is accuracy; to display it as misclassification error we use 1 - x\n",
    "results['score_acc'] = 1 - best_model.cv_results_['mean_test_accuracy']\n",
    "\n",
    "# get rid of classifierXX in columns\n",
    "cols = results.columns.to_series().str.split('__').apply(lambda x: x[-1])\n",
    "results.columns = cols\n",
    "\n",
    "# lets show the results for the saga solver across penalty & C values\n",
    "#sns.heatmap( results.query('solver==\"saga\"').pivot('C','penalty','score_acc'),\n",
    "#             annot=True, fmt='.3f')\n",
    "\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cec21d10",
   "metadata": {},
   "outputs": [],
   "source": [
    "acc_list.append(results[0:1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8e72c958",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'classifier': LogisticRegression(max_iter=10000),\n",
       " 'classifier__C': 0.0001,\n",
       " 'classifier__penalty': 'l2',\n",
       " 'classifier__solver': 'saga'}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_model.cv_results_['params'][ np.argmin(best_model.cv_results_['rank_test_precision'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "426600e6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>classifier</th>\n",
       "      <th>C</th>\n",
       "      <th>penalty</th>\n",
       "      <th>solver</th>\n",
       "      <th>score_precision</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.245991</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.274630</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.296503</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0100</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.305961</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0100</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.310645</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.1000</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.311650</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.1000</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.311995</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.311362</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.312041</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>10.0000</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.311750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>10.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.311750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>100.0000</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.311750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>100.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.311750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>1000.0000</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.311750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>1000.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.311750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>10000.0000</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.311750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>10000.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.311750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.245991</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.296503</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0100</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.310645</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.1000</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.311995</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.312041</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>10.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.311750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>100.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.311750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>1000.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.311750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>10000.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.311750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>NaN</td>\n",
       "      <td>none</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.311750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>NaN</td>\n",
       "      <td>none</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.311750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.312041</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                            classifier           C penalty solver  \\\n",
       "0   LogisticRegression(max_iter=10000)      0.0001      l1   saga   \n",
       "1   LogisticRegression(max_iter=10000)      0.0001      l2   saga   \n",
       "2   LogisticRegression(max_iter=10000)      0.0010      l1   saga   \n",
       "3   LogisticRegression(max_iter=10000)      0.0010      l2   saga   \n",
       "4   LogisticRegression(max_iter=10000)      0.0100      l1   saga   \n",
       "5   LogisticRegression(max_iter=10000)      0.0100      l2   saga   \n",
       "6   LogisticRegression(max_iter=10000)      0.1000      l1   saga   \n",
       "7   LogisticRegression(max_iter=10000)      0.1000      l2   saga   \n",
       "8   LogisticRegression(max_iter=10000)      1.0000      l1   saga   \n",
       "9   LogisticRegression(max_iter=10000)      1.0000      l2   saga   \n",
       "10  LogisticRegression(max_iter=10000)     10.0000      l1   saga   \n",
       "11  LogisticRegression(max_iter=10000)     10.0000      l2   saga   \n",
       "12  LogisticRegression(max_iter=10000)    100.0000      l1   saga   \n",
       "13  LogisticRegression(max_iter=10000)    100.0000      l2   saga   \n",
       "14  LogisticRegression(max_iter=10000)   1000.0000      l1   saga   \n",
       "15  LogisticRegression(max_iter=10000)   1000.0000      l2   saga   \n",
       "16  LogisticRegression(max_iter=10000)  10000.0000      l1   saga   \n",
       "17  LogisticRegression(max_iter=10000)  10000.0000      l2   saga   \n",
       "18  LogisticRegression(max_iter=10000)      0.0001      l2  lbfgs   \n",
       "19  LogisticRegression(max_iter=10000)      0.0010      l2  lbfgs   \n",
       "20  LogisticRegression(max_iter=10000)      0.0100      l2  lbfgs   \n",
       "21  LogisticRegression(max_iter=10000)      0.1000      l2  lbfgs   \n",
       "22  LogisticRegression(max_iter=10000)      1.0000      l2  lbfgs   \n",
       "23  LogisticRegression(max_iter=10000)     10.0000      l2  lbfgs   \n",
       "24  LogisticRegression(max_iter=10000)    100.0000      l2  lbfgs   \n",
       "25  LogisticRegression(max_iter=10000)   1000.0000      l2  lbfgs   \n",
       "26  LogisticRegression(max_iter=10000)  10000.0000      l2  lbfgs   \n",
       "27  LogisticRegression(max_iter=10000)         NaN    none  lbfgs   \n",
       "28  LogisticRegression(max_iter=10000)         NaN    none   saga   \n",
       "29  LogisticRegression(max_iter=10000)         NaN     NaN    NaN   \n",
       "\n",
       "    score_precision  \n",
       "0          1.000000  \n",
       "1          0.245991  \n",
       "2          0.274630  \n",
       "3          0.296503  \n",
       "4          0.305961  \n",
       "5          0.310645  \n",
       "6          0.311650  \n",
       "7          0.311995  \n",
       "8          0.311362  \n",
       "9          0.312041  \n",
       "10         0.311750  \n",
       "11         0.311750  \n",
       "12         0.311750  \n",
       "13         0.311750  \n",
       "14         0.311750  \n",
       "15         0.311750  \n",
       "16         0.311750  \n",
       "17         0.311750  \n",
       "18         0.245991  \n",
       "19         0.296503  \n",
       "20         0.310645  \n",
       "21         0.311995  \n",
       "22         0.312041  \n",
       "23         0.311750  \n",
       "24         0.311750  \n",
       "25         0.311750  \n",
       "26         0.311750  \n",
       "27         0.311750  \n",
       "28         0.311750  \n",
       "29         0.312041  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results = pd.DataFrame( best_model.cv_results_['params'] )\n",
    "\n",
    "# next grab the score resulting from those parameters, add it to the data\n",
    "# score is accuracy; to display it as misclassification error we use 1 - x\n",
    "results['score_precision'] = 1 - best_model.cv_results_['mean_test_precision']\n",
    "\n",
    "# get rid of classifierXX in columns\n",
    "cols = results.columns.to_series().str.split('__').apply(lambda x: x[-1])\n",
    "results.columns = cols\n",
    "\n",
    "# lets show the results for the saga solver across penalty & C values\n",
    "#sns.heatmap( results.query('solver==\"saga\"').pivot('C','penalty','score_precision'),\n",
    "#             annot=True, fmt='.3f')\n",
    "\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "46a6f899",
   "metadata": {},
   "outputs": [],
   "source": [
    "roc_auc_ovr_List.append(results[1:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6ceb54de",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'classifier': LogisticRegression(max_iter=10000),\n",
       " 'classifier__C': 1.0,\n",
       " 'classifier__penalty': 'l1',\n",
       " 'classifier__solver': 'saga'}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_model.cv_results_['params'][ np.argmin(best_model.cv_results_['rank_test_f1_micro'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "886a47b2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>classifier</th>\n",
       "      <th>C</th>\n",
       "      <th>penalty</th>\n",
       "      <th>solver</th>\n",
       "      <th>score_f1_micro</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.3961</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.3082</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.3143</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.2636</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0100</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.2595</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0100</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.2599</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.1000</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.2595</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.1000</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.2592</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.2586</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.2590</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>10.0000</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.2588</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>10.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.2588</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>100.0000</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.2588</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>100.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.2588</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>1000.0000</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.2588</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>1000.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.2588</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>10000.0000</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.2588</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>10000.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.2588</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.3082</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.2636</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0100</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.2599</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.1000</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.2592</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.2590</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>10.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.2588</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>100.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.2588</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>1000.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.2588</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>10000.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.2588</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>NaN</td>\n",
       "      <td>none</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.2588</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>NaN</td>\n",
       "      <td>none</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.2588</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.2590</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                            classifier           C penalty solver  \\\n",
       "0   LogisticRegression(max_iter=10000)      0.0001      l1   saga   \n",
       "1   LogisticRegression(max_iter=10000)      0.0001      l2   saga   \n",
       "2   LogisticRegression(max_iter=10000)      0.0010      l1   saga   \n",
       "3   LogisticRegression(max_iter=10000)      0.0010      l2   saga   \n",
       "4   LogisticRegression(max_iter=10000)      0.0100      l1   saga   \n",
       "5   LogisticRegression(max_iter=10000)      0.0100      l2   saga   \n",
       "6   LogisticRegression(max_iter=10000)      0.1000      l1   saga   \n",
       "7   LogisticRegression(max_iter=10000)      0.1000      l2   saga   \n",
       "8   LogisticRegression(max_iter=10000)      1.0000      l1   saga   \n",
       "9   LogisticRegression(max_iter=10000)      1.0000      l2   saga   \n",
       "10  LogisticRegression(max_iter=10000)     10.0000      l1   saga   \n",
       "11  LogisticRegression(max_iter=10000)     10.0000      l2   saga   \n",
       "12  LogisticRegression(max_iter=10000)    100.0000      l1   saga   \n",
       "13  LogisticRegression(max_iter=10000)    100.0000      l2   saga   \n",
       "14  LogisticRegression(max_iter=10000)   1000.0000      l1   saga   \n",
       "15  LogisticRegression(max_iter=10000)   1000.0000      l2   saga   \n",
       "16  LogisticRegression(max_iter=10000)  10000.0000      l1   saga   \n",
       "17  LogisticRegression(max_iter=10000)  10000.0000      l2   saga   \n",
       "18  LogisticRegression(max_iter=10000)      0.0001      l2  lbfgs   \n",
       "19  LogisticRegression(max_iter=10000)      0.0010      l2  lbfgs   \n",
       "20  LogisticRegression(max_iter=10000)      0.0100      l2  lbfgs   \n",
       "21  LogisticRegression(max_iter=10000)      0.1000      l2  lbfgs   \n",
       "22  LogisticRegression(max_iter=10000)      1.0000      l2  lbfgs   \n",
       "23  LogisticRegression(max_iter=10000)     10.0000      l2  lbfgs   \n",
       "24  LogisticRegression(max_iter=10000)    100.0000      l2  lbfgs   \n",
       "25  LogisticRegression(max_iter=10000)   1000.0000      l2  lbfgs   \n",
       "26  LogisticRegression(max_iter=10000)  10000.0000      l2  lbfgs   \n",
       "27  LogisticRegression(max_iter=10000)         NaN    none  lbfgs   \n",
       "28  LogisticRegression(max_iter=10000)         NaN    none   saga   \n",
       "29  LogisticRegression(max_iter=10000)         NaN     NaN    NaN   \n",
       "\n",
       "    score_f1_micro  \n",
       "0           0.3961  \n",
       "1           0.3082  \n",
       "2           0.3143  \n",
       "3           0.2636  \n",
       "4           0.2595  \n",
       "5           0.2599  \n",
       "6           0.2595  \n",
       "7           0.2592  \n",
       "8           0.2586  \n",
       "9           0.2590  \n",
       "10          0.2588  \n",
       "11          0.2588  \n",
       "12          0.2588  \n",
       "13          0.2588  \n",
       "14          0.2588  \n",
       "15          0.2588  \n",
       "16          0.2588  \n",
       "17          0.2588  \n",
       "18          0.3082  \n",
       "19          0.2636  \n",
       "20          0.2599  \n",
       "21          0.2592  \n",
       "22          0.2590  \n",
       "23          0.2588  \n",
       "24          0.2588  \n",
       "25          0.2588  \n",
       "26          0.2588  \n",
       "27          0.2588  \n",
       "28          0.2588  \n",
       "29          0.2590  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results = pd.DataFrame( best_model.cv_results_['params'] )\n",
    "\n",
    "# next grab the score resulting from those parameters, add it to the data\n",
    "# score is accuracy; to display it as misclassification error we use 1 - x\n",
    "results['score_f1_micro'] = 1 - best_model.cv_results_['mean_test_f1_micro']\n",
    "\n",
    "# get rid of classifierXX in columns\n",
    "cols = results.columns.to_series().str.split('__').apply(lambda x: x[-1])\n",
    "results.columns = cols\n",
    "\n",
    "# lets show the results for the saga solver across penalty & C values\n",
    "#sns.heatmap( results.query('solver==\"saga\"').pivot('C','penalty','score_f1_micro'),\n",
    "#             annot=True, fmt='.3f')\n",
    "\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "943aea19",
   "metadata": {},
   "outputs": [],
   "source": [
    "f1_micro_List.append(results[8:9])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02458d54",
   "metadata": {},
   "source": [
    "## Trial 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7bf26ccd",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, Y_train, Y_test = train_test_split(X, y, train_size=10000, shuffle=True, random_state = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4628ee31",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Guest-All\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1469: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\Guest-All\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1469: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\Guest-All\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1469: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\Guest-All\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1469: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\Guest-All\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1469: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\Guest-All\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1469: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\Guest-All\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1469: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\Guest-All\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1469: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\Guest-All\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1469: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\Guest-All\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1469: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\Guest-All\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1182: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Guest-All\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1182: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Guest-All\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1182: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Guest-All\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1182: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Guest-All\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1182: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Guest-All\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1182: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Guest-All\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1182: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Guest-All\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1182: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Guest-All\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1182: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Guest-All\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1182: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Guest-All\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1182: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Guest-All\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1182: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Guest-All\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1182: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Guest-All\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1182: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Guest-All\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1182: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Guest-All\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1182: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Guest-All\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1182: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Guest-All\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1182: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Guest-All\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1182: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Guest-All\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1182: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Create a pipeline - RF is a stand in, we will populate the classifier part below\n",
    "pipe = Pipeline([('std', StandardScaler()),\n",
    "                 ('classifier', LogisticRegression())])\n",
    "\n",
    "# Create search space of candidate learning algorithms and their hyperparameters\n",
    "# note lbfgs can't do l1, and if you pass penalty='none' it expects no C value\n",
    "search_space = [{'classifier': [LogisticRegression(max_iter=10000)],\n",
    "                 'classifier__solver': ['saga'],\n",
    "                 'classifier__penalty': ['l1', 'l2'],\n",
    "                 'classifier__C': np.logspace(-4, 4, 9)},\n",
    "                {'classifier': [LogisticRegression(max_iter=10000)],\n",
    "                 'classifier__solver': ['lbfgs'],\n",
    "                 'classifier__penalty': ['l2'],\n",
    "                 'classifier__C': np.logspace(-4, 4, 9)},\n",
    "                {'classifier': [LogisticRegression(max_iter=10000)],\n",
    "                 'classifier__solver': ['lbfgs','saga'],\n",
    "                 'classifier__penalty': ['none']},\n",
    "                {'classifier': [LogisticRegression(max_iter=10000)]}\n",
    "                ]\n",
    "\n",
    "# Create grid search\n",
    "clf = GridSearchCV(pipe, search_space, cv=StratifiedKFold(n_splits=10),\n",
    "                   scoring=['accuracy', 'precision', 'f1_micro'], refit=False,\n",
    "                   verbose=0)\n",
    "\n",
    "# Fit grid search\n",
    "best_model = clf.fit(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2bf2b4d0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'mean_fit_time': array([0.01931539, 0.06725841, 0.07058723, 0.07815807, 0.08596368,\n",
       "        0.06513565, 0.07429421, 0.06068981, 0.09605699, 0.07257047,\n",
       "        0.08978846, 0.06927693, 0.10030444, 0.06447778, 0.08035533,\n",
       "        0.07719707, 0.10697193, 0.09166915, 0.01990266, 0.01997254,\n",
       "        0.01551003, 0.02186108, 0.02316077, 0.02047017, 0.02221661,\n",
       "        0.01846945, 0.02109344, 0.02554944, 0.08343911, 0.02281282]),\n",
       " 'std_fit_time': array([0.00946069, 0.01009995, 0.01028016, 0.00815188, 0.00936993,\n",
       "        0.01546043, 0.01214767, 0.01173103, 0.0145212 , 0.01300433,\n",
       "        0.01672919, 0.0142411 , 0.02178602, 0.01306638, 0.01478341,\n",
       "        0.01096509, 0.02272507, 0.0161696 , 0.00684441, 0.00633065,\n",
       "        0.00142041, 0.00791449, 0.0071933 , 0.00713376, 0.00794164,\n",
       "        0.00666907, 0.00710497, 0.00828161, 0.0190903 , 0.00740047]),\n",
       " 'mean_score_time': array([0.00947127, 0.01139772, 0.00956721, 0.00591559, 0.00255997,\n",
       "        0.00986736, 0.00597911, 0.00419984, 0.00924733, 0.00865092,\n",
       "        0.00650198, 0.00787199, 0.00644038, 0.01106734, 0.00629642,\n",
       "        0.00767465, 0.00624108, 0.0063205 , 0.00827508, 0.01145787,\n",
       "        0.01293714, 0.00759225, 0.00699375, 0.00628219, 0.00397382,\n",
       "        0.01142364, 0.00725281, 0.0068378 , 0.00842154, 0.01094246]),\n",
       " 'std_score_time': array([0.00530692, 0.00608752, 0.00660713, 0.00729175, 0.00527353,\n",
       "        0.00711124, 0.00734948, 0.00653719, 0.00755693, 0.00737307,\n",
       "        0.0074434 , 0.00787259, 0.00679089, 0.007247  , 0.00771293,\n",
       "        0.00768961, 0.00764377, 0.00774193, 0.00720649, 0.00618698,\n",
       "        0.00660931, 0.0077108 , 0.0073566 , 0.00769505, 0.00633773,\n",
       "        0.00655945, 0.00744483, 0.00715263, 0.00627766, 0.00622018]),\n",
       " 'param_classifier': masked_array(data=[LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000)],\n",
       "              mask=[False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'param_classifier__C': masked_array(data=[0.0001, 0.0001, 0.001, 0.001, 0.01, 0.01, 0.1, 0.1,\n",
       "                    1.0, 1.0, 10.0, 10.0, 100.0, 100.0, 1000.0, 1000.0,\n",
       "                    10000.0, 10000.0, 0.0001, 0.001, 0.01, 0.1, 1.0, 10.0,\n",
       "                    100.0, 1000.0, 10000.0, --, --, --],\n",
       "              mask=[False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False,  True,  True,  True],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'param_classifier__penalty': masked_array(data=['l1', 'l2', 'l1', 'l2', 'l1', 'l2', 'l1', 'l2', 'l1',\n",
       "                    'l2', 'l1', 'l2', 'l1', 'l2', 'l1', 'l2', 'l1', 'l2',\n",
       "                    'l2', 'l2', 'l2', 'l2', 'l2', 'l2', 'l2', 'l2', 'l2',\n",
       "                    'none', 'none', --],\n",
       "              mask=[False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False,  True],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'param_classifier__solver': masked_array(data=['saga', 'saga', 'saga', 'saga', 'saga', 'saga', 'saga',\n",
       "                    'saga', 'saga', 'saga', 'saga', 'saga', 'saga', 'saga',\n",
       "                    'saga', 'saga', 'saga', 'saga', 'lbfgs', 'lbfgs',\n",
       "                    'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs',\n",
       "                    'lbfgs', 'lbfgs', 'saga', --],\n",
       "              mask=[False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False,  True],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'params': [{'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 0.0001,\n",
       "   'classifier__penalty': 'l1',\n",
       "   'classifier__solver': 'saga'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 0.0001,\n",
       "   'classifier__penalty': 'l2',\n",
       "   'classifier__solver': 'saga'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 0.001,\n",
       "   'classifier__penalty': 'l1',\n",
       "   'classifier__solver': 'saga'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 0.001,\n",
       "   'classifier__penalty': 'l2',\n",
       "   'classifier__solver': 'saga'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 0.01,\n",
       "   'classifier__penalty': 'l1',\n",
       "   'classifier__solver': 'saga'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 0.01,\n",
       "   'classifier__penalty': 'l2',\n",
       "   'classifier__solver': 'saga'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 0.1,\n",
       "   'classifier__penalty': 'l1',\n",
       "   'classifier__solver': 'saga'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 0.1,\n",
       "   'classifier__penalty': 'l2',\n",
       "   'classifier__solver': 'saga'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 1.0,\n",
       "   'classifier__penalty': 'l1',\n",
       "   'classifier__solver': 'saga'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 1.0,\n",
       "   'classifier__penalty': 'l2',\n",
       "   'classifier__solver': 'saga'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 10.0,\n",
       "   'classifier__penalty': 'l1',\n",
       "   'classifier__solver': 'saga'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 10.0,\n",
       "   'classifier__penalty': 'l2',\n",
       "   'classifier__solver': 'saga'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 100.0,\n",
       "   'classifier__penalty': 'l1',\n",
       "   'classifier__solver': 'saga'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 100.0,\n",
       "   'classifier__penalty': 'l2',\n",
       "   'classifier__solver': 'saga'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 1000.0,\n",
       "   'classifier__penalty': 'l1',\n",
       "   'classifier__solver': 'saga'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 1000.0,\n",
       "   'classifier__penalty': 'l2',\n",
       "   'classifier__solver': 'saga'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 10000.0,\n",
       "   'classifier__penalty': 'l1',\n",
       "   'classifier__solver': 'saga'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 10000.0,\n",
       "   'classifier__penalty': 'l2',\n",
       "   'classifier__solver': 'saga'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 0.0001,\n",
       "   'classifier__penalty': 'l2',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 0.001,\n",
       "   'classifier__penalty': 'l2',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 0.01,\n",
       "   'classifier__penalty': 'l2',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 0.1,\n",
       "   'classifier__penalty': 'l2',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 1.0,\n",
       "   'classifier__penalty': 'l2',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 10.0,\n",
       "   'classifier__penalty': 'l2',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 100.0,\n",
       "   'classifier__penalty': 'l2',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 1000.0,\n",
       "   'classifier__penalty': 'l2',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 10000.0,\n",
       "   'classifier__penalty': 'l2',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__penalty': 'none',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__penalty': 'none',\n",
       "   'classifier__solver': 'saga'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000)}],\n",
       " 'split0_test_accuracy': array([0.601, 0.709, 0.689, 0.765, 0.748, 0.756, 0.755, 0.757, 0.756,\n",
       "        0.756, 0.755, 0.755, 0.755, 0.755, 0.755, 0.755, 0.755, 0.755,\n",
       "        0.709, 0.765, 0.756, 0.757, 0.756, 0.755, 0.755, 0.755, 0.755,\n",
       "        0.755, 0.755, 0.756]),\n",
       " 'split1_test_accuracy': array([0.601, 0.7  , 0.684, 0.73 , 0.734, 0.735, 0.736, 0.735, 0.734,\n",
       "        0.735, 0.736, 0.736, 0.736, 0.736, 0.736, 0.735, 0.736, 0.736,\n",
       "        0.7  , 0.73 , 0.735, 0.735, 0.735, 0.736, 0.736, 0.736, 0.736,\n",
       "        0.736, 0.736, 0.735]),\n",
       " 'split2_test_accuracy': array([0.601, 0.686, 0.676, 0.729, 0.726, 0.726, 0.725, 0.726, 0.728,\n",
       "        0.728, 0.728, 0.728, 0.728, 0.728, 0.728, 0.728, 0.728, 0.728,\n",
       "        0.686, 0.729, 0.726, 0.726, 0.728, 0.728, 0.728, 0.728, 0.728,\n",
       "        0.728, 0.728, 0.728]),\n",
       " 'split3_test_accuracy': array([0.601, 0.698, 0.685, 0.748, 0.741, 0.746, 0.743, 0.746, 0.747,\n",
       "        0.748, 0.748, 0.748, 0.748, 0.748, 0.748, 0.748, 0.748, 0.748,\n",
       "        0.698, 0.748, 0.746, 0.746, 0.748, 0.748, 0.748, 0.748, 0.748,\n",
       "        0.748, 0.748, 0.748]),\n",
       " 'split4_test_accuracy': array([0.6  , 0.712, 0.696, 0.767, 0.76 , 0.765, 0.762, 0.762, 0.762,\n",
       "        0.763, 0.763, 0.763, 0.763, 0.763, 0.763, 0.763, 0.763, 0.763,\n",
       "        0.712, 0.767, 0.765, 0.762, 0.763, 0.763, 0.763, 0.763, 0.763,\n",
       "        0.763, 0.763, 0.763]),\n",
       " 'split5_test_accuracy': array([0.6  , 0.67 , 0.678, 0.727, 0.726, 0.731, 0.73 , 0.73 , 0.73 ,\n",
       "        0.731, 0.73 , 0.73 , 0.73 , 0.73 , 0.73 , 0.73 , 0.73 , 0.73 ,\n",
       "        0.67 , 0.727, 0.731, 0.73 , 0.731, 0.73 , 0.73 , 0.73 , 0.73 ,\n",
       "        0.73 , 0.73 , 0.731]),\n",
       " 'split6_test_accuracy': array([0.6  , 0.679, 0.67 , 0.72 , 0.725, 0.724, 0.723, 0.721, 0.722,\n",
       "        0.722, 0.722, 0.722, 0.722, 0.722, 0.722, 0.722, 0.722, 0.722,\n",
       "        0.679, 0.72 , 0.724, 0.721, 0.722, 0.722, 0.722, 0.722, 0.722,\n",
       "        0.722, 0.722, 0.722]),\n",
       " 'split7_test_accuracy': array([0.6  , 0.697, 0.69 , 0.741, 0.741, 0.74 , 0.74 , 0.739, 0.74 ,\n",
       "        0.74 , 0.74 , 0.74 , 0.74 , 0.74 , 0.74 , 0.74 , 0.74 , 0.74 ,\n",
       "        0.697, 0.741, 0.74 , 0.739, 0.74 , 0.74 , 0.74 , 0.74 , 0.74 ,\n",
       "        0.74 , 0.74 , 0.74 ]),\n",
       " 'split8_test_accuracy': array([0.6  , 0.699, 0.695, 0.763, 0.755, 0.756, 0.757, 0.756, 0.756,\n",
       "        0.756, 0.757, 0.756, 0.756, 0.756, 0.756, 0.756, 0.756, 0.756,\n",
       "        0.699, 0.763, 0.756, 0.756, 0.756, 0.756, 0.756, 0.756, 0.756,\n",
       "        0.756, 0.756, 0.756]),\n",
       " 'split9_test_accuracy': array([0.6  , 0.667, 0.674, 0.722, 0.734, 0.735, 0.732, 0.735, 0.736,\n",
       "        0.735, 0.735, 0.735, 0.735, 0.735, 0.735, 0.735, 0.735, 0.735,\n",
       "        0.667, 0.722, 0.735, 0.735, 0.735, 0.735, 0.735, 0.735, 0.735,\n",
       "        0.735, 0.735, 0.735]),\n",
       " 'mean_test_accuracy': array([0.6004, 0.6917, 0.6837, 0.7412, 0.739 , 0.7414, 0.7403, 0.7407,\n",
       "        0.7411, 0.7414, 0.7414, 0.7413, 0.7413, 0.7413, 0.7413, 0.7412,\n",
       "        0.7413, 0.7413, 0.6917, 0.7412, 0.7414, 0.7407, 0.7414, 0.7413,\n",
       "        0.7413, 0.7413, 0.7413, 0.7413, 0.7413, 0.7414]),\n",
       " 'std_test_accuracy': array([0.0004899 , 0.01475161, 0.00849765, 0.01745738, 0.0117047 ,\n",
       "        0.01317725, 0.01303879, 0.01329699, 0.01285652, 0.01297844,\n",
       "        0.01302459, 0.01290775, 0.01290775, 0.01290775, 0.01290775,\n",
       "        0.01295222, 0.01290775, 0.01290775, 0.01475161, 0.01745738,\n",
       "        0.01317725, 0.01329699, 0.01297844, 0.01290775, 0.01290775,\n",
       "        0.01290775, 0.01290775, 0.01290775, 0.01290775, 0.01297844]),\n",
       " 'rank_test_accuracy': array([30, 27, 29, 20, 26,  1, 25, 23, 22,  1,  1,  7,  7,  7,  7, 19,  7,\n",
       "         7, 27, 20,  1, 23,  1,  7,  7,  7,  7,  7,  7,  1]),\n",
       " 'split0_test_precision': array([0.        , 0.78421053, 0.72      , 0.72162162, 0.68797954,\n",
       "        0.69230769, 0.68965517, 0.69306931, 0.69230769, 0.69230769,\n",
       "        0.69059406, 0.69059406, 0.69059406, 0.69059406, 0.69059406,\n",
       "        0.69059406, 0.69059406, 0.69059406, 0.78421053, 0.72162162,\n",
       "        0.69230769, 0.69306931, 0.69230769, 0.69059406, 0.69059406,\n",
       "        0.69059406, 0.69059406, 0.69059406, 0.69059406, 0.69230769]),\n",
       " 'split1_test_precision': array([0.        , 0.77348066, 0.73184358, 0.6996904 , 0.68838527,\n",
       "        0.68406593, 0.68493151, 0.68306011, 0.68219178, 0.68306011,\n",
       "        0.68392371, 0.68392371, 0.68392371, 0.68392371, 0.68392371,\n",
       "        0.68306011, 0.68392371, 0.68392371, 0.77348066, 0.6996904 ,\n",
       "        0.68406593, 0.68306011, 0.68306011, 0.68392371, 0.68392371,\n",
       "        0.68392371, 0.68392371, 0.68392371, 0.68392371, 0.68306011]),\n",
       " 'split2_test_precision': array([0.        , 0.75757576, 0.74834437, 0.69753086, 0.68221574,\n",
       "        0.67313019, 0.67032967, 0.67123288, 0.67493113, 0.67493113,\n",
       "        0.67493113, 0.67493113, 0.67493113, 0.67493113, 0.67493113,\n",
       "        0.67493113, 0.67493113, 0.67493113, 0.75757576, 0.69753086,\n",
       "        0.67313019, 0.67123288, 0.67493113, 0.67493113, 0.67493113,\n",
       "        0.67493113, 0.67493113, 0.67493113, 0.67493113, 0.67493113]),\n",
       " 'split3_test_precision': array([0.        , 0.76502732, 0.73595506, 0.71940299, 0.69444444,\n",
       "        0.69230769, 0.68783069, 0.69028871, 0.69210526, 0.69291339,\n",
       "        0.69291339, 0.69291339, 0.69291339, 0.69291339, 0.69291339,\n",
       "        0.69291339, 0.69291339, 0.69291339, 0.76502732, 0.71940299,\n",
       "        0.69230769, 0.69028871, 0.69291339, 0.69291339, 0.69291339,\n",
       "        0.69291339, 0.69291339, 0.69291339, 0.69291339, 0.69291339]),\n",
       " 'split4_test_precision': array([0.        , 0.83333333, 0.76373626, 0.73789174, 0.71390374,\n",
       "        0.7154047 , 0.70984456, 0.70984456, 0.70984456, 0.71059432,\n",
       "        0.71059432, 0.71059432, 0.71059432, 0.71059432, 0.71059432,\n",
       "        0.71059432, 0.71059432, 0.71059432, 0.83333333, 0.73789174,\n",
       "        0.7154047 , 0.70984456, 0.71059432, 0.71059432, 0.71059432,\n",
       "        0.71059432, 0.71059432, 0.71059432, 0.71059432, 0.71059432]),\n",
       " 'split5_test_precision': array([0.        , 0.70833333, 0.74683544, 0.6884273 , 0.67897727,\n",
       "        0.67945205, 0.67473118, 0.67379679, 0.67379679, 0.67466667,\n",
       "        0.67379679, 0.67379679, 0.67379679, 0.67379679, 0.67379679,\n",
       "        0.67379679, 0.67379679, 0.67379679, 0.70833333, 0.6884273 ,\n",
       "        0.67945205, 0.67379679, 0.67466667, 0.67379679, 0.67379679,\n",
       "        0.67379679, 0.67379679, 0.67379679, 0.67379679, 0.67466667]),\n",
       " 'split6_test_precision': array([0.        , 0.71122995, 0.70114943, 0.66666667, 0.65743073,\n",
       "        0.65656566, 0.65336658, 0.65087282, 0.6525    , 0.6525    ,\n",
       "        0.6525    , 0.6525    , 0.6525    , 0.6525    , 0.6525    ,\n",
       "        0.6525    , 0.6525    , 0.6525    , 0.71122995, 0.66666667,\n",
       "        0.65656566, 0.65087282, 0.6525    , 0.6525    , 0.6525    ,\n",
       "        0.6525    , 0.6525    , 0.6525    , 0.6525    , 0.6525    ]),\n",
       " 'split7_test_precision': array([0.        , 0.75935829, 0.734375  , 0.69747899, 0.67938931,\n",
       "        0.68134715, 0.67676768, 0.67594937, 0.67676768, 0.67676768,\n",
       "        0.67676768, 0.67676768, 0.67676768, 0.67676768, 0.67676768,\n",
       "        0.67676768, 0.67676768, 0.67676768, 0.75935829, 0.69747899,\n",
       "        0.68134715, 0.67594937, 0.67676768, 0.67676768, 0.67676768,\n",
       "        0.67676768, 0.67676768, 0.67676768, 0.67676768, 0.67676768]),\n",
       " 'split8_test_precision': array([0.        , 0.75126904, 0.77456647, 0.74183976, 0.71587744,\n",
       "        0.71311475, 0.7115903 , 0.71081081, 0.70967742, 0.71081081,\n",
       "        0.7115903 , 0.71081081, 0.71081081, 0.71081081, 0.71081081,\n",
       "        0.71081081, 0.71081081, 0.71081081, 0.75126904, 0.74183976,\n",
       "        0.71311475, 0.71081081, 0.71081081, 0.71081081, 0.71081081,\n",
       "        0.71081081, 0.71081081, 0.71081081, 0.71081081, 0.71081081]),\n",
       " 'split9_test_precision': array([0.        , 0.69822485, 0.7202381 , 0.68154762, 0.69034091,\n",
       "        0.68698061, 0.68032787, 0.68493151, 0.68579235, 0.68493151,\n",
       "        0.68493151, 0.68493151, 0.68493151, 0.68493151, 0.68493151,\n",
       "        0.68493151, 0.68493151, 0.68493151, 0.69822485, 0.68154762,\n",
       "        0.68698061, 0.68493151, 0.68493151, 0.68493151, 0.68493151,\n",
       "        0.68493151, 0.68493151, 0.68493151, 0.68493151, 0.68493151]),\n",
       " 'mean_test_precision': array([0.        , 0.75420431, 0.73770437, 0.7052098 , 0.68889444,\n",
       "        0.68746764, 0.68393752, 0.68438569, 0.68499147, 0.68534833,\n",
       "        0.68525429, 0.68517634, 0.68517634, 0.68517634, 0.68517634,\n",
       "        0.68508998, 0.68517634, 0.68517634, 0.75420431, 0.7052098 ,\n",
       "        0.68746764, 0.68438569, 0.68534833, 0.68517634, 0.68517634,\n",
       "        0.68517634, 0.68517634, 0.68517634, 0.68517634, 0.68534833]),\n",
       " 'std_test_precision': array([0.        , 0.03844695, 0.02058236, 0.02314514, 0.01615799,\n",
       "        0.0166106 , 0.01659408, 0.01714104, 0.01639983, 0.0166494 ,\n",
       "        0.01675548, 0.01663415, 0.01663415, 0.01663415, 0.01663415,\n",
       "        0.01664267, 0.01663415, 0.01663415, 0.03844695, 0.02314514,\n",
       "        0.0166106 , 0.01714104, 0.0166494 , 0.01663415, 0.01663415,\n",
       "        0.01663415, 0.01663415, 0.01663415, 0.01663415, 0.0166494 ]),\n",
       " 'rank_test_precision': array([30,  1,  3,  4,  6,  7, 29, 27, 26,  9, 12, 13, 13, 13, 13, 25, 13,\n",
       "        13,  1,  4,  7, 27,  9, 13, 13, 13, 13, 13, 13,  9]),\n",
       " 'split0_test_f1_micro': array([0.601, 0.709, 0.689, 0.765, 0.748, 0.756, 0.755, 0.757, 0.756,\n",
       "        0.756, 0.755, 0.755, 0.755, 0.755, 0.755, 0.755, 0.755, 0.755,\n",
       "        0.709, 0.765, 0.756, 0.757, 0.756, 0.755, 0.755, 0.755, 0.755,\n",
       "        0.755, 0.755, 0.756]),\n",
       " 'split1_test_f1_micro': array([0.601, 0.7  , 0.684, 0.73 , 0.734, 0.735, 0.736, 0.735, 0.734,\n",
       "        0.735, 0.736, 0.736, 0.736, 0.736, 0.736, 0.735, 0.736, 0.736,\n",
       "        0.7  , 0.73 , 0.735, 0.735, 0.735, 0.736, 0.736, 0.736, 0.736,\n",
       "        0.736, 0.736, 0.735]),\n",
       " 'split2_test_f1_micro': array([0.601, 0.686, 0.676, 0.729, 0.726, 0.726, 0.725, 0.726, 0.728,\n",
       "        0.728, 0.728, 0.728, 0.728, 0.728, 0.728, 0.728, 0.728, 0.728,\n",
       "        0.686, 0.729, 0.726, 0.726, 0.728, 0.728, 0.728, 0.728, 0.728,\n",
       "        0.728, 0.728, 0.728]),\n",
       " 'split3_test_f1_micro': array([0.601, 0.698, 0.685, 0.748, 0.741, 0.746, 0.743, 0.746, 0.747,\n",
       "        0.748, 0.748, 0.748, 0.748, 0.748, 0.748, 0.748, 0.748, 0.748,\n",
       "        0.698, 0.748, 0.746, 0.746, 0.748, 0.748, 0.748, 0.748, 0.748,\n",
       "        0.748, 0.748, 0.748]),\n",
       " 'split4_test_f1_micro': array([0.6  , 0.712, 0.696, 0.767, 0.76 , 0.765, 0.762, 0.762, 0.762,\n",
       "        0.763, 0.763, 0.763, 0.763, 0.763, 0.763, 0.763, 0.763, 0.763,\n",
       "        0.712, 0.767, 0.765, 0.762, 0.763, 0.763, 0.763, 0.763, 0.763,\n",
       "        0.763, 0.763, 0.763]),\n",
       " 'split5_test_f1_micro': array([0.6  , 0.67 , 0.678, 0.727, 0.726, 0.731, 0.73 , 0.73 , 0.73 ,\n",
       "        0.731, 0.73 , 0.73 , 0.73 , 0.73 , 0.73 , 0.73 , 0.73 , 0.73 ,\n",
       "        0.67 , 0.727, 0.731, 0.73 , 0.731, 0.73 , 0.73 , 0.73 , 0.73 ,\n",
       "        0.73 , 0.73 , 0.731]),\n",
       " 'split6_test_f1_micro': array([0.6  , 0.679, 0.67 , 0.72 , 0.725, 0.724, 0.723, 0.721, 0.722,\n",
       "        0.722, 0.722, 0.722, 0.722, 0.722, 0.722, 0.722, 0.722, 0.722,\n",
       "        0.679, 0.72 , 0.724, 0.721, 0.722, 0.722, 0.722, 0.722, 0.722,\n",
       "        0.722, 0.722, 0.722]),\n",
       " 'split7_test_f1_micro': array([0.6  , 0.697, 0.69 , 0.741, 0.741, 0.74 , 0.74 , 0.739, 0.74 ,\n",
       "        0.74 , 0.74 , 0.74 , 0.74 , 0.74 , 0.74 , 0.74 , 0.74 , 0.74 ,\n",
       "        0.697, 0.741, 0.74 , 0.739, 0.74 , 0.74 , 0.74 , 0.74 , 0.74 ,\n",
       "        0.74 , 0.74 , 0.74 ]),\n",
       " 'split8_test_f1_micro': array([0.6  , 0.699, 0.695, 0.763, 0.755, 0.756, 0.757, 0.756, 0.756,\n",
       "        0.756, 0.757, 0.756, 0.756, 0.756, 0.756, 0.756, 0.756, 0.756,\n",
       "        0.699, 0.763, 0.756, 0.756, 0.756, 0.756, 0.756, 0.756, 0.756,\n",
       "        0.756, 0.756, 0.756]),\n",
       " 'split9_test_f1_micro': array([0.6  , 0.667, 0.674, 0.722, 0.734, 0.735, 0.732, 0.735, 0.736,\n",
       "        0.735, 0.735, 0.735, 0.735, 0.735, 0.735, 0.735, 0.735, 0.735,\n",
       "        0.667, 0.722, 0.735, 0.735, 0.735, 0.735, 0.735, 0.735, 0.735,\n",
       "        0.735, 0.735, 0.735]),\n",
       " 'mean_test_f1_micro': array([0.6004, 0.6917, 0.6837, 0.7412, 0.739 , 0.7414, 0.7403, 0.7407,\n",
       "        0.7411, 0.7414, 0.7414, 0.7413, 0.7413, 0.7413, 0.7413, 0.7412,\n",
       "        0.7413, 0.7413, 0.6917, 0.7412, 0.7414, 0.7407, 0.7414, 0.7413,\n",
       "        0.7413, 0.7413, 0.7413, 0.7413, 0.7413, 0.7414]),\n",
       " 'std_test_f1_micro': array([0.0004899 , 0.01475161, 0.00849765, 0.01745738, 0.0117047 ,\n",
       "        0.01317725, 0.01303879, 0.01329699, 0.01285652, 0.01297844,\n",
       "        0.01302459, 0.01290775, 0.01290775, 0.01290775, 0.01290775,\n",
       "        0.01295222, 0.01290775, 0.01290775, 0.01475161, 0.01745738,\n",
       "        0.01317725, 0.01329699, 0.01297844, 0.01290775, 0.01290775,\n",
       "        0.01290775, 0.01290775, 0.01290775, 0.01290775, 0.01297844]),\n",
       " 'rank_test_f1_micro': array([30, 27, 29, 20, 26,  1, 25, 23, 22,  1,  1,  7,  7,  7,  7, 19,  7,\n",
       "         7, 27, 20,  1, 23,  1,  7,  7,  7,  7,  7,  7,  1])}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_model.cv_results_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6e3313e2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([30, 27, 29, 20, 26,  1, 25, 23, 22,  1,  1,  7,  7,  7,  7, 19,  7,\n",
       "        7, 27, 20,  1, 23,  1,  7,  7,  7,  7,  7,  7,  1])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_model.cv_results_['rank_test_accuracy']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3d08f33e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'classifier': LogisticRegression(max_iter=10000),\n",
       " 'classifier__C': 0.0001,\n",
       " 'classifier__penalty': 'l1',\n",
       " 'classifier__solver': 'saga'}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_model.cv_results_['params'][ np.argmin(best_model.cv_results_['mean_test_accuracy'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c8da90d4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>classifier</th>\n",
       "      <th>C</th>\n",
       "      <th>penalty</th>\n",
       "      <th>solver</th>\n",
       "      <th>score_acc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.3996</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.3083</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.3163</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.2588</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0100</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.2610</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0100</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.2586</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.1000</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.2597</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.1000</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.2593</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.2589</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.2586</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>10.0000</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.2586</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>10.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.2587</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>100.0000</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.2587</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>100.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.2587</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>1000.0000</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.2587</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>1000.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.2588</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>10000.0000</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.2587</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>10000.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.2587</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.3083</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.2588</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0100</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.2586</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.1000</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.2593</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.2586</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>10.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.2587</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>100.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.2587</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>1000.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.2587</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>10000.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.2587</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>NaN</td>\n",
       "      <td>none</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.2587</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>NaN</td>\n",
       "      <td>none</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.2587</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.2586</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                            classifier           C penalty solver  score_acc\n",
       "0   LogisticRegression(max_iter=10000)      0.0001      l1   saga     0.3996\n",
       "1   LogisticRegression(max_iter=10000)      0.0001      l2   saga     0.3083\n",
       "2   LogisticRegression(max_iter=10000)      0.0010      l1   saga     0.3163\n",
       "3   LogisticRegression(max_iter=10000)      0.0010      l2   saga     0.2588\n",
       "4   LogisticRegression(max_iter=10000)      0.0100      l1   saga     0.2610\n",
       "5   LogisticRegression(max_iter=10000)      0.0100      l2   saga     0.2586\n",
       "6   LogisticRegression(max_iter=10000)      0.1000      l1   saga     0.2597\n",
       "7   LogisticRegression(max_iter=10000)      0.1000      l2   saga     0.2593\n",
       "8   LogisticRegression(max_iter=10000)      1.0000      l1   saga     0.2589\n",
       "9   LogisticRegression(max_iter=10000)      1.0000      l2   saga     0.2586\n",
       "10  LogisticRegression(max_iter=10000)     10.0000      l1   saga     0.2586\n",
       "11  LogisticRegression(max_iter=10000)     10.0000      l2   saga     0.2587\n",
       "12  LogisticRegression(max_iter=10000)    100.0000      l1   saga     0.2587\n",
       "13  LogisticRegression(max_iter=10000)    100.0000      l2   saga     0.2587\n",
       "14  LogisticRegression(max_iter=10000)   1000.0000      l1   saga     0.2587\n",
       "15  LogisticRegression(max_iter=10000)   1000.0000      l2   saga     0.2588\n",
       "16  LogisticRegression(max_iter=10000)  10000.0000      l1   saga     0.2587\n",
       "17  LogisticRegression(max_iter=10000)  10000.0000      l2   saga     0.2587\n",
       "18  LogisticRegression(max_iter=10000)      0.0001      l2  lbfgs     0.3083\n",
       "19  LogisticRegression(max_iter=10000)      0.0010      l2  lbfgs     0.2588\n",
       "20  LogisticRegression(max_iter=10000)      0.0100      l2  lbfgs     0.2586\n",
       "21  LogisticRegression(max_iter=10000)      0.1000      l2  lbfgs     0.2593\n",
       "22  LogisticRegression(max_iter=10000)      1.0000      l2  lbfgs     0.2586\n",
       "23  LogisticRegression(max_iter=10000)     10.0000      l2  lbfgs     0.2587\n",
       "24  LogisticRegression(max_iter=10000)    100.0000      l2  lbfgs     0.2587\n",
       "25  LogisticRegression(max_iter=10000)   1000.0000      l2  lbfgs     0.2587\n",
       "26  LogisticRegression(max_iter=10000)  10000.0000      l2  lbfgs     0.2587\n",
       "27  LogisticRegression(max_iter=10000)         NaN    none  lbfgs     0.2587\n",
       "28  LogisticRegression(max_iter=10000)         NaN    none   saga     0.2587\n",
       "29  LogisticRegression(max_iter=10000)         NaN     NaN    NaN     0.2586"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results = pd.DataFrame( best_model.cv_results_['params'] )\n",
    "\n",
    "# next grab the score resulting from those parameters, add it to the data\n",
    "# score is accuracy; to display it as misclassification error we use 1 - x\n",
    "results['score_acc'] = 1 - best_model.cv_results_['mean_test_accuracy']\n",
    "\n",
    "# get rid of classifierXX in columns\n",
    "cols = results.columns.to_series().str.split('__').apply(lambda x: x[-1])\n",
    "results.columns = cols\n",
    "\n",
    "# lets show the results for the saga solver across penalty & C values\n",
    "#sns.heatmap( results.query('solver==\"saga\"').pivot('C','penalty','score_acc'),\n",
    "#             annot=True, fmt='.3f')\n",
    "\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "eb7e7292",
   "metadata": {},
   "outputs": [],
   "source": [
    "acc_list.append(results[0:1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ce1a13cb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'classifier': LogisticRegression(max_iter=10000),\n",
       " 'classifier__C': 0.0001,\n",
       " 'classifier__penalty': 'l2',\n",
       " 'classifier__solver': 'saga'}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_model.cv_results_['params'][ np.argmin(best_model.cv_results_['rank_test_precision'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e6654f1a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>classifier</th>\n",
       "      <th>C</th>\n",
       "      <th>penalty</th>\n",
       "      <th>solver</th>\n",
       "      <th>score_precision</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.245796</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.262296</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.294790</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0100</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.311106</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0100</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.312532</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.1000</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.316062</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.1000</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.315614</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.315009</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.314652</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>10.0000</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.314746</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>10.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.314824</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>100.0000</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.314824</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>100.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.314824</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>1000.0000</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.314824</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>1000.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.314910</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>10000.0000</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.314824</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>10000.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.314824</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.245796</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.294790</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0100</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.312532</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.1000</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.315614</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.314652</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>10.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.314824</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>100.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.314824</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>1000.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.314824</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>10000.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.314824</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>NaN</td>\n",
       "      <td>none</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.314824</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>NaN</td>\n",
       "      <td>none</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.314824</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.314652</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                            classifier           C penalty solver  \\\n",
       "0   LogisticRegression(max_iter=10000)      0.0001      l1   saga   \n",
       "1   LogisticRegression(max_iter=10000)      0.0001      l2   saga   \n",
       "2   LogisticRegression(max_iter=10000)      0.0010      l1   saga   \n",
       "3   LogisticRegression(max_iter=10000)      0.0010      l2   saga   \n",
       "4   LogisticRegression(max_iter=10000)      0.0100      l1   saga   \n",
       "5   LogisticRegression(max_iter=10000)      0.0100      l2   saga   \n",
       "6   LogisticRegression(max_iter=10000)      0.1000      l1   saga   \n",
       "7   LogisticRegression(max_iter=10000)      0.1000      l2   saga   \n",
       "8   LogisticRegression(max_iter=10000)      1.0000      l1   saga   \n",
       "9   LogisticRegression(max_iter=10000)      1.0000      l2   saga   \n",
       "10  LogisticRegression(max_iter=10000)     10.0000      l1   saga   \n",
       "11  LogisticRegression(max_iter=10000)     10.0000      l2   saga   \n",
       "12  LogisticRegression(max_iter=10000)    100.0000      l1   saga   \n",
       "13  LogisticRegression(max_iter=10000)    100.0000      l2   saga   \n",
       "14  LogisticRegression(max_iter=10000)   1000.0000      l1   saga   \n",
       "15  LogisticRegression(max_iter=10000)   1000.0000      l2   saga   \n",
       "16  LogisticRegression(max_iter=10000)  10000.0000      l1   saga   \n",
       "17  LogisticRegression(max_iter=10000)  10000.0000      l2   saga   \n",
       "18  LogisticRegression(max_iter=10000)      0.0001      l2  lbfgs   \n",
       "19  LogisticRegression(max_iter=10000)      0.0010      l2  lbfgs   \n",
       "20  LogisticRegression(max_iter=10000)      0.0100      l2  lbfgs   \n",
       "21  LogisticRegression(max_iter=10000)      0.1000      l2  lbfgs   \n",
       "22  LogisticRegression(max_iter=10000)      1.0000      l2  lbfgs   \n",
       "23  LogisticRegression(max_iter=10000)     10.0000      l2  lbfgs   \n",
       "24  LogisticRegression(max_iter=10000)    100.0000      l2  lbfgs   \n",
       "25  LogisticRegression(max_iter=10000)   1000.0000      l2  lbfgs   \n",
       "26  LogisticRegression(max_iter=10000)  10000.0000      l2  lbfgs   \n",
       "27  LogisticRegression(max_iter=10000)         NaN    none  lbfgs   \n",
       "28  LogisticRegression(max_iter=10000)         NaN    none   saga   \n",
       "29  LogisticRegression(max_iter=10000)         NaN     NaN    NaN   \n",
       "\n",
       "    score_precision  \n",
       "0          1.000000  \n",
       "1          0.245796  \n",
       "2          0.262296  \n",
       "3          0.294790  \n",
       "4          0.311106  \n",
       "5          0.312532  \n",
       "6          0.316062  \n",
       "7          0.315614  \n",
       "8          0.315009  \n",
       "9          0.314652  \n",
       "10         0.314746  \n",
       "11         0.314824  \n",
       "12         0.314824  \n",
       "13         0.314824  \n",
       "14         0.314824  \n",
       "15         0.314910  \n",
       "16         0.314824  \n",
       "17         0.314824  \n",
       "18         0.245796  \n",
       "19         0.294790  \n",
       "20         0.312532  \n",
       "21         0.315614  \n",
       "22         0.314652  \n",
       "23         0.314824  \n",
       "24         0.314824  \n",
       "25         0.314824  \n",
       "26         0.314824  \n",
       "27         0.314824  \n",
       "28         0.314824  \n",
       "29         0.314652  "
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results = pd.DataFrame( best_model.cv_results_['params'] )\n",
    "\n",
    "# next grab the score resulting from those parameters, add it to the data\n",
    "# score is accuracy; to display it as misclassification error we use 1 - x\n",
    "results['score_precision'] = 1 - best_model.cv_results_['mean_test_precision']\n",
    "\n",
    "# get rid of classifierXX in columns\n",
    "cols = results.columns.to_series().str.split('__').apply(lambda x: x[-1])\n",
    "results.columns = cols\n",
    "\n",
    "# lets show the results for the saga solver across penalty & C values\n",
    "#sns.heatmap( results.query('solver==\"saga\"').pivot('C','penalty','score_precision'),\n",
    "#             annot=True, fmt='.3f')\n",
    "\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "71d804bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "roc_auc_ovr_List.append(results[1:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "c8742a4b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'classifier': LogisticRegression(max_iter=10000),\n",
       " 'classifier__C': 0.01,\n",
       " 'classifier__penalty': 'l2',\n",
       " 'classifier__solver': 'saga'}"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_model.cv_results_['params'][ np.argmin(best_model.cv_results_['rank_test_f1_micro'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "4bf5cd96",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>classifier</th>\n",
       "      <th>C</th>\n",
       "      <th>penalty</th>\n",
       "      <th>solver</th>\n",
       "      <th>score_f1_micro</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.3996</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.3083</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.3163</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.2588</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0100</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.2610</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0100</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.2586</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.1000</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.2597</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.1000</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.2593</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.2589</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.2586</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>10.0000</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.2586</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>10.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.2587</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>100.0000</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.2587</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>100.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.2587</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>1000.0000</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.2587</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>1000.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.2588</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>10000.0000</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.2587</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>10000.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.2587</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.3083</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.2588</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0100</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.2586</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.1000</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.2593</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.2586</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>10.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.2587</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>100.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.2587</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>1000.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.2587</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>10000.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.2587</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>NaN</td>\n",
       "      <td>none</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.2587</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>NaN</td>\n",
       "      <td>none</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.2587</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.2586</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                            classifier           C penalty solver  \\\n",
       "0   LogisticRegression(max_iter=10000)      0.0001      l1   saga   \n",
       "1   LogisticRegression(max_iter=10000)      0.0001      l2   saga   \n",
       "2   LogisticRegression(max_iter=10000)      0.0010      l1   saga   \n",
       "3   LogisticRegression(max_iter=10000)      0.0010      l2   saga   \n",
       "4   LogisticRegression(max_iter=10000)      0.0100      l1   saga   \n",
       "5   LogisticRegression(max_iter=10000)      0.0100      l2   saga   \n",
       "6   LogisticRegression(max_iter=10000)      0.1000      l1   saga   \n",
       "7   LogisticRegression(max_iter=10000)      0.1000      l2   saga   \n",
       "8   LogisticRegression(max_iter=10000)      1.0000      l1   saga   \n",
       "9   LogisticRegression(max_iter=10000)      1.0000      l2   saga   \n",
       "10  LogisticRegression(max_iter=10000)     10.0000      l1   saga   \n",
       "11  LogisticRegression(max_iter=10000)     10.0000      l2   saga   \n",
       "12  LogisticRegression(max_iter=10000)    100.0000      l1   saga   \n",
       "13  LogisticRegression(max_iter=10000)    100.0000      l2   saga   \n",
       "14  LogisticRegression(max_iter=10000)   1000.0000      l1   saga   \n",
       "15  LogisticRegression(max_iter=10000)   1000.0000      l2   saga   \n",
       "16  LogisticRegression(max_iter=10000)  10000.0000      l1   saga   \n",
       "17  LogisticRegression(max_iter=10000)  10000.0000      l2   saga   \n",
       "18  LogisticRegression(max_iter=10000)      0.0001      l2  lbfgs   \n",
       "19  LogisticRegression(max_iter=10000)      0.0010      l2  lbfgs   \n",
       "20  LogisticRegression(max_iter=10000)      0.0100      l2  lbfgs   \n",
       "21  LogisticRegression(max_iter=10000)      0.1000      l2  lbfgs   \n",
       "22  LogisticRegression(max_iter=10000)      1.0000      l2  lbfgs   \n",
       "23  LogisticRegression(max_iter=10000)     10.0000      l2  lbfgs   \n",
       "24  LogisticRegression(max_iter=10000)    100.0000      l2  lbfgs   \n",
       "25  LogisticRegression(max_iter=10000)   1000.0000      l2  lbfgs   \n",
       "26  LogisticRegression(max_iter=10000)  10000.0000      l2  lbfgs   \n",
       "27  LogisticRegression(max_iter=10000)         NaN    none  lbfgs   \n",
       "28  LogisticRegression(max_iter=10000)         NaN    none   saga   \n",
       "29  LogisticRegression(max_iter=10000)         NaN     NaN    NaN   \n",
       "\n",
       "    score_f1_micro  \n",
       "0           0.3996  \n",
       "1           0.3083  \n",
       "2           0.3163  \n",
       "3           0.2588  \n",
       "4           0.2610  \n",
       "5           0.2586  \n",
       "6           0.2597  \n",
       "7           0.2593  \n",
       "8           0.2589  \n",
       "9           0.2586  \n",
       "10          0.2586  \n",
       "11          0.2587  \n",
       "12          0.2587  \n",
       "13          0.2587  \n",
       "14          0.2587  \n",
       "15          0.2588  \n",
       "16          0.2587  \n",
       "17          0.2587  \n",
       "18          0.3083  \n",
       "19          0.2588  \n",
       "20          0.2586  \n",
       "21          0.2593  \n",
       "22          0.2586  \n",
       "23          0.2587  \n",
       "24          0.2587  \n",
       "25          0.2587  \n",
       "26          0.2587  \n",
       "27          0.2587  \n",
       "28          0.2587  \n",
       "29          0.2586  "
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results = pd.DataFrame( best_model.cv_results_['params'] )\n",
    "\n",
    "# next grab the score resulting from those parameters, add it to the data\n",
    "# score is accuracy; to display it as misclassification error we use 1 - x\n",
    "results['score_f1_micro'] = 1 - best_model.cv_results_['mean_test_f1_micro']\n",
    "\n",
    "# get rid of classifierXX in columns\n",
    "cols = results.columns.to_series().str.split('__').apply(lambda x: x[-1])\n",
    "results.columns = cols\n",
    "\n",
    "# lets show the results for the saga solver across penalty & C values\n",
    "#sns.heatmap( results.query('solver==\"saga\"').pivot('C','penalty','score_f1_micro'),\n",
    "#             annot=True, fmt='.3f')\n",
    "\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "8cebbf79",
   "metadata": {},
   "outputs": [],
   "source": [
    "f1_micro_List.append(results[5:6])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b315f477",
   "metadata": {},
   "source": [
    "## Trial 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "8e5bcf1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, Y_train, Y_test = train_test_split(X, y, train_size=10000, shuffle=True, random_state = 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "e91ba61f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Guest-All\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1469: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\Guest-All\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1469: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\Guest-All\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1469: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\Guest-All\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1469: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\Guest-All\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1469: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\Guest-All\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1469: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\Guest-All\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1469: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\Guest-All\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1469: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\Guest-All\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1469: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\Guest-All\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1469: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\Guest-All\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1182: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Guest-All\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1182: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Guest-All\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1182: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Guest-All\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1182: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Guest-All\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1182: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Guest-All\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1182: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Guest-All\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1182: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Guest-All\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1182: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Guest-All\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1182: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Guest-All\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1182: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Guest-All\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1182: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Guest-All\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1182: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Guest-All\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1182: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Guest-All\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1182: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Guest-All\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1182: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Guest-All\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1182: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Guest-All\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1182: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Guest-All\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1182: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Guest-All\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1182: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Guest-All\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1182: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Create a pipeline - RF is a stand in, we will populate the classifier part below\n",
    "pipe = Pipeline([('std', StandardScaler()),\n",
    "                 ('classifier', LogisticRegression())])\n",
    "\n",
    "# Create search space of candidate learning algorithms and their hyperparameters\n",
    "# note lbfgs can't do l1, and if you pass penalty='none' it expects no C value\n",
    "search_space = [{'classifier': [LogisticRegression(max_iter=10000)],\n",
    "                 'classifier__solver': ['saga'],\n",
    "                 'classifier__penalty': ['l1', 'l2'],\n",
    "                 'classifier__C': np.logspace(-4, 4, 9)},\n",
    "                {'classifier': [LogisticRegression(max_iter=10000)],\n",
    "                 'classifier__solver': ['lbfgs'],\n",
    "                 'classifier__penalty': ['l2'],\n",
    "                 'classifier__C': np.logspace(-4, 4, 9)},\n",
    "                {'classifier': [LogisticRegression(max_iter=10000)],\n",
    "                 'classifier__solver': ['lbfgs','saga'],\n",
    "                 'classifier__penalty': ['none']},\n",
    "                {'classifier': [LogisticRegression(max_iter=10000)]}\n",
    "                ]\n",
    "\n",
    "# Create grid search\n",
    "clf = GridSearchCV(pipe, search_space, cv=StratifiedKFold(n_splits=10),\n",
    "                   scoring=['accuracy', 'precision', 'f1_micro'], refit=False,\n",
    "                   verbose=0)\n",
    "\n",
    "# Fit grid search\n",
    "best_model = clf.fit(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "36bb3f82",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'mean_fit_time': array([0.02211905, 0.08105481, 0.07513254, 0.06575971, 0.08989377,\n",
       "        0.07971194, 0.09664969, 0.0853009 , 0.09677811, 0.07151542,\n",
       "        0.09715207, 0.0766947 , 0.09966738, 0.08958099, 0.12130175,\n",
       "        0.06307886, 0.11692622, 0.09505532, 0.02223752, 0.02380641,\n",
       "        0.02312493, 0.02728791, 0.02727604, 0.02962677, 0.02674415,\n",
       "        0.02199893, 0.02418122, 0.02611551, 0.07725699, 0.02045481]),\n",
       " 'std_fit_time': array([0.005711  , 0.0170219 , 0.01187235, 0.00967367, 0.01801439,\n",
       "        0.01366517, 0.01783743, 0.0141837 , 0.01320169, 0.0074765 ,\n",
       "        0.01627375, 0.01064508, 0.01795209, 0.00631787, 0.02378282,\n",
       "        0.00032192, 0.0269617 , 0.0142864 , 0.00751808, 0.00757814,\n",
       "        0.00842681, 0.00783307, 0.00638048, 0.0058297 , 0.00727265,\n",
       "        0.00771574, 0.00743941, 0.0071573 , 0.01493379, 0.0059865 ]),\n",
       " 'mean_score_time': array([0.0092324 , 0.00633562, 0.00794089, 0.00978112, 0.01072121,\n",
       "        0.00910316, 0.00872109, 0.00625455, 0.01014206, 0.00557055,\n",
       "        0.01304338, 0.00355527, 0.00737929, 0.01250455, 0.00771148,\n",
       "        0.01096978, 0.00811782, 0.01246285, 0.00757153, 0.00628281,\n",
       "        0.00778112, 0.00687945, 0.00468752, 0.00500677, 0.00628073,\n",
       "        0.00931628, 0.01192191, 0.00996733, 0.00618298, 0.00827754]),\n",
       " 'std_score_time': array([0.0069884 , 0.00684131, 0.00703228, 0.00687691, 0.00662645,\n",
       "        0.00715343, 0.00735227, 0.00766024, 0.00708406, 0.00712581,\n",
       "        0.00484413, 0.00617482, 0.00751317, 0.00662329, 0.00708848,\n",
       "        0.0071827 , 0.00709392, 0.00623252, 0.00761963, 0.00769502,\n",
       "        0.00778169, 0.00720843, 0.00716031, 0.00606184, 0.0076926 ,\n",
       "        0.00760722, 0.00623088, 0.00693154, 0.00677676, 0.0071259 ]),\n",
       " 'param_classifier': masked_array(data=[LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000)],\n",
       "              mask=[False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'param_classifier__C': masked_array(data=[0.0001, 0.0001, 0.001, 0.001, 0.01, 0.01, 0.1, 0.1,\n",
       "                    1.0, 1.0, 10.0, 10.0, 100.0, 100.0, 1000.0, 1000.0,\n",
       "                    10000.0, 10000.0, 0.0001, 0.001, 0.01, 0.1, 1.0, 10.0,\n",
       "                    100.0, 1000.0, 10000.0, --, --, --],\n",
       "              mask=[False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False,  True,  True,  True],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'param_classifier__penalty': masked_array(data=['l1', 'l2', 'l1', 'l2', 'l1', 'l2', 'l1', 'l2', 'l1',\n",
       "                    'l2', 'l1', 'l2', 'l1', 'l2', 'l1', 'l2', 'l1', 'l2',\n",
       "                    'l2', 'l2', 'l2', 'l2', 'l2', 'l2', 'l2', 'l2', 'l2',\n",
       "                    'none', 'none', --],\n",
       "              mask=[False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False,  True],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'param_classifier__solver': masked_array(data=['saga', 'saga', 'saga', 'saga', 'saga', 'saga', 'saga',\n",
       "                    'saga', 'saga', 'saga', 'saga', 'saga', 'saga', 'saga',\n",
       "                    'saga', 'saga', 'saga', 'saga', 'lbfgs', 'lbfgs',\n",
       "                    'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs',\n",
       "                    'lbfgs', 'lbfgs', 'saga', --],\n",
       "              mask=[False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False,  True],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'params': [{'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 0.0001,\n",
       "   'classifier__penalty': 'l1',\n",
       "   'classifier__solver': 'saga'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 0.0001,\n",
       "   'classifier__penalty': 'l2',\n",
       "   'classifier__solver': 'saga'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 0.001,\n",
       "   'classifier__penalty': 'l1',\n",
       "   'classifier__solver': 'saga'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 0.001,\n",
       "   'classifier__penalty': 'l2',\n",
       "   'classifier__solver': 'saga'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 0.01,\n",
       "   'classifier__penalty': 'l1',\n",
       "   'classifier__solver': 'saga'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 0.01,\n",
       "   'classifier__penalty': 'l2',\n",
       "   'classifier__solver': 'saga'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 0.1,\n",
       "   'classifier__penalty': 'l1',\n",
       "   'classifier__solver': 'saga'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 0.1,\n",
       "   'classifier__penalty': 'l2',\n",
       "   'classifier__solver': 'saga'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 1.0,\n",
       "   'classifier__penalty': 'l1',\n",
       "   'classifier__solver': 'saga'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 1.0,\n",
       "   'classifier__penalty': 'l2',\n",
       "   'classifier__solver': 'saga'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 10.0,\n",
       "   'classifier__penalty': 'l1',\n",
       "   'classifier__solver': 'saga'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 10.0,\n",
       "   'classifier__penalty': 'l2',\n",
       "   'classifier__solver': 'saga'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 100.0,\n",
       "   'classifier__penalty': 'l1',\n",
       "   'classifier__solver': 'saga'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 100.0,\n",
       "   'classifier__penalty': 'l2',\n",
       "   'classifier__solver': 'saga'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 1000.0,\n",
       "   'classifier__penalty': 'l1',\n",
       "   'classifier__solver': 'saga'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 1000.0,\n",
       "   'classifier__penalty': 'l2',\n",
       "   'classifier__solver': 'saga'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 10000.0,\n",
       "   'classifier__penalty': 'l1',\n",
       "   'classifier__solver': 'saga'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 10000.0,\n",
       "   'classifier__penalty': 'l2',\n",
       "   'classifier__solver': 'saga'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 0.0001,\n",
       "   'classifier__penalty': 'l2',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 0.001,\n",
       "   'classifier__penalty': 'l2',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 0.01,\n",
       "   'classifier__penalty': 'l2',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 0.1,\n",
       "   'classifier__penalty': 'l2',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 1.0,\n",
       "   'classifier__penalty': 'l2',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 10.0,\n",
       "   'classifier__penalty': 'l2',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 100.0,\n",
       "   'classifier__penalty': 'l2',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 1000.0,\n",
       "   'classifier__penalty': 'l2',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 10000.0,\n",
       "   'classifier__penalty': 'l2',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__penalty': 'none',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__penalty': 'none',\n",
       "   'classifier__solver': 'saga'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000)}],\n",
       " 'split0_test_accuracy': array([0.601, 0.697, 0.668, 0.743, 0.747, 0.745, 0.745, 0.745, 0.743,\n",
       "        0.744, 0.744, 0.744, 0.744, 0.744, 0.744, 0.744, 0.744, 0.744,\n",
       "        0.697, 0.743, 0.745, 0.745, 0.744, 0.744, 0.744, 0.744, 0.744,\n",
       "        0.744, 0.744, 0.744]),\n",
       " 'split1_test_accuracy': array([0.601, 0.681, 0.679, 0.751, 0.749, 0.745, 0.745, 0.743, 0.742,\n",
       "        0.742, 0.742, 0.742, 0.742, 0.742, 0.742, 0.742, 0.742, 0.742,\n",
       "        0.681, 0.751, 0.745, 0.743, 0.742, 0.742, 0.742, 0.742, 0.742,\n",
       "        0.742, 0.742, 0.742]),\n",
       " 'split2_test_accuracy': array([0.601, 0.703, 0.691, 0.762, 0.749, 0.758, 0.76 , 0.759, 0.759,\n",
       "        0.759, 0.759, 0.759, 0.759, 0.759, 0.759, 0.759, 0.759, 0.759,\n",
       "        0.703, 0.762, 0.758, 0.759, 0.759, 0.759, 0.759, 0.759, 0.759,\n",
       "        0.759, 0.759, 0.759]),\n",
       " 'split3_test_accuracy': array([0.601, 0.69 , 0.686, 0.766, 0.769, 0.77 , 0.764, 0.767, 0.766,\n",
       "        0.766, 0.766, 0.766, 0.766, 0.766, 0.766, 0.766, 0.766, 0.766,\n",
       "        0.69 , 0.766, 0.77 , 0.767, 0.766, 0.766, 0.766, 0.766, 0.766,\n",
       "        0.766, 0.766, 0.766]),\n",
       " 'split4_test_accuracy': array([0.601, 0.689, 0.673, 0.738, 0.738, 0.736, 0.737, 0.738, 0.738,\n",
       "        0.738, 0.738, 0.738, 0.738, 0.738, 0.738, 0.738, 0.738, 0.738,\n",
       "        0.689, 0.738, 0.736, 0.738, 0.738, 0.738, 0.738, 0.738, 0.738,\n",
       "        0.738, 0.738, 0.738]),\n",
       " 'split5_test_accuracy': array([0.601, 0.683, 0.671, 0.742, 0.745, 0.747, 0.748, 0.745, 0.745,\n",
       "        0.746, 0.747, 0.747, 0.747, 0.747, 0.747, 0.747, 0.747, 0.747,\n",
       "        0.683, 0.742, 0.747, 0.745, 0.746, 0.747, 0.747, 0.747, 0.747,\n",
       "        0.747, 0.747, 0.746]),\n",
       " 'split6_test_accuracy': array([0.601, 0.674, 0.677, 0.722, 0.72 , 0.727, 0.728, 0.726, 0.727,\n",
       "        0.726, 0.726, 0.726, 0.726, 0.726, 0.726, 0.726, 0.726, 0.726,\n",
       "        0.674, 0.722, 0.727, 0.726, 0.726, 0.726, 0.726, 0.726, 0.726,\n",
       "        0.726, 0.726, 0.726]),\n",
       " 'split7_test_accuracy': array([0.601, 0.694, 0.687, 0.745, 0.761, 0.751, 0.751, 0.752, 0.752,\n",
       "        0.751, 0.751, 0.751, 0.751, 0.751, 0.751, 0.751, 0.751, 0.751,\n",
       "        0.694, 0.745, 0.751, 0.752, 0.751, 0.751, 0.751, 0.751, 0.751,\n",
       "        0.751, 0.751, 0.751]),\n",
       " 'split8_test_accuracy': array([0.601, 0.68 , 0.679, 0.726, 0.743, 0.746, 0.75 , 0.75 , 0.75 ,\n",
       "        0.75 , 0.75 , 0.75 , 0.75 , 0.75 , 0.75 , 0.75 , 0.75 , 0.75 ,\n",
       "        0.68 , 0.726, 0.746, 0.75 , 0.75 , 0.75 , 0.75 , 0.75 , 0.75 ,\n",
       "        0.75 , 0.75 , 0.75 ]),\n",
       " 'split9_test_accuracy': array([0.602, 0.69 , 0.679, 0.73 , 0.73 , 0.736, 0.731, 0.734, 0.734,\n",
       "        0.733, 0.733, 0.733, 0.733, 0.733, 0.733, 0.733, 0.733, 0.733,\n",
       "        0.69 , 0.73 , 0.736, 0.734, 0.733, 0.733, 0.733, 0.733, 0.733,\n",
       "        0.733, 0.733, 0.733]),\n",
       " 'mean_test_accuracy': array([0.6011, 0.6881, 0.679 , 0.7425, 0.7451, 0.7461, 0.7459, 0.7459,\n",
       "        0.7456, 0.7455, 0.7456, 0.7456, 0.7456, 0.7456, 0.7456, 0.7456,\n",
       "        0.7456, 0.7456, 0.6881, 0.7425, 0.7461, 0.7459, 0.7455, 0.7456,\n",
       "        0.7456, 0.7456, 0.7456, 0.7456, 0.7456, 0.7455]),\n",
       " 'std_test_accuracy': array([0.0003    , 0.00827587, 0.00694262, 0.01371313, 0.01330752,\n",
       "        0.01144072, 0.01094029, 0.01131769, 0.01101998, 0.0112272 ,\n",
       "        0.01123566, 0.01123566, 0.01123566, 0.01123566, 0.01123566,\n",
       "        0.01123566, 0.01123566, 0.01123566, 0.00827587, 0.01371313,\n",
       "        0.01144072, 0.01131769, 0.0112272 , 0.01123566, 0.01123566,\n",
       "        0.01123566, 0.01123566, 0.01123566, 0.01123566, 0.0112272 ]),\n",
       " 'rank_test_accuracy': array([30, 27, 29, 25, 24,  1,  3,  3,  6, 21,  6,  6,  6,  6,  6,  6,  6,\n",
       "         6, 27, 25,  1,  3, 21,  6,  6,  6,  6,  6,  6, 21]),\n",
       " 'split0_test_precision': array([0.        , 0.78571429, 0.7005988 , 0.71005917, 0.70391061,\n",
       "        0.69672131, 0.69354839, 0.69251337, 0.68983957, 0.69066667,\n",
       "        0.69066667, 0.69066667, 0.69066667, 0.69066667, 0.69066667,\n",
       "        0.69066667, 0.69066667, 0.69066667, 0.78571429, 0.71005917,\n",
       "        0.69672131, 0.69251337, 0.69066667, 0.69066667, 0.69066667,\n",
       "        0.69066667, 0.69066667, 0.69066667, 0.69066667, 0.69066667]),\n",
       " 'split1_test_precision': array([0.        , 0.72727273, 0.73214286, 0.734375  , 0.71264368,\n",
       "        0.70454545, 0.70111732, 0.6994382 , 0.69747899, 0.69859155,\n",
       "        0.69859155, 0.69859155, 0.69859155, 0.69859155, 0.69859155,\n",
       "        0.69859155, 0.69859155, 0.69859155, 0.72727273, 0.734375  ,\n",
       "        0.70454545, 0.6994382 , 0.69859155, 0.69859155, 0.69859155,\n",
       "        0.69859155, 0.69859155, 0.69859155, 0.69859155, 0.69859155]),\n",
       " 'split2_test_precision': array([0.        , 0.76842105, 0.734375  , 0.72054795, 0.68686869,\n",
       "        0.69674185, 0.6953317 , 0.69458128, 0.69458128, 0.69458128,\n",
       "        0.69458128, 0.69458128, 0.69458128, 0.69458128, 0.69458128,\n",
       "        0.69458128, 0.69458128, 0.69458128, 0.76842105, 0.72054795,\n",
       "        0.69674185, 0.69458128, 0.69458128, 0.69458128, 0.69458128,\n",
       "        0.69458128, 0.69458128, 0.69458128, 0.69458128, 0.69458128]),\n",
       " 'split3_test_precision': array([0.        , 0.76331361, 0.74853801, 0.74924471, 0.73728814,\n",
       "        0.73669468, 0.72576177, 0.72928177, 0.72727273, 0.72727273,\n",
       "        0.72727273, 0.72727273, 0.72727273, 0.72727273, 0.72727273,\n",
       "        0.72727273, 0.72727273, 0.72727273, 0.76331361, 0.74924471,\n",
       "        0.73669468, 0.72928177, 0.72727273, 0.72727273, 0.72727273,\n",
       "        0.72727273, 0.72727273, 0.72727273, 0.72727273, 0.72727273]),\n",
       " 'split4_test_precision': array([0.        , 0.74175824, 0.70224719, 0.6951567 , 0.68169761,\n",
       "        0.67810026, 0.67708333, 0.67700258, 0.67700258, 0.67700258,\n",
       "        0.67700258, 0.67700258, 0.67700258, 0.67700258, 0.67700258,\n",
       "        0.67700258, 0.67700258, 0.67700258, 0.74175824, 0.6951567 ,\n",
       "        0.67810026, 0.67700258, 0.67700258, 0.67700258, 0.67700258,\n",
       "        0.67700258, 0.67700258, 0.67700258, 0.67700258, 0.67700258]),\n",
       " 'split5_test_precision': array([0.        , 0.75      , 0.70114943, 0.71559633, 0.70338983,\n",
       "        0.70165746, 0.70027248, 0.69459459, 0.69459459, 0.69541779,\n",
       "        0.6972973 , 0.6972973 , 0.6972973 , 0.6972973 , 0.6972973 ,\n",
       "        0.6972973 , 0.6972973 , 0.6972973 , 0.75      , 0.71559633,\n",
       "        0.70165746, 0.69459459, 0.69541779, 0.6972973 , 0.6972973 ,\n",
       "        0.6972973 , 0.6972973 , 0.6972973 , 0.6972973 , 0.69541779]),\n",
       " 'split6_test_precision': array([0.        , 0.70857143, 0.70212766, 0.67952522, 0.66573816,\n",
       "        0.67696629, 0.67493113, 0.67507003, 0.67597765, 0.67507003,\n",
       "        0.67507003, 0.67507003, 0.67507003, 0.67507003, 0.67507003,\n",
       "        0.67507003, 0.67507003, 0.67507003, 0.70857143, 0.67952522,\n",
       "        0.67696629, 0.67507003, 0.67507003, 0.67507003, 0.67507003,\n",
       "        0.67507003, 0.67507003, 0.67507003, 0.67507003, 0.67507003]),\n",
       " 'split7_test_precision': array([0.        , 0.78527607, 0.73888889, 0.71428571, 0.7247191 ,\n",
       "        0.70718232, 0.70380435, 0.70460705, 0.70460705, 0.70380435,\n",
       "        0.70380435, 0.70380435, 0.70380435, 0.70380435, 0.70380435,\n",
       "        0.70380435, 0.70380435, 0.70380435, 0.78527607, 0.71428571,\n",
       "        0.70718232, 0.70460705, 0.70380435, 0.70380435, 0.70380435,\n",
       "        0.70380435, 0.70380435, 0.70380435, 0.70380435, 0.70380435]),\n",
       " 'split8_test_precision': array([0.        , 0.74842767, 0.71428571, 0.68545994, 0.6994382 ,\n",
       "        0.69647696, 0.6997319 , 0.69866667, 0.69866667, 0.69866667,\n",
       "        0.69866667, 0.69866667, 0.69866667, 0.69866667, 0.69866667,\n",
       "        0.69866667, 0.69866667, 0.69866667, 0.74842767, 0.68545994,\n",
       "        0.69647696, 0.69866667, 0.69866667, 0.69866667, 0.69866667,\n",
       "        0.69866667, 0.69866667, 0.69866667, 0.69866667, 0.69866667]),\n",
       " 'split9_test_precision': array([0.        , 0.73404255, 0.71751412, 0.6849711 , 0.67297297,\n",
       "        0.67819149, 0.6701847 , 0.67368421, 0.67368421, 0.67191601,\n",
       "        0.67191601, 0.67191601, 0.67191601, 0.67191601, 0.67191601,\n",
       "        0.67191601, 0.67191601, 0.67191601, 0.73404255, 0.6849711 ,\n",
       "        0.67819149, 0.67368421, 0.67191601, 0.67191601, 0.67191601,\n",
       "        0.67191601, 0.67191601, 0.67191601, 0.67191601, 0.67191601]),\n",
       " 'mean_test_precision': array([0.        , 0.75127976, 0.71918677, 0.70892218, 0.6988667 ,\n",
       "        0.69732781, 0.69417671, 0.69394397, 0.69337053, 0.69329897,\n",
       "        0.69348692, 0.69348692, 0.69348692, 0.69348692, 0.69348692,\n",
       "        0.69348692, 0.69348692, 0.69348692, 0.75127976, 0.70892218,\n",
       "        0.69732781, 0.69394397, 0.69329897, 0.69348692, 0.69348692,\n",
       "        0.69348692, 0.69348692, 0.69348692, 0.69348692, 0.69329897]),\n",
       " 'std_test_precision': array([0.        , 0.02361782, 0.01709927, 0.02159008, 0.02141798,\n",
       "        0.01693997, 0.01564103, 0.01572107, 0.0151458 , 0.01545821,\n",
       "        0.01549421, 0.01549421, 0.01549421, 0.01549421, 0.01549421,\n",
       "        0.01549421, 0.01549421, 0.01549421, 0.02361782, 0.02159008,\n",
       "        0.01693997, 0.01572107, 0.01545821, 0.01549421, 0.01549421,\n",
       "        0.01549421, 0.01549421, 0.01549421, 0.01549421, 0.01545821]),\n",
       " 'rank_test_precision': array([30,  1,  3,  4,  6,  7,  9, 10, 26, 27, 12, 12, 12, 12, 12, 12, 12,\n",
       "        12,  1,  4,  7, 10, 27, 12, 12, 12, 12, 12, 12, 27]),\n",
       " 'split0_test_f1_micro': array([0.601, 0.697, 0.668, 0.743, 0.747, 0.745, 0.745, 0.745, 0.743,\n",
       "        0.744, 0.744, 0.744, 0.744, 0.744, 0.744, 0.744, 0.744, 0.744,\n",
       "        0.697, 0.743, 0.745, 0.745, 0.744, 0.744, 0.744, 0.744, 0.744,\n",
       "        0.744, 0.744, 0.744]),\n",
       " 'split1_test_f1_micro': array([0.601, 0.681, 0.679, 0.751, 0.749, 0.745, 0.745, 0.743, 0.742,\n",
       "        0.742, 0.742, 0.742, 0.742, 0.742, 0.742, 0.742, 0.742, 0.742,\n",
       "        0.681, 0.751, 0.745, 0.743, 0.742, 0.742, 0.742, 0.742, 0.742,\n",
       "        0.742, 0.742, 0.742]),\n",
       " 'split2_test_f1_micro': array([0.601, 0.703, 0.691, 0.762, 0.749, 0.758, 0.76 , 0.759, 0.759,\n",
       "        0.759, 0.759, 0.759, 0.759, 0.759, 0.759, 0.759, 0.759, 0.759,\n",
       "        0.703, 0.762, 0.758, 0.759, 0.759, 0.759, 0.759, 0.759, 0.759,\n",
       "        0.759, 0.759, 0.759]),\n",
       " 'split3_test_f1_micro': array([0.601, 0.69 , 0.686, 0.766, 0.769, 0.77 , 0.764, 0.767, 0.766,\n",
       "        0.766, 0.766, 0.766, 0.766, 0.766, 0.766, 0.766, 0.766, 0.766,\n",
       "        0.69 , 0.766, 0.77 , 0.767, 0.766, 0.766, 0.766, 0.766, 0.766,\n",
       "        0.766, 0.766, 0.766]),\n",
       " 'split4_test_f1_micro': array([0.601, 0.689, 0.673, 0.738, 0.738, 0.736, 0.737, 0.738, 0.738,\n",
       "        0.738, 0.738, 0.738, 0.738, 0.738, 0.738, 0.738, 0.738, 0.738,\n",
       "        0.689, 0.738, 0.736, 0.738, 0.738, 0.738, 0.738, 0.738, 0.738,\n",
       "        0.738, 0.738, 0.738]),\n",
       " 'split5_test_f1_micro': array([0.601, 0.683, 0.671, 0.742, 0.745, 0.747, 0.748, 0.745, 0.745,\n",
       "        0.746, 0.747, 0.747, 0.747, 0.747, 0.747, 0.747, 0.747, 0.747,\n",
       "        0.683, 0.742, 0.747, 0.745, 0.746, 0.747, 0.747, 0.747, 0.747,\n",
       "        0.747, 0.747, 0.746]),\n",
       " 'split6_test_f1_micro': array([0.601, 0.674, 0.677, 0.722, 0.72 , 0.727, 0.728, 0.726, 0.727,\n",
       "        0.726, 0.726, 0.726, 0.726, 0.726, 0.726, 0.726, 0.726, 0.726,\n",
       "        0.674, 0.722, 0.727, 0.726, 0.726, 0.726, 0.726, 0.726, 0.726,\n",
       "        0.726, 0.726, 0.726]),\n",
       " 'split7_test_f1_micro': array([0.601, 0.694, 0.687, 0.745, 0.761, 0.751, 0.751, 0.752, 0.752,\n",
       "        0.751, 0.751, 0.751, 0.751, 0.751, 0.751, 0.751, 0.751, 0.751,\n",
       "        0.694, 0.745, 0.751, 0.752, 0.751, 0.751, 0.751, 0.751, 0.751,\n",
       "        0.751, 0.751, 0.751]),\n",
       " 'split8_test_f1_micro': array([0.601, 0.68 , 0.679, 0.726, 0.743, 0.746, 0.75 , 0.75 , 0.75 ,\n",
       "        0.75 , 0.75 , 0.75 , 0.75 , 0.75 , 0.75 , 0.75 , 0.75 , 0.75 ,\n",
       "        0.68 , 0.726, 0.746, 0.75 , 0.75 , 0.75 , 0.75 , 0.75 , 0.75 ,\n",
       "        0.75 , 0.75 , 0.75 ]),\n",
       " 'split9_test_f1_micro': array([0.602, 0.69 , 0.679, 0.73 , 0.73 , 0.736, 0.731, 0.734, 0.734,\n",
       "        0.733, 0.733, 0.733, 0.733, 0.733, 0.733, 0.733, 0.733, 0.733,\n",
       "        0.69 , 0.73 , 0.736, 0.734, 0.733, 0.733, 0.733, 0.733, 0.733,\n",
       "        0.733, 0.733, 0.733]),\n",
       " 'mean_test_f1_micro': array([0.6011, 0.6881, 0.679 , 0.7425, 0.7451, 0.7461, 0.7459, 0.7459,\n",
       "        0.7456, 0.7455, 0.7456, 0.7456, 0.7456, 0.7456, 0.7456, 0.7456,\n",
       "        0.7456, 0.7456, 0.6881, 0.7425, 0.7461, 0.7459, 0.7455, 0.7456,\n",
       "        0.7456, 0.7456, 0.7456, 0.7456, 0.7456, 0.7455]),\n",
       " 'std_test_f1_micro': array([0.0003    , 0.00827587, 0.00694262, 0.01371313, 0.01330752,\n",
       "        0.01144072, 0.01094029, 0.01131769, 0.01101998, 0.0112272 ,\n",
       "        0.01123566, 0.01123566, 0.01123566, 0.01123566, 0.01123566,\n",
       "        0.01123566, 0.01123566, 0.01123566, 0.00827587, 0.01371313,\n",
       "        0.01144072, 0.01131769, 0.0112272 , 0.01123566, 0.01123566,\n",
       "        0.01123566, 0.01123566, 0.01123566, 0.01123566, 0.0112272 ]),\n",
       " 'rank_test_f1_micro': array([30, 27, 29, 25, 24,  1,  3,  3,  6, 21,  6,  6,  6,  6,  6,  6,  6,\n",
       "         6, 27, 25,  1,  3, 21,  6,  6,  6,  6,  6,  6, 21])}"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_model.cv_results_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "19851888",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([30, 27, 29, 25, 24,  1,  3,  3,  6, 21,  6,  6,  6,  6,  6,  6,  6,\n",
       "        6, 27, 25,  1,  3, 21,  6,  6,  6,  6,  6,  6, 21])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_model.cv_results_['rank_test_accuracy']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "09bdbbb9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'classifier': LogisticRegression(max_iter=10000),\n",
       " 'classifier__C': 0.0001,\n",
       " 'classifier__penalty': 'l1',\n",
       " 'classifier__solver': 'saga'}"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_model.cv_results_['params'][ np.argmin(best_model.cv_results_['mean_test_accuracy'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "ff293d70",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>classifier</th>\n",
       "      <th>C</th>\n",
       "      <th>penalty</th>\n",
       "      <th>solver</th>\n",
       "      <th>score_acc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.3989</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.3119</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.3210</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.2575</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0100</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.2549</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0100</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.2539</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.1000</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.2541</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.1000</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.2541</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.2544</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.2545</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>10.0000</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.2544</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>10.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.2544</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>100.0000</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.2544</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>100.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.2544</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>1000.0000</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.2544</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>1000.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.2544</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>10000.0000</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.2544</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>10000.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.2544</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.3119</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.2575</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0100</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.2539</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.1000</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.2541</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.2545</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>10.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.2544</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>100.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.2544</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>1000.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.2544</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>10000.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.2544</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>NaN</td>\n",
       "      <td>none</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.2544</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>NaN</td>\n",
       "      <td>none</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.2544</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.2545</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                            classifier           C penalty solver  score_acc\n",
       "0   LogisticRegression(max_iter=10000)      0.0001      l1   saga     0.3989\n",
       "1   LogisticRegression(max_iter=10000)      0.0001      l2   saga     0.3119\n",
       "2   LogisticRegression(max_iter=10000)      0.0010      l1   saga     0.3210\n",
       "3   LogisticRegression(max_iter=10000)      0.0010      l2   saga     0.2575\n",
       "4   LogisticRegression(max_iter=10000)      0.0100      l1   saga     0.2549\n",
       "5   LogisticRegression(max_iter=10000)      0.0100      l2   saga     0.2539\n",
       "6   LogisticRegression(max_iter=10000)      0.1000      l1   saga     0.2541\n",
       "7   LogisticRegression(max_iter=10000)      0.1000      l2   saga     0.2541\n",
       "8   LogisticRegression(max_iter=10000)      1.0000      l1   saga     0.2544\n",
       "9   LogisticRegression(max_iter=10000)      1.0000      l2   saga     0.2545\n",
       "10  LogisticRegression(max_iter=10000)     10.0000      l1   saga     0.2544\n",
       "11  LogisticRegression(max_iter=10000)     10.0000      l2   saga     0.2544\n",
       "12  LogisticRegression(max_iter=10000)    100.0000      l1   saga     0.2544\n",
       "13  LogisticRegression(max_iter=10000)    100.0000      l2   saga     0.2544\n",
       "14  LogisticRegression(max_iter=10000)   1000.0000      l1   saga     0.2544\n",
       "15  LogisticRegression(max_iter=10000)   1000.0000      l2   saga     0.2544\n",
       "16  LogisticRegression(max_iter=10000)  10000.0000      l1   saga     0.2544\n",
       "17  LogisticRegression(max_iter=10000)  10000.0000      l2   saga     0.2544\n",
       "18  LogisticRegression(max_iter=10000)      0.0001      l2  lbfgs     0.3119\n",
       "19  LogisticRegression(max_iter=10000)      0.0010      l2  lbfgs     0.2575\n",
       "20  LogisticRegression(max_iter=10000)      0.0100      l2  lbfgs     0.2539\n",
       "21  LogisticRegression(max_iter=10000)      0.1000      l2  lbfgs     0.2541\n",
       "22  LogisticRegression(max_iter=10000)      1.0000      l2  lbfgs     0.2545\n",
       "23  LogisticRegression(max_iter=10000)     10.0000      l2  lbfgs     0.2544\n",
       "24  LogisticRegression(max_iter=10000)    100.0000      l2  lbfgs     0.2544\n",
       "25  LogisticRegression(max_iter=10000)   1000.0000      l2  lbfgs     0.2544\n",
       "26  LogisticRegression(max_iter=10000)  10000.0000      l2  lbfgs     0.2544\n",
       "27  LogisticRegression(max_iter=10000)         NaN    none  lbfgs     0.2544\n",
       "28  LogisticRegression(max_iter=10000)         NaN    none   saga     0.2544\n",
       "29  LogisticRegression(max_iter=10000)         NaN     NaN    NaN     0.2545"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results = pd.DataFrame( best_model.cv_results_['params'] )\n",
    "\n",
    "# next grab the score resulting from those parameters, add it to the data\n",
    "# score is accuracy; to display it as misclassification error we use 1 - x\n",
    "results['score_acc'] = 1 - best_model.cv_results_['mean_test_accuracy']\n",
    "\n",
    "# get rid of classifierXX in columns\n",
    "cols = results.columns.to_series().str.split('__').apply(lambda x: x[-1])\n",
    "results.columns = cols\n",
    "\n",
    "# lets show the results for the saga solver across penalty & C values\n",
    "#sns.heatmap( results.query('solver==\"saga\"').pivot('C','penalty','score_acc'),\n",
    "#             annot=True, fmt='.3f')\n",
    "\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "69c1c465",
   "metadata": {},
   "outputs": [],
   "source": [
    "acc_list.append(results[0:1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "05385bec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'classifier': LogisticRegression(max_iter=10000),\n",
       " 'classifier__C': 0.0001,\n",
       " 'classifier__penalty': 'l2',\n",
       " 'classifier__solver': 'saga'}"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_model.cv_results_['params'][ np.argmin(best_model.cv_results_['rank_test_precision'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "2ef6f409",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>classifier</th>\n",
       "      <th>C</th>\n",
       "      <th>penalty</th>\n",
       "      <th>solver</th>\n",
       "      <th>score_precision</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.248720</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.280813</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.291078</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0100</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.301133</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0100</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.302672</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.1000</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.305823</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.1000</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.306056</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.306629</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.306701</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>10.0000</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.306513</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>10.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.306513</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>100.0000</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.306513</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>100.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.306513</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>1000.0000</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.306513</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>1000.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.306513</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>10000.0000</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.306513</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>10000.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.306513</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.248720</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.291078</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0100</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.302672</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.1000</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.306056</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.306701</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>10.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.306513</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>100.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.306513</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>1000.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.306513</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>10000.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.306513</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>NaN</td>\n",
       "      <td>none</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.306513</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>NaN</td>\n",
       "      <td>none</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.306513</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.306701</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                            classifier           C penalty solver  \\\n",
       "0   LogisticRegression(max_iter=10000)      0.0001      l1   saga   \n",
       "1   LogisticRegression(max_iter=10000)      0.0001      l2   saga   \n",
       "2   LogisticRegression(max_iter=10000)      0.0010      l1   saga   \n",
       "3   LogisticRegression(max_iter=10000)      0.0010      l2   saga   \n",
       "4   LogisticRegression(max_iter=10000)      0.0100      l1   saga   \n",
       "5   LogisticRegression(max_iter=10000)      0.0100      l2   saga   \n",
       "6   LogisticRegression(max_iter=10000)      0.1000      l1   saga   \n",
       "7   LogisticRegression(max_iter=10000)      0.1000      l2   saga   \n",
       "8   LogisticRegression(max_iter=10000)      1.0000      l1   saga   \n",
       "9   LogisticRegression(max_iter=10000)      1.0000      l2   saga   \n",
       "10  LogisticRegression(max_iter=10000)     10.0000      l1   saga   \n",
       "11  LogisticRegression(max_iter=10000)     10.0000      l2   saga   \n",
       "12  LogisticRegression(max_iter=10000)    100.0000      l1   saga   \n",
       "13  LogisticRegression(max_iter=10000)    100.0000      l2   saga   \n",
       "14  LogisticRegression(max_iter=10000)   1000.0000      l1   saga   \n",
       "15  LogisticRegression(max_iter=10000)   1000.0000      l2   saga   \n",
       "16  LogisticRegression(max_iter=10000)  10000.0000      l1   saga   \n",
       "17  LogisticRegression(max_iter=10000)  10000.0000      l2   saga   \n",
       "18  LogisticRegression(max_iter=10000)      0.0001      l2  lbfgs   \n",
       "19  LogisticRegression(max_iter=10000)      0.0010      l2  lbfgs   \n",
       "20  LogisticRegression(max_iter=10000)      0.0100      l2  lbfgs   \n",
       "21  LogisticRegression(max_iter=10000)      0.1000      l2  lbfgs   \n",
       "22  LogisticRegression(max_iter=10000)      1.0000      l2  lbfgs   \n",
       "23  LogisticRegression(max_iter=10000)     10.0000      l2  lbfgs   \n",
       "24  LogisticRegression(max_iter=10000)    100.0000      l2  lbfgs   \n",
       "25  LogisticRegression(max_iter=10000)   1000.0000      l2  lbfgs   \n",
       "26  LogisticRegression(max_iter=10000)  10000.0000      l2  lbfgs   \n",
       "27  LogisticRegression(max_iter=10000)         NaN    none  lbfgs   \n",
       "28  LogisticRegression(max_iter=10000)         NaN    none   saga   \n",
       "29  LogisticRegression(max_iter=10000)         NaN     NaN    NaN   \n",
       "\n",
       "    score_precision  \n",
       "0          1.000000  \n",
       "1          0.248720  \n",
       "2          0.280813  \n",
       "3          0.291078  \n",
       "4          0.301133  \n",
       "5          0.302672  \n",
       "6          0.305823  \n",
       "7          0.306056  \n",
       "8          0.306629  \n",
       "9          0.306701  \n",
       "10         0.306513  \n",
       "11         0.306513  \n",
       "12         0.306513  \n",
       "13         0.306513  \n",
       "14         0.306513  \n",
       "15         0.306513  \n",
       "16         0.306513  \n",
       "17         0.306513  \n",
       "18         0.248720  \n",
       "19         0.291078  \n",
       "20         0.302672  \n",
       "21         0.306056  \n",
       "22         0.306701  \n",
       "23         0.306513  \n",
       "24         0.306513  \n",
       "25         0.306513  \n",
       "26         0.306513  \n",
       "27         0.306513  \n",
       "28         0.306513  \n",
       "29         0.306701  "
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results = pd.DataFrame( best_model.cv_results_['params'] )\n",
    "\n",
    "# next grab the score resulting from those parameters, add it to the data\n",
    "# score is accuracy; to display it as misclassification error we use 1 - x\n",
    "results['score_precision'] = 1 - best_model.cv_results_['mean_test_precision']\n",
    "\n",
    "# get rid of classifierXX in columns\n",
    "cols = results.columns.to_series().str.split('__').apply(lambda x: x[-1])\n",
    "results.columns = cols\n",
    "\n",
    "# lets show the results for the saga solver across penalty & C values\n",
    "#sns.heatmap( results.query('solver==\"saga\"').pivot('C','penalty','score_precision'),\n",
    "#             annot=True, fmt='.3f')\n",
    "\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "be514fd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "roc_auc_ovr_List.append(results[1:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "8cb4c8d2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'classifier': LogisticRegression(max_iter=10000),\n",
       " 'classifier__C': 0.01,\n",
       " 'classifier__penalty': 'l2',\n",
       " 'classifier__solver': 'saga'}"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_model.cv_results_['params'][ np.argmin(best_model.cv_results_['rank_test_f1_micro'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "e1fc8e88",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>classifier</th>\n",
       "      <th>C</th>\n",
       "      <th>penalty</th>\n",
       "      <th>solver</th>\n",
       "      <th>score_f1_micro</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.3989</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.3119</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.3210</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.2575</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0100</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.2549</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0100</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.2539</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.1000</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.2541</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.1000</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.2541</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.2544</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.2545</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>10.0000</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.2544</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>10.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.2544</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>100.0000</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.2544</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>100.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.2544</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>1000.0000</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.2544</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>1000.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.2544</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>10000.0000</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.2544</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>10000.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.2544</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.3119</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.2575</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0100</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.2539</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.1000</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.2541</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.2545</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>10.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.2544</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>100.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.2544</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>1000.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.2544</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>10000.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.2544</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>NaN</td>\n",
       "      <td>none</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.2544</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>NaN</td>\n",
       "      <td>none</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.2544</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.2545</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                            classifier           C penalty solver  \\\n",
       "0   LogisticRegression(max_iter=10000)      0.0001      l1   saga   \n",
       "1   LogisticRegression(max_iter=10000)      0.0001      l2   saga   \n",
       "2   LogisticRegression(max_iter=10000)      0.0010      l1   saga   \n",
       "3   LogisticRegression(max_iter=10000)      0.0010      l2   saga   \n",
       "4   LogisticRegression(max_iter=10000)      0.0100      l1   saga   \n",
       "5   LogisticRegression(max_iter=10000)      0.0100      l2   saga   \n",
       "6   LogisticRegression(max_iter=10000)      0.1000      l1   saga   \n",
       "7   LogisticRegression(max_iter=10000)      0.1000      l2   saga   \n",
       "8   LogisticRegression(max_iter=10000)      1.0000      l1   saga   \n",
       "9   LogisticRegression(max_iter=10000)      1.0000      l2   saga   \n",
       "10  LogisticRegression(max_iter=10000)     10.0000      l1   saga   \n",
       "11  LogisticRegression(max_iter=10000)     10.0000      l2   saga   \n",
       "12  LogisticRegression(max_iter=10000)    100.0000      l1   saga   \n",
       "13  LogisticRegression(max_iter=10000)    100.0000      l2   saga   \n",
       "14  LogisticRegression(max_iter=10000)   1000.0000      l1   saga   \n",
       "15  LogisticRegression(max_iter=10000)   1000.0000      l2   saga   \n",
       "16  LogisticRegression(max_iter=10000)  10000.0000      l1   saga   \n",
       "17  LogisticRegression(max_iter=10000)  10000.0000      l2   saga   \n",
       "18  LogisticRegression(max_iter=10000)      0.0001      l2  lbfgs   \n",
       "19  LogisticRegression(max_iter=10000)      0.0010      l2  lbfgs   \n",
       "20  LogisticRegression(max_iter=10000)      0.0100      l2  lbfgs   \n",
       "21  LogisticRegression(max_iter=10000)      0.1000      l2  lbfgs   \n",
       "22  LogisticRegression(max_iter=10000)      1.0000      l2  lbfgs   \n",
       "23  LogisticRegression(max_iter=10000)     10.0000      l2  lbfgs   \n",
       "24  LogisticRegression(max_iter=10000)    100.0000      l2  lbfgs   \n",
       "25  LogisticRegression(max_iter=10000)   1000.0000      l2  lbfgs   \n",
       "26  LogisticRegression(max_iter=10000)  10000.0000      l2  lbfgs   \n",
       "27  LogisticRegression(max_iter=10000)         NaN    none  lbfgs   \n",
       "28  LogisticRegression(max_iter=10000)         NaN    none   saga   \n",
       "29  LogisticRegression(max_iter=10000)         NaN     NaN    NaN   \n",
       "\n",
       "    score_f1_micro  \n",
       "0           0.3989  \n",
       "1           0.3119  \n",
       "2           0.3210  \n",
       "3           0.2575  \n",
       "4           0.2549  \n",
       "5           0.2539  \n",
       "6           0.2541  \n",
       "7           0.2541  \n",
       "8           0.2544  \n",
       "9           0.2545  \n",
       "10          0.2544  \n",
       "11          0.2544  \n",
       "12          0.2544  \n",
       "13          0.2544  \n",
       "14          0.2544  \n",
       "15          0.2544  \n",
       "16          0.2544  \n",
       "17          0.2544  \n",
       "18          0.3119  \n",
       "19          0.2575  \n",
       "20          0.2539  \n",
       "21          0.2541  \n",
       "22          0.2545  \n",
       "23          0.2544  \n",
       "24          0.2544  \n",
       "25          0.2544  \n",
       "26          0.2544  \n",
       "27          0.2544  \n",
       "28          0.2544  \n",
       "29          0.2545  "
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results = pd.DataFrame( best_model.cv_results_['params'] )\n",
    "\n",
    "# next grab the score resulting from those parameters, add it to the data\n",
    "# score is accuracy; to display it as misclassification error we use 1 - x\n",
    "results['score_f1_micro'] = 1 - best_model.cv_results_['mean_test_f1_micro']\n",
    "\n",
    "# get rid of classifierXX in columns\n",
    "cols = results.columns.to_series().str.split('__').apply(lambda x: x[-1])\n",
    "results.columns = cols\n",
    "\n",
    "# lets show the results for the saga solver across penalty & C values\n",
    "#sns.heatmap( results.query('solver==\"saga\"').pivot('C','penalty','score_f1_micro'),\n",
    "#             annot=True, fmt='.3f')\n",
    "\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "a8c83f8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "f1_micro_List.append(results[5:6])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cd3a031",
   "metadata": {},
   "source": [
    "## Trial 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "f0c67633",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, Y_train, Y_test = train_test_split(X, y, train_size=10000, shuffle=True, random_state = 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "382815ad",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Guest-All\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1469: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\Guest-All\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1469: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\Guest-All\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1469: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\Guest-All\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1469: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\Guest-All\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1469: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\Guest-All\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1469: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\Guest-All\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1469: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\Guest-All\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1469: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\Guest-All\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1469: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\Guest-All\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1469: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\Guest-All\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1182: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Guest-All\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1182: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Guest-All\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1182: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Guest-All\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1182: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Guest-All\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1182: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Guest-All\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1182: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Guest-All\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1182: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Guest-All\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1182: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Guest-All\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1182: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Guest-All\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1182: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Guest-All\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1182: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Guest-All\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1182: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Guest-All\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1182: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Guest-All\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1182: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Guest-All\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1182: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Guest-All\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1182: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Guest-All\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1182: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Guest-All\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1182: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Guest-All\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1182: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Guest-All\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1182: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Create a pipeline - RF is a stand in, we will populate the classifier part below\n",
    "pipe = Pipeline([('std', StandardScaler()),\n",
    "                 ('classifier', LogisticRegression())])\n",
    "\n",
    "# Create search space of candidate learning algorithms and their hyperparameters\n",
    "# note lbfgs can't do l1, and if you pass penalty='none' it expects no C value\n",
    "search_space = [{'classifier': [LogisticRegression(max_iter=10000)],\n",
    "                 'classifier__solver': ['saga'],\n",
    "                 'classifier__penalty': ['l1', 'l2'],\n",
    "                 'classifier__C': np.logspace(-4, 4, 9)},\n",
    "                {'classifier': [LogisticRegression(max_iter=10000)],\n",
    "                 'classifier__solver': ['lbfgs'],\n",
    "                 'classifier__penalty': ['l2'],\n",
    "                 'classifier__C': np.logspace(-4, 4, 9)},\n",
    "                {'classifier': [LogisticRegression(max_iter=10000)],\n",
    "                 'classifier__solver': ['lbfgs','saga'],\n",
    "                 'classifier__penalty': ['none']},\n",
    "                {'classifier': [LogisticRegression(max_iter=10000)]}\n",
    "                ]\n",
    "\n",
    "# Create grid search\n",
    "clf = GridSearchCV(pipe, search_space, cv=StratifiedKFold(n_splits=10),\n",
    "                   scoring=['accuracy', 'precision', 'f1_micro'], refit=False,\n",
    "                   verbose=0)\n",
    "\n",
    "# Fit grid search\n",
    "best_model = clf.fit(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "befaa8ab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'mean_fit_time': array([0.01858034, 0.0841594 , 0.0995791 , 0.08477128, 0.07921925,\n",
       "        0.08084464, 0.09603736, 0.08845074, 0.11162031, 0.08697999,\n",
       "        0.10200436, 0.07311423, 0.09666393, 0.08619237, 0.11343811,\n",
       "        0.07996111, 0.09644594, 0.08088238, 0.02229652, 0.02040348,\n",
       "        0.02649186, 0.02387068, 0.02358284, 0.02576334, 0.02211382,\n",
       "        0.02775686, 0.02411215, 0.0282222 , 0.08803594, 0.02193959]),\n",
       " 'std_fit_time': array([0.00411804, 0.01603266, 0.01450352, 0.01418431, 0.01934171,\n",
       "        0.01858413, 0.01647499, 0.0143033 , 0.01898208, 0.01519758,\n",
       "        0.01911303, 0.02024067, 0.01816598, 0.01091002, 0.00903352,\n",
       "        0.01798353, 0.0257164 , 0.01034268, 0.00764347, 0.00722532,\n",
       "        0.00759949, 0.00742728, 0.00772676, 0.00742143, 0.00759646,\n",
       "        0.00626972, 0.00720665, 0.00695451, 0.01812412, 0.00680425]),\n",
       " 'mean_score_time': array([0.00908272, 0.00972142, 0.00471222, 0.00937762, 0.00892999,\n",
       "        0.00874228, 0.01092696, 0.00447421, 0.00627153, 0.00722342,\n",
       "        0.00789559, 0.0072021 , 0.00541348, 0.0083117 , 0.00293376,\n",
       "        0.00968425, 0.00883977, 0.00722675, 0.00893188, 0.00950415,\n",
       "        0.0034014 , 0.00761034, 0.0078819 , 0.00729232, 0.00687237,\n",
       "        0.0046922 , 0.00422156, 0.00634906, 0.00868032, 0.00943551]),\n",
       " 'std_score_time': array([0.00697716, 0.00645079, 0.00719835, 0.00765895, 0.00744573,\n",
       "        0.00743767, 0.00715366, 0.00629639, 0.00768159, 0.00690924,\n",
       "        0.0078965 , 0.00731208, 0.00679946, 0.00763566, 0.00501976,\n",
       "        0.00794406, 0.0074595 , 0.00741436, 0.0073855 , 0.0077622 ,\n",
       "        0.00616978, 0.00734072, 0.00788332, 0.0071932 , 0.00733162,\n",
       "        0.00716746, 0.00617312, 0.00688104, 0.00659806, 0.00591449]),\n",
       " 'param_classifier': masked_array(data=[LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000)],\n",
       "              mask=[False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'param_classifier__C': masked_array(data=[0.0001, 0.0001, 0.001, 0.001, 0.01, 0.01, 0.1, 0.1,\n",
       "                    1.0, 1.0, 10.0, 10.0, 100.0, 100.0, 1000.0, 1000.0,\n",
       "                    10000.0, 10000.0, 0.0001, 0.001, 0.01, 0.1, 1.0, 10.0,\n",
       "                    100.0, 1000.0, 10000.0, --, --, --],\n",
       "              mask=[False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False,  True,  True,  True],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'param_classifier__penalty': masked_array(data=['l1', 'l2', 'l1', 'l2', 'l1', 'l2', 'l1', 'l2', 'l1',\n",
       "                    'l2', 'l1', 'l2', 'l1', 'l2', 'l1', 'l2', 'l1', 'l2',\n",
       "                    'l2', 'l2', 'l2', 'l2', 'l2', 'l2', 'l2', 'l2', 'l2',\n",
       "                    'none', 'none', --],\n",
       "              mask=[False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False,  True],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'param_classifier__solver': masked_array(data=['saga', 'saga', 'saga', 'saga', 'saga', 'saga', 'saga',\n",
       "                    'saga', 'saga', 'saga', 'saga', 'saga', 'saga', 'saga',\n",
       "                    'saga', 'saga', 'saga', 'saga', 'lbfgs', 'lbfgs',\n",
       "                    'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs',\n",
       "                    'lbfgs', 'lbfgs', 'saga', --],\n",
       "              mask=[False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False,  True],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'params': [{'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 0.0001,\n",
       "   'classifier__penalty': 'l1',\n",
       "   'classifier__solver': 'saga'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 0.0001,\n",
       "   'classifier__penalty': 'l2',\n",
       "   'classifier__solver': 'saga'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 0.001,\n",
       "   'classifier__penalty': 'l1',\n",
       "   'classifier__solver': 'saga'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 0.001,\n",
       "   'classifier__penalty': 'l2',\n",
       "   'classifier__solver': 'saga'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 0.01,\n",
       "   'classifier__penalty': 'l1',\n",
       "   'classifier__solver': 'saga'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 0.01,\n",
       "   'classifier__penalty': 'l2',\n",
       "   'classifier__solver': 'saga'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 0.1,\n",
       "   'classifier__penalty': 'l1',\n",
       "   'classifier__solver': 'saga'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 0.1,\n",
       "   'classifier__penalty': 'l2',\n",
       "   'classifier__solver': 'saga'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 1.0,\n",
       "   'classifier__penalty': 'l1',\n",
       "   'classifier__solver': 'saga'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 1.0,\n",
       "   'classifier__penalty': 'l2',\n",
       "   'classifier__solver': 'saga'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 10.0,\n",
       "   'classifier__penalty': 'l1',\n",
       "   'classifier__solver': 'saga'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 10.0,\n",
       "   'classifier__penalty': 'l2',\n",
       "   'classifier__solver': 'saga'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 100.0,\n",
       "   'classifier__penalty': 'l1',\n",
       "   'classifier__solver': 'saga'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 100.0,\n",
       "   'classifier__penalty': 'l2',\n",
       "   'classifier__solver': 'saga'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 1000.0,\n",
       "   'classifier__penalty': 'l1',\n",
       "   'classifier__solver': 'saga'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 1000.0,\n",
       "   'classifier__penalty': 'l2',\n",
       "   'classifier__solver': 'saga'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 10000.0,\n",
       "   'classifier__penalty': 'l1',\n",
       "   'classifier__solver': 'saga'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 10000.0,\n",
       "   'classifier__penalty': 'l2',\n",
       "   'classifier__solver': 'saga'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 0.0001,\n",
       "   'classifier__penalty': 'l2',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 0.001,\n",
       "   'classifier__penalty': 'l2',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 0.01,\n",
       "   'classifier__penalty': 'l2',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 0.1,\n",
       "   'classifier__penalty': 'l2',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 1.0,\n",
       "   'classifier__penalty': 'l2',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 10.0,\n",
       "   'classifier__penalty': 'l2',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 100.0,\n",
       "   'classifier__penalty': 'l2',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 1000.0,\n",
       "   'classifier__penalty': 'l2',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 10000.0,\n",
       "   'classifier__penalty': 'l2',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__penalty': 'none',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__penalty': 'none',\n",
       "   'classifier__solver': 'saga'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000)}],\n",
       " 'split0_test_accuracy': array([0.602, 0.717, 0.687, 0.761, 0.759, 0.762, 0.762, 0.761, 0.763,\n",
       "        0.763, 0.763, 0.763, 0.763, 0.763, 0.763, 0.763, 0.763, 0.763,\n",
       "        0.717, 0.761, 0.762, 0.761, 0.763, 0.763, 0.763, 0.763, 0.763,\n",
       "        0.763, 0.763, 0.763]),\n",
       " 'split1_test_accuracy': array([0.601, 0.695, 0.678, 0.742, 0.744, 0.747, 0.75 , 0.752, 0.752,\n",
       "        0.753, 0.753, 0.753, 0.753, 0.753, 0.753, 0.753, 0.753, 0.753,\n",
       "        0.695, 0.742, 0.747, 0.752, 0.753, 0.753, 0.753, 0.753, 0.753,\n",
       "        0.753, 0.753, 0.753]),\n",
       " 'split2_test_accuracy': array([0.601, 0.706, 0.703, 0.741, 0.734, 0.74 , 0.743, 0.746, 0.746,\n",
       "        0.746, 0.746, 0.746, 0.746, 0.746, 0.746, 0.746, 0.746, 0.746,\n",
       "        0.706, 0.741, 0.74 , 0.746, 0.746, 0.746, 0.746, 0.746, 0.746,\n",
       "        0.746, 0.746, 0.746]),\n",
       " 'split3_test_accuracy': array([0.601, 0.676, 0.681, 0.724, 0.725, 0.727, 0.723, 0.725, 0.726,\n",
       "        0.726, 0.726, 0.726, 0.726, 0.726, 0.726, 0.726, 0.726, 0.726,\n",
       "        0.676, 0.724, 0.727, 0.725, 0.726, 0.726, 0.726, 0.726, 0.726,\n",
       "        0.726, 0.726, 0.726]),\n",
       " 'split4_test_accuracy': array([0.601, 0.683, 0.673, 0.712, 0.711, 0.711, 0.71 , 0.712, 0.711,\n",
       "        0.711, 0.711, 0.711, 0.711, 0.711, 0.711, 0.711, 0.711, 0.711,\n",
       "        0.683, 0.712, 0.711, 0.712, 0.711, 0.711, 0.711, 0.711, 0.711,\n",
       "        0.711, 0.711, 0.711]),\n",
       " 'split5_test_accuracy': array([0.601, 0.702, 0.687, 0.742, 0.745, 0.751, 0.752, 0.756, 0.755,\n",
       "        0.755, 0.755, 0.755, 0.755, 0.755, 0.755, 0.755, 0.755, 0.755,\n",
       "        0.702, 0.742, 0.751, 0.756, 0.755, 0.755, 0.755, 0.755, 0.755,\n",
       "        0.755, 0.755, 0.755]),\n",
       " 'split6_test_accuracy': array([0.601, 0.722, 0.69 , 0.749, 0.754, 0.751, 0.745, 0.749, 0.75 ,\n",
       "        0.75 , 0.75 , 0.75 , 0.75 , 0.75 , 0.75 , 0.75 , 0.75 , 0.75 ,\n",
       "        0.722, 0.749, 0.751, 0.749, 0.75 , 0.75 , 0.75 , 0.75 , 0.75 ,\n",
       "        0.75 , 0.75 , 0.75 ]),\n",
       " 'split7_test_accuracy': array([0.601, 0.685, 0.687, 0.74 , 0.753, 0.749, 0.746, 0.748, 0.747,\n",
       "        0.747, 0.747, 0.747, 0.747, 0.747, 0.747, 0.747, 0.747, 0.747,\n",
       "        0.685, 0.74 , 0.749, 0.748, 0.747, 0.747, 0.747, 0.747, 0.747,\n",
       "        0.747, 0.747, 0.747]),\n",
       " 'split8_test_accuracy': array([0.601, 0.69 , 0.675, 0.754, 0.746, 0.749, 0.747, 0.747, 0.748,\n",
       "        0.748, 0.748, 0.748, 0.748, 0.748, 0.748, 0.748, 0.748, 0.748,\n",
       "        0.69 , 0.755, 0.749, 0.747, 0.748, 0.748, 0.748, 0.748, 0.748,\n",
       "        0.748, 0.748, 0.748]),\n",
       " 'split9_test_accuracy': array([0.601, 0.689, 0.683, 0.733, 0.726, 0.731, 0.733, 0.734, 0.734,\n",
       "        0.734, 0.734, 0.734, 0.734, 0.734, 0.734, 0.734, 0.734, 0.734,\n",
       "        0.689, 0.733, 0.731, 0.734, 0.734, 0.734, 0.734, 0.734, 0.734,\n",
       "        0.734, 0.734, 0.734]),\n",
       " 'mean_test_accuracy': array([0.6011, 0.6965, 0.6844, 0.7398, 0.7397, 0.7418, 0.7411, 0.743 ,\n",
       "        0.7432, 0.7433, 0.7433, 0.7433, 0.7433, 0.7433, 0.7433, 0.7433,\n",
       "        0.7433, 0.7433, 0.6965, 0.7399, 0.7418, 0.743 , 0.7433, 0.7433,\n",
       "        0.7433, 0.7433, 0.7433, 0.7433, 0.7433, 0.7433]),\n",
       " 'std_test_accuracy': array([0.0003    , 0.01423552, 0.0081878 , 0.01347442, 0.01449172,\n",
       "        0.01412657, 0.01439757, 0.01423376, 0.01455198, 0.0146154 ,\n",
       "        0.0146154 , 0.0146154 , 0.0146154 , 0.0146154 , 0.0146154 ,\n",
       "        0.0146154 , 0.0146154 , 0.0146154 , 0.01423552, 0.01358271,\n",
       "        0.01412657, 0.01423376, 0.0146154 , 0.0146154 , 0.0146154 ,\n",
       "        0.0146154 , 0.0146154 , 0.0146154 , 0.0146154 , 0.0146154 ]),\n",
       " 'rank_test_accuracy': array([30, 27, 29, 25, 26, 21, 23, 19, 18,  1,  1,  1,  1,  1,  1,  1,  1,\n",
       "         1, 27, 24, 21, 19,  1,  1,  1,  1,  1,  1,  1,  1]),\n",
       " 'split0_test_precision': array([0.        , 0.8042328 , 0.72486772, 0.73451327, 0.7115903 ,\n",
       "        0.71390374, 0.71164021, 0.712     , 0.71352785, 0.71352785,\n",
       "        0.71352785, 0.71352785, 0.71352785, 0.71352785, 0.71352785,\n",
       "        0.71352785, 0.71352785, 0.71352785, 0.8042328 , 0.73451327,\n",
       "        0.71390374, 0.712     , 0.71352785, 0.71352785, 0.71352785,\n",
       "        0.71352785, 0.71352785, 0.71352785, 0.71352785, 0.71352785]),\n",
       " 'split1_test_precision': array([0.        , 0.77325581, 0.72781065, 0.71826625, 0.70605187,\n",
       "        0.70621469, 0.70637119, 0.70914127, 0.70914127, 0.70994475,\n",
       "        0.70994475, 0.70994475, 0.70994475, 0.70994475, 0.70994475,\n",
       "        0.70994475, 0.70994475, 0.70994475, 0.77325581, 0.71826625,\n",
       "        0.70621469, 0.70914127, 0.70994475, 0.70994475, 0.70994475,\n",
       "        0.70994475, 0.70994475, 0.70994475, 0.70994475, 0.70994475]),\n",
       " 'split2_test_precision': array([0.        , 0.79005525, 0.78333333, 0.70114943, 0.6802168 ,\n",
       "        0.68733154, 0.68882979, 0.69230769, 0.69230769, 0.69230769,\n",
       "        0.69230769, 0.69230769, 0.69230769, 0.69230769, 0.69230769,\n",
       "        0.69230769, 0.69230769, 0.69230769, 0.79005525, 0.70114943,\n",
       "        0.68733154, 0.69230769, 0.69230769, 0.69230769, 0.69230769,\n",
       "        0.69230769, 0.69230769, 0.69230769, 0.69230769, 0.69230769]),\n",
       " 'split3_test_precision': array([0.        , 0.71428571, 0.71276596, 0.67521368, 0.66756757,\n",
       "        0.66935484, 0.66223404, 0.66489362, 0.66578249, 0.66578249,\n",
       "        0.66578249, 0.66578249, 0.66578249, 0.66578249, 0.66578249,\n",
       "        0.66578249, 0.66578249, 0.66578249, 0.71428571, 0.67521368,\n",
       "        0.66935484, 0.66489362, 0.66578249, 0.66578249, 0.66578249,\n",
       "        0.66578249, 0.66578249, 0.66578249, 0.66578249, 0.66578249]),\n",
       " 'split4_test_precision': array([0.        , 0.73837209, 0.70930233, 0.67619048, 0.65895954,\n",
       "        0.65625   , 0.65266106, 0.65546218, 0.65449438, 0.65449438,\n",
       "        0.65449438, 0.65449438, 0.65449438, 0.65449438, 0.65449438,\n",
       "        0.65449438, 0.65449438, 0.65449438, 0.73837209, 0.67619048,\n",
       "        0.65625   , 0.65546218, 0.65449438, 0.65449438, 0.65449438,\n",
       "        0.65449438, 0.65449438, 0.65449438, 0.65449438, 0.65449438]),\n",
       " 'split5_test_precision': array([0.        , 0.77297297, 0.72631579, 0.69971671, 0.68947368,\n",
       "        0.69946809, 0.69816273, 0.70448549, 0.70263158, 0.70263158,\n",
       "        0.70263158, 0.70263158, 0.70263158, 0.70263158, 0.70263158,\n",
       "        0.70263158, 0.70263158, 0.70263158, 0.77297297, 0.69971671,\n",
       "        0.69946809, 0.70448549, 0.70263158, 0.70263158, 0.70263158,\n",
       "        0.70263158, 0.70263158, 0.70263158, 0.70263158, 0.70263158]),\n",
       " 'split6_test_precision': array([0.        , 0.8040201 , 0.73298429, 0.71142857, 0.70291777,\n",
       "        0.69736842, 0.6865285 , 0.69270833, 0.69350649, 0.69350649,\n",
       "        0.69350649, 0.69350649, 0.69350649, 0.69350649, 0.69350649,\n",
       "        0.69350649, 0.69350649, 0.69350649, 0.8040201 , 0.71142857,\n",
       "        0.69736842, 0.69270833, 0.69350649, 0.69350649, 0.69350649,\n",
       "        0.69350649, 0.69350649, 0.69350649, 0.69350649, 0.69350649]),\n",
       " 'split7_test_precision': array([0.        , 0.72580645, 0.74157303, 0.70381232, 0.70994475,\n",
       "        0.70218579, 0.69647696, 0.69918699, 0.6972973 , 0.6972973 ,\n",
       "        0.6972973 , 0.6972973 , 0.6972973 , 0.6972973 , 0.6972973 ,\n",
       "        0.6972973 , 0.6972973 , 0.6972973 , 0.72580645, 0.70381232,\n",
       "        0.70218579, 0.69918699, 0.6972973 , 0.6972973 , 0.6972973 ,\n",
       "        0.6972973 , 0.6972973 , 0.6972973 , 0.6972973 , 0.6972973 ]),\n",
       " 'split8_test_precision': array([0.        , 0.77300613, 0.74025974, 0.73538462, 0.70893372,\n",
       "        0.71022727, 0.70391061, 0.70277778, 0.70473538, 0.70473538,\n",
       "        0.70473538, 0.70473538, 0.70473538, 0.70473538, 0.70473538,\n",
       "        0.70473538, 0.70473538, 0.70473538, 0.77300613, 0.73619632,\n",
       "        0.71022727, 0.70277778, 0.70473538, 0.70473538, 0.70473538,\n",
       "        0.70473538, 0.70473538, 0.70473538, 0.70473538, 0.70473538]),\n",
       " 'split9_test_precision': array([0.        , 0.73913043, 0.71134021, 0.67837838, 0.65984655,\n",
       "        0.6625    , 0.66256158, 0.66339066, 0.66339066, 0.66339066,\n",
       "        0.66339066, 0.66339066, 0.66339066, 0.66339066, 0.66339066,\n",
       "        0.66339066, 0.66339066, 0.66339066, 0.73913043, 0.67837838,\n",
       "        0.6625    , 0.66339066, 0.66339066, 0.66339066, 0.66339066,\n",
       "        0.66339066, 0.66339066, 0.66339066, 0.66339066, 0.66339066]),\n",
       " 'mean_test_precision': array([0.        , 0.76351378, 0.73105531, 0.70340537, 0.68955025,\n",
       "        0.69048044, 0.68693767, 0.6896354 , 0.68968151, 0.68976186,\n",
       "        0.68976186, 0.68976186, 0.68976186, 0.68976186, 0.68976186,\n",
       "        0.68976186, 0.68976186, 0.68976186, 0.76351378, 0.70348654,\n",
       "        0.69048044, 0.6896354 , 0.68976186, 0.68976186, 0.68976186,\n",
       "        0.68976186, 0.68976186, 0.68976186, 0.68976186, 0.68976186]),\n",
       " 'std_test_precision': array([0.        , 0.03057717, 0.0204932 , 0.02110977, 0.02029008,\n",
       "        0.01965255, 0.0196877 , 0.01962261, 0.01980325, 0.01988351,\n",
       "        0.01988351, 0.01988351, 0.01988351, 0.01988351, 0.01988351,\n",
       "        0.01988351, 0.01988351, 0.01988351, 0.03057717, 0.02123378,\n",
       "        0.01965255, 0.01962261, 0.01988351, 0.01988351, 0.01988351,\n",
       "        0.01988351, 0.01988351, 0.01988351, 0.01988351, 0.01988351]),\n",
       " 'rank_test_precision': array([30,  1,  3,  5, 28,  6, 29, 26, 25,  8,  8,  8,  8,  8,  8,  8,  8,\n",
       "         8,  1,  4,  6, 26,  8,  8,  8,  8,  8,  8,  8,  8]),\n",
       " 'split0_test_f1_micro': array([0.602, 0.717, 0.687, 0.761, 0.759, 0.762, 0.762, 0.761, 0.763,\n",
       "        0.763, 0.763, 0.763, 0.763, 0.763, 0.763, 0.763, 0.763, 0.763,\n",
       "        0.717, 0.761, 0.762, 0.761, 0.763, 0.763, 0.763, 0.763, 0.763,\n",
       "        0.763, 0.763, 0.763]),\n",
       " 'split1_test_f1_micro': array([0.601, 0.695, 0.678, 0.742, 0.744, 0.747, 0.75 , 0.752, 0.752,\n",
       "        0.753, 0.753, 0.753, 0.753, 0.753, 0.753, 0.753, 0.753, 0.753,\n",
       "        0.695, 0.742, 0.747, 0.752, 0.753, 0.753, 0.753, 0.753, 0.753,\n",
       "        0.753, 0.753, 0.753]),\n",
       " 'split2_test_f1_micro': array([0.601, 0.706, 0.703, 0.741, 0.734, 0.74 , 0.743, 0.746, 0.746,\n",
       "        0.746, 0.746, 0.746, 0.746, 0.746, 0.746, 0.746, 0.746, 0.746,\n",
       "        0.706, 0.741, 0.74 , 0.746, 0.746, 0.746, 0.746, 0.746, 0.746,\n",
       "        0.746, 0.746, 0.746]),\n",
       " 'split3_test_f1_micro': array([0.601, 0.676, 0.681, 0.724, 0.725, 0.727, 0.723, 0.725, 0.726,\n",
       "        0.726, 0.726, 0.726, 0.726, 0.726, 0.726, 0.726, 0.726, 0.726,\n",
       "        0.676, 0.724, 0.727, 0.725, 0.726, 0.726, 0.726, 0.726, 0.726,\n",
       "        0.726, 0.726, 0.726]),\n",
       " 'split4_test_f1_micro': array([0.601, 0.683, 0.673, 0.712, 0.711, 0.711, 0.71 , 0.712, 0.711,\n",
       "        0.711, 0.711, 0.711, 0.711, 0.711, 0.711, 0.711, 0.711, 0.711,\n",
       "        0.683, 0.712, 0.711, 0.712, 0.711, 0.711, 0.711, 0.711, 0.711,\n",
       "        0.711, 0.711, 0.711]),\n",
       " 'split5_test_f1_micro': array([0.601, 0.702, 0.687, 0.742, 0.745, 0.751, 0.752, 0.756, 0.755,\n",
       "        0.755, 0.755, 0.755, 0.755, 0.755, 0.755, 0.755, 0.755, 0.755,\n",
       "        0.702, 0.742, 0.751, 0.756, 0.755, 0.755, 0.755, 0.755, 0.755,\n",
       "        0.755, 0.755, 0.755]),\n",
       " 'split6_test_f1_micro': array([0.601, 0.722, 0.69 , 0.749, 0.754, 0.751, 0.745, 0.749, 0.75 ,\n",
       "        0.75 , 0.75 , 0.75 , 0.75 , 0.75 , 0.75 , 0.75 , 0.75 , 0.75 ,\n",
       "        0.722, 0.749, 0.751, 0.749, 0.75 , 0.75 , 0.75 , 0.75 , 0.75 ,\n",
       "        0.75 , 0.75 , 0.75 ]),\n",
       " 'split7_test_f1_micro': array([0.601, 0.685, 0.687, 0.74 , 0.753, 0.749, 0.746, 0.748, 0.747,\n",
       "        0.747, 0.747, 0.747, 0.747, 0.747, 0.747, 0.747, 0.747, 0.747,\n",
       "        0.685, 0.74 , 0.749, 0.748, 0.747, 0.747, 0.747, 0.747, 0.747,\n",
       "        0.747, 0.747, 0.747]),\n",
       " 'split8_test_f1_micro': array([0.601, 0.69 , 0.675, 0.754, 0.746, 0.749, 0.747, 0.747, 0.748,\n",
       "        0.748, 0.748, 0.748, 0.748, 0.748, 0.748, 0.748, 0.748, 0.748,\n",
       "        0.69 , 0.755, 0.749, 0.747, 0.748, 0.748, 0.748, 0.748, 0.748,\n",
       "        0.748, 0.748, 0.748]),\n",
       " 'split9_test_f1_micro': array([0.601, 0.689, 0.683, 0.733, 0.726, 0.731, 0.733, 0.734, 0.734,\n",
       "        0.734, 0.734, 0.734, 0.734, 0.734, 0.734, 0.734, 0.734, 0.734,\n",
       "        0.689, 0.733, 0.731, 0.734, 0.734, 0.734, 0.734, 0.734, 0.734,\n",
       "        0.734, 0.734, 0.734]),\n",
       " 'mean_test_f1_micro': array([0.6011, 0.6965, 0.6844, 0.7398, 0.7397, 0.7418, 0.7411, 0.743 ,\n",
       "        0.7432, 0.7433, 0.7433, 0.7433, 0.7433, 0.7433, 0.7433, 0.7433,\n",
       "        0.7433, 0.7433, 0.6965, 0.7399, 0.7418, 0.743 , 0.7433, 0.7433,\n",
       "        0.7433, 0.7433, 0.7433, 0.7433, 0.7433, 0.7433]),\n",
       " 'std_test_f1_micro': array([0.0003    , 0.01423552, 0.0081878 , 0.01347442, 0.01449172,\n",
       "        0.01412657, 0.01439757, 0.01423376, 0.01455198, 0.0146154 ,\n",
       "        0.0146154 , 0.0146154 , 0.0146154 , 0.0146154 , 0.0146154 ,\n",
       "        0.0146154 , 0.0146154 , 0.0146154 , 0.01423552, 0.01358271,\n",
       "        0.01412657, 0.01423376, 0.0146154 , 0.0146154 , 0.0146154 ,\n",
       "        0.0146154 , 0.0146154 , 0.0146154 , 0.0146154 , 0.0146154 ]),\n",
       " 'rank_test_f1_micro': array([30, 27, 29, 25, 26, 21, 23, 19, 18,  1,  1,  1,  1,  1,  1,  1,  1,\n",
       "         1, 27, 24, 21, 19,  1,  1,  1,  1,  1,  1,  1,  1])}"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_model.cv_results_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "9aa08645",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([30, 27, 29, 25, 26, 21, 23, 19, 18,  1,  1,  1,  1,  1,  1,  1,  1,\n",
       "        1, 27, 24, 21, 19,  1,  1,  1,  1,  1,  1,  1,  1])"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_model.cv_results_['rank_test_accuracy']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "abdb8b79",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'classifier': LogisticRegression(max_iter=10000),\n",
       " 'classifier__C': 0.0001,\n",
       " 'classifier__penalty': 'l1',\n",
       " 'classifier__solver': 'saga'}"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_model.cv_results_['params'][ np.argmin(best_model.cv_results_['mean_test_accuracy'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "f4602352",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>classifier</th>\n",
       "      <th>C</th>\n",
       "      <th>penalty</th>\n",
       "      <th>solver</th>\n",
       "      <th>score_acc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.3989</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.3035</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.3156</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.2602</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0100</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.2603</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0100</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.2582</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.1000</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.2589</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.1000</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.2570</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.2568</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.2567</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>10.0000</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.2567</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>10.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.2567</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>100.0000</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.2567</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>100.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.2567</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>1000.0000</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.2567</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>1000.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.2567</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>10000.0000</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.2567</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>10000.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.2567</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.3035</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.2601</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0100</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.2582</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.1000</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.2570</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.2567</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>10.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.2567</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>100.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.2567</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>1000.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.2567</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>10000.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.2567</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>NaN</td>\n",
       "      <td>none</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.2567</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>NaN</td>\n",
       "      <td>none</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.2567</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.2567</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                            classifier           C penalty solver  score_acc\n",
       "0   LogisticRegression(max_iter=10000)      0.0001      l1   saga     0.3989\n",
       "1   LogisticRegression(max_iter=10000)      0.0001      l2   saga     0.3035\n",
       "2   LogisticRegression(max_iter=10000)      0.0010      l1   saga     0.3156\n",
       "3   LogisticRegression(max_iter=10000)      0.0010      l2   saga     0.2602\n",
       "4   LogisticRegression(max_iter=10000)      0.0100      l1   saga     0.2603\n",
       "5   LogisticRegression(max_iter=10000)      0.0100      l2   saga     0.2582\n",
       "6   LogisticRegression(max_iter=10000)      0.1000      l1   saga     0.2589\n",
       "7   LogisticRegression(max_iter=10000)      0.1000      l2   saga     0.2570\n",
       "8   LogisticRegression(max_iter=10000)      1.0000      l1   saga     0.2568\n",
       "9   LogisticRegression(max_iter=10000)      1.0000      l2   saga     0.2567\n",
       "10  LogisticRegression(max_iter=10000)     10.0000      l1   saga     0.2567\n",
       "11  LogisticRegression(max_iter=10000)     10.0000      l2   saga     0.2567\n",
       "12  LogisticRegression(max_iter=10000)    100.0000      l1   saga     0.2567\n",
       "13  LogisticRegression(max_iter=10000)    100.0000      l2   saga     0.2567\n",
       "14  LogisticRegression(max_iter=10000)   1000.0000      l1   saga     0.2567\n",
       "15  LogisticRegression(max_iter=10000)   1000.0000      l2   saga     0.2567\n",
       "16  LogisticRegression(max_iter=10000)  10000.0000      l1   saga     0.2567\n",
       "17  LogisticRegression(max_iter=10000)  10000.0000      l2   saga     0.2567\n",
       "18  LogisticRegression(max_iter=10000)      0.0001      l2  lbfgs     0.3035\n",
       "19  LogisticRegression(max_iter=10000)      0.0010      l2  lbfgs     0.2601\n",
       "20  LogisticRegression(max_iter=10000)      0.0100      l2  lbfgs     0.2582\n",
       "21  LogisticRegression(max_iter=10000)      0.1000      l2  lbfgs     0.2570\n",
       "22  LogisticRegression(max_iter=10000)      1.0000      l2  lbfgs     0.2567\n",
       "23  LogisticRegression(max_iter=10000)     10.0000      l2  lbfgs     0.2567\n",
       "24  LogisticRegression(max_iter=10000)    100.0000      l2  lbfgs     0.2567\n",
       "25  LogisticRegression(max_iter=10000)   1000.0000      l2  lbfgs     0.2567\n",
       "26  LogisticRegression(max_iter=10000)  10000.0000      l2  lbfgs     0.2567\n",
       "27  LogisticRegression(max_iter=10000)         NaN    none  lbfgs     0.2567\n",
       "28  LogisticRegression(max_iter=10000)         NaN    none   saga     0.2567\n",
       "29  LogisticRegression(max_iter=10000)         NaN     NaN    NaN     0.2567"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results = pd.DataFrame( best_model.cv_results_['params'] )\n",
    "\n",
    "# next grab the score resulting from those parameters, add it to the data\n",
    "# score is accuracy; to display it as misclassification error we use 1 - x\n",
    "results['score_acc'] = 1 - best_model.cv_results_['mean_test_accuracy']\n",
    "\n",
    "# get rid of classifierXX in columns\n",
    "cols = results.columns.to_series().str.split('__').apply(lambda x: x[-1])\n",
    "results.columns = cols\n",
    "\n",
    "# lets show the results for the saga solver across penalty & C values\n",
    "#sns.heatmap( results.query('solver==\"saga\"').pivot('C','penalty','score_acc'),\n",
    "#             annot=True, fmt='.3f')\n",
    "\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "dce721ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "acc_list.append(results[0:1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "ab7ddcc5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'classifier': LogisticRegression(max_iter=10000),\n",
       " 'classifier__C': 0.0001,\n",
       " 'classifier__penalty': 'l2',\n",
       " 'classifier__solver': 'saga'}"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_model.cv_results_['params'][ np.argmin(best_model.cv_results_['rank_test_precision'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "c6c37019",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>classifier</th>\n",
       "      <th>C</th>\n",
       "      <th>penalty</th>\n",
       "      <th>solver</th>\n",
       "      <th>score_precision</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.236486</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.268945</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.296595</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0100</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.310450</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0100</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.309520</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.1000</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.313062</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.1000</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.310365</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.310318</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.310238</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>10.0000</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.310238</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>10.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.310238</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>100.0000</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.310238</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>100.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.310238</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>1000.0000</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.310238</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>1000.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.310238</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>10000.0000</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.310238</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>10000.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.310238</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.236486</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.296513</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0100</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.309520</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.1000</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.310365</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.310238</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>10.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.310238</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>100.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.310238</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>1000.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.310238</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>10000.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.310238</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>NaN</td>\n",
       "      <td>none</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.310238</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>NaN</td>\n",
       "      <td>none</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.310238</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.310238</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                            classifier           C penalty solver  \\\n",
       "0   LogisticRegression(max_iter=10000)      0.0001      l1   saga   \n",
       "1   LogisticRegression(max_iter=10000)      0.0001      l2   saga   \n",
       "2   LogisticRegression(max_iter=10000)      0.0010      l1   saga   \n",
       "3   LogisticRegression(max_iter=10000)      0.0010      l2   saga   \n",
       "4   LogisticRegression(max_iter=10000)      0.0100      l1   saga   \n",
       "5   LogisticRegression(max_iter=10000)      0.0100      l2   saga   \n",
       "6   LogisticRegression(max_iter=10000)      0.1000      l1   saga   \n",
       "7   LogisticRegression(max_iter=10000)      0.1000      l2   saga   \n",
       "8   LogisticRegression(max_iter=10000)      1.0000      l1   saga   \n",
       "9   LogisticRegression(max_iter=10000)      1.0000      l2   saga   \n",
       "10  LogisticRegression(max_iter=10000)     10.0000      l1   saga   \n",
       "11  LogisticRegression(max_iter=10000)     10.0000      l2   saga   \n",
       "12  LogisticRegression(max_iter=10000)    100.0000      l1   saga   \n",
       "13  LogisticRegression(max_iter=10000)    100.0000      l2   saga   \n",
       "14  LogisticRegression(max_iter=10000)   1000.0000      l1   saga   \n",
       "15  LogisticRegression(max_iter=10000)   1000.0000      l2   saga   \n",
       "16  LogisticRegression(max_iter=10000)  10000.0000      l1   saga   \n",
       "17  LogisticRegression(max_iter=10000)  10000.0000      l2   saga   \n",
       "18  LogisticRegression(max_iter=10000)      0.0001      l2  lbfgs   \n",
       "19  LogisticRegression(max_iter=10000)      0.0010      l2  lbfgs   \n",
       "20  LogisticRegression(max_iter=10000)      0.0100      l2  lbfgs   \n",
       "21  LogisticRegression(max_iter=10000)      0.1000      l2  lbfgs   \n",
       "22  LogisticRegression(max_iter=10000)      1.0000      l2  lbfgs   \n",
       "23  LogisticRegression(max_iter=10000)     10.0000      l2  lbfgs   \n",
       "24  LogisticRegression(max_iter=10000)    100.0000      l2  lbfgs   \n",
       "25  LogisticRegression(max_iter=10000)   1000.0000      l2  lbfgs   \n",
       "26  LogisticRegression(max_iter=10000)  10000.0000      l2  lbfgs   \n",
       "27  LogisticRegression(max_iter=10000)         NaN    none  lbfgs   \n",
       "28  LogisticRegression(max_iter=10000)         NaN    none   saga   \n",
       "29  LogisticRegression(max_iter=10000)         NaN     NaN    NaN   \n",
       "\n",
       "    score_precision  \n",
       "0          1.000000  \n",
       "1          0.236486  \n",
       "2          0.268945  \n",
       "3          0.296595  \n",
       "4          0.310450  \n",
       "5          0.309520  \n",
       "6          0.313062  \n",
       "7          0.310365  \n",
       "8          0.310318  \n",
       "9          0.310238  \n",
       "10         0.310238  \n",
       "11         0.310238  \n",
       "12         0.310238  \n",
       "13         0.310238  \n",
       "14         0.310238  \n",
       "15         0.310238  \n",
       "16         0.310238  \n",
       "17         0.310238  \n",
       "18         0.236486  \n",
       "19         0.296513  \n",
       "20         0.309520  \n",
       "21         0.310365  \n",
       "22         0.310238  \n",
       "23         0.310238  \n",
       "24         0.310238  \n",
       "25         0.310238  \n",
       "26         0.310238  \n",
       "27         0.310238  \n",
       "28         0.310238  \n",
       "29         0.310238  "
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results = pd.DataFrame( best_model.cv_results_['params'] )\n",
    "\n",
    "# next grab the score resulting from those parameters, add it to the data\n",
    "# score is accuracy; to display it as misclassification error we use 1 - x\n",
    "results['score_precision'] = 1 - best_model.cv_results_['mean_test_precision']\n",
    "\n",
    "# get rid of classifierXX in columns\n",
    "cols = results.columns.to_series().str.split('__').apply(lambda x: x[-1])\n",
    "results.columns = cols\n",
    "\n",
    "# lets show the results for the saga solver across penalty & C values\n",
    "#sns.heatmap( results.query('solver==\"saga\"').pivot('C','penalty','score_precision'),\n",
    "#             annot=True, fmt='.3f')\n",
    "\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "30976105",
   "metadata": {},
   "outputs": [],
   "source": [
    "roc_auc_ovr_List.append(results[1:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "ca7bc287",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'classifier': LogisticRegression(max_iter=10000),\n",
       " 'classifier__C': 1.0,\n",
       " 'classifier__penalty': 'l2',\n",
       " 'classifier__solver': 'saga'}"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_model.cv_results_['params'][ np.argmin(best_model.cv_results_['rank_test_f1_micro'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "affe1adf",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>classifier</th>\n",
       "      <th>C</th>\n",
       "      <th>penalty</th>\n",
       "      <th>solver</th>\n",
       "      <th>score_f1_micro</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.3989</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.3035</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.3156</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.2602</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0100</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.2603</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0100</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.2582</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.1000</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.2589</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.1000</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.2570</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.2568</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.2567</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>10.0000</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.2567</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>10.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.2567</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>100.0000</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.2567</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>100.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.2567</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>1000.0000</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.2567</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>1000.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.2567</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>10000.0000</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.2567</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>10000.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.2567</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.3035</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.2601</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0100</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.2582</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.1000</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.2570</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.2567</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>10.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.2567</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>100.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.2567</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>1000.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.2567</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>10000.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.2567</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>NaN</td>\n",
       "      <td>none</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.2567</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>NaN</td>\n",
       "      <td>none</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.2567</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.2567</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                            classifier           C penalty solver  \\\n",
       "0   LogisticRegression(max_iter=10000)      0.0001      l1   saga   \n",
       "1   LogisticRegression(max_iter=10000)      0.0001      l2   saga   \n",
       "2   LogisticRegression(max_iter=10000)      0.0010      l1   saga   \n",
       "3   LogisticRegression(max_iter=10000)      0.0010      l2   saga   \n",
       "4   LogisticRegression(max_iter=10000)      0.0100      l1   saga   \n",
       "5   LogisticRegression(max_iter=10000)      0.0100      l2   saga   \n",
       "6   LogisticRegression(max_iter=10000)      0.1000      l1   saga   \n",
       "7   LogisticRegression(max_iter=10000)      0.1000      l2   saga   \n",
       "8   LogisticRegression(max_iter=10000)      1.0000      l1   saga   \n",
       "9   LogisticRegression(max_iter=10000)      1.0000      l2   saga   \n",
       "10  LogisticRegression(max_iter=10000)     10.0000      l1   saga   \n",
       "11  LogisticRegression(max_iter=10000)     10.0000      l2   saga   \n",
       "12  LogisticRegression(max_iter=10000)    100.0000      l1   saga   \n",
       "13  LogisticRegression(max_iter=10000)    100.0000      l2   saga   \n",
       "14  LogisticRegression(max_iter=10000)   1000.0000      l1   saga   \n",
       "15  LogisticRegression(max_iter=10000)   1000.0000      l2   saga   \n",
       "16  LogisticRegression(max_iter=10000)  10000.0000      l1   saga   \n",
       "17  LogisticRegression(max_iter=10000)  10000.0000      l2   saga   \n",
       "18  LogisticRegression(max_iter=10000)      0.0001      l2  lbfgs   \n",
       "19  LogisticRegression(max_iter=10000)      0.0010      l2  lbfgs   \n",
       "20  LogisticRegression(max_iter=10000)      0.0100      l2  lbfgs   \n",
       "21  LogisticRegression(max_iter=10000)      0.1000      l2  lbfgs   \n",
       "22  LogisticRegression(max_iter=10000)      1.0000      l2  lbfgs   \n",
       "23  LogisticRegression(max_iter=10000)     10.0000      l2  lbfgs   \n",
       "24  LogisticRegression(max_iter=10000)    100.0000      l2  lbfgs   \n",
       "25  LogisticRegression(max_iter=10000)   1000.0000      l2  lbfgs   \n",
       "26  LogisticRegression(max_iter=10000)  10000.0000      l2  lbfgs   \n",
       "27  LogisticRegression(max_iter=10000)         NaN    none  lbfgs   \n",
       "28  LogisticRegression(max_iter=10000)         NaN    none   saga   \n",
       "29  LogisticRegression(max_iter=10000)         NaN     NaN    NaN   \n",
       "\n",
       "    score_f1_micro  \n",
       "0           0.3989  \n",
       "1           0.3035  \n",
       "2           0.3156  \n",
       "3           0.2602  \n",
       "4           0.2603  \n",
       "5           0.2582  \n",
       "6           0.2589  \n",
       "7           0.2570  \n",
       "8           0.2568  \n",
       "9           0.2567  \n",
       "10          0.2567  \n",
       "11          0.2567  \n",
       "12          0.2567  \n",
       "13          0.2567  \n",
       "14          0.2567  \n",
       "15          0.2567  \n",
       "16          0.2567  \n",
       "17          0.2567  \n",
       "18          0.3035  \n",
       "19          0.2601  \n",
       "20          0.2582  \n",
       "21          0.2570  \n",
       "22          0.2567  \n",
       "23          0.2567  \n",
       "24          0.2567  \n",
       "25          0.2567  \n",
       "26          0.2567  \n",
       "27          0.2567  \n",
       "28          0.2567  \n",
       "29          0.2567  "
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results = pd.DataFrame( best_model.cv_results_['params'] )\n",
    "\n",
    "# next grab the score resulting from those parameters, add it to the data\n",
    "# score is accuracy; to display it as misclassification error we use 1 - x\n",
    "results['score_f1_micro'] = 1 - best_model.cv_results_['mean_test_f1_micro']\n",
    "\n",
    "# get rid of classifierXX in columns\n",
    "cols = results.columns.to_series().str.split('__').apply(lambda x: x[-1])\n",
    "results.columns = cols\n",
    "\n",
    "# lets show the results for the saga solver across penalty & C values\n",
    "#sns.heatmap( results.query('solver==\"saga\"').pivot('C','penalty','score_f1_micro'),\n",
    "#             annot=True, fmt='.3f')\n",
    "\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "7439a073",
   "metadata": {},
   "outputs": [],
   "source": [
    "f1_micro_List.append(results[9:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efd1babe",
   "metadata": {},
   "source": [
    "## Trial 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "1a0f0acc",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, Y_train, Y_test = train_test_split(X, y, train_size=10000, shuffle=True, random_state = 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "7eb408a1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Guest-All\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1469: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\Guest-All\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1469: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\Guest-All\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1469: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\Guest-All\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1469: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\Guest-All\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1469: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\Guest-All\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1469: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\Guest-All\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1469: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\Guest-All\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1469: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\Guest-All\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1469: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\Guest-All\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1469: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\Guest-All\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1182: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Guest-All\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1182: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Guest-All\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1182: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Guest-All\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1182: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Guest-All\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1182: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Guest-All\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1182: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Guest-All\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1182: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Guest-All\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1182: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Guest-All\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1182: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Guest-All\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1182: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Guest-All\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1182: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Guest-All\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1182: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Guest-All\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1182: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Guest-All\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1182: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Guest-All\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1182: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Guest-All\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1182: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Guest-All\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1182: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Guest-All\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1182: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Guest-All\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1182: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Guest-All\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1182: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Create a pipeline - RF is a stand in, we will populate the classifier part below\n",
    "pipe = Pipeline([('std', StandardScaler()),\n",
    "                 ('classifier', LogisticRegression())])\n",
    "\n",
    "# Create search space of candidate learning algorithms and their hyperparameters\n",
    "# note lbfgs can't do l1, and if you pass penalty='none' it expects no C value\n",
    "search_space = [{'classifier': [LogisticRegression(max_iter=10000)],\n",
    "                 'classifier__solver': ['saga'],\n",
    "                 'classifier__penalty': ['l1', 'l2'],\n",
    "                 'classifier__C': np.logspace(-4, 4, 9)},\n",
    "                {'classifier': [LogisticRegression(max_iter=10000)],\n",
    "                 'classifier__solver': ['lbfgs'],\n",
    "                 'classifier__penalty': ['l2'],\n",
    "                 'classifier__C': np.logspace(-4, 4, 9)},\n",
    "                {'classifier': [LogisticRegression(max_iter=10000)],\n",
    "                 'classifier__solver': ['lbfgs','saga'],\n",
    "                 'classifier__penalty': ['none']},\n",
    "                {'classifier': [LogisticRegression(max_iter=10000)]}\n",
    "                ]\n",
    "\n",
    "# Create grid search\n",
    "clf = GridSearchCV(pipe, search_space, cv=StratifiedKFold(n_splits=10),\n",
    "                   scoring=['accuracy', 'precision', 'f1_micro'], refit=False,\n",
    "                   verbose=0)\n",
    "\n",
    "# Fit grid search\n",
    "best_model = clf.fit(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "bfa6938c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'mean_fit_time': array([0.02308955, 0.08153121, 0.09174404, 0.07765205, 0.097838  ,\n",
       "        0.09905071, 0.10192332, 0.0841186 , 0.12345746, 0.0996366 ,\n",
       "        0.11807191, 0.10311475, 0.12374725, 0.10500593, 0.11119056,\n",
       "        0.07990174, 0.08287246, 0.05497427, 0.01159201, 0.01191318,\n",
       "        0.01442766, 0.01416473, 0.01580408, 0.01551905, 0.01591287,\n",
       "        0.01684818, 0.01673398, 0.01532507, 0.06331613, 0.01323433]),\n",
       " 'std_fit_time': array([0.00705376, 0.01441821, 0.00873333, 0.01881778, 0.02095909,\n",
       "        0.01008592, 0.02203431, 0.01764827, 0.01106505, 0.01851002,\n",
       "        0.0225421 , 0.01077031, 0.01380524, 0.01093791, 0.02982655,\n",
       "        0.02165508, 0.01249654, 0.01043635, 0.00610058, 0.00622558,\n",
       "        0.00489551, 0.00472476, 0.007544  , 0.0078442 , 0.00701444,\n",
       "        0.00746329, 0.00298112, 0.00993653, 0.0101787 , 0.00543821]),\n",
       " 'mean_score_time': array([0.00755734, 0.00777135, 0.00800302, 0.0075686 , 0.00883584,\n",
       "        0.00943551, 0.00578816, 0.00687647, 0.00853822, 0.00783014,\n",
       "        0.00844147, 0.00976493, 0.00849986, 0.00934527, 0.00689726,\n",
       "        0.00972748, 0.00677118, 0.00156252, 0.00573039, 0.00466192,\n",
       "        0.00599148, 0.00629597, 0.00624003, 0.00490661, 0.00448616,\n",
       "        0.00673728, 0.00532649, 0.00807891, 0.00814922, 0.00737174]),\n",
       " 'std_score_time': array([0.00561391, 0.00780559, 0.00615008, 0.00744764, 0.00781488,\n",
       "        0.00770542, 0.00754787, 0.00709671, 0.007433  , 0.00783065,\n",
       "        0.00723326, 0.00684065, 0.0073177 , 0.00969108, 0.00724897,\n",
       "        0.00825774, 0.00744016, 0.00468757, 0.00763347, 0.00712138,\n",
       "        0.00737211, 0.007712  , 0.00690116, 0.00705296, 0.00686185,\n",
       "        0.00710903, 0.00701411, 0.00706445, 0.00704544, 0.00750666]),\n",
       " 'param_classifier': masked_array(data=[LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000)],\n",
       "              mask=[False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'param_classifier__C': masked_array(data=[0.0001, 0.0001, 0.001, 0.001, 0.01, 0.01, 0.1, 0.1,\n",
       "                    1.0, 1.0, 10.0, 10.0, 100.0, 100.0, 1000.0, 1000.0,\n",
       "                    10000.0, 10000.0, 0.0001, 0.001, 0.01, 0.1, 1.0, 10.0,\n",
       "                    100.0, 1000.0, 10000.0, --, --, --],\n",
       "              mask=[False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False,  True,  True,  True],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'param_classifier__penalty': masked_array(data=['l1', 'l2', 'l1', 'l2', 'l1', 'l2', 'l1', 'l2', 'l1',\n",
       "                    'l2', 'l1', 'l2', 'l1', 'l2', 'l1', 'l2', 'l1', 'l2',\n",
       "                    'l2', 'l2', 'l2', 'l2', 'l2', 'l2', 'l2', 'l2', 'l2',\n",
       "                    'none', 'none', --],\n",
       "              mask=[False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False,  True],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'param_classifier__solver': masked_array(data=['saga', 'saga', 'saga', 'saga', 'saga', 'saga', 'saga',\n",
       "                    'saga', 'saga', 'saga', 'saga', 'saga', 'saga', 'saga',\n",
       "                    'saga', 'saga', 'saga', 'saga', 'lbfgs', 'lbfgs',\n",
       "                    'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs',\n",
       "                    'lbfgs', 'lbfgs', 'saga', --],\n",
       "              mask=[False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False,  True],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'params': [{'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 0.0001,\n",
       "   'classifier__penalty': 'l1',\n",
       "   'classifier__solver': 'saga'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 0.0001,\n",
       "   'classifier__penalty': 'l2',\n",
       "   'classifier__solver': 'saga'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 0.001,\n",
       "   'classifier__penalty': 'l1',\n",
       "   'classifier__solver': 'saga'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 0.001,\n",
       "   'classifier__penalty': 'l2',\n",
       "   'classifier__solver': 'saga'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 0.01,\n",
       "   'classifier__penalty': 'l1',\n",
       "   'classifier__solver': 'saga'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 0.01,\n",
       "   'classifier__penalty': 'l2',\n",
       "   'classifier__solver': 'saga'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 0.1,\n",
       "   'classifier__penalty': 'l1',\n",
       "   'classifier__solver': 'saga'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 0.1,\n",
       "   'classifier__penalty': 'l2',\n",
       "   'classifier__solver': 'saga'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 1.0,\n",
       "   'classifier__penalty': 'l1',\n",
       "   'classifier__solver': 'saga'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 1.0,\n",
       "   'classifier__penalty': 'l2',\n",
       "   'classifier__solver': 'saga'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 10.0,\n",
       "   'classifier__penalty': 'l1',\n",
       "   'classifier__solver': 'saga'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 10.0,\n",
       "   'classifier__penalty': 'l2',\n",
       "   'classifier__solver': 'saga'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 100.0,\n",
       "   'classifier__penalty': 'l1',\n",
       "   'classifier__solver': 'saga'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 100.0,\n",
       "   'classifier__penalty': 'l2',\n",
       "   'classifier__solver': 'saga'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 1000.0,\n",
       "   'classifier__penalty': 'l1',\n",
       "   'classifier__solver': 'saga'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 1000.0,\n",
       "   'classifier__penalty': 'l2',\n",
       "   'classifier__solver': 'saga'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 10000.0,\n",
       "   'classifier__penalty': 'l1',\n",
       "   'classifier__solver': 'saga'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 10000.0,\n",
       "   'classifier__penalty': 'l2',\n",
       "   'classifier__solver': 'saga'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 0.0001,\n",
       "   'classifier__penalty': 'l2',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 0.001,\n",
       "   'classifier__penalty': 'l2',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 0.01,\n",
       "   'classifier__penalty': 'l2',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 0.1,\n",
       "   'classifier__penalty': 'l2',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 1.0,\n",
       "   'classifier__penalty': 'l2',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 10.0,\n",
       "   'classifier__penalty': 'l2',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 100.0,\n",
       "   'classifier__penalty': 'l2',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 1000.0,\n",
       "   'classifier__penalty': 'l2',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 10000.0,\n",
       "   'classifier__penalty': 'l2',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__penalty': 'none',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__penalty': 'none',\n",
       "   'classifier__solver': 'saga'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000)}],\n",
       " 'split0_test_accuracy': array([0.598, 0.69 , 0.682, 0.746, 0.749, 0.75 , 0.748, 0.75 , 0.751,\n",
       "        0.751, 0.751, 0.751, 0.751, 0.751, 0.751, 0.751, 0.751, 0.751,\n",
       "        0.69 , 0.746, 0.75 , 0.75 , 0.751, 0.751, 0.751, 0.751, 0.751,\n",
       "        0.751, 0.751, 0.751]),\n",
       " 'split1_test_accuracy': array([0.598, 0.683, 0.695, 0.739, 0.742, 0.746, 0.745, 0.744, 0.745,\n",
       "        0.745, 0.745, 0.745, 0.745, 0.745, 0.745, 0.745, 0.745, 0.745,\n",
       "        0.683, 0.739, 0.746, 0.744, 0.745, 0.745, 0.745, 0.745, 0.745,\n",
       "        0.745, 0.745, 0.745]),\n",
       " 'split2_test_accuracy': array([0.598, 0.698, 0.674, 0.72 , 0.722, 0.723, 0.726, 0.731, 0.731,\n",
       "        0.732, 0.732, 0.732, 0.732, 0.732, 0.732, 0.732, 0.732, 0.732,\n",
       "        0.698, 0.72 , 0.723, 0.731, 0.732, 0.732, 0.732, 0.732, 0.732,\n",
       "        0.732, 0.732, 0.732]),\n",
       " 'split3_test_accuracy': array([0.598, 0.691, 0.687, 0.733, 0.739, 0.739, 0.738, 0.736, 0.737,\n",
       "        0.737, 0.737, 0.737, 0.737, 0.737, 0.737, 0.737, 0.737, 0.737,\n",
       "        0.691, 0.733, 0.739, 0.736, 0.737, 0.737, 0.737, 0.737, 0.737,\n",
       "        0.737, 0.737, 0.737]),\n",
       " 'split4_test_accuracy': array([0.598, 0.701, 0.684, 0.729, 0.74 , 0.748, 0.751, 0.749, 0.749,\n",
       "        0.749, 0.749, 0.749, 0.749, 0.749, 0.749, 0.749, 0.749, 0.749,\n",
       "        0.701, 0.729, 0.748, 0.749, 0.749, 0.749, 0.749, 0.749, 0.749,\n",
       "        0.749, 0.749, 0.749]),\n",
       " 'split5_test_accuracy': array([0.598, 0.719, 0.698, 0.734, 0.729, 0.733, 0.735, 0.733, 0.733,\n",
       "        0.733, 0.733, 0.733, 0.733, 0.733, 0.733, 0.733, 0.733, 0.733,\n",
       "        0.719, 0.734, 0.733, 0.733, 0.733, 0.733, 0.733, 0.733, 0.733,\n",
       "        0.733, 0.733, 0.733]),\n",
       " 'split6_test_accuracy': array([0.598, 0.678, 0.678, 0.724, 0.747, 0.737, 0.738, 0.739, 0.738,\n",
       "        0.738, 0.738, 0.738, 0.738, 0.738, 0.738, 0.738, 0.738, 0.738,\n",
       "        0.678, 0.724, 0.737, 0.739, 0.738, 0.738, 0.738, 0.738, 0.738,\n",
       "        0.738, 0.738, 0.738]),\n",
       " 'split7_test_accuracy': array([0.598, 0.696, 0.689, 0.75 , 0.757, 0.761, 0.768, 0.765, 0.766,\n",
       "        0.765, 0.765, 0.765, 0.765, 0.765, 0.765, 0.765, 0.765, 0.765,\n",
       "        0.696, 0.75 , 0.761, 0.765, 0.765, 0.765, 0.765, 0.765, 0.765,\n",
       "        0.765, 0.765, 0.765]),\n",
       " 'split8_test_accuracy': array([0.598, 0.698, 0.68 , 0.746, 0.753, 0.756, 0.759, 0.757, 0.758,\n",
       "        0.756, 0.757, 0.757, 0.757, 0.757, 0.757, 0.757, 0.757, 0.757,\n",
       "        0.698, 0.746, 0.756, 0.757, 0.756, 0.757, 0.757, 0.757, 0.757,\n",
       "        0.757, 0.757, 0.756]),\n",
       " 'split9_test_accuracy': array([0.597, 0.704, 0.684, 0.762, 0.756, 0.756, 0.76 , 0.761, 0.761,\n",
       "        0.761, 0.761, 0.761, 0.761, 0.761, 0.761, 0.761, 0.761, 0.761,\n",
       "        0.704, 0.762, 0.756, 0.761, 0.761, 0.761, 0.761, 0.761, 0.761,\n",
       "        0.761, 0.761, 0.761]),\n",
       " 'mean_test_accuracy': array([0.5979, 0.6958, 0.6851, 0.7383, 0.7434, 0.7449, 0.7468, 0.7465,\n",
       "        0.7469, 0.7467, 0.7468, 0.7468, 0.7468, 0.7468, 0.7468, 0.7468,\n",
       "        0.7468, 0.7468, 0.6958, 0.7383, 0.7449, 0.7465, 0.7467, 0.7468,\n",
       "        0.7468, 0.7468, 0.7468, 0.7468, 0.7468, 0.7467]),\n",
       " 'std_test_accuracy': array([0.0003    , 0.01086094, 0.00703491, 0.01220697, 0.01085541,\n",
       "        0.01122898, 0.01233532, 0.01129823, 0.01155379, 0.01107294,\n",
       "        0.01116065, 0.01116065, 0.01116065, 0.01116065, 0.01116065,\n",
       "        0.01116065, 0.01116065, 0.01116065, 0.01086094, 0.01220697,\n",
       "        0.01122898, 0.01129823, 0.01107294, 0.01116065, 0.01116065,\n",
       "        0.01116065, 0.01116065, 0.01116065, 0.01116065, 0.01107294]),\n",
       " 'rank_test_accuracy': array([30, 27, 29, 25, 24, 22,  2, 20,  1, 17,  2,  2,  2,  2,  2,  2,  2,\n",
       "         2, 27, 25, 22, 20, 17,  2,  2,  2,  2,  2,  2, 17]),\n",
       " 'split0_test_precision': array([0.        , 0.73958333, 0.72826087, 0.7032967 , 0.69920844,\n",
       "        0.69587629, 0.69430052, 0.69487179, 0.69565217, 0.69565217,\n",
       "        0.69565217, 0.69565217, 0.69565217, 0.69565217, 0.69565217,\n",
       "        0.69565217, 0.69565217, 0.69565217, 0.73958333, 0.7032967 ,\n",
       "        0.69587629, 0.69487179, 0.69565217, 0.69565217, 0.69565217,\n",
       "        0.69565217, 0.69565217, 0.69565217, 0.69565217, 0.69565217]),\n",
       " 'split1_test_precision': array([0.        , 0.74566474, 0.78362573, 0.71559633, 0.70809249,\n",
       "        0.70903955, 0.70473538, 0.70391061, 0.70588235, 0.70588235,\n",
       "        0.70588235, 0.70588235, 0.70588235, 0.70588235, 0.70588235,\n",
       "        0.70588235, 0.70588235, 0.70588235, 0.74566474, 0.71559633,\n",
       "        0.70903955, 0.70391061, 0.70588235, 0.70588235, 0.70588235,\n",
       "        0.70588235, 0.70588235, 0.70588235, 0.70588235, 0.70588235]),\n",
       " 'split2_test_precision': array([0.        , 0.76595745, 0.72619048, 0.67329545, 0.66315789,\n",
       "        0.65903308, 0.66243655, 0.66835443, 0.66835443, 0.66919192,\n",
       "        0.66919192, 0.66919192, 0.66919192, 0.66919192, 0.66919192,\n",
       "        0.66919192, 0.66919192, 0.66919192, 0.76595745, 0.67329545,\n",
       "        0.65903308, 0.66835443, 0.66919192, 0.66919192, 0.66919192,\n",
       "        0.66919192, 0.66919192, 0.66919192, 0.66919192, 0.66919192]),\n",
       " 'split3_test_precision': array([0.        , 0.71627907, 0.72588832, 0.68802228, 0.68311688,\n",
       "        0.68311688, 0.68041237, 0.67783505, 0.67866324, 0.67866324,\n",
       "        0.67866324, 0.67866324, 0.67866324, 0.67866324, 0.67866324,\n",
       "        0.67866324, 0.67866324, 0.67866324, 0.71627907, 0.68802228,\n",
       "        0.68311688, 0.67783505, 0.67866324, 0.67866324, 0.67866324,\n",
       "        0.67866324, 0.67866324, 0.67866324, 0.67866324, 0.67866324]),\n",
       " 'split4_test_precision': array([0.        , 0.75879397, 0.71078431, 0.68144044, 0.69086022,\n",
       "        0.6984127 , 0.7007874 , 0.69712794, 0.69712794, 0.69712794,\n",
       "        0.69712794, 0.69712794, 0.69712794, 0.69712794, 0.69712794,\n",
       "        0.69712794, 0.69712794, 0.69712794, 0.75879397, 0.68144044,\n",
       "        0.6984127 , 0.69712794, 0.69712794, 0.69712794, 0.69712794,\n",
       "        0.69712794, 0.69712794, 0.69712794, 0.69712794, 0.69712794]),\n",
       " 'split5_test_precision': array([0.        , 0.80099502, 0.76315789, 0.69653179, 0.67654987,\n",
       "        0.67716535, 0.67885117, 0.67532468, 0.67532468, 0.67532468,\n",
       "        0.67532468, 0.67532468, 0.67532468, 0.67532468, 0.67532468,\n",
       "        0.67532468, 0.67532468, 0.67532468, 0.80099502, 0.69653179,\n",
       "        0.67716535, 0.67532468, 0.67532468, 0.67532468, 0.67532468,\n",
       "        0.67532468, 0.67532468, 0.67532468, 0.67532468, 0.67532468]),\n",
       " 'split6_test_precision': array([0.        , 0.74390244, 0.74096386, 0.69325153, 0.72106825,\n",
       "        0.70144928, 0.70348837, 0.70434783, 0.70231214, 0.70231214,\n",
       "        0.70231214, 0.70231214, 0.70231214, 0.70231214, 0.70231214,\n",
       "        0.70231214, 0.70231214, 0.70231214, 0.74390244, 0.69325153,\n",
       "        0.70144928, 0.70434783, 0.70231214, 0.70231214, 0.70231214,\n",
       "        0.70231214, 0.70231214, 0.70231214, 0.70231214, 0.70231214]),\n",
       " 'split7_test_precision': array([0.        , 0.7816092 , 0.76923077, 0.7345679 , 0.72268908,\n",
       "        0.72829132, 0.73480663, 0.73002755, 0.7320442 , 0.73002755,\n",
       "        0.73002755, 0.73002755, 0.73002755, 0.73002755, 0.73002755,\n",
       "        0.73002755, 0.73002755, 0.73002755, 0.7816092 , 0.7345679 ,\n",
       "        0.72829132, 0.73002755, 0.73002755, 0.73002755, 0.73002755,\n",
       "        0.73002755, 0.73002755, 0.73002755, 0.73002755, 0.73002755]),\n",
       " 'split8_test_precision': array([0.        , 0.73809524, 0.71354167, 0.70786517, 0.70889488,\n",
       "        0.71010638, 0.71240106, 0.7075718 , 0.70942408, 0.70572917,\n",
       "        0.70649351, 0.70649351, 0.70649351, 0.70649351, 0.70649351,\n",
       "        0.70649351, 0.70649351, 0.70649351, 0.73809524, 0.70786517,\n",
       "        0.71010638, 0.7075718 , 0.70572917, 0.70649351, 0.70649351,\n",
       "        0.70649351, 0.70649351, 0.70649351, 0.70649351, 0.70572917]),\n",
       " 'split9_test_precision': array([0.        , 0.77435897, 0.74033149, 0.73504274, 0.71428571,\n",
       "        0.71313673, 0.71618037, 0.71693122, 0.71693122, 0.71693122,\n",
       "        0.71693122, 0.71693122, 0.71693122, 0.71693122, 0.71693122,\n",
       "        0.71693122, 0.71693122, 0.71693122, 0.77435897, 0.73504274,\n",
       "        0.71313673, 0.71693122, 0.71693122, 0.71693122, 0.71693122,\n",
       "        0.71693122, 0.71693122, 0.71693122, 0.71693122, 0.71693122]),\n",
       " 'mean_test_precision': array([0.        , 0.75652394, 0.74019754, 0.70289103, 0.69879237,\n",
       "        0.69756276, 0.69883998, 0.69763029, 0.69817164, 0.69768424,\n",
       "        0.69776067, 0.69776067, 0.69776067, 0.69776067, 0.69776067,\n",
       "        0.69776067, 0.69776067, 0.69776067, 0.75652394, 0.70289103,\n",
       "        0.69756276, 0.69763029, 0.69768424, 0.69776067, 0.69776067,\n",
       "        0.69776067, 0.69776067, 0.69776067, 0.69776067, 0.69768424]),\n",
       " 'std_test_precision': array([0.        , 0.02354284, 0.02318583, 0.01978371, 0.01889238,\n",
       "        0.01898367, 0.0198073 , 0.01832986, 0.01871244, 0.01802188,\n",
       "        0.01805743, 0.01805743, 0.01805743, 0.01805743, 0.01805743,\n",
       "        0.01805743, 0.01805743, 0.01805743, 0.02354284, 0.01978371,\n",
       "        0.01898367, 0.01832986, 0.01802188, 0.01805743, 0.01805743,\n",
       "        0.01805743, 0.01805743, 0.01805743, 0.01805743, 0.01802188]),\n",
       " 'rank_test_precision': array([30,  1,  3,  4,  7, 28,  6, 26,  8, 23,  9,  9,  9,  9,  9,  9,  9,\n",
       "         9,  1,  4, 28, 26, 23,  9,  9,  9,  9,  9,  9, 23]),\n",
       " 'split0_test_f1_micro': array([0.598, 0.69 , 0.682, 0.746, 0.749, 0.75 , 0.748, 0.75 , 0.751,\n",
       "        0.751, 0.751, 0.751, 0.751, 0.751, 0.751, 0.751, 0.751, 0.751,\n",
       "        0.69 , 0.746, 0.75 , 0.75 , 0.751, 0.751, 0.751, 0.751, 0.751,\n",
       "        0.751, 0.751, 0.751]),\n",
       " 'split1_test_f1_micro': array([0.598, 0.683, 0.695, 0.739, 0.742, 0.746, 0.745, 0.744, 0.745,\n",
       "        0.745, 0.745, 0.745, 0.745, 0.745, 0.745, 0.745, 0.745, 0.745,\n",
       "        0.683, 0.739, 0.746, 0.744, 0.745, 0.745, 0.745, 0.745, 0.745,\n",
       "        0.745, 0.745, 0.745]),\n",
       " 'split2_test_f1_micro': array([0.598, 0.698, 0.674, 0.72 , 0.722, 0.723, 0.726, 0.731, 0.731,\n",
       "        0.732, 0.732, 0.732, 0.732, 0.732, 0.732, 0.732, 0.732, 0.732,\n",
       "        0.698, 0.72 , 0.723, 0.731, 0.732, 0.732, 0.732, 0.732, 0.732,\n",
       "        0.732, 0.732, 0.732]),\n",
       " 'split3_test_f1_micro': array([0.598, 0.691, 0.687, 0.733, 0.739, 0.739, 0.738, 0.736, 0.737,\n",
       "        0.737, 0.737, 0.737, 0.737, 0.737, 0.737, 0.737, 0.737, 0.737,\n",
       "        0.691, 0.733, 0.739, 0.736, 0.737, 0.737, 0.737, 0.737, 0.737,\n",
       "        0.737, 0.737, 0.737]),\n",
       " 'split4_test_f1_micro': array([0.598, 0.701, 0.684, 0.729, 0.74 , 0.748, 0.751, 0.749, 0.749,\n",
       "        0.749, 0.749, 0.749, 0.749, 0.749, 0.749, 0.749, 0.749, 0.749,\n",
       "        0.701, 0.729, 0.748, 0.749, 0.749, 0.749, 0.749, 0.749, 0.749,\n",
       "        0.749, 0.749, 0.749]),\n",
       " 'split5_test_f1_micro': array([0.598, 0.719, 0.698, 0.734, 0.729, 0.733, 0.735, 0.733, 0.733,\n",
       "        0.733, 0.733, 0.733, 0.733, 0.733, 0.733, 0.733, 0.733, 0.733,\n",
       "        0.719, 0.734, 0.733, 0.733, 0.733, 0.733, 0.733, 0.733, 0.733,\n",
       "        0.733, 0.733, 0.733]),\n",
       " 'split6_test_f1_micro': array([0.598, 0.678, 0.678, 0.724, 0.747, 0.737, 0.738, 0.739, 0.738,\n",
       "        0.738, 0.738, 0.738, 0.738, 0.738, 0.738, 0.738, 0.738, 0.738,\n",
       "        0.678, 0.724, 0.737, 0.739, 0.738, 0.738, 0.738, 0.738, 0.738,\n",
       "        0.738, 0.738, 0.738]),\n",
       " 'split7_test_f1_micro': array([0.598, 0.696, 0.689, 0.75 , 0.757, 0.761, 0.768, 0.765, 0.766,\n",
       "        0.765, 0.765, 0.765, 0.765, 0.765, 0.765, 0.765, 0.765, 0.765,\n",
       "        0.696, 0.75 , 0.761, 0.765, 0.765, 0.765, 0.765, 0.765, 0.765,\n",
       "        0.765, 0.765, 0.765]),\n",
       " 'split8_test_f1_micro': array([0.598, 0.698, 0.68 , 0.746, 0.753, 0.756, 0.759, 0.757, 0.758,\n",
       "        0.756, 0.757, 0.757, 0.757, 0.757, 0.757, 0.757, 0.757, 0.757,\n",
       "        0.698, 0.746, 0.756, 0.757, 0.756, 0.757, 0.757, 0.757, 0.757,\n",
       "        0.757, 0.757, 0.756]),\n",
       " 'split9_test_f1_micro': array([0.597, 0.704, 0.684, 0.762, 0.756, 0.756, 0.76 , 0.761, 0.761,\n",
       "        0.761, 0.761, 0.761, 0.761, 0.761, 0.761, 0.761, 0.761, 0.761,\n",
       "        0.704, 0.762, 0.756, 0.761, 0.761, 0.761, 0.761, 0.761, 0.761,\n",
       "        0.761, 0.761, 0.761]),\n",
       " 'mean_test_f1_micro': array([0.5979, 0.6958, 0.6851, 0.7383, 0.7434, 0.7449, 0.7468, 0.7465,\n",
       "        0.7469, 0.7467, 0.7468, 0.7468, 0.7468, 0.7468, 0.7468, 0.7468,\n",
       "        0.7468, 0.7468, 0.6958, 0.7383, 0.7449, 0.7465, 0.7467, 0.7468,\n",
       "        0.7468, 0.7468, 0.7468, 0.7468, 0.7468, 0.7467]),\n",
       " 'std_test_f1_micro': array([0.0003    , 0.01086094, 0.00703491, 0.01220697, 0.01085541,\n",
       "        0.01122898, 0.01233532, 0.01129823, 0.01155379, 0.01107294,\n",
       "        0.01116065, 0.01116065, 0.01116065, 0.01116065, 0.01116065,\n",
       "        0.01116065, 0.01116065, 0.01116065, 0.01086094, 0.01220697,\n",
       "        0.01122898, 0.01129823, 0.01107294, 0.01116065, 0.01116065,\n",
       "        0.01116065, 0.01116065, 0.01116065, 0.01116065, 0.01107294]),\n",
       " 'rank_test_f1_micro': array([30, 27, 29, 25, 24, 22,  2, 20,  1, 17,  2,  2,  2,  2,  2,  2,  2,\n",
       "         2, 27, 25, 22, 20, 17,  2,  2,  2,  2,  2,  2, 17])}"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_model.cv_results_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "d443f4d3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([30, 27, 29, 25, 24, 22,  2, 20,  1, 17,  2,  2,  2,  2,  2,  2,  2,\n",
       "        2, 27, 25, 22, 20, 17,  2,  2,  2,  2,  2,  2, 17])"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_model.cv_results_['rank_test_accuracy']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "4f10e207",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'classifier': LogisticRegression(max_iter=10000),\n",
       " 'classifier__C': 0.0001,\n",
       " 'classifier__penalty': 'l1',\n",
       " 'classifier__solver': 'saga'}"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_model.cv_results_['params'][ np.argmin(best_model.cv_results_['mean_test_accuracy'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "51791136",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>classifier</th>\n",
       "      <th>C</th>\n",
       "      <th>penalty</th>\n",
       "      <th>solver</th>\n",
       "      <th>score_acc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.4021</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.3042</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.3149</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.2617</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0100</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.2566</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0100</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.2551</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.1000</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.2532</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.1000</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.2535</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.2531</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.2533</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>10.0000</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.2532</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>10.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.2532</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>100.0000</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.2532</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>100.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.2532</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>1000.0000</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.2532</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>1000.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.2532</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>10000.0000</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.2532</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>10000.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.2532</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.3042</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.2617</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0100</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.2551</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.1000</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.2535</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.2533</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>10.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.2532</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>100.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.2532</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>1000.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.2532</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>10000.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.2532</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>NaN</td>\n",
       "      <td>none</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.2532</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>NaN</td>\n",
       "      <td>none</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.2532</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.2533</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                            classifier           C penalty solver  score_acc\n",
       "0   LogisticRegression(max_iter=10000)      0.0001      l1   saga     0.4021\n",
       "1   LogisticRegression(max_iter=10000)      0.0001      l2   saga     0.3042\n",
       "2   LogisticRegression(max_iter=10000)      0.0010      l1   saga     0.3149\n",
       "3   LogisticRegression(max_iter=10000)      0.0010      l2   saga     0.2617\n",
       "4   LogisticRegression(max_iter=10000)      0.0100      l1   saga     0.2566\n",
       "5   LogisticRegression(max_iter=10000)      0.0100      l2   saga     0.2551\n",
       "6   LogisticRegression(max_iter=10000)      0.1000      l1   saga     0.2532\n",
       "7   LogisticRegression(max_iter=10000)      0.1000      l2   saga     0.2535\n",
       "8   LogisticRegression(max_iter=10000)      1.0000      l1   saga     0.2531\n",
       "9   LogisticRegression(max_iter=10000)      1.0000      l2   saga     0.2533\n",
       "10  LogisticRegression(max_iter=10000)     10.0000      l1   saga     0.2532\n",
       "11  LogisticRegression(max_iter=10000)     10.0000      l2   saga     0.2532\n",
       "12  LogisticRegression(max_iter=10000)    100.0000      l1   saga     0.2532\n",
       "13  LogisticRegression(max_iter=10000)    100.0000      l2   saga     0.2532\n",
       "14  LogisticRegression(max_iter=10000)   1000.0000      l1   saga     0.2532\n",
       "15  LogisticRegression(max_iter=10000)   1000.0000      l2   saga     0.2532\n",
       "16  LogisticRegression(max_iter=10000)  10000.0000      l1   saga     0.2532\n",
       "17  LogisticRegression(max_iter=10000)  10000.0000      l2   saga     0.2532\n",
       "18  LogisticRegression(max_iter=10000)      0.0001      l2  lbfgs     0.3042\n",
       "19  LogisticRegression(max_iter=10000)      0.0010      l2  lbfgs     0.2617\n",
       "20  LogisticRegression(max_iter=10000)      0.0100      l2  lbfgs     0.2551\n",
       "21  LogisticRegression(max_iter=10000)      0.1000      l2  lbfgs     0.2535\n",
       "22  LogisticRegression(max_iter=10000)      1.0000      l2  lbfgs     0.2533\n",
       "23  LogisticRegression(max_iter=10000)     10.0000      l2  lbfgs     0.2532\n",
       "24  LogisticRegression(max_iter=10000)    100.0000      l2  lbfgs     0.2532\n",
       "25  LogisticRegression(max_iter=10000)   1000.0000      l2  lbfgs     0.2532\n",
       "26  LogisticRegression(max_iter=10000)  10000.0000      l2  lbfgs     0.2532\n",
       "27  LogisticRegression(max_iter=10000)         NaN    none  lbfgs     0.2532\n",
       "28  LogisticRegression(max_iter=10000)         NaN    none   saga     0.2532\n",
       "29  LogisticRegression(max_iter=10000)         NaN     NaN    NaN     0.2533"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results = pd.DataFrame( best_model.cv_results_['params'] )\n",
    "\n",
    "# next grab the score resulting from those parameters, add it to the data\n",
    "# score is accuracy; to display it as misclassification error we use 1 - x\n",
    "results['score_acc'] = 1 - best_model.cv_results_['mean_test_accuracy']\n",
    "\n",
    "# get rid of classifierXX in columns\n",
    "cols = results.columns.to_series().str.split('__').apply(lambda x: x[-1])\n",
    "results.columns = cols\n",
    "\n",
    "# lets show the results for the saga solver across penalty & C values\n",
    "#sns.heatmap( results.query('solver==\"saga\"').pivot('C','penalty','score_acc'),\n",
    "#             annot=True, fmt='.3f')\n",
    "\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "153b94ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "acc_list.append(results[0:1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "a7cbb487",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'classifier': LogisticRegression(max_iter=10000),\n",
       " 'classifier__C': 0.0001,\n",
       " 'classifier__penalty': 'l2',\n",
       " 'classifier__solver': 'saga'}"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_model.cv_results_['params'][ np.argmin(best_model.cv_results_['rank_test_precision'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "155a06f9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>classifier</th>\n",
       "      <th>C</th>\n",
       "      <th>penalty</th>\n",
       "      <th>solver</th>\n",
       "      <th>score_precision</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.243476</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.259802</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.297109</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0100</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.301208</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0100</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.302437</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.1000</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.301160</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.1000</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.302370</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.301828</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.302316</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>10.0000</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.302239</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>10.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.302239</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>100.0000</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.302239</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>100.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.302239</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>1000.0000</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.302239</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>1000.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.302239</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>10000.0000</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.302239</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>10000.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.302239</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.243476</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.297109</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0100</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.302437</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.1000</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.302370</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.302316</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>10.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.302239</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>100.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.302239</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>1000.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.302239</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>10000.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.302239</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>NaN</td>\n",
       "      <td>none</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.302239</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>NaN</td>\n",
       "      <td>none</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.302239</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.302316</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                            classifier           C penalty solver  \\\n",
       "0   LogisticRegression(max_iter=10000)      0.0001      l1   saga   \n",
       "1   LogisticRegression(max_iter=10000)      0.0001      l2   saga   \n",
       "2   LogisticRegression(max_iter=10000)      0.0010      l1   saga   \n",
       "3   LogisticRegression(max_iter=10000)      0.0010      l2   saga   \n",
       "4   LogisticRegression(max_iter=10000)      0.0100      l1   saga   \n",
       "5   LogisticRegression(max_iter=10000)      0.0100      l2   saga   \n",
       "6   LogisticRegression(max_iter=10000)      0.1000      l1   saga   \n",
       "7   LogisticRegression(max_iter=10000)      0.1000      l2   saga   \n",
       "8   LogisticRegression(max_iter=10000)      1.0000      l1   saga   \n",
       "9   LogisticRegression(max_iter=10000)      1.0000      l2   saga   \n",
       "10  LogisticRegression(max_iter=10000)     10.0000      l1   saga   \n",
       "11  LogisticRegression(max_iter=10000)     10.0000      l2   saga   \n",
       "12  LogisticRegression(max_iter=10000)    100.0000      l1   saga   \n",
       "13  LogisticRegression(max_iter=10000)    100.0000      l2   saga   \n",
       "14  LogisticRegression(max_iter=10000)   1000.0000      l1   saga   \n",
       "15  LogisticRegression(max_iter=10000)   1000.0000      l2   saga   \n",
       "16  LogisticRegression(max_iter=10000)  10000.0000      l1   saga   \n",
       "17  LogisticRegression(max_iter=10000)  10000.0000      l2   saga   \n",
       "18  LogisticRegression(max_iter=10000)      0.0001      l2  lbfgs   \n",
       "19  LogisticRegression(max_iter=10000)      0.0010      l2  lbfgs   \n",
       "20  LogisticRegression(max_iter=10000)      0.0100      l2  lbfgs   \n",
       "21  LogisticRegression(max_iter=10000)      0.1000      l2  lbfgs   \n",
       "22  LogisticRegression(max_iter=10000)      1.0000      l2  lbfgs   \n",
       "23  LogisticRegression(max_iter=10000)     10.0000      l2  lbfgs   \n",
       "24  LogisticRegression(max_iter=10000)    100.0000      l2  lbfgs   \n",
       "25  LogisticRegression(max_iter=10000)   1000.0000      l2  lbfgs   \n",
       "26  LogisticRegression(max_iter=10000)  10000.0000      l2  lbfgs   \n",
       "27  LogisticRegression(max_iter=10000)         NaN    none  lbfgs   \n",
       "28  LogisticRegression(max_iter=10000)         NaN    none   saga   \n",
       "29  LogisticRegression(max_iter=10000)         NaN     NaN    NaN   \n",
       "\n",
       "    score_precision  \n",
       "0          1.000000  \n",
       "1          0.243476  \n",
       "2          0.259802  \n",
       "3          0.297109  \n",
       "4          0.301208  \n",
       "5          0.302437  \n",
       "6          0.301160  \n",
       "7          0.302370  \n",
       "8          0.301828  \n",
       "9          0.302316  \n",
       "10         0.302239  \n",
       "11         0.302239  \n",
       "12         0.302239  \n",
       "13         0.302239  \n",
       "14         0.302239  \n",
       "15         0.302239  \n",
       "16         0.302239  \n",
       "17         0.302239  \n",
       "18         0.243476  \n",
       "19         0.297109  \n",
       "20         0.302437  \n",
       "21         0.302370  \n",
       "22         0.302316  \n",
       "23         0.302239  \n",
       "24         0.302239  \n",
       "25         0.302239  \n",
       "26         0.302239  \n",
       "27         0.302239  \n",
       "28         0.302239  \n",
       "29         0.302316  "
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results = pd.DataFrame( best_model.cv_results_['params'] )\n",
    "\n",
    "# next grab the score resulting from those parameters, add it to the data\n",
    "# score is accuracy; to display it as misclassification error we use 1 - x\n",
    "results['score_precision'] = 1 - best_model.cv_results_['mean_test_precision']\n",
    "\n",
    "# get rid of classifierXX in columns\n",
    "cols = results.columns.to_series().str.split('__').apply(lambda x: x[-1])\n",
    "results.columns = cols\n",
    "\n",
    "# lets show the results for the saga solver across penalty & C values\n",
    "#sns.heatmap( results.query('solver==\"saga\"').pivot('C','penalty','score_precision'),\n",
    "#             annot=True, fmt='.3f')\n",
    "\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "a4de3306",
   "metadata": {},
   "outputs": [],
   "source": [
    "roc_auc_ovr_List.append(results[1:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "58513be7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'classifier': LogisticRegression(max_iter=10000),\n",
       " 'classifier__C': 1.0,\n",
       " 'classifier__penalty': 'l1',\n",
       " 'classifier__solver': 'saga'}"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_model.cv_results_['params'][ np.argmin(best_model.cv_results_['rank_test_f1_micro'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "6f4588d3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>classifier</th>\n",
       "      <th>C</th>\n",
       "      <th>penalty</th>\n",
       "      <th>solver</th>\n",
       "      <th>score_f1_micro</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.4021</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.3042</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.3149</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.2617</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0100</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.2566</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0100</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.2551</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.1000</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.2532</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.1000</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.2535</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.2531</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.2533</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>10.0000</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.2532</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>10.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.2532</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>100.0000</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.2532</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>100.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.2532</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>1000.0000</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.2532</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>1000.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.2532</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>10000.0000</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.2532</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>10000.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.2532</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.3042</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.2617</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0100</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.2551</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.1000</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.2535</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.2533</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>10.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.2532</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>100.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.2532</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>1000.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.2532</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>10000.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.2532</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>NaN</td>\n",
       "      <td>none</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.2532</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>NaN</td>\n",
       "      <td>none</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.2532</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.2533</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                            classifier           C penalty solver  \\\n",
       "0   LogisticRegression(max_iter=10000)      0.0001      l1   saga   \n",
       "1   LogisticRegression(max_iter=10000)      0.0001      l2   saga   \n",
       "2   LogisticRegression(max_iter=10000)      0.0010      l1   saga   \n",
       "3   LogisticRegression(max_iter=10000)      0.0010      l2   saga   \n",
       "4   LogisticRegression(max_iter=10000)      0.0100      l1   saga   \n",
       "5   LogisticRegression(max_iter=10000)      0.0100      l2   saga   \n",
       "6   LogisticRegression(max_iter=10000)      0.1000      l1   saga   \n",
       "7   LogisticRegression(max_iter=10000)      0.1000      l2   saga   \n",
       "8   LogisticRegression(max_iter=10000)      1.0000      l1   saga   \n",
       "9   LogisticRegression(max_iter=10000)      1.0000      l2   saga   \n",
       "10  LogisticRegression(max_iter=10000)     10.0000      l1   saga   \n",
       "11  LogisticRegression(max_iter=10000)     10.0000      l2   saga   \n",
       "12  LogisticRegression(max_iter=10000)    100.0000      l1   saga   \n",
       "13  LogisticRegression(max_iter=10000)    100.0000      l2   saga   \n",
       "14  LogisticRegression(max_iter=10000)   1000.0000      l1   saga   \n",
       "15  LogisticRegression(max_iter=10000)   1000.0000      l2   saga   \n",
       "16  LogisticRegression(max_iter=10000)  10000.0000      l1   saga   \n",
       "17  LogisticRegression(max_iter=10000)  10000.0000      l2   saga   \n",
       "18  LogisticRegression(max_iter=10000)      0.0001      l2  lbfgs   \n",
       "19  LogisticRegression(max_iter=10000)      0.0010      l2  lbfgs   \n",
       "20  LogisticRegression(max_iter=10000)      0.0100      l2  lbfgs   \n",
       "21  LogisticRegression(max_iter=10000)      0.1000      l2  lbfgs   \n",
       "22  LogisticRegression(max_iter=10000)      1.0000      l2  lbfgs   \n",
       "23  LogisticRegression(max_iter=10000)     10.0000      l2  lbfgs   \n",
       "24  LogisticRegression(max_iter=10000)    100.0000      l2  lbfgs   \n",
       "25  LogisticRegression(max_iter=10000)   1000.0000      l2  lbfgs   \n",
       "26  LogisticRegression(max_iter=10000)  10000.0000      l2  lbfgs   \n",
       "27  LogisticRegression(max_iter=10000)         NaN    none  lbfgs   \n",
       "28  LogisticRegression(max_iter=10000)         NaN    none   saga   \n",
       "29  LogisticRegression(max_iter=10000)         NaN     NaN    NaN   \n",
       "\n",
       "    score_f1_micro  \n",
       "0           0.4021  \n",
       "1           0.3042  \n",
       "2           0.3149  \n",
       "3           0.2617  \n",
       "4           0.2566  \n",
       "5           0.2551  \n",
       "6           0.2532  \n",
       "7           0.2535  \n",
       "8           0.2531  \n",
       "9           0.2533  \n",
       "10          0.2532  \n",
       "11          0.2532  \n",
       "12          0.2532  \n",
       "13          0.2532  \n",
       "14          0.2532  \n",
       "15          0.2532  \n",
       "16          0.2532  \n",
       "17          0.2532  \n",
       "18          0.3042  \n",
       "19          0.2617  \n",
       "20          0.2551  \n",
       "21          0.2535  \n",
       "22          0.2533  \n",
       "23          0.2532  \n",
       "24          0.2532  \n",
       "25          0.2532  \n",
       "26          0.2532  \n",
       "27          0.2532  \n",
       "28          0.2532  \n",
       "29          0.2533  "
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results = pd.DataFrame( best_model.cv_results_['params'] )\n",
    "\n",
    "# next grab the score resulting from those parameters, add it to the data\n",
    "# score is accuracy; to display it as misclassification error we use 1 - x\n",
    "results['score_f1_micro'] = 1 - best_model.cv_results_['mean_test_f1_micro']\n",
    "\n",
    "# get rid of classifierXX in columns\n",
    "cols = results.columns.to_series().str.split('__').apply(lambda x: x[-1])\n",
    "results.columns = cols\n",
    "\n",
    "# lets show the results for the saga solver across penalty & C values\n",
    "#sns.heatmap( results.query('solver==\"saga\"').pivot('C','penalty','score_f1_micro'),\n",
    "#             annot=True, fmt='.3f')\n",
    "\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "4f243c59",
   "metadata": {},
   "outputs": [],
   "source": [
    "f1_micro_List.append(results[6:7])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94d1e17e",
   "metadata": {},
   "source": [
    "## Trial 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "82374d4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, Y_train, Y_test = train_test_split(X, y, train_size=10000, shuffle=True, random_state = 6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "64a330b7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Guest-All\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1469: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\Guest-All\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1469: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\Guest-All\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1469: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\Guest-All\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1469: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\Guest-All\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1469: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\Guest-All\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1469: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\Guest-All\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1469: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\Guest-All\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1469: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\Guest-All\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1469: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\Guest-All\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1469: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\Guest-All\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1182: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Guest-All\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1182: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Guest-All\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1182: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Guest-All\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1182: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Guest-All\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1182: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Guest-All\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1182: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Guest-All\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1182: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Guest-All\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1182: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Guest-All\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1182: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Guest-All\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1182: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Guest-All\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1182: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Guest-All\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1182: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Guest-All\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1182: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Guest-All\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1182: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Guest-All\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1182: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Guest-All\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1182: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Guest-All\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1182: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Guest-All\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1182: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Guest-All\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1182: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Guest-All\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1182: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Create a pipeline - RF is a stand in, we will populate the classifier part below\n",
    "pipe = Pipeline([('std', StandardScaler()),\n",
    "                 ('classifier', LogisticRegression())])\n",
    "\n",
    "# Create search space of candidate learning algorithms and their hyperparameters\n",
    "# note lbfgs can't do l1, and if you pass penalty='none' it expects no C value\n",
    "search_space = [{'classifier': [LogisticRegression(max_iter=10000)],\n",
    "                 'classifier__solver': ['saga'],\n",
    "                 'classifier__penalty': ['l1', 'l2'],\n",
    "                 'classifier__C': np.logspace(-4, 4, 9)},\n",
    "                {'classifier': [LogisticRegression(max_iter=10000)],\n",
    "                 'classifier__solver': ['lbfgs'],\n",
    "                 'classifier__penalty': ['l2'],\n",
    "                 'classifier__C': np.logspace(-4, 4, 9)},\n",
    "                {'classifier': [LogisticRegression(max_iter=10000)],\n",
    "                 'classifier__solver': ['lbfgs','saga'],\n",
    "                 'classifier__penalty': ['none']},\n",
    "                {'classifier': [LogisticRegression(max_iter=10000)]}\n",
    "                ]\n",
    "\n",
    "# Create grid search\n",
    "clf = GridSearchCV(pipe, search_space, cv=StratifiedKFold(n_splits=10),\n",
    "                   scoring=['accuracy', 'precision', 'f1_micro'], refit=False,\n",
    "                   verbose=0)\n",
    "\n",
    "# Fit grid search\n",
    "best_model = clf.fit(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "b0094308",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'mean_fit_time': array([0.01737266, 0.04388099, 0.06550457, 0.05065863, 0.06950455,\n",
       "        0.0529268 , 0.07073174, 0.04736278, 0.06015787, 0.0656549 ,\n",
       "        0.06823819, 0.04518266, 0.07253709, 0.05298262, 0.0795907 ,\n",
       "        0.04823523, 0.06614304, 0.04614034, 0.01425509, 0.01751101,\n",
       "        0.01432757, 0.01526265, 0.0152184 , 0.01628709, 0.01428044,\n",
       "        0.01482353, 0.01339905, 0.01592877, 0.05461521, 0.01631558]),\n",
       " 'std_fit_time': array([0.00472452, 0.01103916, 0.0150384 , 0.01371228, 0.01028058,\n",
       "        0.00937371, 0.01233207, 0.00983137, 0.01622413, 0.00883396,\n",
       "        0.00991749, 0.01083791, 0.01612537, 0.00680829, 0.01088086,\n",
       "        0.00938027, 0.01152216, 0.00496467, 0.00535707, 0.00421253,\n",
       "        0.00703245, 0.00726437, 0.00644241, 0.00547372, 0.00929048,\n",
       "        0.00334867, 0.004935  , 0.0071696 , 0.01089254, 0.00729176]),\n",
       " 'mean_score_time': array([0.00716038, 0.01086452, 0.0052675 , 0.00591424, 0.00627882,\n",
       "        0.00686533, 0.00439818, 0.00654185, 0.00155678, 0.00356038,\n",
       "        0.00415192, 0.00764093, 0.00758464, 0.00510166, 0.00284607,\n",
       "        0.00213306, 0.00626681, 0.00742552, 0.00619142, 0.0050925 ,\n",
       "        0.00848837, 0.00619173, 0.00715725, 0.0033916 , 0.00935879,\n",
       "        0.00724287, 0.00854847, 0.00781062, 0.00595546, 0.00555379]),\n",
       " 'std_score_time': array([0.00598313, 0.00630249, 0.00696301, 0.00673233, 0.00769068,\n",
       "        0.00716381, 0.00675786, 0.00732927, 0.00467033, 0.0062263 ,\n",
       "        0.00649176, 0.0076577 , 0.00760128, 0.0066406 , 0.00573705,\n",
       "        0.00486139, 0.00767716, 0.00753433, 0.0079799 , 0.00699843,\n",
       "        0.00819381, 0.00697562, 0.00773257, 0.00613997, 0.00688861,\n",
       "        0.00734102, 0.00739125, 0.00705713, 0.00590836, 0.00628018]),\n",
       " 'param_classifier': masked_array(data=[LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000)],\n",
       "              mask=[False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'param_classifier__C': masked_array(data=[0.0001, 0.0001, 0.001, 0.001, 0.01, 0.01, 0.1, 0.1,\n",
       "                    1.0, 1.0, 10.0, 10.0, 100.0, 100.0, 1000.0, 1000.0,\n",
       "                    10000.0, 10000.0, 0.0001, 0.001, 0.01, 0.1, 1.0, 10.0,\n",
       "                    100.0, 1000.0, 10000.0, --, --, --],\n",
       "              mask=[False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False,  True,  True,  True],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'param_classifier__penalty': masked_array(data=['l1', 'l2', 'l1', 'l2', 'l1', 'l2', 'l1', 'l2', 'l1',\n",
       "                    'l2', 'l1', 'l2', 'l1', 'l2', 'l1', 'l2', 'l1', 'l2',\n",
       "                    'l2', 'l2', 'l2', 'l2', 'l2', 'l2', 'l2', 'l2', 'l2',\n",
       "                    'none', 'none', --],\n",
       "              mask=[False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False,  True],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'param_classifier__solver': masked_array(data=['saga', 'saga', 'saga', 'saga', 'saga', 'saga', 'saga',\n",
       "                    'saga', 'saga', 'saga', 'saga', 'saga', 'saga', 'saga',\n",
       "                    'saga', 'saga', 'saga', 'saga', 'lbfgs', 'lbfgs',\n",
       "                    'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs',\n",
       "                    'lbfgs', 'lbfgs', 'saga', --],\n",
       "              mask=[False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False,  True],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'params': [{'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 0.0001,\n",
       "   'classifier__penalty': 'l1',\n",
       "   'classifier__solver': 'saga'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 0.0001,\n",
       "   'classifier__penalty': 'l2',\n",
       "   'classifier__solver': 'saga'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 0.001,\n",
       "   'classifier__penalty': 'l1',\n",
       "   'classifier__solver': 'saga'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 0.001,\n",
       "   'classifier__penalty': 'l2',\n",
       "   'classifier__solver': 'saga'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 0.01,\n",
       "   'classifier__penalty': 'l1',\n",
       "   'classifier__solver': 'saga'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 0.01,\n",
       "   'classifier__penalty': 'l2',\n",
       "   'classifier__solver': 'saga'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 0.1,\n",
       "   'classifier__penalty': 'l1',\n",
       "   'classifier__solver': 'saga'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 0.1,\n",
       "   'classifier__penalty': 'l2',\n",
       "   'classifier__solver': 'saga'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 1.0,\n",
       "   'classifier__penalty': 'l1',\n",
       "   'classifier__solver': 'saga'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 1.0,\n",
       "   'classifier__penalty': 'l2',\n",
       "   'classifier__solver': 'saga'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 10.0,\n",
       "   'classifier__penalty': 'l1',\n",
       "   'classifier__solver': 'saga'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 10.0,\n",
       "   'classifier__penalty': 'l2',\n",
       "   'classifier__solver': 'saga'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 100.0,\n",
       "   'classifier__penalty': 'l1',\n",
       "   'classifier__solver': 'saga'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 100.0,\n",
       "   'classifier__penalty': 'l2',\n",
       "   'classifier__solver': 'saga'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 1000.0,\n",
       "   'classifier__penalty': 'l1',\n",
       "   'classifier__solver': 'saga'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 1000.0,\n",
       "   'classifier__penalty': 'l2',\n",
       "   'classifier__solver': 'saga'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 10000.0,\n",
       "   'classifier__penalty': 'l1',\n",
       "   'classifier__solver': 'saga'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 10000.0,\n",
       "   'classifier__penalty': 'l2',\n",
       "   'classifier__solver': 'saga'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 0.0001,\n",
       "   'classifier__penalty': 'l2',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 0.001,\n",
       "   'classifier__penalty': 'l2',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 0.01,\n",
       "   'classifier__penalty': 'l2',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 0.1,\n",
       "   'classifier__penalty': 'l2',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 1.0,\n",
       "   'classifier__penalty': 'l2',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 10.0,\n",
       "   'classifier__penalty': 'l2',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 100.0,\n",
       "   'classifier__penalty': 'l2',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 1000.0,\n",
       "   'classifier__penalty': 'l2',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 10000.0,\n",
       "   'classifier__penalty': 'l2',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__penalty': 'none',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__penalty': 'none',\n",
       "   'classifier__solver': 'saga'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000)}],\n",
       " 'split0_test_accuracy': array([0.601, 0.709, 0.701, 0.753, 0.758, 0.764, 0.764, 0.762, 0.764,\n",
       "        0.764, 0.764, 0.764, 0.764, 0.764, 0.764, 0.764, 0.764, 0.764,\n",
       "        0.709, 0.753, 0.764, 0.762, 0.764, 0.764, 0.764, 0.764, 0.764,\n",
       "        0.764, 0.764, 0.764]),\n",
       " 'split1_test_accuracy': array([0.601, 0.707, 0.708, 0.738, 0.75 , 0.744, 0.75 , 0.748, 0.747,\n",
       "        0.748, 0.748, 0.748, 0.748, 0.748, 0.748, 0.748, 0.748, 0.748,\n",
       "        0.707, 0.738, 0.744, 0.748, 0.748, 0.748, 0.748, 0.748, 0.748,\n",
       "        0.748, 0.748, 0.748]),\n",
       " 'split2_test_accuracy': array([0.601, 0.712, 0.696, 0.743, 0.752, 0.748, 0.743, 0.741, 0.741,\n",
       "        0.741, 0.741, 0.741, 0.741, 0.741, 0.741, 0.741, 0.741, 0.741,\n",
       "        0.712, 0.743, 0.748, 0.741, 0.741, 0.741, 0.741, 0.741, 0.741,\n",
       "        0.741, 0.741, 0.741]),\n",
       " 'split3_test_accuracy': array([0.6  , 0.688, 0.69 , 0.757, 0.756, 0.764, 0.762, 0.761, 0.761,\n",
       "        0.762, 0.762, 0.762, 0.762, 0.762, 0.762, 0.762, 0.762, 0.762,\n",
       "        0.688, 0.757, 0.764, 0.761, 0.762, 0.762, 0.762, 0.762, 0.762,\n",
       "        0.762, 0.762, 0.762]),\n",
       " 'split4_test_accuracy': array([0.6  , 0.707, 0.706, 0.758, 0.769, 0.768, 0.768, 0.768, 0.768,\n",
       "        0.768, 0.768, 0.768, 0.768, 0.768, 0.768, 0.768, 0.768, 0.768,\n",
       "        0.707, 0.758, 0.768, 0.768, 0.768, 0.768, 0.768, 0.768, 0.768,\n",
       "        0.768, 0.768, 0.768]),\n",
       " 'split5_test_accuracy': array([0.6  , 0.712, 0.694, 0.736, 0.731, 0.733, 0.733, 0.733, 0.734,\n",
       "        0.733, 0.733, 0.733, 0.733, 0.733, 0.733, 0.733, 0.733, 0.733,\n",
       "        0.712, 0.736, 0.733, 0.733, 0.733, 0.733, 0.733, 0.733, 0.733,\n",
       "        0.733, 0.733, 0.733]),\n",
       " 'split6_test_accuracy': array([0.6  , 0.704, 0.686, 0.766, 0.758, 0.771, 0.773, 0.774, 0.774,\n",
       "        0.774, 0.774, 0.774, 0.774, 0.774, 0.774, 0.774, 0.774, 0.774,\n",
       "        0.704, 0.766, 0.771, 0.774, 0.774, 0.774, 0.774, 0.774, 0.774,\n",
       "        0.774, 0.774, 0.774]),\n",
       " 'split7_test_accuracy': array([0.6  , 0.689, 0.684, 0.717, 0.719, 0.725, 0.725, 0.725, 0.725,\n",
       "        0.726, 0.726, 0.726, 0.725, 0.725, 0.725, 0.725, 0.725, 0.725,\n",
       "        0.689, 0.717, 0.725, 0.725, 0.726, 0.726, 0.726, 0.726, 0.726,\n",
       "        0.726, 0.725, 0.726]),\n",
       " 'split8_test_accuracy': array([0.6  , 0.704, 0.69 , 0.731, 0.747, 0.756, 0.757, 0.755, 0.756,\n",
       "        0.756, 0.756, 0.756, 0.756, 0.756, 0.756, 0.756, 0.756, 0.756,\n",
       "        0.704, 0.731, 0.756, 0.755, 0.756, 0.756, 0.756, 0.756, 0.756,\n",
       "        0.756, 0.756, 0.756]),\n",
       " 'split9_test_accuracy': array([0.6  , 0.702, 0.686, 0.763, 0.771, 0.765, 0.765, 0.766, 0.765,\n",
       "        0.766, 0.766, 0.766, 0.766, 0.766, 0.766, 0.766, 0.766, 0.766,\n",
       "        0.702, 0.763, 0.765, 0.766, 0.766, 0.766, 0.766, 0.766, 0.766,\n",
       "        0.766, 0.766, 0.766]),\n",
       " 'mean_test_accuracy': array([0.6003, 0.7034, 0.6941, 0.7462, 0.7511, 0.7538, 0.754 , 0.7533,\n",
       "        0.7535, 0.7538, 0.7538, 0.7538, 0.7537, 0.7537, 0.7537, 0.7537,\n",
       "        0.7537, 0.7537, 0.7034, 0.7462, 0.7538, 0.7533, 0.7538, 0.7538,\n",
       "        0.7538, 0.7538, 0.7538, 0.7538, 0.7537, 0.7538]),\n",
       " 'std_test_accuracy': array([0.00045826, 0.00807713, 0.00808022, 0.01497197, 0.01512911,\n",
       "        0.01492515, 0.01506652, 0.01528431, 0.01525287, 0.01528921,\n",
       "        0.01528921, 0.01528921, 0.01547288, 0.01547288, 0.01547288,\n",
       "        0.01547288, 0.01547288, 0.01547288, 0.00807713, 0.01497197,\n",
       "        0.01492515, 0.01528431, 0.01528921, 0.01528921, 0.01528921,\n",
       "        0.01528921, 0.01528921, 0.01528921, 0.01547288, 0.01528921]),\n",
       " 'rank_test_accuracy': array([30, 27, 29, 25, 24, 12,  1, 22, 21,  2,  2,  2, 14, 14, 14, 14, 14,\n",
       "        14, 27, 25, 12, 22,  2,  2,  2,  2,  2,  2, 14,  2]),\n",
       " 'split0_test_precision': array([0.        , 0.75714286, 0.75      , 0.70994475, 0.70496084,\n",
       "        0.71059432, 0.7084399 , 0.70694087, 0.7084399 , 0.7084399 ,\n",
       "        0.7084399 , 0.7084399 , 0.7084399 , 0.7084399 , 0.7084399 ,\n",
       "        0.7084399 , 0.7084399 , 0.7084399 , 0.75714286, 0.70994475,\n",
       "        0.71059432, 0.70694087, 0.7084399 , 0.7084399 , 0.7084399 ,\n",
       "        0.7084399 , 0.7084399 , 0.7084399 , 0.7084399 , 0.7084399 ]),\n",
       " 'split1_test_precision': array([0.        , 0.79444444, 0.80571429, 0.69970845, 0.70410959,\n",
       "        0.6969697 , 0.70189702, 0.70027248, 0.69836957, 0.70027248,\n",
       "        0.70027248, 0.70027248, 0.70027248, 0.70027248, 0.70027248,\n",
       "        0.70027248, 0.70027248, 0.70027248, 0.79444444, 0.69970845,\n",
       "        0.6969697 , 0.70027248, 0.70027248, 0.70027248, 0.70027248,\n",
       "        0.70027248, 0.70027248, 0.70027248, 0.70027248, 0.70027248]),\n",
       " 'split2_test_precision': array([0.        , 0.78756477, 0.75675676, 0.70285714, 0.70350404,\n",
       "        0.69496021, 0.68882979, 0.68518519, 0.68518519, 0.68518519,\n",
       "        0.68518519, 0.68518519, 0.68518519, 0.68518519, 0.68518519,\n",
       "        0.68518519, 0.68518519, 0.68518519, 0.78756477, 0.70285714,\n",
       "        0.69496021, 0.68518519, 0.68518519, 0.68518519, 0.68518519,\n",
       "        0.68518519, 0.68518519, 0.68518519, 0.68518519, 0.68518519]),\n",
       " 'split3_test_precision': array([0.        , 0.75882353, 0.73684211, 0.73716012, 0.72285714,\n",
       "        0.72404372, 0.72131148, 0.71815718, 0.71815718, 0.71891892,\n",
       "        0.71891892, 0.71891892, 0.71891892, 0.71891892, 0.71891892,\n",
       "        0.71891892, 0.71891892, 0.71891892, 0.75882353, 0.73716012,\n",
       "        0.72404372, 0.71815718, 0.71891892, 0.71891892, 0.71891892,\n",
       "        0.71891892, 0.71891892, 0.71891892, 0.71891892, 0.71891892]),\n",
       " 'split4_test_precision': array([0.        , 0.78306878, 0.78191489, 0.71467391, 0.72295515,\n",
       "        0.71649485, 0.71319797, 0.71212121, 0.71212121, 0.71212121,\n",
       "        0.71212121, 0.71212121, 0.71212121, 0.71212121, 0.71212121,\n",
       "        0.71212121, 0.71212121, 0.71212121, 0.78306878, 0.71467391,\n",
       "        0.71649485, 0.71212121, 0.71212121, 0.71212121, 0.71212121,\n",
       "        0.71212121, 0.71212121, 0.71212121, 0.71212121, 0.71212121]),\n",
       " 'split5_test_precision': array([0.        , 0.77722772, 0.73737374, 0.6954023 , 0.67945205,\n",
       "        0.67924528, 0.67733333, 0.67733333, 0.67819149, 0.67733333,\n",
       "        0.67733333, 0.67733333, 0.67733333, 0.67733333, 0.67733333,\n",
       "        0.67733333, 0.67733333, 0.67733333, 0.77722772, 0.6954023 ,\n",
       "        0.67924528, 0.67733333, 0.67733333, 0.67733333, 0.67733333,\n",
       "        0.67733333, 0.67733333, 0.67733333, 0.67733333, 0.67733333]),\n",
       " 'split6_test_precision': array([0.        , 0.78571429, 0.74157303, 0.74702381, 0.71823204,\n",
       "        0.73170732, 0.73190349, 0.73262032, 0.73262032, 0.73262032,\n",
       "        0.73262032, 0.73262032, 0.73262032, 0.73262032, 0.73262032,\n",
       "        0.73262032, 0.73262032, 0.73262032, 0.78571429, 0.74702381,\n",
       "        0.73170732, 0.73262032, 0.73262032, 0.73262032, 0.73262032,\n",
       "        0.73262032, 0.73262032, 0.73262032, 0.73262032, 0.73262032]),\n",
       " 'split7_test_precision': array([0.        , 0.74054054, 0.74137931, 0.66386555, 0.66124661,\n",
       "        0.66404199, 0.66233766, 0.66318538, 0.66233766, 0.6640625 ,\n",
       "        0.6640625 , 0.6640625 , 0.66318538, 0.66318538, 0.66318538,\n",
       "        0.66318538, 0.66318538, 0.66318538, 0.74054054, 0.66386555,\n",
       "        0.66404199, 0.66318538, 0.6640625 , 0.6640625 , 0.6640625 ,\n",
       "        0.6640625 , 0.6640625 , 0.6640625 , 0.66318538, 0.6640625 ]),\n",
       " 'split8_test_precision': array([0.        , 0.81325301, 0.77108434, 0.70926518, 0.71810089,\n",
       "        0.72807018, 0.72622478, 0.72334294, 0.72413793, 0.72413793,\n",
       "        0.72413793, 0.72413793, 0.72413793, 0.72413793, 0.72413793,\n",
       "        0.72413793, 0.72413793, 0.72413793, 0.81325301, 0.70926518,\n",
       "        0.72807018, 0.72334294, 0.72413793, 0.72413793, 0.72413793,\n",
       "        0.72413793, 0.72413793, 0.72413793, 0.72413793, 0.72413793]),\n",
       " 'split9_test_precision': array([0.        , 0.78977273, 0.74431818, 0.74183976, 0.73170732,\n",
       "        0.72237197, 0.71883289, 0.72192513, 0.72      , 0.72074468,\n",
       "        0.72074468, 0.72074468, 0.72074468, 0.72074468, 0.72074468,\n",
       "        0.72074468, 0.72074468, 0.72074468, 0.78977273, 0.74183976,\n",
       "        0.72237197, 0.72192513, 0.72074468, 0.72074468, 0.72074468,\n",
       "        0.72074468, 0.72074468, 0.72074468, 0.72074468, 0.72074468]),\n",
       " 'mean_test_precision': array([0.        , 0.77875527, 0.75669566, 0.7121741 , 0.70671257,\n",
       "        0.70684995, 0.70503083, 0.7041084 , 0.70395604, 0.70438365,\n",
       "        0.70438365, 0.70438365, 0.70429593, 0.70429593, 0.70429593,\n",
       "        0.70429593, 0.70429593, 0.70429593, 0.77875527, 0.7121741 ,\n",
       "        0.70684995, 0.7041084 , 0.70438365, 0.70438365, 0.70438365,\n",
       "        0.70438365, 0.70438365, 0.70438365, 0.70429593, 0.70438365]),\n",
       " 'std_test_precision': array([0.        , 0.02007322, 0.02162104, 0.02365532, 0.02059618,\n",
       "        0.02131974, 0.02137137, 0.02129397, 0.02134071, 0.02117975,\n",
       "        0.02117975, 0.02117975, 0.0213477 , 0.0213477 , 0.0213477 ,\n",
       "        0.0213477 , 0.0213477 , 0.0213477 , 0.02007322, 0.02365532,\n",
       "        0.02131974, 0.02129397, 0.02117975, 0.02117975, 0.02117975,\n",
       "        0.02117975, 0.02117975, 0.02117975, 0.0213477 , 0.02117975]),\n",
       " 'rank_test_precision': array([30,  1,  3,  4,  8,  6,  9, 27, 29, 10, 10, 10, 20, 20, 20, 20, 20,\n",
       "        20,  1,  4,  6, 27, 10, 10, 10, 10, 10, 10, 20, 10]),\n",
       " 'split0_test_f1_micro': array([0.601, 0.709, 0.701, 0.753, 0.758, 0.764, 0.764, 0.762, 0.764,\n",
       "        0.764, 0.764, 0.764, 0.764, 0.764, 0.764, 0.764, 0.764, 0.764,\n",
       "        0.709, 0.753, 0.764, 0.762, 0.764, 0.764, 0.764, 0.764, 0.764,\n",
       "        0.764, 0.764, 0.764]),\n",
       " 'split1_test_f1_micro': array([0.601, 0.707, 0.708, 0.738, 0.75 , 0.744, 0.75 , 0.748, 0.747,\n",
       "        0.748, 0.748, 0.748, 0.748, 0.748, 0.748, 0.748, 0.748, 0.748,\n",
       "        0.707, 0.738, 0.744, 0.748, 0.748, 0.748, 0.748, 0.748, 0.748,\n",
       "        0.748, 0.748, 0.748]),\n",
       " 'split2_test_f1_micro': array([0.601, 0.712, 0.696, 0.743, 0.752, 0.748, 0.743, 0.741, 0.741,\n",
       "        0.741, 0.741, 0.741, 0.741, 0.741, 0.741, 0.741, 0.741, 0.741,\n",
       "        0.712, 0.743, 0.748, 0.741, 0.741, 0.741, 0.741, 0.741, 0.741,\n",
       "        0.741, 0.741, 0.741]),\n",
       " 'split3_test_f1_micro': array([0.6  , 0.688, 0.69 , 0.757, 0.756, 0.764, 0.762, 0.761, 0.761,\n",
       "        0.762, 0.762, 0.762, 0.762, 0.762, 0.762, 0.762, 0.762, 0.762,\n",
       "        0.688, 0.757, 0.764, 0.761, 0.762, 0.762, 0.762, 0.762, 0.762,\n",
       "        0.762, 0.762, 0.762]),\n",
       " 'split4_test_f1_micro': array([0.6  , 0.707, 0.706, 0.758, 0.769, 0.768, 0.768, 0.768, 0.768,\n",
       "        0.768, 0.768, 0.768, 0.768, 0.768, 0.768, 0.768, 0.768, 0.768,\n",
       "        0.707, 0.758, 0.768, 0.768, 0.768, 0.768, 0.768, 0.768, 0.768,\n",
       "        0.768, 0.768, 0.768]),\n",
       " 'split5_test_f1_micro': array([0.6  , 0.712, 0.694, 0.736, 0.731, 0.733, 0.733, 0.733, 0.734,\n",
       "        0.733, 0.733, 0.733, 0.733, 0.733, 0.733, 0.733, 0.733, 0.733,\n",
       "        0.712, 0.736, 0.733, 0.733, 0.733, 0.733, 0.733, 0.733, 0.733,\n",
       "        0.733, 0.733, 0.733]),\n",
       " 'split6_test_f1_micro': array([0.6  , 0.704, 0.686, 0.766, 0.758, 0.771, 0.773, 0.774, 0.774,\n",
       "        0.774, 0.774, 0.774, 0.774, 0.774, 0.774, 0.774, 0.774, 0.774,\n",
       "        0.704, 0.766, 0.771, 0.774, 0.774, 0.774, 0.774, 0.774, 0.774,\n",
       "        0.774, 0.774, 0.774]),\n",
       " 'split7_test_f1_micro': array([0.6  , 0.689, 0.684, 0.717, 0.719, 0.725, 0.725, 0.725, 0.725,\n",
       "        0.726, 0.726, 0.726, 0.725, 0.725, 0.725, 0.725, 0.725, 0.725,\n",
       "        0.689, 0.717, 0.725, 0.725, 0.726, 0.726, 0.726, 0.726, 0.726,\n",
       "        0.726, 0.725, 0.726]),\n",
       " 'split8_test_f1_micro': array([0.6  , 0.704, 0.69 , 0.731, 0.747, 0.756, 0.757, 0.755, 0.756,\n",
       "        0.756, 0.756, 0.756, 0.756, 0.756, 0.756, 0.756, 0.756, 0.756,\n",
       "        0.704, 0.731, 0.756, 0.755, 0.756, 0.756, 0.756, 0.756, 0.756,\n",
       "        0.756, 0.756, 0.756]),\n",
       " 'split9_test_f1_micro': array([0.6  , 0.702, 0.686, 0.763, 0.771, 0.765, 0.765, 0.766, 0.765,\n",
       "        0.766, 0.766, 0.766, 0.766, 0.766, 0.766, 0.766, 0.766, 0.766,\n",
       "        0.702, 0.763, 0.765, 0.766, 0.766, 0.766, 0.766, 0.766, 0.766,\n",
       "        0.766, 0.766, 0.766]),\n",
       " 'mean_test_f1_micro': array([0.6003, 0.7034, 0.6941, 0.7462, 0.7511, 0.7538, 0.754 , 0.7533,\n",
       "        0.7535, 0.7538, 0.7538, 0.7538, 0.7537, 0.7537, 0.7537, 0.7537,\n",
       "        0.7537, 0.7537, 0.7034, 0.7462, 0.7538, 0.7533, 0.7538, 0.7538,\n",
       "        0.7538, 0.7538, 0.7538, 0.7538, 0.7537, 0.7538]),\n",
       " 'std_test_f1_micro': array([0.00045826, 0.00807713, 0.00808022, 0.01497197, 0.01512911,\n",
       "        0.01492515, 0.01506652, 0.01528431, 0.01525287, 0.01528921,\n",
       "        0.01528921, 0.01528921, 0.01547288, 0.01547288, 0.01547288,\n",
       "        0.01547288, 0.01547288, 0.01547288, 0.00807713, 0.01497197,\n",
       "        0.01492515, 0.01528431, 0.01528921, 0.01528921, 0.01528921,\n",
       "        0.01528921, 0.01528921, 0.01528921, 0.01547288, 0.01528921]),\n",
       " 'rank_test_f1_micro': array([30, 27, 29, 25, 24, 12,  1, 22, 21,  2,  2,  2, 14, 14, 14, 14, 14,\n",
       "        14, 27, 25, 12, 22,  2,  2,  2,  2,  2,  2, 14,  2])}"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_model.cv_results_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "50a53953",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([30, 27, 29, 25, 24, 12,  1, 22, 21,  2,  2,  2, 14, 14, 14, 14, 14,\n",
       "       14, 27, 25, 12, 22,  2,  2,  2,  2,  2,  2, 14,  2])"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_model.cv_results_['rank_test_accuracy']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "ed852058",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'classifier': LogisticRegression(max_iter=10000),\n",
       " 'classifier__C': 0.0001,\n",
       " 'classifier__penalty': 'l1',\n",
       " 'classifier__solver': 'saga'}"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_model.cv_results_['params'][ np.argmin(best_model.cv_results_['mean_test_accuracy'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "cb83803b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>classifier</th>\n",
       "      <th>C</th>\n",
       "      <th>penalty</th>\n",
       "      <th>solver</th>\n",
       "      <th>score_acc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.3997</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.2966</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.3059</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.2538</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0100</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.2489</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0100</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.2462</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.1000</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.2460</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.1000</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.2467</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.2465</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.2462</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>10.0000</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.2462</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>10.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.2462</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>100.0000</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.2463</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>100.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.2463</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>1000.0000</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.2463</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>1000.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.2463</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>10000.0000</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.2463</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>10000.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.2463</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.2966</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.2538</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0100</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.2462</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.1000</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.2467</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.2462</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>10.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.2462</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>100.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.2462</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>1000.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.2462</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>10000.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.2462</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>NaN</td>\n",
       "      <td>none</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.2462</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>NaN</td>\n",
       "      <td>none</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.2463</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.2462</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                            classifier           C penalty solver  score_acc\n",
       "0   LogisticRegression(max_iter=10000)      0.0001      l1   saga     0.3997\n",
       "1   LogisticRegression(max_iter=10000)      0.0001      l2   saga     0.2966\n",
       "2   LogisticRegression(max_iter=10000)      0.0010      l1   saga     0.3059\n",
       "3   LogisticRegression(max_iter=10000)      0.0010      l2   saga     0.2538\n",
       "4   LogisticRegression(max_iter=10000)      0.0100      l1   saga     0.2489\n",
       "5   LogisticRegression(max_iter=10000)      0.0100      l2   saga     0.2462\n",
       "6   LogisticRegression(max_iter=10000)      0.1000      l1   saga     0.2460\n",
       "7   LogisticRegression(max_iter=10000)      0.1000      l2   saga     0.2467\n",
       "8   LogisticRegression(max_iter=10000)      1.0000      l1   saga     0.2465\n",
       "9   LogisticRegression(max_iter=10000)      1.0000      l2   saga     0.2462\n",
       "10  LogisticRegression(max_iter=10000)     10.0000      l1   saga     0.2462\n",
       "11  LogisticRegression(max_iter=10000)     10.0000      l2   saga     0.2462\n",
       "12  LogisticRegression(max_iter=10000)    100.0000      l1   saga     0.2463\n",
       "13  LogisticRegression(max_iter=10000)    100.0000      l2   saga     0.2463\n",
       "14  LogisticRegression(max_iter=10000)   1000.0000      l1   saga     0.2463\n",
       "15  LogisticRegression(max_iter=10000)   1000.0000      l2   saga     0.2463\n",
       "16  LogisticRegression(max_iter=10000)  10000.0000      l1   saga     0.2463\n",
       "17  LogisticRegression(max_iter=10000)  10000.0000      l2   saga     0.2463\n",
       "18  LogisticRegression(max_iter=10000)      0.0001      l2  lbfgs     0.2966\n",
       "19  LogisticRegression(max_iter=10000)      0.0010      l2  lbfgs     0.2538\n",
       "20  LogisticRegression(max_iter=10000)      0.0100      l2  lbfgs     0.2462\n",
       "21  LogisticRegression(max_iter=10000)      0.1000      l2  lbfgs     0.2467\n",
       "22  LogisticRegression(max_iter=10000)      1.0000      l2  lbfgs     0.2462\n",
       "23  LogisticRegression(max_iter=10000)     10.0000      l2  lbfgs     0.2462\n",
       "24  LogisticRegression(max_iter=10000)    100.0000      l2  lbfgs     0.2462\n",
       "25  LogisticRegression(max_iter=10000)   1000.0000      l2  lbfgs     0.2462\n",
       "26  LogisticRegression(max_iter=10000)  10000.0000      l2  lbfgs     0.2462\n",
       "27  LogisticRegression(max_iter=10000)         NaN    none  lbfgs     0.2462\n",
       "28  LogisticRegression(max_iter=10000)         NaN    none   saga     0.2463\n",
       "29  LogisticRegression(max_iter=10000)         NaN     NaN    NaN     0.2462"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results = pd.DataFrame( best_model.cv_results_['params'] )\n",
    "\n",
    "# next grab the score resulting from those parameters, add it to the data\n",
    "# score is accuracy; to display it as misclassification error we use 1 - x\n",
    "results['score_acc'] = 1 - best_model.cv_results_['mean_test_accuracy']\n",
    "\n",
    "# get rid of classifierXX in columns\n",
    "cols = results.columns.to_series().str.split('__').apply(lambda x: x[-1])\n",
    "results.columns = cols\n",
    "\n",
    "# lets show the results for the saga solver across penalty & C values\n",
    "#sns.heatmap( results.query('solver==\"saga\"').pivot('C','penalty','score_acc'),\n",
    "#             annot=True, fmt='.3f')\n",
    "\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "e5605c9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "acc_list.append(results[0:1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "687c473b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'classifier': LogisticRegression(max_iter=10000),\n",
       " 'classifier__C': 0.0001,\n",
       " 'classifier__penalty': 'l2',\n",
       " 'classifier__solver': 'saga'}"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_model.cv_results_['params'][ np.argmin(best_model.cv_results_['rank_test_precision'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "f9394193",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>classifier</th>\n",
       "      <th>C</th>\n",
       "      <th>penalty</th>\n",
       "      <th>solver</th>\n",
       "      <th>score_precision</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.221245</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.243304</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.287826</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0100</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.293287</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0100</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.293150</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.1000</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.294969</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.1000</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.295892</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.296044</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.295616</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>10.0000</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.295616</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>10.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.295616</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>100.0000</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.295704</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>100.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.295704</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>1000.0000</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.295704</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>1000.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.295704</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>10000.0000</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.295704</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>10000.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.295704</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.221245</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.287826</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0100</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.293150</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.1000</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.295892</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.295616</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>10.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.295616</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>100.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.295616</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>1000.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.295616</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>10000.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.295616</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>NaN</td>\n",
       "      <td>none</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.295616</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>NaN</td>\n",
       "      <td>none</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.295704</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.295616</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                            classifier           C penalty solver  \\\n",
       "0   LogisticRegression(max_iter=10000)      0.0001      l1   saga   \n",
       "1   LogisticRegression(max_iter=10000)      0.0001      l2   saga   \n",
       "2   LogisticRegression(max_iter=10000)      0.0010      l1   saga   \n",
       "3   LogisticRegression(max_iter=10000)      0.0010      l2   saga   \n",
       "4   LogisticRegression(max_iter=10000)      0.0100      l1   saga   \n",
       "5   LogisticRegression(max_iter=10000)      0.0100      l2   saga   \n",
       "6   LogisticRegression(max_iter=10000)      0.1000      l1   saga   \n",
       "7   LogisticRegression(max_iter=10000)      0.1000      l2   saga   \n",
       "8   LogisticRegression(max_iter=10000)      1.0000      l1   saga   \n",
       "9   LogisticRegression(max_iter=10000)      1.0000      l2   saga   \n",
       "10  LogisticRegression(max_iter=10000)     10.0000      l1   saga   \n",
       "11  LogisticRegression(max_iter=10000)     10.0000      l2   saga   \n",
       "12  LogisticRegression(max_iter=10000)    100.0000      l1   saga   \n",
       "13  LogisticRegression(max_iter=10000)    100.0000      l2   saga   \n",
       "14  LogisticRegression(max_iter=10000)   1000.0000      l1   saga   \n",
       "15  LogisticRegression(max_iter=10000)   1000.0000      l2   saga   \n",
       "16  LogisticRegression(max_iter=10000)  10000.0000      l1   saga   \n",
       "17  LogisticRegression(max_iter=10000)  10000.0000      l2   saga   \n",
       "18  LogisticRegression(max_iter=10000)      0.0001      l2  lbfgs   \n",
       "19  LogisticRegression(max_iter=10000)      0.0010      l2  lbfgs   \n",
       "20  LogisticRegression(max_iter=10000)      0.0100      l2  lbfgs   \n",
       "21  LogisticRegression(max_iter=10000)      0.1000      l2  lbfgs   \n",
       "22  LogisticRegression(max_iter=10000)      1.0000      l2  lbfgs   \n",
       "23  LogisticRegression(max_iter=10000)     10.0000      l2  lbfgs   \n",
       "24  LogisticRegression(max_iter=10000)    100.0000      l2  lbfgs   \n",
       "25  LogisticRegression(max_iter=10000)   1000.0000      l2  lbfgs   \n",
       "26  LogisticRegression(max_iter=10000)  10000.0000      l2  lbfgs   \n",
       "27  LogisticRegression(max_iter=10000)         NaN    none  lbfgs   \n",
       "28  LogisticRegression(max_iter=10000)         NaN    none   saga   \n",
       "29  LogisticRegression(max_iter=10000)         NaN     NaN    NaN   \n",
       "\n",
       "    score_precision  \n",
       "0          1.000000  \n",
       "1          0.221245  \n",
       "2          0.243304  \n",
       "3          0.287826  \n",
       "4          0.293287  \n",
       "5          0.293150  \n",
       "6          0.294969  \n",
       "7          0.295892  \n",
       "8          0.296044  \n",
       "9          0.295616  \n",
       "10         0.295616  \n",
       "11         0.295616  \n",
       "12         0.295704  \n",
       "13         0.295704  \n",
       "14         0.295704  \n",
       "15         0.295704  \n",
       "16         0.295704  \n",
       "17         0.295704  \n",
       "18         0.221245  \n",
       "19         0.287826  \n",
       "20         0.293150  \n",
       "21         0.295892  \n",
       "22         0.295616  \n",
       "23         0.295616  \n",
       "24         0.295616  \n",
       "25         0.295616  \n",
       "26         0.295616  \n",
       "27         0.295616  \n",
       "28         0.295704  \n",
       "29         0.295616  "
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results = pd.DataFrame( best_model.cv_results_['params'] )\n",
    "\n",
    "# next grab the score resulting from those parameters, add it to the data\n",
    "# score is accuracy; to display it as misclassification error we use 1 - x\n",
    "results['score_precision'] = 1 - best_model.cv_results_['mean_test_precision']\n",
    "\n",
    "# get rid of classifierXX in columns\n",
    "cols = results.columns.to_series().str.split('__').apply(lambda x: x[-1])\n",
    "results.columns = cols\n",
    "\n",
    "# lets show the results for the saga solver across penalty & C values\n",
    "#sns.heatmap( results.query('solver==\"saga\"').pivot('C','penalty','score_precision'),\n",
    "#             annot=True, fmt='.3f')\n",
    "\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "f6cdf507",
   "metadata": {},
   "outputs": [],
   "source": [
    "roc_auc_ovr_List.append(results[1:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "342313d7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'classifier': LogisticRegression(max_iter=10000),\n",
       " 'classifier__C': 0.1,\n",
       " 'classifier__penalty': 'l1',\n",
       " 'classifier__solver': 'saga'}"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_model.cv_results_['params'][ np.argmin(best_model.cv_results_['rank_test_f1_micro'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "2d1e6c10",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>classifier</th>\n",
       "      <th>C</th>\n",
       "      <th>penalty</th>\n",
       "      <th>solver</th>\n",
       "      <th>score_f1_micro</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.3997</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.2966</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.3059</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.2538</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0100</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.2489</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0100</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.2462</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.1000</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.2460</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.1000</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.2467</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.2465</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.2462</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>10.0000</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.2462</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>10.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.2462</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>100.0000</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.2463</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>100.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.2463</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>1000.0000</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.2463</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>1000.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.2463</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>10000.0000</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.2463</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>10000.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.2463</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.2966</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.2538</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0100</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.2462</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.1000</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.2467</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.2462</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>10.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.2462</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>100.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.2462</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>1000.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.2462</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>10000.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.2462</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>NaN</td>\n",
       "      <td>none</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.2462</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>NaN</td>\n",
       "      <td>none</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.2463</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.2462</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                            classifier           C penalty solver  \\\n",
       "0   LogisticRegression(max_iter=10000)      0.0001      l1   saga   \n",
       "1   LogisticRegression(max_iter=10000)      0.0001      l2   saga   \n",
       "2   LogisticRegression(max_iter=10000)      0.0010      l1   saga   \n",
       "3   LogisticRegression(max_iter=10000)      0.0010      l2   saga   \n",
       "4   LogisticRegression(max_iter=10000)      0.0100      l1   saga   \n",
       "5   LogisticRegression(max_iter=10000)      0.0100      l2   saga   \n",
       "6   LogisticRegression(max_iter=10000)      0.1000      l1   saga   \n",
       "7   LogisticRegression(max_iter=10000)      0.1000      l2   saga   \n",
       "8   LogisticRegression(max_iter=10000)      1.0000      l1   saga   \n",
       "9   LogisticRegression(max_iter=10000)      1.0000      l2   saga   \n",
       "10  LogisticRegression(max_iter=10000)     10.0000      l1   saga   \n",
       "11  LogisticRegression(max_iter=10000)     10.0000      l2   saga   \n",
       "12  LogisticRegression(max_iter=10000)    100.0000      l1   saga   \n",
       "13  LogisticRegression(max_iter=10000)    100.0000      l2   saga   \n",
       "14  LogisticRegression(max_iter=10000)   1000.0000      l1   saga   \n",
       "15  LogisticRegression(max_iter=10000)   1000.0000      l2   saga   \n",
       "16  LogisticRegression(max_iter=10000)  10000.0000      l1   saga   \n",
       "17  LogisticRegression(max_iter=10000)  10000.0000      l2   saga   \n",
       "18  LogisticRegression(max_iter=10000)      0.0001      l2  lbfgs   \n",
       "19  LogisticRegression(max_iter=10000)      0.0010      l2  lbfgs   \n",
       "20  LogisticRegression(max_iter=10000)      0.0100      l2  lbfgs   \n",
       "21  LogisticRegression(max_iter=10000)      0.1000      l2  lbfgs   \n",
       "22  LogisticRegression(max_iter=10000)      1.0000      l2  lbfgs   \n",
       "23  LogisticRegression(max_iter=10000)     10.0000      l2  lbfgs   \n",
       "24  LogisticRegression(max_iter=10000)    100.0000      l2  lbfgs   \n",
       "25  LogisticRegression(max_iter=10000)   1000.0000      l2  lbfgs   \n",
       "26  LogisticRegression(max_iter=10000)  10000.0000      l2  lbfgs   \n",
       "27  LogisticRegression(max_iter=10000)         NaN    none  lbfgs   \n",
       "28  LogisticRegression(max_iter=10000)         NaN    none   saga   \n",
       "29  LogisticRegression(max_iter=10000)         NaN     NaN    NaN   \n",
       "\n",
       "    score_f1_micro  \n",
       "0           0.3997  \n",
       "1           0.2966  \n",
       "2           0.3059  \n",
       "3           0.2538  \n",
       "4           0.2489  \n",
       "5           0.2462  \n",
       "6           0.2460  \n",
       "7           0.2467  \n",
       "8           0.2465  \n",
       "9           0.2462  \n",
       "10          0.2462  \n",
       "11          0.2462  \n",
       "12          0.2463  \n",
       "13          0.2463  \n",
       "14          0.2463  \n",
       "15          0.2463  \n",
       "16          0.2463  \n",
       "17          0.2463  \n",
       "18          0.2966  \n",
       "19          0.2538  \n",
       "20          0.2462  \n",
       "21          0.2467  \n",
       "22          0.2462  \n",
       "23          0.2462  \n",
       "24          0.2462  \n",
       "25          0.2462  \n",
       "26          0.2462  \n",
       "27          0.2462  \n",
       "28          0.2463  \n",
       "29          0.2462  "
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results = pd.DataFrame( best_model.cv_results_['params'] )\n",
    "\n",
    "# next grab the score resulting from those parameters, add it to the data\n",
    "# score is accuracy; to display it as misclassification error we use 1 - x\n",
    "results['score_f1_micro'] = 1 - best_model.cv_results_['mean_test_f1_micro']\n",
    "\n",
    "# get rid of classifierXX in columns\n",
    "cols = results.columns.to_series().str.split('__').apply(lambda x: x[-1])\n",
    "results.columns = cols\n",
    "\n",
    "# lets show the results for the saga solver across penalty & C values\n",
    "#sns.heatmap( results.query('solver==\"saga\"').pivot('C','penalty','score_f1_micro'),\n",
    "#             annot=True, fmt='.3f')\n",
    "\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "cd4d8e65",
   "metadata": {},
   "outputs": [],
   "source": [
    "f1_micro_List.append(results[6:7])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c967a56e",
   "metadata": {},
   "source": [
    "## Trial 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "1542f63b",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, Y_train, Y_test = train_test_split(X, y, train_size=10000, shuffle=True, random_state = 7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "5380e5ce",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Guest-All\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1469: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\Guest-All\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1469: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\Guest-All\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1469: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\Guest-All\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1469: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\Guest-All\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1469: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\Guest-All\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1469: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\Guest-All\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1469: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\Guest-All\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1469: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\Guest-All\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1469: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\Guest-All\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1469: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\Guest-All\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1182: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Guest-All\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1182: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Guest-All\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1182: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Guest-All\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1182: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Guest-All\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1182: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Guest-All\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1182: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Guest-All\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1182: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Guest-All\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1182: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Guest-All\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1182: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Guest-All\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1182: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Guest-All\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1182: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Guest-All\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1182: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Guest-All\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1182: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Guest-All\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1182: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Guest-All\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1182: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Guest-All\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1182: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Guest-All\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1182: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Guest-All\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1182: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Guest-All\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1182: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Guest-All\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1182: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Create a pipeline - RF is a stand in, we will populate the classifier part below\n",
    "pipe = Pipeline([('std', StandardScaler()),\n",
    "                 ('classifier', LogisticRegression())])\n",
    "\n",
    "# Create search space of candidate learning algorithms and their hyperparameters\n",
    "# note lbfgs can't do l1, and if you pass penalty='none' it expects no C value\n",
    "search_space = [{'classifier': [LogisticRegression(max_iter=10000)],\n",
    "                 'classifier__solver': ['saga'],\n",
    "                 'classifier__penalty': ['l1', 'l2'],\n",
    "                 'classifier__C': np.logspace(-4, 4, 9)},\n",
    "                {'classifier': [LogisticRegression(max_iter=10000)],\n",
    "                 'classifier__solver': ['lbfgs'],\n",
    "                 'classifier__penalty': ['l2'],\n",
    "                 'classifier__C': np.logspace(-4, 4, 9)},\n",
    "                {'classifier': [LogisticRegression(max_iter=10000)],\n",
    "                 'classifier__solver': ['lbfgs','saga'],\n",
    "                 'classifier__penalty': ['none']},\n",
    "                {'classifier': [LogisticRegression(max_iter=10000)]}\n",
    "                ]\n",
    "\n",
    "# Create grid search\n",
    "clf = GridSearchCV(pipe, search_space, cv=StratifiedKFold(n_splits=10),\n",
    "                   scoring=['accuracy', 'precision', 'f1_micro'], refit=False,\n",
    "                   verbose=0)\n",
    "\n",
    "# Fit grid search\n",
    "best_model = clf.fit(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "2795f9d4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'mean_fit_time': array([0.01452353, 0.0551595 , 0.06368232, 0.04734917, 0.07077663,\n",
       "        0.06042213, 0.05941138, 0.05519729, 0.0742033 , 0.07545686,\n",
       "        0.0682111 , 0.06295273, 0.07559428, 0.06773322, 0.0816906 ,\n",
       "        0.05791404, 0.07374501, 0.05019147, 0.0112505 , 0.01309164,\n",
       "        0.01345232, 0.01732965, 0.01458604, 0.01585441, 0.01480513,\n",
       "        0.0165906 , 0.01589704, 0.01734266, 0.05895212, 0.01252215]),\n",
       " 'std_fit_time': array([0.00666532, 0.01266862, 0.01200235, 0.00688629, 0.01619007,\n",
       "        0.01237139, 0.01358514, 0.00964634, 0.01455343, 0.01112782,\n",
       "        0.02212531, 0.01402939, 0.01835173, 0.00939087, 0.02084697,\n",
       "        0.01244084, 0.01336013, 0.00938906, 0.00649839, 0.00477087,\n",
       "        0.00483802, 0.00477827, 0.00512133, 0.00700503, 0.00415294,\n",
       "        0.00760235, 0.00402348, 0.0055061 , 0.01258054, 0.00718736]),\n",
       " 'mean_score_time': array([0.00444281, 0.00622821, 0.00946808, 0.00627441, 0.00624731,\n",
       "        0.00261033, 0.00666175, 0.00762055, 0.01047239, 0.00297029,\n",
       "        0.01049151, 0.00777357, 0.00932   , 0.00465925, 0.00796297,\n",
       "        0.00657589, 0.00796325, 0.00479095, 0.00604234, 0.00568545,\n",
       "        0.00696523, 0.00468211, 0.00633333, 0.00315115, 0.00750294,\n",
       "        0.00536287, 0.00769022, 0.00567658, 0.00451941, 0.00911729]),\n",
       " 'std_score_time': array([0.00617408, 0.00678645, 0.00681724, 0.00768617, 0.00765246,\n",
       "        0.00556737, 0.00714624, 0.0080015 , 0.00694253, 0.00508936,\n",
       "        0.00778885, 0.00777508, 0.00761129, 0.0071173 , 0.00797106,\n",
       "        0.00809486, 0.00809519, 0.00708768, 0.00709656, 0.00769644,\n",
       "        0.00743999, 0.00715207, 0.00775782, 0.00630277, 0.00756575,\n",
       "        0.00706823, 0.00760512, 0.00626411, 0.00600041, 0.00682826]),\n",
       " 'param_classifier': masked_array(data=[LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000)],\n",
       "              mask=[False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'param_classifier__C': masked_array(data=[0.0001, 0.0001, 0.001, 0.001, 0.01, 0.01, 0.1, 0.1,\n",
       "                    1.0, 1.0, 10.0, 10.0, 100.0, 100.0, 1000.0, 1000.0,\n",
       "                    10000.0, 10000.0, 0.0001, 0.001, 0.01, 0.1, 1.0, 10.0,\n",
       "                    100.0, 1000.0, 10000.0, --, --, --],\n",
       "              mask=[False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False,  True,  True,  True],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'param_classifier__penalty': masked_array(data=['l1', 'l2', 'l1', 'l2', 'l1', 'l2', 'l1', 'l2', 'l1',\n",
       "                    'l2', 'l1', 'l2', 'l1', 'l2', 'l1', 'l2', 'l1', 'l2',\n",
       "                    'l2', 'l2', 'l2', 'l2', 'l2', 'l2', 'l2', 'l2', 'l2',\n",
       "                    'none', 'none', --],\n",
       "              mask=[False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False,  True],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'param_classifier__solver': masked_array(data=['saga', 'saga', 'saga', 'saga', 'saga', 'saga', 'saga',\n",
       "                    'saga', 'saga', 'saga', 'saga', 'saga', 'saga', 'saga',\n",
       "                    'saga', 'saga', 'saga', 'saga', 'lbfgs', 'lbfgs',\n",
       "                    'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs',\n",
       "                    'lbfgs', 'lbfgs', 'saga', --],\n",
       "              mask=[False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False,  True],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'params': [{'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 0.0001,\n",
       "   'classifier__penalty': 'l1',\n",
       "   'classifier__solver': 'saga'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 0.0001,\n",
       "   'classifier__penalty': 'l2',\n",
       "   'classifier__solver': 'saga'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 0.001,\n",
       "   'classifier__penalty': 'l1',\n",
       "   'classifier__solver': 'saga'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 0.001,\n",
       "   'classifier__penalty': 'l2',\n",
       "   'classifier__solver': 'saga'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 0.01,\n",
       "   'classifier__penalty': 'l1',\n",
       "   'classifier__solver': 'saga'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 0.01,\n",
       "   'classifier__penalty': 'l2',\n",
       "   'classifier__solver': 'saga'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 0.1,\n",
       "   'classifier__penalty': 'l1',\n",
       "   'classifier__solver': 'saga'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 0.1,\n",
       "   'classifier__penalty': 'l2',\n",
       "   'classifier__solver': 'saga'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 1.0,\n",
       "   'classifier__penalty': 'l1',\n",
       "   'classifier__solver': 'saga'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 1.0,\n",
       "   'classifier__penalty': 'l2',\n",
       "   'classifier__solver': 'saga'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 10.0,\n",
       "   'classifier__penalty': 'l1',\n",
       "   'classifier__solver': 'saga'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 10.0,\n",
       "   'classifier__penalty': 'l2',\n",
       "   'classifier__solver': 'saga'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 100.0,\n",
       "   'classifier__penalty': 'l1',\n",
       "   'classifier__solver': 'saga'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 100.0,\n",
       "   'classifier__penalty': 'l2',\n",
       "   'classifier__solver': 'saga'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 1000.0,\n",
       "   'classifier__penalty': 'l1',\n",
       "   'classifier__solver': 'saga'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 1000.0,\n",
       "   'classifier__penalty': 'l2',\n",
       "   'classifier__solver': 'saga'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 10000.0,\n",
       "   'classifier__penalty': 'l1',\n",
       "   'classifier__solver': 'saga'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 10000.0,\n",
       "   'classifier__penalty': 'l2',\n",
       "   'classifier__solver': 'saga'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 0.0001,\n",
       "   'classifier__penalty': 'l2',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 0.001,\n",
       "   'classifier__penalty': 'l2',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 0.01,\n",
       "   'classifier__penalty': 'l2',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 0.1,\n",
       "   'classifier__penalty': 'l2',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 1.0,\n",
       "   'classifier__penalty': 'l2',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 10.0,\n",
       "   'classifier__penalty': 'l2',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 100.0,\n",
       "   'classifier__penalty': 'l2',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 1000.0,\n",
       "   'classifier__penalty': 'l2',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 10000.0,\n",
       "   'classifier__penalty': 'l2',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__penalty': 'none',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__penalty': 'none',\n",
       "   'classifier__solver': 'saga'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000)}],\n",
       " 'split0_test_accuracy': array([0.597, 0.699, 0.679, 0.746, 0.749, 0.743, 0.744, 0.743, 0.744,\n",
       "        0.744, 0.744, 0.744, 0.744, 0.744, 0.744, 0.744, 0.744, 0.744,\n",
       "        0.699, 0.746, 0.743, 0.743, 0.744, 0.744, 0.744, 0.744, 0.744,\n",
       "        0.744, 0.744, 0.744]),\n",
       " 'split1_test_accuracy': array([0.597, 0.671, 0.664, 0.725, 0.726, 0.728, 0.724, 0.726, 0.726,\n",
       "        0.726, 0.726, 0.726, 0.726, 0.726, 0.726, 0.726, 0.726, 0.726,\n",
       "        0.671, 0.725, 0.728, 0.726, 0.726, 0.726, 0.726, 0.726, 0.726,\n",
       "        0.726, 0.726, 0.726]),\n",
       " 'split2_test_accuracy': array([0.597, 0.703, 0.684, 0.731, 0.747, 0.753, 0.755, 0.755, 0.755,\n",
       "        0.754, 0.754, 0.754, 0.754, 0.754, 0.754, 0.754, 0.754, 0.754,\n",
       "        0.703, 0.731, 0.753, 0.755, 0.754, 0.754, 0.754, 0.754, 0.754,\n",
       "        0.754, 0.754, 0.754]),\n",
       " 'split3_test_accuracy': array([0.597, 0.68 , 0.677, 0.729, 0.729, 0.726, 0.727, 0.726, 0.725,\n",
       "        0.725, 0.725, 0.725, 0.725, 0.725, 0.725, 0.725, 0.725, 0.725,\n",
       "        0.68 , 0.729, 0.726, 0.726, 0.725, 0.725, 0.725, 0.725, 0.725,\n",
       "        0.725, 0.725, 0.725]),\n",
       " 'split4_test_accuracy': array([0.597, 0.676, 0.654, 0.697, 0.705, 0.714, 0.714, 0.715, 0.715,\n",
       "        0.715, 0.715, 0.715, 0.715, 0.715, 0.715, 0.715, 0.715, 0.715,\n",
       "        0.676, 0.697, 0.714, 0.715, 0.715, 0.715, 0.715, 0.715, 0.715,\n",
       "        0.715, 0.715, 0.715]),\n",
       " 'split5_test_accuracy': array([0.597, 0.698, 0.683, 0.747, 0.749, 0.756, 0.753, 0.756, 0.756,\n",
       "        0.756, 0.756, 0.756, 0.756, 0.756, 0.756, 0.756, 0.756, 0.756,\n",
       "        0.698, 0.747, 0.756, 0.756, 0.756, 0.756, 0.756, 0.756, 0.756,\n",
       "        0.756, 0.756, 0.756]),\n",
       " 'split6_test_accuracy': array([0.596, 0.705, 0.705, 0.746, 0.745, 0.746, 0.749, 0.746, 0.747,\n",
       "        0.747, 0.747, 0.747, 0.747, 0.747, 0.747, 0.747, 0.747, 0.747,\n",
       "        0.705, 0.746, 0.746, 0.746, 0.747, 0.747, 0.747, 0.747, 0.747,\n",
       "        0.747, 0.747, 0.747]),\n",
       " 'split7_test_accuracy': array([0.596, 0.705, 0.685, 0.775, 0.771, 0.775, 0.774, 0.774, 0.775,\n",
       "        0.775, 0.775, 0.775, 0.775, 0.775, 0.775, 0.775, 0.775, 0.775,\n",
       "        0.705, 0.775, 0.775, 0.774, 0.775, 0.775, 0.775, 0.775, 0.775,\n",
       "        0.775, 0.775, 0.775]),\n",
       " 'split8_test_accuracy': array([0.596, 0.71 , 0.704, 0.757, 0.769, 0.771, 0.774, 0.773, 0.773,\n",
       "        0.773, 0.773, 0.773, 0.773, 0.773, 0.773, 0.773, 0.773, 0.773,\n",
       "        0.71 , 0.757, 0.771, 0.773, 0.773, 0.773, 0.773, 0.773, 0.773,\n",
       "        0.773, 0.773, 0.773]),\n",
       " 'split9_test_accuracy': array([0.596, 0.701, 0.693, 0.747, 0.762, 0.764, 0.765, 0.764, 0.764,\n",
       "        0.763, 0.763, 0.763, 0.763, 0.763, 0.763, 0.763, 0.763, 0.763,\n",
       "        0.701, 0.747, 0.764, 0.764, 0.763, 0.763, 0.763, 0.763, 0.763,\n",
       "        0.763, 0.763, 0.763]),\n",
       " 'mean_test_accuracy': array([0.5966, 0.6948, 0.6828, 0.74  , 0.7452, 0.7476, 0.7479, 0.7478,\n",
       "        0.748 , 0.7478, 0.7478, 0.7478, 0.7478, 0.7478, 0.7478, 0.7478,\n",
       "        0.7478, 0.7478, 0.6948, 0.74  , 0.7476, 0.7478, 0.7478, 0.7478,\n",
       "        0.7478, 0.7478, 0.7478, 0.7478, 0.7478, 0.7478]),\n",
       " 'std_test_accuracy': array([0.0004899 , 0.01308281, 0.01511158, 0.01989975, 0.01947717,\n",
       "        0.01915829, 0.01976082, 0.01937937, 0.01960102, 0.01948743,\n",
       "        0.01948743, 0.01948743, 0.01948743, 0.01948743, 0.01948743,\n",
       "        0.01948743, 0.01948743, 0.01948743, 0.01308281, 0.01989975,\n",
       "        0.01915829, 0.01937937, 0.01948743, 0.01948743, 0.01948743,\n",
       "        0.01948743, 0.01948743, 0.01948743, 0.01948743, 0.01948743]),\n",
       " 'rank_test_accuracy': array([30, 27, 29, 25, 24, 22,  2,  3,  1,  3,  3,  3,  3,  3,  3,  3,  3,\n",
       "         3, 27, 25, 22,  3,  3,  3,  3,  3,  3,  3,  3,  3]),\n",
       " 'split0_test_precision': array([0.        , 0.78333333, 0.73563218, 0.71346705, 0.70540541,\n",
       "        0.6972973 , 0.696     , 0.69518717, 0.696     , 0.696     ,\n",
       "        0.696     , 0.696     , 0.696     , 0.696     , 0.696     ,\n",
       "        0.696     , 0.696     , 0.696     , 0.78333333, 0.71346705,\n",
       "        0.6972973 , 0.69518717, 0.696     , 0.696     , 0.696     ,\n",
       "        0.696     , 0.696     , 0.696     , 0.696     , 0.696     ]),\n",
       " 'split1_test_precision': array([0.        , 0.68877551, 0.68306011, 0.68604651, 0.6701847 ,\n",
       "        0.67466667, 0.66579634, 0.66840731, 0.66840731, 0.66840731,\n",
       "        0.66840731, 0.66840731, 0.66840731, 0.66840731, 0.66840731,\n",
       "        0.66840731, 0.66840731, 0.66840731, 0.68877551, 0.68604651,\n",
       "        0.67466667, 0.66840731, 0.66840731, 0.66840731, 0.66840731,\n",
       "        0.66840731, 0.66840731, 0.66840731, 0.66840731, 0.66840731]),\n",
       " 'split2_test_precision': array([0.        , 0.74537037, 0.71428571, 0.68108108, 0.69329897,\n",
       "        0.6969697 , 0.69849246, 0.69849246, 0.69849246, 0.69674185,\n",
       "        0.69674185, 0.69674185, 0.69674185, 0.69674185, 0.69674185,\n",
       "        0.69674185, 0.69674185, 0.69674185, 0.74537037, 0.68108108,\n",
       "        0.6969697 , 0.69849246, 0.69674185, 0.69674185, 0.69674185,\n",
       "        0.69674185, 0.69674185, 0.69674185, 0.69674185, 0.69674185]),\n",
       " 'split3_test_precision': array([0.        , 0.73184358, 0.71505376, 0.68965517, 0.67741935,\n",
       "        0.67292225, 0.67287234, 0.672     , 0.67021277, 0.67021277,\n",
       "        0.67021277, 0.67021277, 0.67021277, 0.67021277, 0.67021277,\n",
       "        0.67021277, 0.67021277, 0.67021277, 0.73184358, 0.68965517,\n",
       "        0.67292225, 0.672     , 0.67021277, 0.67021277, 0.67021277,\n",
       "        0.67021277, 0.67021277, 0.67021277, 0.67021277, 0.67021277]),\n",
       " 'split4_test_precision': array([0.        , 0.72067039, 0.67272727, 0.64450867, 0.64285714,\n",
       "        0.65116279, 0.64810127, 0.64824121, 0.64824121, 0.64824121,\n",
       "        0.64824121, 0.64824121, 0.64824121, 0.64824121, 0.64824121,\n",
       "        0.64824121, 0.64824121, 0.64824121, 0.72067039, 0.64450867,\n",
       "        0.65116279, 0.64824121, 0.64824121, 0.64824121, 0.64824121,\n",
       "        0.64824121, 0.64824121, 0.64824121, 0.64824121, 0.64824121]),\n",
       " 'split5_test_precision': array([0.        , 0.77297297, 0.715     , 0.71802326, 0.70540541,\n",
       "        0.71313673, 0.70744681, 0.712     , 0.712     , 0.712     ,\n",
       "        0.712     , 0.712     , 0.712     , 0.712     , 0.712     ,\n",
       "        0.712     , 0.712     , 0.712     , 0.77297297, 0.71802326,\n",
       "        0.71313673, 0.712     , 0.712     , 0.712     , 0.712     ,\n",
       "        0.712     , 0.712     , 0.712     , 0.712     , 0.712     ]),\n",
       " 'split6_test_precision': array([0.        , 0.79781421, 0.79781421, 0.71551724, 0.6997319 ,\n",
       "        0.7016129 , 0.70291777, 0.69946809, 0.70026525, 0.70026525,\n",
       "        0.70026525, 0.70026525, 0.70026525, 0.70026525, 0.70026525,\n",
       "        0.70026525, 0.70026525, 0.70026525, 0.79781421, 0.71551724,\n",
       "        0.7016129 , 0.69946809, 0.70026525, 0.70026525, 0.70026525,\n",
       "        0.70026525, 0.70026525, 0.70026525, 0.70026525, 0.70026525]),\n",
       " 'split7_test_precision': array([0.        , 0.78534031, 0.72588832, 0.75644699, 0.73209549,\n",
       "        0.73740053, 0.73177083, 0.73421053, 0.73490814, 0.73490814,\n",
       "        0.73490814, 0.73490814, 0.73490814, 0.73490814, 0.73490814,\n",
       "        0.73490814, 0.73490814, 0.73490814, 0.78534031, 0.75644699,\n",
       "        0.73740053, 0.73421053, 0.73490814, 0.73490814, 0.73490814,\n",
       "        0.73490814, 0.73490814, 0.73490814, 0.73490814, 0.73490814]),\n",
       " 'split8_test_precision': array([0.        , 0.8       , 0.7967033 , 0.74029851, 0.74643875,\n",
       "        0.74104683, 0.74585635, 0.74246575, 0.74246575, 0.74246575,\n",
       "        0.74246575, 0.74246575, 0.74246575, 0.74246575, 0.74246575,\n",
       "        0.74246575, 0.74246575, 0.74246575, 0.8       , 0.74029851,\n",
       "        0.74104683, 0.74246575, 0.74246575, 0.74246575, 0.74246575,\n",
       "        0.74246575, 0.74246575, 0.74246575, 0.74246575, 0.74246575]),\n",
       " 'split9_test_precision': array([0.        , 0.78688525, 0.75129534, 0.72271386, 0.72432432,\n",
       "        0.72580645, 0.72295515, 0.72459893, 0.72459893, 0.72386059,\n",
       "        0.72386059, 0.72386059, 0.72386059, 0.72386059, 0.72386059,\n",
       "        0.72386059, 0.72386059, 0.72386059, 0.78688525, 0.72271386,\n",
       "        0.72580645, 0.72459893, 0.72386059, 0.72386059, 0.72386059,\n",
       "        0.72386059, 0.72386059, 0.72386059, 0.72386059, 0.72386059]),\n",
       " 'mean_test_precision': array([0.        , 0.76130059, 0.73074602, 0.70677583, 0.69971614,\n",
       "        0.70120222, 0.69922093, 0.69950714, 0.69955918, 0.69931029,\n",
       "        0.69931029, 0.69931029, 0.69931029, 0.69931029, 0.69931029,\n",
       "        0.69931029, 0.69931029, 0.69931029, 0.76130059, 0.70677583,\n",
       "        0.70120222, 0.69950714, 0.69931029, 0.69931029, 0.69931029,\n",
       "        0.69931029, 0.69931029, 0.69931029, 0.69931029, 0.69931029]),\n",
       " 'std_test_precision': array([0.        , 0.03566125, 0.03965287, 0.03058259, 0.02923201,\n",
       "        0.02772522, 0.02883501, 0.02864816, 0.02889944, 0.02884705,\n",
       "        0.02884705, 0.02884705, 0.02884705, 0.02884705, 0.02884705,\n",
       "        0.02884705, 0.02884705, 0.02884705, 0.03566125, 0.03058259,\n",
       "        0.02772522, 0.02864816, 0.02884705, 0.02884705, 0.02884705,\n",
       "        0.02884705, 0.02884705, 0.02884705, 0.02884705, 0.02884705]),\n",
       " 'rank_test_precision': array([30,  1,  3,  4,  8,  6, 29, 10,  9, 12, 12, 12, 12, 12, 12, 12, 12,\n",
       "        12,  1,  4,  6, 10, 12, 12, 12, 12, 12, 12, 12, 12]),\n",
       " 'split0_test_f1_micro': array([0.597, 0.699, 0.679, 0.746, 0.749, 0.743, 0.744, 0.743, 0.744,\n",
       "        0.744, 0.744, 0.744, 0.744, 0.744, 0.744, 0.744, 0.744, 0.744,\n",
       "        0.699, 0.746, 0.743, 0.743, 0.744, 0.744, 0.744, 0.744, 0.744,\n",
       "        0.744, 0.744, 0.744]),\n",
       " 'split1_test_f1_micro': array([0.597, 0.671, 0.664, 0.725, 0.726, 0.728, 0.724, 0.726, 0.726,\n",
       "        0.726, 0.726, 0.726, 0.726, 0.726, 0.726, 0.726, 0.726, 0.726,\n",
       "        0.671, 0.725, 0.728, 0.726, 0.726, 0.726, 0.726, 0.726, 0.726,\n",
       "        0.726, 0.726, 0.726]),\n",
       " 'split2_test_f1_micro': array([0.597, 0.703, 0.684, 0.731, 0.747, 0.753, 0.755, 0.755, 0.755,\n",
       "        0.754, 0.754, 0.754, 0.754, 0.754, 0.754, 0.754, 0.754, 0.754,\n",
       "        0.703, 0.731, 0.753, 0.755, 0.754, 0.754, 0.754, 0.754, 0.754,\n",
       "        0.754, 0.754, 0.754]),\n",
       " 'split3_test_f1_micro': array([0.597, 0.68 , 0.677, 0.729, 0.729, 0.726, 0.727, 0.726, 0.725,\n",
       "        0.725, 0.725, 0.725, 0.725, 0.725, 0.725, 0.725, 0.725, 0.725,\n",
       "        0.68 , 0.729, 0.726, 0.726, 0.725, 0.725, 0.725, 0.725, 0.725,\n",
       "        0.725, 0.725, 0.725]),\n",
       " 'split4_test_f1_micro': array([0.597, 0.676, 0.654, 0.697, 0.705, 0.714, 0.714, 0.715, 0.715,\n",
       "        0.715, 0.715, 0.715, 0.715, 0.715, 0.715, 0.715, 0.715, 0.715,\n",
       "        0.676, 0.697, 0.714, 0.715, 0.715, 0.715, 0.715, 0.715, 0.715,\n",
       "        0.715, 0.715, 0.715]),\n",
       " 'split5_test_f1_micro': array([0.597, 0.698, 0.683, 0.747, 0.749, 0.756, 0.753, 0.756, 0.756,\n",
       "        0.756, 0.756, 0.756, 0.756, 0.756, 0.756, 0.756, 0.756, 0.756,\n",
       "        0.698, 0.747, 0.756, 0.756, 0.756, 0.756, 0.756, 0.756, 0.756,\n",
       "        0.756, 0.756, 0.756]),\n",
       " 'split6_test_f1_micro': array([0.596, 0.705, 0.705, 0.746, 0.745, 0.746, 0.749, 0.746, 0.747,\n",
       "        0.747, 0.747, 0.747, 0.747, 0.747, 0.747, 0.747, 0.747, 0.747,\n",
       "        0.705, 0.746, 0.746, 0.746, 0.747, 0.747, 0.747, 0.747, 0.747,\n",
       "        0.747, 0.747, 0.747]),\n",
       " 'split7_test_f1_micro': array([0.596, 0.705, 0.685, 0.775, 0.771, 0.775, 0.774, 0.774, 0.775,\n",
       "        0.775, 0.775, 0.775, 0.775, 0.775, 0.775, 0.775, 0.775, 0.775,\n",
       "        0.705, 0.775, 0.775, 0.774, 0.775, 0.775, 0.775, 0.775, 0.775,\n",
       "        0.775, 0.775, 0.775]),\n",
       " 'split8_test_f1_micro': array([0.596, 0.71 , 0.704, 0.757, 0.769, 0.771, 0.774, 0.773, 0.773,\n",
       "        0.773, 0.773, 0.773, 0.773, 0.773, 0.773, 0.773, 0.773, 0.773,\n",
       "        0.71 , 0.757, 0.771, 0.773, 0.773, 0.773, 0.773, 0.773, 0.773,\n",
       "        0.773, 0.773, 0.773]),\n",
       " 'split9_test_f1_micro': array([0.596, 0.701, 0.693, 0.747, 0.762, 0.764, 0.765, 0.764, 0.764,\n",
       "        0.763, 0.763, 0.763, 0.763, 0.763, 0.763, 0.763, 0.763, 0.763,\n",
       "        0.701, 0.747, 0.764, 0.764, 0.763, 0.763, 0.763, 0.763, 0.763,\n",
       "        0.763, 0.763, 0.763]),\n",
       " 'mean_test_f1_micro': array([0.5966, 0.6948, 0.6828, 0.74  , 0.7452, 0.7476, 0.7479, 0.7478,\n",
       "        0.748 , 0.7478, 0.7478, 0.7478, 0.7478, 0.7478, 0.7478, 0.7478,\n",
       "        0.7478, 0.7478, 0.6948, 0.74  , 0.7476, 0.7478, 0.7478, 0.7478,\n",
       "        0.7478, 0.7478, 0.7478, 0.7478, 0.7478, 0.7478]),\n",
       " 'std_test_f1_micro': array([0.0004899 , 0.01308281, 0.01511158, 0.01989975, 0.01947717,\n",
       "        0.01915829, 0.01976082, 0.01937937, 0.01960102, 0.01948743,\n",
       "        0.01948743, 0.01948743, 0.01948743, 0.01948743, 0.01948743,\n",
       "        0.01948743, 0.01948743, 0.01948743, 0.01308281, 0.01989975,\n",
       "        0.01915829, 0.01937937, 0.01948743, 0.01948743, 0.01948743,\n",
       "        0.01948743, 0.01948743, 0.01948743, 0.01948743, 0.01948743]),\n",
       " 'rank_test_f1_micro': array([30, 27, 29, 25, 24, 22,  2,  3,  1,  3,  3,  3,  3,  3,  3,  3,  3,\n",
       "         3, 27, 25, 22,  3,  3,  3,  3,  3,  3,  3,  3,  3])}"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_model.cv_results_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "5a869ea2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([30, 27, 29, 25, 24, 22,  2,  3,  1,  3,  3,  3,  3,  3,  3,  3,  3,\n",
       "        3, 27, 25, 22,  3,  3,  3,  3,  3,  3,  3,  3,  3])"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_model.cv_results_['rank_test_accuracy']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "3bcc2637",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'classifier': LogisticRegression(max_iter=10000),\n",
       " 'classifier__C': 0.0001,\n",
       " 'classifier__penalty': 'l1',\n",
       " 'classifier__solver': 'saga'}"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_model.cv_results_['params'][ np.argmin(best_model.cv_results_['mean_test_accuracy'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "0216cb96",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>classifier</th>\n",
       "      <th>C</th>\n",
       "      <th>penalty</th>\n",
       "      <th>solver</th>\n",
       "      <th>score_acc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.4034</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.3052</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.3172</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.2600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0100</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.2548</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0100</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.2524</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.1000</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.2521</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.1000</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.2522</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.2520</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.2522</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>10.0000</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.2522</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>10.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.2522</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>100.0000</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.2522</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>100.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.2522</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>1000.0000</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.2522</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>1000.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.2522</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>10000.0000</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.2522</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>10000.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.2522</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.3052</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.2600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0100</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.2524</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.1000</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.2522</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.2522</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>10.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.2522</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>100.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.2522</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>1000.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.2522</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>10000.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.2522</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>NaN</td>\n",
       "      <td>none</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.2522</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>NaN</td>\n",
       "      <td>none</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.2522</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.2522</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                            classifier           C penalty solver  score_acc\n",
       "0   LogisticRegression(max_iter=10000)      0.0001      l1   saga     0.4034\n",
       "1   LogisticRegression(max_iter=10000)      0.0001      l2   saga     0.3052\n",
       "2   LogisticRegression(max_iter=10000)      0.0010      l1   saga     0.3172\n",
       "3   LogisticRegression(max_iter=10000)      0.0010      l2   saga     0.2600\n",
       "4   LogisticRegression(max_iter=10000)      0.0100      l1   saga     0.2548\n",
       "5   LogisticRegression(max_iter=10000)      0.0100      l2   saga     0.2524\n",
       "6   LogisticRegression(max_iter=10000)      0.1000      l1   saga     0.2521\n",
       "7   LogisticRegression(max_iter=10000)      0.1000      l2   saga     0.2522\n",
       "8   LogisticRegression(max_iter=10000)      1.0000      l1   saga     0.2520\n",
       "9   LogisticRegression(max_iter=10000)      1.0000      l2   saga     0.2522\n",
       "10  LogisticRegression(max_iter=10000)     10.0000      l1   saga     0.2522\n",
       "11  LogisticRegression(max_iter=10000)     10.0000      l2   saga     0.2522\n",
       "12  LogisticRegression(max_iter=10000)    100.0000      l1   saga     0.2522\n",
       "13  LogisticRegression(max_iter=10000)    100.0000      l2   saga     0.2522\n",
       "14  LogisticRegression(max_iter=10000)   1000.0000      l1   saga     0.2522\n",
       "15  LogisticRegression(max_iter=10000)   1000.0000      l2   saga     0.2522\n",
       "16  LogisticRegression(max_iter=10000)  10000.0000      l1   saga     0.2522\n",
       "17  LogisticRegression(max_iter=10000)  10000.0000      l2   saga     0.2522\n",
       "18  LogisticRegression(max_iter=10000)      0.0001      l2  lbfgs     0.3052\n",
       "19  LogisticRegression(max_iter=10000)      0.0010      l2  lbfgs     0.2600\n",
       "20  LogisticRegression(max_iter=10000)      0.0100      l2  lbfgs     0.2524\n",
       "21  LogisticRegression(max_iter=10000)      0.1000      l2  lbfgs     0.2522\n",
       "22  LogisticRegression(max_iter=10000)      1.0000      l2  lbfgs     0.2522\n",
       "23  LogisticRegression(max_iter=10000)     10.0000      l2  lbfgs     0.2522\n",
       "24  LogisticRegression(max_iter=10000)    100.0000      l2  lbfgs     0.2522\n",
       "25  LogisticRegression(max_iter=10000)   1000.0000      l2  lbfgs     0.2522\n",
       "26  LogisticRegression(max_iter=10000)  10000.0000      l2  lbfgs     0.2522\n",
       "27  LogisticRegression(max_iter=10000)         NaN    none  lbfgs     0.2522\n",
       "28  LogisticRegression(max_iter=10000)         NaN    none   saga     0.2522\n",
       "29  LogisticRegression(max_iter=10000)         NaN     NaN    NaN     0.2522"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results = pd.DataFrame( best_model.cv_results_['params'] )\n",
    "\n",
    "# next grab the score resulting from those parameters, add it to the data\n",
    "# score is accuracy; to display it as misclassification error we use 1 - x\n",
    "results['score_acc'] = 1 - best_model.cv_results_['mean_test_accuracy']\n",
    "\n",
    "# get rid of classifierXX in columns\n",
    "cols = results.columns.to_series().str.split('__').apply(lambda x: x[-1])\n",
    "results.columns = cols\n",
    "\n",
    "# lets show the results for the saga solver across penalty & C values\n",
    "#sns.heatmap( results.query('solver==\"saga\"').pivot('C','penalty','score_acc'),\n",
    "#             annot=True, fmt='.3f')\n",
    "\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "83604c0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "acc_list.append(results[0:1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "9c89d3be",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'classifier': LogisticRegression(max_iter=10000),\n",
       " 'classifier__C': 0.0001,\n",
       " 'classifier__penalty': 'l2',\n",
       " 'classifier__solver': 'saga'}"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_model.cv_results_['params'][ np.argmin(best_model.cv_results_['rank_test_precision'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "69ab11b2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>classifier</th>\n",
       "      <th>C</th>\n",
       "      <th>penalty</th>\n",
       "      <th>solver</th>\n",
       "      <th>score_precision</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.238699</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.269254</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.293224</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0100</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.300284</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0100</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.298798</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.1000</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.300779</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.1000</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.300493</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.300441</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.300690</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>10.0000</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.300690</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>10.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.300690</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>100.0000</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.300690</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>100.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.300690</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>1000.0000</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.300690</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>1000.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.300690</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>10000.0000</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.300690</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>10000.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.300690</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.238699</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.293224</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0100</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.298798</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.1000</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.300493</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.300690</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>10.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.300690</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>100.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.300690</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>1000.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.300690</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>10000.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.300690</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>NaN</td>\n",
       "      <td>none</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.300690</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>NaN</td>\n",
       "      <td>none</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.300690</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.300690</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                            classifier           C penalty solver  \\\n",
       "0   LogisticRegression(max_iter=10000)      0.0001      l1   saga   \n",
       "1   LogisticRegression(max_iter=10000)      0.0001      l2   saga   \n",
       "2   LogisticRegression(max_iter=10000)      0.0010      l1   saga   \n",
       "3   LogisticRegression(max_iter=10000)      0.0010      l2   saga   \n",
       "4   LogisticRegression(max_iter=10000)      0.0100      l1   saga   \n",
       "5   LogisticRegression(max_iter=10000)      0.0100      l2   saga   \n",
       "6   LogisticRegression(max_iter=10000)      0.1000      l1   saga   \n",
       "7   LogisticRegression(max_iter=10000)      0.1000      l2   saga   \n",
       "8   LogisticRegression(max_iter=10000)      1.0000      l1   saga   \n",
       "9   LogisticRegression(max_iter=10000)      1.0000      l2   saga   \n",
       "10  LogisticRegression(max_iter=10000)     10.0000      l1   saga   \n",
       "11  LogisticRegression(max_iter=10000)     10.0000      l2   saga   \n",
       "12  LogisticRegression(max_iter=10000)    100.0000      l1   saga   \n",
       "13  LogisticRegression(max_iter=10000)    100.0000      l2   saga   \n",
       "14  LogisticRegression(max_iter=10000)   1000.0000      l1   saga   \n",
       "15  LogisticRegression(max_iter=10000)   1000.0000      l2   saga   \n",
       "16  LogisticRegression(max_iter=10000)  10000.0000      l1   saga   \n",
       "17  LogisticRegression(max_iter=10000)  10000.0000      l2   saga   \n",
       "18  LogisticRegression(max_iter=10000)      0.0001      l2  lbfgs   \n",
       "19  LogisticRegression(max_iter=10000)      0.0010      l2  lbfgs   \n",
       "20  LogisticRegression(max_iter=10000)      0.0100      l2  lbfgs   \n",
       "21  LogisticRegression(max_iter=10000)      0.1000      l2  lbfgs   \n",
       "22  LogisticRegression(max_iter=10000)      1.0000      l2  lbfgs   \n",
       "23  LogisticRegression(max_iter=10000)     10.0000      l2  lbfgs   \n",
       "24  LogisticRegression(max_iter=10000)    100.0000      l2  lbfgs   \n",
       "25  LogisticRegression(max_iter=10000)   1000.0000      l2  lbfgs   \n",
       "26  LogisticRegression(max_iter=10000)  10000.0000      l2  lbfgs   \n",
       "27  LogisticRegression(max_iter=10000)         NaN    none  lbfgs   \n",
       "28  LogisticRegression(max_iter=10000)         NaN    none   saga   \n",
       "29  LogisticRegression(max_iter=10000)         NaN     NaN    NaN   \n",
       "\n",
       "    score_precision  \n",
       "0          1.000000  \n",
       "1          0.238699  \n",
       "2          0.269254  \n",
       "3          0.293224  \n",
       "4          0.300284  \n",
       "5          0.298798  \n",
       "6          0.300779  \n",
       "7          0.300493  \n",
       "8          0.300441  \n",
       "9          0.300690  \n",
       "10         0.300690  \n",
       "11         0.300690  \n",
       "12         0.300690  \n",
       "13         0.300690  \n",
       "14         0.300690  \n",
       "15         0.300690  \n",
       "16         0.300690  \n",
       "17         0.300690  \n",
       "18         0.238699  \n",
       "19         0.293224  \n",
       "20         0.298798  \n",
       "21         0.300493  \n",
       "22         0.300690  \n",
       "23         0.300690  \n",
       "24         0.300690  \n",
       "25         0.300690  \n",
       "26         0.300690  \n",
       "27         0.300690  \n",
       "28         0.300690  \n",
       "29         0.300690  "
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results = pd.DataFrame( best_model.cv_results_['params'] )\n",
    "\n",
    "# next grab the score resulting from those parameters, add it to the data\n",
    "# score is accuracy; to display it as misclassification error we use 1 - x\n",
    "results['score_precision'] = 1 - best_model.cv_results_['mean_test_precision']\n",
    "\n",
    "# get rid of classifierXX in columns\n",
    "cols = results.columns.to_series().str.split('__').apply(lambda x: x[-1])\n",
    "results.columns = cols\n",
    "\n",
    "# lets show the results for the saga solver across penalty & C values\n",
    "#sns.heatmap( results.query('solver==\"saga\"').pivot('C','penalty','score_precision'),\n",
    "#             annot=True, fmt='.3f')\n",
    "\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "c584772f",
   "metadata": {},
   "outputs": [],
   "source": [
    "roc_auc_ovr_List.append(results[1:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "70f619b8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'classifier': LogisticRegression(max_iter=10000),\n",
       " 'classifier__C': 1.0,\n",
       " 'classifier__penalty': 'l1',\n",
       " 'classifier__solver': 'saga'}"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_model.cv_results_['params'][ np.argmin(best_model.cv_results_['rank_test_f1_micro'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "11465a39",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>classifier</th>\n",
       "      <th>C</th>\n",
       "      <th>penalty</th>\n",
       "      <th>solver</th>\n",
       "      <th>score_f1_micro</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.4034</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.3052</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.3172</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.2600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0100</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.2548</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0100</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.2524</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.1000</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.2521</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.1000</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.2522</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.2520</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.2522</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>10.0000</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.2522</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>10.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.2522</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>100.0000</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.2522</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>100.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.2522</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>1000.0000</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.2522</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>1000.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.2522</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>10000.0000</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.2522</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>10000.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.2522</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.3052</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.2600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0100</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.2524</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.1000</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.2522</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.2522</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>10.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.2522</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>100.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.2522</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>1000.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.2522</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>10000.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.2522</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>NaN</td>\n",
       "      <td>none</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.2522</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>NaN</td>\n",
       "      <td>none</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.2522</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.2522</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                            classifier           C penalty solver  \\\n",
       "0   LogisticRegression(max_iter=10000)      0.0001      l1   saga   \n",
       "1   LogisticRegression(max_iter=10000)      0.0001      l2   saga   \n",
       "2   LogisticRegression(max_iter=10000)      0.0010      l1   saga   \n",
       "3   LogisticRegression(max_iter=10000)      0.0010      l2   saga   \n",
       "4   LogisticRegression(max_iter=10000)      0.0100      l1   saga   \n",
       "5   LogisticRegression(max_iter=10000)      0.0100      l2   saga   \n",
       "6   LogisticRegression(max_iter=10000)      0.1000      l1   saga   \n",
       "7   LogisticRegression(max_iter=10000)      0.1000      l2   saga   \n",
       "8   LogisticRegression(max_iter=10000)      1.0000      l1   saga   \n",
       "9   LogisticRegression(max_iter=10000)      1.0000      l2   saga   \n",
       "10  LogisticRegression(max_iter=10000)     10.0000      l1   saga   \n",
       "11  LogisticRegression(max_iter=10000)     10.0000      l2   saga   \n",
       "12  LogisticRegression(max_iter=10000)    100.0000      l1   saga   \n",
       "13  LogisticRegression(max_iter=10000)    100.0000      l2   saga   \n",
       "14  LogisticRegression(max_iter=10000)   1000.0000      l1   saga   \n",
       "15  LogisticRegression(max_iter=10000)   1000.0000      l2   saga   \n",
       "16  LogisticRegression(max_iter=10000)  10000.0000      l1   saga   \n",
       "17  LogisticRegression(max_iter=10000)  10000.0000      l2   saga   \n",
       "18  LogisticRegression(max_iter=10000)      0.0001      l2  lbfgs   \n",
       "19  LogisticRegression(max_iter=10000)      0.0010      l2  lbfgs   \n",
       "20  LogisticRegression(max_iter=10000)      0.0100      l2  lbfgs   \n",
       "21  LogisticRegression(max_iter=10000)      0.1000      l2  lbfgs   \n",
       "22  LogisticRegression(max_iter=10000)      1.0000      l2  lbfgs   \n",
       "23  LogisticRegression(max_iter=10000)     10.0000      l2  lbfgs   \n",
       "24  LogisticRegression(max_iter=10000)    100.0000      l2  lbfgs   \n",
       "25  LogisticRegression(max_iter=10000)   1000.0000      l2  lbfgs   \n",
       "26  LogisticRegression(max_iter=10000)  10000.0000      l2  lbfgs   \n",
       "27  LogisticRegression(max_iter=10000)         NaN    none  lbfgs   \n",
       "28  LogisticRegression(max_iter=10000)         NaN    none   saga   \n",
       "29  LogisticRegression(max_iter=10000)         NaN     NaN    NaN   \n",
       "\n",
       "    score_f1_micro  \n",
       "0           0.4034  \n",
       "1           0.3052  \n",
       "2           0.3172  \n",
       "3           0.2600  \n",
       "4           0.2548  \n",
       "5           0.2524  \n",
       "6           0.2521  \n",
       "7           0.2522  \n",
       "8           0.2520  \n",
       "9           0.2522  \n",
       "10          0.2522  \n",
       "11          0.2522  \n",
       "12          0.2522  \n",
       "13          0.2522  \n",
       "14          0.2522  \n",
       "15          0.2522  \n",
       "16          0.2522  \n",
       "17          0.2522  \n",
       "18          0.3052  \n",
       "19          0.2600  \n",
       "20          0.2524  \n",
       "21          0.2522  \n",
       "22          0.2522  \n",
       "23          0.2522  \n",
       "24          0.2522  \n",
       "25          0.2522  \n",
       "26          0.2522  \n",
       "27          0.2522  \n",
       "28          0.2522  \n",
       "29          0.2522  "
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results = pd.DataFrame( best_model.cv_results_['params'] )\n",
    "\n",
    "# next grab the score resulting from those parameters, add it to the data\n",
    "# score is accuracy; to display it as misclassification error we use 1 - x\n",
    "results['score_f1_micro'] = 1 - best_model.cv_results_['mean_test_f1_micro']\n",
    "\n",
    "# get rid of classifierXX in columns\n",
    "cols = results.columns.to_series().str.split('__').apply(lambda x: x[-1])\n",
    "results.columns = cols\n",
    "\n",
    "# lets show the results for the saga solver across penalty & C values\n",
    "#sns.heatmap( results.query('solver==\"saga\"').pivot('C','penalty','score_f1_micro'),\n",
    "#             annot=True, fmt='.3f')\n",
    "\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "4a693627",
   "metadata": {},
   "outputs": [],
   "source": [
    "f1_micro_List.append(results[8:9])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a0b1170",
   "metadata": {},
   "source": [
    "## Trial 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "d430cae5",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, Y_train, Y_test = train_test_split(X, y, train_size=10000, shuffle=True, random_state = 8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "ec6bf17a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Guest-All\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1469: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\Guest-All\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1469: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\Guest-All\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1469: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\Guest-All\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1469: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\Guest-All\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1469: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\Guest-All\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1469: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\Guest-All\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1469: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\Guest-All\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1469: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\Guest-All\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1469: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\Guest-All\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1469: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\Guest-All\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1182: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Guest-All\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1182: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Guest-All\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1182: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Guest-All\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1182: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Guest-All\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1182: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Guest-All\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1182: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Guest-All\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1182: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Guest-All\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1182: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Guest-All\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1182: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Guest-All\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1182: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Guest-All\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1182: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Guest-All\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1182: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Guest-All\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1182: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Guest-All\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1182: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Guest-All\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1182: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Guest-All\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1182: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Guest-All\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1182: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Guest-All\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1182: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Guest-All\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1182: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Guest-All\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1182: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Create a pipeline - RF is a stand in, we will populate the classifier part below\n",
    "pipe = Pipeline([('std', StandardScaler()),\n",
    "                 ('classifier', LogisticRegression())])\n",
    "\n",
    "# Create search space of candidate learning algorithms and their hyperparameters\n",
    "# note lbfgs can't do l1, and if you pass penalty='none' it expects no C value\n",
    "search_space = [{'classifier': [LogisticRegression(max_iter=10000)],\n",
    "                 'classifier__solver': ['saga'],\n",
    "                 'classifier__penalty': ['l1', 'l2'],\n",
    "                 'classifier__C': np.logspace(-4, 4, 9)},\n",
    "                {'classifier': [LogisticRegression(max_iter=10000)],\n",
    "                 'classifier__solver': ['lbfgs'],\n",
    "                 'classifier__penalty': ['l2'],\n",
    "                 'classifier__C': np.logspace(-4, 4, 9)},\n",
    "                {'classifier': [LogisticRegression(max_iter=10000)],\n",
    "                 'classifier__solver': ['lbfgs','saga'],\n",
    "                 'classifier__penalty': ['none']},\n",
    "                {'classifier': [LogisticRegression(max_iter=10000)]}\n",
    "                ]\n",
    "\n",
    "# Create grid search\n",
    "clf = GridSearchCV(pipe, search_space, cv=StratifiedKFold(n_splits=10),\n",
    "                   scoring=['accuracy', 'precision', 'f1_micro'], refit=False,\n",
    "                   verbose=0)\n",
    "\n",
    "# Fit grid search\n",
    "best_model = clf.fit(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "b6aa2042",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'mean_fit_time': array([0.01573954, 0.04967554, 0.05961335, 0.04956481, 0.06310413,\n",
       "        0.05118489, 0.06650667, 0.0556365 , 0.06461713, 0.06610632,\n",
       "        0.05824928, 0.04425726, 0.04875224, 0.04719474, 0.05545895,\n",
       "        0.04992056, 0.06162398, 0.04736364, 0.01355455, 0.01496539,\n",
       "        0.01490083, 0.01364672, 0.01300013, 0.01553354, 0.01287289,\n",
       "        0.01052482, 0.01677923, 0.01535447, 0.04790475, 0.01499112]),\n",
       " 'std_fit_time': array([0.00693178, 0.00866263, 0.01341329, 0.00691133, 0.01190741,\n",
       "        0.01169518, 0.00994149, 0.01164398, 0.00875937, 0.01168523,\n",
       "        0.00748653, 0.00564171, 0.00320547, 0.00024357, 0.00737897,\n",
       "        0.00797433, 0.00362978, 0.00407446, 0.00465534, 0.00231565,\n",
       "        0.00553434, 0.00760383, 0.00526596, 0.00421368, 0.00571954,\n",
       "        0.00702524, 0.00322277, 0.00589278, 0.00806003, 0.00282629]),\n",
       " 'mean_score_time': array([0.00797052, 0.005515  , 0.00646329, 0.00703733, 0.00471327,\n",
       "        0.00702324, 0.00783663, 0.00417101, 0.00844791, 0.00315092,\n",
       "        0.00468583, 0.00592082, 0.01098332, 0.00311172, 0.00892684,\n",
       "        0.00496526, 0.00679591, 0.0084166 , 0.00376282, 0.0039073 ,\n",
       "        0.00541811, 0.00514128, 0.00585046, 0.00491056, 0.00443962,\n",
       "        0.00836365, 0.00523274, 0.00533211, 0.00627656, 0.00552168]),\n",
       " 'std_score_time': array([0.00517234, 0.00707273, 0.00695712, 0.00743885, 0.00720013,\n",
       "        0.00736657, 0.00783864, 0.00591161, 0.00779143, 0.0063021 ,\n",
       "        0.00645331, 0.00729872, 0.00624378, 0.00622351, 0.00740896,\n",
       "        0.00662945, 0.00707707, 0.00740801, 0.0066424 , 0.006273  ,\n",
       "        0.00702134, 0.00684004, 0.00732375, 0.00706169, 0.0068158 ,\n",
       "        0.00848833, 0.00697567, 0.00705279, 0.00682435, 0.00705529]),\n",
       " 'param_classifier': masked_array(data=[LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000)],\n",
       "              mask=[False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'param_classifier__C': masked_array(data=[0.0001, 0.0001, 0.001, 0.001, 0.01, 0.01, 0.1, 0.1,\n",
       "                    1.0, 1.0, 10.0, 10.0, 100.0, 100.0, 1000.0, 1000.0,\n",
       "                    10000.0, 10000.0, 0.0001, 0.001, 0.01, 0.1, 1.0, 10.0,\n",
       "                    100.0, 1000.0, 10000.0, --, --, --],\n",
       "              mask=[False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False,  True,  True,  True],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'param_classifier__penalty': masked_array(data=['l1', 'l2', 'l1', 'l2', 'l1', 'l2', 'l1', 'l2', 'l1',\n",
       "                    'l2', 'l1', 'l2', 'l1', 'l2', 'l1', 'l2', 'l1', 'l2',\n",
       "                    'l2', 'l2', 'l2', 'l2', 'l2', 'l2', 'l2', 'l2', 'l2',\n",
       "                    'none', 'none', --],\n",
       "              mask=[False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False,  True],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'param_classifier__solver': masked_array(data=['saga', 'saga', 'saga', 'saga', 'saga', 'saga', 'saga',\n",
       "                    'saga', 'saga', 'saga', 'saga', 'saga', 'saga', 'saga',\n",
       "                    'saga', 'saga', 'saga', 'saga', 'lbfgs', 'lbfgs',\n",
       "                    'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs',\n",
       "                    'lbfgs', 'lbfgs', 'saga', --],\n",
       "              mask=[False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False,  True],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'params': [{'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 0.0001,\n",
       "   'classifier__penalty': 'l1',\n",
       "   'classifier__solver': 'saga'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 0.0001,\n",
       "   'classifier__penalty': 'l2',\n",
       "   'classifier__solver': 'saga'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 0.001,\n",
       "   'classifier__penalty': 'l1',\n",
       "   'classifier__solver': 'saga'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 0.001,\n",
       "   'classifier__penalty': 'l2',\n",
       "   'classifier__solver': 'saga'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 0.01,\n",
       "   'classifier__penalty': 'l1',\n",
       "   'classifier__solver': 'saga'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 0.01,\n",
       "   'classifier__penalty': 'l2',\n",
       "   'classifier__solver': 'saga'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 0.1,\n",
       "   'classifier__penalty': 'l1',\n",
       "   'classifier__solver': 'saga'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 0.1,\n",
       "   'classifier__penalty': 'l2',\n",
       "   'classifier__solver': 'saga'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 1.0,\n",
       "   'classifier__penalty': 'l1',\n",
       "   'classifier__solver': 'saga'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 1.0,\n",
       "   'classifier__penalty': 'l2',\n",
       "   'classifier__solver': 'saga'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 10.0,\n",
       "   'classifier__penalty': 'l1',\n",
       "   'classifier__solver': 'saga'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 10.0,\n",
       "   'classifier__penalty': 'l2',\n",
       "   'classifier__solver': 'saga'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 100.0,\n",
       "   'classifier__penalty': 'l1',\n",
       "   'classifier__solver': 'saga'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 100.0,\n",
       "   'classifier__penalty': 'l2',\n",
       "   'classifier__solver': 'saga'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 1000.0,\n",
       "   'classifier__penalty': 'l1',\n",
       "   'classifier__solver': 'saga'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 1000.0,\n",
       "   'classifier__penalty': 'l2',\n",
       "   'classifier__solver': 'saga'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 10000.0,\n",
       "   'classifier__penalty': 'l1',\n",
       "   'classifier__solver': 'saga'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 10000.0,\n",
       "   'classifier__penalty': 'l2',\n",
       "   'classifier__solver': 'saga'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 0.0001,\n",
       "   'classifier__penalty': 'l2',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 0.001,\n",
       "   'classifier__penalty': 'l2',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 0.01,\n",
       "   'classifier__penalty': 'l2',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 0.1,\n",
       "   'classifier__penalty': 'l2',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 1.0,\n",
       "   'classifier__penalty': 'l2',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 10.0,\n",
       "   'classifier__penalty': 'l2',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 100.0,\n",
       "   'classifier__penalty': 'l2',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 1000.0,\n",
       "   'classifier__penalty': 'l2',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 10000.0,\n",
       "   'classifier__penalty': 'l2',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__penalty': 'none',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__penalty': 'none',\n",
       "   'classifier__solver': 'saga'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000)}],\n",
       " 'split0_test_accuracy': array([0.594, 0.701, 0.686, 0.764, 0.764, 0.759, 0.762, 0.761, 0.761,\n",
       "        0.761, 0.761, 0.761, 0.761, 0.761, 0.761, 0.761, 0.761, 0.761,\n",
       "        0.701, 0.764, 0.759, 0.761, 0.761, 0.761, 0.761, 0.761, 0.761,\n",
       "        0.761, 0.761, 0.761]),\n",
       " 'split1_test_accuracy': array([0.593, 0.681, 0.665, 0.731, 0.736, 0.738, 0.739, 0.738, 0.738,\n",
       "        0.738, 0.738, 0.738, 0.738, 0.738, 0.738, 0.738, 0.738, 0.738,\n",
       "        0.681, 0.731, 0.738, 0.738, 0.738, 0.738, 0.738, 0.738, 0.738,\n",
       "        0.738, 0.738, 0.738]),\n",
       " 'split2_test_accuracy': array([0.593, 0.705, 0.69 , 0.763, 0.769, 0.774, 0.774, 0.773, 0.773,\n",
       "        0.773, 0.773, 0.773, 0.773, 0.773, 0.773, 0.773, 0.773, 0.773,\n",
       "        0.705, 0.763, 0.774, 0.773, 0.773, 0.773, 0.773, 0.773, 0.773,\n",
       "        0.773, 0.773, 0.773]),\n",
       " 'split3_test_accuracy': array([0.593, 0.699, 0.688, 0.749, 0.763, 0.758, 0.762, 0.762, 0.762,\n",
       "        0.763, 0.763, 0.763, 0.763, 0.763, 0.763, 0.763, 0.763, 0.763,\n",
       "        0.699, 0.749, 0.758, 0.762, 0.763, 0.763, 0.763, 0.763, 0.763,\n",
       "        0.763, 0.763, 0.763]),\n",
       " 'split4_test_accuracy': array([0.593, 0.704, 0.699, 0.754, 0.753, 0.76 , 0.756, 0.759, 0.758,\n",
       "        0.759, 0.758, 0.758, 0.758, 0.758, 0.758, 0.758, 0.758, 0.758,\n",
       "        0.704, 0.754, 0.76 , 0.759, 0.759, 0.758, 0.758, 0.758, 0.758,\n",
       "        0.758, 0.758, 0.759]),\n",
       " 'split5_test_accuracy': array([0.593, 0.691, 0.679, 0.746, 0.74 , 0.739, 0.738, 0.736, 0.736,\n",
       "        0.736, 0.736, 0.736, 0.736, 0.736, 0.736, 0.736, 0.736, 0.736,\n",
       "        0.691, 0.746, 0.739, 0.736, 0.736, 0.736, 0.736, 0.736, 0.736,\n",
       "        0.736, 0.736, 0.736]),\n",
       " 'split6_test_accuracy': array([0.593, 0.706, 0.699, 0.771, 0.774, 0.777, 0.777, 0.775, 0.775,\n",
       "        0.774, 0.774, 0.774, 0.774, 0.774, 0.774, 0.774, 0.774, 0.774,\n",
       "        0.706, 0.771, 0.777, 0.775, 0.774, 0.774, 0.774, 0.774, 0.774,\n",
       "        0.774, 0.774, 0.774]),\n",
       " 'split7_test_accuracy': array([0.593, 0.687, 0.675, 0.731, 0.722, 0.717, 0.721, 0.722, 0.722,\n",
       "        0.722, 0.721, 0.721, 0.721, 0.721, 0.721, 0.721, 0.721, 0.721,\n",
       "        0.687, 0.731, 0.717, 0.722, 0.722, 0.721, 0.721, 0.721, 0.721,\n",
       "        0.721, 0.721, 0.722]),\n",
       " 'split8_test_accuracy': array([0.593, 0.7  , 0.696, 0.752, 0.759, 0.76 , 0.765, 0.76 , 0.761,\n",
       "        0.761, 0.761, 0.761, 0.761, 0.761, 0.761, 0.761, 0.761, 0.761,\n",
       "        0.7  , 0.752, 0.76 , 0.76 , 0.761, 0.761, 0.761, 0.761, 0.761,\n",
       "        0.761, 0.761, 0.761]),\n",
       " 'split9_test_accuracy': array([0.593, 0.718, 0.688, 0.756, 0.749, 0.755, 0.753, 0.75 , 0.75 ,\n",
       "        0.749, 0.749, 0.749, 0.749, 0.749, 0.749, 0.749, 0.749, 0.749,\n",
       "        0.718, 0.756, 0.755, 0.75 , 0.749, 0.749, 0.749, 0.749, 0.749,\n",
       "        0.749, 0.749, 0.749]),\n",
       " 'mean_test_accuracy': array([0.5931, 0.6992, 0.6865, 0.7517, 0.7529, 0.7537, 0.7547, 0.7536,\n",
       "        0.7536, 0.7536, 0.7534, 0.7534, 0.7534, 0.7534, 0.7534, 0.7534,\n",
       "        0.7534, 0.7534, 0.6992, 0.7517, 0.7537, 0.7536, 0.7536, 0.7534,\n",
       "        0.7534, 0.7534, 0.7534, 0.7534, 0.7534, 0.7536]),\n",
       " 'std_test_accuracy': array([0.0003    , 0.01003793, 0.0103465 , 0.01253834, 0.01545607,\n",
       "        0.01700618, 0.01657739, 0.01610714, 0.01611955, 0.01610093,\n",
       "        0.01626776, 0.01626776, 0.01626776, 0.01626776, 0.01626776,\n",
       "        0.01626776, 0.01626776, 0.01626776, 0.01003793, 0.01253834,\n",
       "        0.01700618, 0.01610714, 0.01610093, 0.01626776, 0.01626776,\n",
       "        0.01626776, 0.01626776, 0.01626776, 0.01626776, 0.01610093]),\n",
       " 'rank_test_accuracy': array([30, 27, 29, 25, 24,  2,  1,  5,  4,  5, 10, 10, 10, 10, 10, 10, 10,\n",
       "        10, 27, 25,  2,  5,  5, 10, 10, 10, 10, 10, 10,  5]),\n",
       " 'split0_test_precision': array([0.        , 0.78918919, 0.77058824, 0.74425287, 0.72972973,\n",
       "        0.71883289, 0.72105263, 0.7191601 , 0.7191601 , 0.7191601 ,\n",
       "        0.7191601 , 0.7191601 , 0.7191601 , 0.7191601 , 0.7191601 ,\n",
       "        0.7191601 , 0.7191601 , 0.7191601 , 0.78918919, 0.74425287,\n",
       "        0.71883289, 0.7191601 , 0.7191601 , 0.7191601 , 0.7191601 ,\n",
       "        0.7191601 , 0.7191601 , 0.7191601 , 0.7191601 , 0.7191601 ]),\n",
       " 'split1_test_precision': array([0.        , 0.74444444, 0.70454545, 0.69060773, 0.68766404,\n",
       "        0.68637532, 0.68814433, 0.6873385 , 0.6873385 , 0.6873385 ,\n",
       "        0.6873385 , 0.6873385 , 0.6873385 , 0.6873385 , 0.6873385 ,\n",
       "        0.6873385 , 0.6873385 , 0.6873385 , 0.74444444, 0.69060773,\n",
       "        0.68637532, 0.6873385 , 0.6873385 , 0.6873385 , 0.6873385 ,\n",
       "        0.6873385 , 0.6873385 , 0.6873385 , 0.6873385 , 0.6873385 ]),\n",
       " 'split2_test_precision': array([0.        , 0.79473684, 0.79041916, 0.75449102, 0.74581006,\n",
       "        0.74931129, 0.74659401, 0.74456522, 0.74456522, 0.74456522,\n",
       "        0.74456522, 0.74456522, 0.74456522, 0.74456522, 0.74456522,\n",
       "        0.74456522, 0.74456522, 0.74456522, 0.79473684, 0.75449102,\n",
       "        0.74931129, 0.74456522, 0.74456522, 0.74456522, 0.74456522,\n",
       "        0.74456522, 0.74456522, 0.74456522, 0.74456522, 0.74456522]),\n",
       " 'split3_test_precision': array([0.        , 0.75980392, 0.74111675, 0.72159091, 0.72849462,\n",
       "        0.7154047 , 0.71501272, 0.71611253, 0.71611253, 0.71794872,\n",
       "        0.71794872, 0.71794872, 0.71794872, 0.71794872, 0.71794872,\n",
       "        0.71794872, 0.71794872, 0.71794872, 0.75980392, 0.72159091,\n",
       "        0.7154047 , 0.71611253, 0.71794872, 0.71794872, 0.71794872,\n",
       "        0.71794872, 0.71794872, 0.71794872, 0.71794872, 0.71794872]),\n",
       " 'split4_test_precision': array([0.        , 0.73819742, 0.76237624, 0.70588235, 0.69512195,\n",
       "        0.70023981, 0.6935867 , 0.69668246, 0.69503546, 0.69668246,\n",
       "        0.69503546, 0.69503546, 0.69503546, 0.69503546, 0.69503546,\n",
       "        0.69503546, 0.69503546, 0.69503546, 0.73819742, 0.70588235,\n",
       "        0.70023981, 0.69668246, 0.69668246, 0.69503546, 0.69503546,\n",
       "        0.69503546, 0.69503546, 0.69503546, 0.69503546, 0.69668246]),\n",
       " 'split5_test_precision': array([0.        , 0.75      , 0.73626374, 0.71919771, 0.69705094,\n",
       "        0.69518717, 0.69230769, 0.69066667, 0.69066667, 0.69066667,\n",
       "        0.69066667, 0.69066667, 0.69066667, 0.69066667, 0.69066667,\n",
       "        0.69066667, 0.69066667, 0.69066667, 0.75      , 0.71919771,\n",
       "        0.69518717, 0.69066667, 0.69066667, 0.69066667, 0.69066667,\n",
       "        0.69066667, 0.69066667, 0.69066667, 0.69066667, 0.69066667]),\n",
       " 'split6_test_precision': array([0.        , 0.77560976, 0.77319588, 0.75      , 0.75069252,\n",
       "        0.7459893 , 0.7459893 , 0.74331551, 0.74331551, 0.74133333,\n",
       "        0.74133333, 0.74133333, 0.74133333, 0.74133333, 0.74133333,\n",
       "        0.74133333, 0.74133333, 0.74133333, 0.77560976, 0.75      ,\n",
       "        0.7459893 , 0.74331551, 0.74133333, 0.74133333, 0.74133333,\n",
       "        0.74133333, 0.74133333, 0.74133333, 0.74133333, 0.74133333]),\n",
       " 'split7_test_precision': array([0.        , 0.72815534, 0.70707071, 0.6875    , 0.66929134,\n",
       "        0.66062176, 0.66494845, 0.66496164, 0.66496164, 0.66496164,\n",
       "        0.66410256, 0.66410256, 0.66410256, 0.66410256, 0.66410256,\n",
       "        0.66410256, 0.66410256, 0.66410256, 0.72815534, 0.6875    ,\n",
       "        0.66062176, 0.66496164, 0.66496164, 0.66410256, 0.66410256,\n",
       "        0.66410256, 0.66410256, 0.66410256, 0.66410256, 0.66496164]),\n",
       " 'split8_test_precision': array([0.        , 0.78306878, 0.77837838, 0.71662125, 0.7106599 ,\n",
       "        0.71032746, 0.71287129, 0.70617284, 0.70792079, 0.70792079,\n",
       "        0.70792079, 0.70792079, 0.70792079, 0.70792079, 0.70792079,\n",
       "        0.70792079, 0.70792079, 0.70792079, 0.78306878, 0.71662125,\n",
       "        0.71032746, 0.70617284, 0.70792079, 0.70792079, 0.70792079,\n",
       "        0.70792079, 0.70792079, 0.70792079, 0.70792079, 0.70792079]),\n",
       " 'split9_test_precision': array([0.        , 0.78538813, 0.75132275, 0.72829132, 0.70103093,\n",
       "        0.70663265, 0.70100503, 0.697733  , 0.697733  , 0.6969697 ,\n",
       "        0.6969697 , 0.6969697 , 0.6969697 , 0.6969697 , 0.6969697 ,\n",
       "        0.6969697 , 0.6969697 , 0.6969697 , 0.78538813, 0.72829132,\n",
       "        0.70663265, 0.697733  , 0.6969697 , 0.6969697 , 0.6969697 ,\n",
       "        0.6969697 , 0.6969697 , 0.6969697 , 0.6969697 , 0.6969697 ]),\n",
       " 'mean_test_precision': array([0.        , 0.76485938, 0.75152773, 0.72184352, 0.7115546 ,\n",
       "        0.70889224, 0.70815122, 0.70667085, 0.70668094, 0.70675471,\n",
       "        0.70650411, 0.70650411, 0.70650411, 0.70650411, 0.70650411,\n",
       "        0.70650411, 0.70650411, 0.70650411, 0.76485938, 0.72184352,\n",
       "        0.70889224, 0.70667085, 0.70675471, 0.70650411, 0.70650411,\n",
       "        0.70650411, 0.70650411, 0.70650411, 0.70650411, 0.70675471]),\n",
       " 'std_test_precision': array([0.        , 0.02252634, 0.02776542, 0.0220151 , 0.02504275,\n",
       "        0.02501606, 0.02434407, 0.02360799, 0.02368608, 0.02342124,\n",
       "        0.02365027, 0.02365027, 0.02365027, 0.02365027, 0.02365027,\n",
       "        0.02365027, 0.02365027, 0.02365027, 0.02252634, 0.0220151 ,\n",
       "        0.02501606, 0.02360799, 0.02342124, 0.02365027, 0.02365027,\n",
       "        0.02365027, 0.02365027, 0.02365027, 0.02365027, 0.02342124]),\n",
       " 'rank_test_precision': array([30,  1,  3,  4,  6,  7,  9, 14, 13, 10, 16, 16, 16, 16, 16, 16, 16,\n",
       "        16,  1,  4,  7, 14, 10, 16, 16, 16, 16, 16, 16, 10]),\n",
       " 'split0_test_f1_micro': array([0.594, 0.701, 0.686, 0.764, 0.764, 0.759, 0.762, 0.761, 0.761,\n",
       "        0.761, 0.761, 0.761, 0.761, 0.761, 0.761, 0.761, 0.761, 0.761,\n",
       "        0.701, 0.764, 0.759, 0.761, 0.761, 0.761, 0.761, 0.761, 0.761,\n",
       "        0.761, 0.761, 0.761]),\n",
       " 'split1_test_f1_micro': array([0.593, 0.681, 0.665, 0.731, 0.736, 0.738, 0.739, 0.738, 0.738,\n",
       "        0.738, 0.738, 0.738, 0.738, 0.738, 0.738, 0.738, 0.738, 0.738,\n",
       "        0.681, 0.731, 0.738, 0.738, 0.738, 0.738, 0.738, 0.738, 0.738,\n",
       "        0.738, 0.738, 0.738]),\n",
       " 'split2_test_f1_micro': array([0.593, 0.705, 0.69 , 0.763, 0.769, 0.774, 0.774, 0.773, 0.773,\n",
       "        0.773, 0.773, 0.773, 0.773, 0.773, 0.773, 0.773, 0.773, 0.773,\n",
       "        0.705, 0.763, 0.774, 0.773, 0.773, 0.773, 0.773, 0.773, 0.773,\n",
       "        0.773, 0.773, 0.773]),\n",
       " 'split3_test_f1_micro': array([0.593, 0.699, 0.688, 0.749, 0.763, 0.758, 0.762, 0.762, 0.762,\n",
       "        0.763, 0.763, 0.763, 0.763, 0.763, 0.763, 0.763, 0.763, 0.763,\n",
       "        0.699, 0.749, 0.758, 0.762, 0.763, 0.763, 0.763, 0.763, 0.763,\n",
       "        0.763, 0.763, 0.763]),\n",
       " 'split4_test_f1_micro': array([0.593, 0.704, 0.699, 0.754, 0.753, 0.76 , 0.756, 0.759, 0.758,\n",
       "        0.759, 0.758, 0.758, 0.758, 0.758, 0.758, 0.758, 0.758, 0.758,\n",
       "        0.704, 0.754, 0.76 , 0.759, 0.759, 0.758, 0.758, 0.758, 0.758,\n",
       "        0.758, 0.758, 0.759]),\n",
       " 'split5_test_f1_micro': array([0.593, 0.691, 0.679, 0.746, 0.74 , 0.739, 0.738, 0.736, 0.736,\n",
       "        0.736, 0.736, 0.736, 0.736, 0.736, 0.736, 0.736, 0.736, 0.736,\n",
       "        0.691, 0.746, 0.739, 0.736, 0.736, 0.736, 0.736, 0.736, 0.736,\n",
       "        0.736, 0.736, 0.736]),\n",
       " 'split6_test_f1_micro': array([0.593, 0.706, 0.699, 0.771, 0.774, 0.777, 0.777, 0.775, 0.775,\n",
       "        0.774, 0.774, 0.774, 0.774, 0.774, 0.774, 0.774, 0.774, 0.774,\n",
       "        0.706, 0.771, 0.777, 0.775, 0.774, 0.774, 0.774, 0.774, 0.774,\n",
       "        0.774, 0.774, 0.774]),\n",
       " 'split7_test_f1_micro': array([0.593, 0.687, 0.675, 0.731, 0.722, 0.717, 0.721, 0.722, 0.722,\n",
       "        0.722, 0.721, 0.721, 0.721, 0.721, 0.721, 0.721, 0.721, 0.721,\n",
       "        0.687, 0.731, 0.717, 0.722, 0.722, 0.721, 0.721, 0.721, 0.721,\n",
       "        0.721, 0.721, 0.722]),\n",
       " 'split8_test_f1_micro': array([0.593, 0.7  , 0.696, 0.752, 0.759, 0.76 , 0.765, 0.76 , 0.761,\n",
       "        0.761, 0.761, 0.761, 0.761, 0.761, 0.761, 0.761, 0.761, 0.761,\n",
       "        0.7  , 0.752, 0.76 , 0.76 , 0.761, 0.761, 0.761, 0.761, 0.761,\n",
       "        0.761, 0.761, 0.761]),\n",
       " 'split9_test_f1_micro': array([0.593, 0.718, 0.688, 0.756, 0.749, 0.755, 0.753, 0.75 , 0.75 ,\n",
       "        0.749, 0.749, 0.749, 0.749, 0.749, 0.749, 0.749, 0.749, 0.749,\n",
       "        0.718, 0.756, 0.755, 0.75 , 0.749, 0.749, 0.749, 0.749, 0.749,\n",
       "        0.749, 0.749, 0.749]),\n",
       " 'mean_test_f1_micro': array([0.5931, 0.6992, 0.6865, 0.7517, 0.7529, 0.7537, 0.7547, 0.7536,\n",
       "        0.7536, 0.7536, 0.7534, 0.7534, 0.7534, 0.7534, 0.7534, 0.7534,\n",
       "        0.7534, 0.7534, 0.6992, 0.7517, 0.7537, 0.7536, 0.7536, 0.7534,\n",
       "        0.7534, 0.7534, 0.7534, 0.7534, 0.7534, 0.7536]),\n",
       " 'std_test_f1_micro': array([0.0003    , 0.01003793, 0.0103465 , 0.01253834, 0.01545607,\n",
       "        0.01700618, 0.01657739, 0.01610714, 0.01611955, 0.01610093,\n",
       "        0.01626776, 0.01626776, 0.01626776, 0.01626776, 0.01626776,\n",
       "        0.01626776, 0.01626776, 0.01626776, 0.01003793, 0.01253834,\n",
       "        0.01700618, 0.01610714, 0.01610093, 0.01626776, 0.01626776,\n",
       "        0.01626776, 0.01626776, 0.01626776, 0.01626776, 0.01610093]),\n",
       " 'rank_test_f1_micro': array([30, 27, 29, 25, 24,  2,  1,  4,  4,  4, 10, 10, 10, 10, 10, 10, 10,\n",
       "        10, 27, 25,  2,  4,  4, 10, 10, 10, 10, 10, 10,  4])}"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_model.cv_results_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "a76a5ef1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([30, 27, 29, 25, 24,  2,  1,  5,  4,  5, 10, 10, 10, 10, 10, 10, 10,\n",
       "       10, 27, 25,  2,  5,  5, 10, 10, 10, 10, 10, 10,  5])"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_model.cv_results_['rank_test_accuracy']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "80177251",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'classifier': LogisticRegression(max_iter=10000),\n",
       " 'classifier__C': 0.0001,\n",
       " 'classifier__penalty': 'l1',\n",
       " 'classifier__solver': 'saga'}"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_model.cv_results_['params'][ np.argmin(best_model.cv_results_['mean_test_accuracy'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "543c8047",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>classifier</th>\n",
       "      <th>C</th>\n",
       "      <th>penalty</th>\n",
       "      <th>solver</th>\n",
       "      <th>score_acc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.4069</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.3008</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.3135</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.2483</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0100</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.2471</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0100</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.2463</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.1000</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.2453</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.1000</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.2464</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.2464</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.2464</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>10.0000</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.2466</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>10.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.2466</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>100.0000</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.2466</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>100.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.2466</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>1000.0000</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.2466</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>1000.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.2466</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>10000.0000</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.2466</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>10000.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.2466</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.3008</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.2483</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0100</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.2463</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.1000</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.2464</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.2464</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>10.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.2466</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>100.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.2466</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>1000.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.2466</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>10000.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.2466</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>NaN</td>\n",
       "      <td>none</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.2466</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>NaN</td>\n",
       "      <td>none</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.2466</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.2464</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                            classifier           C penalty solver  score_acc\n",
       "0   LogisticRegression(max_iter=10000)      0.0001      l1   saga     0.4069\n",
       "1   LogisticRegression(max_iter=10000)      0.0001      l2   saga     0.3008\n",
       "2   LogisticRegression(max_iter=10000)      0.0010      l1   saga     0.3135\n",
       "3   LogisticRegression(max_iter=10000)      0.0010      l2   saga     0.2483\n",
       "4   LogisticRegression(max_iter=10000)      0.0100      l1   saga     0.2471\n",
       "5   LogisticRegression(max_iter=10000)      0.0100      l2   saga     0.2463\n",
       "6   LogisticRegression(max_iter=10000)      0.1000      l1   saga     0.2453\n",
       "7   LogisticRegression(max_iter=10000)      0.1000      l2   saga     0.2464\n",
       "8   LogisticRegression(max_iter=10000)      1.0000      l1   saga     0.2464\n",
       "9   LogisticRegression(max_iter=10000)      1.0000      l2   saga     0.2464\n",
       "10  LogisticRegression(max_iter=10000)     10.0000      l1   saga     0.2466\n",
       "11  LogisticRegression(max_iter=10000)     10.0000      l2   saga     0.2466\n",
       "12  LogisticRegression(max_iter=10000)    100.0000      l1   saga     0.2466\n",
       "13  LogisticRegression(max_iter=10000)    100.0000      l2   saga     0.2466\n",
       "14  LogisticRegression(max_iter=10000)   1000.0000      l1   saga     0.2466\n",
       "15  LogisticRegression(max_iter=10000)   1000.0000      l2   saga     0.2466\n",
       "16  LogisticRegression(max_iter=10000)  10000.0000      l1   saga     0.2466\n",
       "17  LogisticRegression(max_iter=10000)  10000.0000      l2   saga     0.2466\n",
       "18  LogisticRegression(max_iter=10000)      0.0001      l2  lbfgs     0.3008\n",
       "19  LogisticRegression(max_iter=10000)      0.0010      l2  lbfgs     0.2483\n",
       "20  LogisticRegression(max_iter=10000)      0.0100      l2  lbfgs     0.2463\n",
       "21  LogisticRegression(max_iter=10000)      0.1000      l2  lbfgs     0.2464\n",
       "22  LogisticRegression(max_iter=10000)      1.0000      l2  lbfgs     0.2464\n",
       "23  LogisticRegression(max_iter=10000)     10.0000      l2  lbfgs     0.2466\n",
       "24  LogisticRegression(max_iter=10000)    100.0000      l2  lbfgs     0.2466\n",
       "25  LogisticRegression(max_iter=10000)   1000.0000      l2  lbfgs     0.2466\n",
       "26  LogisticRegression(max_iter=10000)  10000.0000      l2  lbfgs     0.2466\n",
       "27  LogisticRegression(max_iter=10000)         NaN    none  lbfgs     0.2466\n",
       "28  LogisticRegression(max_iter=10000)         NaN    none   saga     0.2466\n",
       "29  LogisticRegression(max_iter=10000)         NaN     NaN    NaN     0.2464"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results = pd.DataFrame( best_model.cv_results_['params'] )\n",
    "\n",
    "# next grab the score resulting from those parameters, add it to the data\n",
    "# score is accuracy; to display it as misclassification error we use 1 - x\n",
    "results['score_acc'] = 1 - best_model.cv_results_['mean_test_accuracy']\n",
    "\n",
    "# get rid of classifierXX in columns\n",
    "cols = results.columns.to_series().str.split('__').apply(lambda x: x[-1])\n",
    "results.columns = cols\n",
    "\n",
    "# lets show the results for the saga solver across penalty & C values\n",
    "#sns.heatmap( results.query('solver==\"saga\"').pivot('C','penalty','score_acc'),\n",
    "#             annot=True, fmt='.3f')\n",
    "\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "557091b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "acc_list.append(results[0:1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "b765827e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'classifier': LogisticRegression(max_iter=10000),\n",
       " 'classifier__C': 0.0001,\n",
       " 'classifier__penalty': 'l2',\n",
       " 'classifier__solver': 'saga'}"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_model.cv_results_['params'][ np.argmin(best_model.cv_results_['rank_test_precision'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "3d3fe844",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>classifier</th>\n",
       "      <th>C</th>\n",
       "      <th>penalty</th>\n",
       "      <th>solver</th>\n",
       "      <th>score_precision</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.235141</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.248472</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.278156</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0100</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.288445</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0100</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.291108</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.1000</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.291849</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.1000</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.293329</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.293319</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.293245</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>10.0000</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.293496</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>10.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.293496</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>100.0000</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.293496</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>100.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.293496</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>1000.0000</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.293496</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>1000.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.293496</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>10000.0000</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.293496</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>10000.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.293496</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.235141</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.278156</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0100</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.291108</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.1000</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.293329</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.293245</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>10.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.293496</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>100.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.293496</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>1000.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.293496</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>10000.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.293496</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>NaN</td>\n",
       "      <td>none</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.293496</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>NaN</td>\n",
       "      <td>none</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.293496</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.293245</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                            classifier           C penalty solver  \\\n",
       "0   LogisticRegression(max_iter=10000)      0.0001      l1   saga   \n",
       "1   LogisticRegression(max_iter=10000)      0.0001      l2   saga   \n",
       "2   LogisticRegression(max_iter=10000)      0.0010      l1   saga   \n",
       "3   LogisticRegression(max_iter=10000)      0.0010      l2   saga   \n",
       "4   LogisticRegression(max_iter=10000)      0.0100      l1   saga   \n",
       "5   LogisticRegression(max_iter=10000)      0.0100      l2   saga   \n",
       "6   LogisticRegression(max_iter=10000)      0.1000      l1   saga   \n",
       "7   LogisticRegression(max_iter=10000)      0.1000      l2   saga   \n",
       "8   LogisticRegression(max_iter=10000)      1.0000      l1   saga   \n",
       "9   LogisticRegression(max_iter=10000)      1.0000      l2   saga   \n",
       "10  LogisticRegression(max_iter=10000)     10.0000      l1   saga   \n",
       "11  LogisticRegression(max_iter=10000)     10.0000      l2   saga   \n",
       "12  LogisticRegression(max_iter=10000)    100.0000      l1   saga   \n",
       "13  LogisticRegression(max_iter=10000)    100.0000      l2   saga   \n",
       "14  LogisticRegression(max_iter=10000)   1000.0000      l1   saga   \n",
       "15  LogisticRegression(max_iter=10000)   1000.0000      l2   saga   \n",
       "16  LogisticRegression(max_iter=10000)  10000.0000      l1   saga   \n",
       "17  LogisticRegression(max_iter=10000)  10000.0000      l2   saga   \n",
       "18  LogisticRegression(max_iter=10000)      0.0001      l2  lbfgs   \n",
       "19  LogisticRegression(max_iter=10000)      0.0010      l2  lbfgs   \n",
       "20  LogisticRegression(max_iter=10000)      0.0100      l2  lbfgs   \n",
       "21  LogisticRegression(max_iter=10000)      0.1000      l2  lbfgs   \n",
       "22  LogisticRegression(max_iter=10000)      1.0000      l2  lbfgs   \n",
       "23  LogisticRegression(max_iter=10000)     10.0000      l2  lbfgs   \n",
       "24  LogisticRegression(max_iter=10000)    100.0000      l2  lbfgs   \n",
       "25  LogisticRegression(max_iter=10000)   1000.0000      l2  lbfgs   \n",
       "26  LogisticRegression(max_iter=10000)  10000.0000      l2  lbfgs   \n",
       "27  LogisticRegression(max_iter=10000)         NaN    none  lbfgs   \n",
       "28  LogisticRegression(max_iter=10000)         NaN    none   saga   \n",
       "29  LogisticRegression(max_iter=10000)         NaN     NaN    NaN   \n",
       "\n",
       "    score_precision  \n",
       "0          1.000000  \n",
       "1          0.235141  \n",
       "2          0.248472  \n",
       "3          0.278156  \n",
       "4          0.288445  \n",
       "5          0.291108  \n",
       "6          0.291849  \n",
       "7          0.293329  \n",
       "8          0.293319  \n",
       "9          0.293245  \n",
       "10         0.293496  \n",
       "11         0.293496  \n",
       "12         0.293496  \n",
       "13         0.293496  \n",
       "14         0.293496  \n",
       "15         0.293496  \n",
       "16         0.293496  \n",
       "17         0.293496  \n",
       "18         0.235141  \n",
       "19         0.278156  \n",
       "20         0.291108  \n",
       "21         0.293329  \n",
       "22         0.293245  \n",
       "23         0.293496  \n",
       "24         0.293496  \n",
       "25         0.293496  \n",
       "26         0.293496  \n",
       "27         0.293496  \n",
       "28         0.293496  \n",
       "29         0.293245  "
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results = pd.DataFrame( best_model.cv_results_['params'] )\n",
    "\n",
    "# next grab the score resulting from those parameters, add it to the data\n",
    "# score is accuracy; to display it as misclassification error we use 1 - x\n",
    "results['score_precision'] = 1 - best_model.cv_results_['mean_test_precision']\n",
    "\n",
    "# get rid of classifierXX in columns\n",
    "cols = results.columns.to_series().str.split('__').apply(lambda x: x[-1])\n",
    "results.columns = cols\n",
    "\n",
    "# lets show the results for the saga solver across penalty & C values\n",
    "#sns.heatmap( results.query('solver==\"saga\"').pivot('C','penalty','score_precision'),\n",
    "#             annot=True, fmt='.3f')\n",
    "\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "b27ecb6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "roc_auc_ovr_List.append(results[1:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "531a42ca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'classifier': LogisticRegression(max_iter=10000),\n",
       " 'classifier__C': 0.1,\n",
       " 'classifier__penalty': 'l1',\n",
       " 'classifier__solver': 'saga'}"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_model.cv_results_['params'][ np.argmin(best_model.cv_results_['rank_test_f1_micro'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "37066f63",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>classifier</th>\n",
       "      <th>C</th>\n",
       "      <th>penalty</th>\n",
       "      <th>solver</th>\n",
       "      <th>score_f1_micro</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.4069</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.3008</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.3135</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.2483</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0100</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.2471</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0100</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.2463</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.1000</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.2453</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.1000</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.2464</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.2464</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.2464</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>10.0000</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.2466</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>10.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.2466</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>100.0000</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.2466</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>100.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.2466</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>1000.0000</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.2466</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>1000.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.2466</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>10000.0000</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.2466</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>10000.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.2466</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.3008</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.2483</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0100</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.2463</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.1000</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.2464</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.2464</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>10.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.2466</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>100.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.2466</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>1000.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.2466</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>10000.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.2466</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>NaN</td>\n",
       "      <td>none</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.2466</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>NaN</td>\n",
       "      <td>none</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.2466</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.2464</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                            classifier           C penalty solver  \\\n",
       "0   LogisticRegression(max_iter=10000)      0.0001      l1   saga   \n",
       "1   LogisticRegression(max_iter=10000)      0.0001      l2   saga   \n",
       "2   LogisticRegression(max_iter=10000)      0.0010      l1   saga   \n",
       "3   LogisticRegression(max_iter=10000)      0.0010      l2   saga   \n",
       "4   LogisticRegression(max_iter=10000)      0.0100      l1   saga   \n",
       "5   LogisticRegression(max_iter=10000)      0.0100      l2   saga   \n",
       "6   LogisticRegression(max_iter=10000)      0.1000      l1   saga   \n",
       "7   LogisticRegression(max_iter=10000)      0.1000      l2   saga   \n",
       "8   LogisticRegression(max_iter=10000)      1.0000      l1   saga   \n",
       "9   LogisticRegression(max_iter=10000)      1.0000      l2   saga   \n",
       "10  LogisticRegression(max_iter=10000)     10.0000      l1   saga   \n",
       "11  LogisticRegression(max_iter=10000)     10.0000      l2   saga   \n",
       "12  LogisticRegression(max_iter=10000)    100.0000      l1   saga   \n",
       "13  LogisticRegression(max_iter=10000)    100.0000      l2   saga   \n",
       "14  LogisticRegression(max_iter=10000)   1000.0000      l1   saga   \n",
       "15  LogisticRegression(max_iter=10000)   1000.0000      l2   saga   \n",
       "16  LogisticRegression(max_iter=10000)  10000.0000      l1   saga   \n",
       "17  LogisticRegression(max_iter=10000)  10000.0000      l2   saga   \n",
       "18  LogisticRegression(max_iter=10000)      0.0001      l2  lbfgs   \n",
       "19  LogisticRegression(max_iter=10000)      0.0010      l2  lbfgs   \n",
       "20  LogisticRegression(max_iter=10000)      0.0100      l2  lbfgs   \n",
       "21  LogisticRegression(max_iter=10000)      0.1000      l2  lbfgs   \n",
       "22  LogisticRegression(max_iter=10000)      1.0000      l2  lbfgs   \n",
       "23  LogisticRegression(max_iter=10000)     10.0000      l2  lbfgs   \n",
       "24  LogisticRegression(max_iter=10000)    100.0000      l2  lbfgs   \n",
       "25  LogisticRegression(max_iter=10000)   1000.0000      l2  lbfgs   \n",
       "26  LogisticRegression(max_iter=10000)  10000.0000      l2  lbfgs   \n",
       "27  LogisticRegression(max_iter=10000)         NaN    none  lbfgs   \n",
       "28  LogisticRegression(max_iter=10000)         NaN    none   saga   \n",
       "29  LogisticRegression(max_iter=10000)         NaN     NaN    NaN   \n",
       "\n",
       "    score_f1_micro  \n",
       "0           0.4069  \n",
       "1           0.3008  \n",
       "2           0.3135  \n",
       "3           0.2483  \n",
       "4           0.2471  \n",
       "5           0.2463  \n",
       "6           0.2453  \n",
       "7           0.2464  \n",
       "8           0.2464  \n",
       "9           0.2464  \n",
       "10          0.2466  \n",
       "11          0.2466  \n",
       "12          0.2466  \n",
       "13          0.2466  \n",
       "14          0.2466  \n",
       "15          0.2466  \n",
       "16          0.2466  \n",
       "17          0.2466  \n",
       "18          0.3008  \n",
       "19          0.2483  \n",
       "20          0.2463  \n",
       "21          0.2464  \n",
       "22          0.2464  \n",
       "23          0.2466  \n",
       "24          0.2466  \n",
       "25          0.2466  \n",
       "26          0.2466  \n",
       "27          0.2466  \n",
       "28          0.2466  \n",
       "29          0.2464  "
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results = pd.DataFrame( best_model.cv_results_['params'] )\n",
    "\n",
    "# next grab the score resulting from those parameters, add it to the data\n",
    "# score is accuracy; to display it as misclassification error we use 1 - x\n",
    "results['score_f1_micro'] = 1 - best_model.cv_results_['mean_test_f1_micro']\n",
    "\n",
    "# get rid of classifierXX in columns\n",
    "cols = results.columns.to_series().str.split('__').apply(lambda x: x[-1])\n",
    "results.columns = cols\n",
    "\n",
    "# lets show the results for the saga solver across penalty & C values\n",
    "#sns.heatmap( results.query('solver==\"saga\"').pivot('C','penalty','score_f1_micro'),\n",
    "#             annot=True, fmt='.3f')\n",
    "\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "56f4f911",
   "metadata": {},
   "outputs": [],
   "source": [
    "f1_micro_List.append(results[6:7])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b7bf1d0",
   "metadata": {},
   "source": [
    "## Trial 9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "b5b093d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, Y_train, Y_test = train_test_split(X, y, train_size=10000, shuffle=True, random_state = 9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "90fdec31",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Guest-All\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1469: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\Guest-All\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1469: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\Guest-All\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1469: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\Guest-All\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1469: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\Guest-All\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1469: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\Guest-All\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1469: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\Guest-All\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1469: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\Guest-All\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1469: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\Guest-All\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1469: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\Guest-All\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1469: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\Guest-All\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1182: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Guest-All\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1182: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Guest-All\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1182: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Guest-All\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1182: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Guest-All\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1182: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Guest-All\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1182: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Guest-All\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1182: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Guest-All\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1182: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Guest-All\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1182: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Guest-All\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1182: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Guest-All\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1182: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Guest-All\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1182: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Guest-All\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1182: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Guest-All\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1182: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Guest-All\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1182: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Guest-All\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1182: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Guest-All\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1182: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Guest-All\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1182: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Guest-All\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1182: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Guest-All\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1182: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Create a pipeline - RF is a stand in, we will populate the classifier part below\n",
    "pipe = Pipeline([('std', StandardScaler()),\n",
    "                 ('classifier', LogisticRegression())])\n",
    "\n",
    "# Create search space of candidate learning algorithms and their hyperparameters\n",
    "# note lbfgs can't do l1, and if you pass penalty='none' it expects no C value\n",
    "search_space = [{'classifier': [LogisticRegression(max_iter=10000)],\n",
    "                 'classifier__solver': ['saga'],\n",
    "                 'classifier__penalty': ['l1', 'l2'],\n",
    "                 'classifier__C': np.logspace(-4, 4, 9)},\n",
    "                {'classifier': [LogisticRegression(max_iter=10000)],\n",
    "                 'classifier__solver': ['lbfgs'],\n",
    "                 'classifier__penalty': ['l2'],\n",
    "                 'classifier__C': np.logspace(-4, 4, 9)},\n",
    "                {'classifier': [LogisticRegression(max_iter=10000)],\n",
    "                 'classifier__solver': ['lbfgs','saga'],\n",
    "                 'classifier__penalty': ['none']},\n",
    "                {'classifier': [LogisticRegression(max_iter=10000)]}\n",
    "                ]\n",
    "\n",
    "# Create grid search\n",
    "clf = GridSearchCV(pipe, search_space, cv=StratifiedKFold(n_splits=10),\n",
    "                   scoring=['accuracy', 'precision', 'f1_micro'], refit=False,\n",
    "                   verbose=0)\n",
    "\n",
    "# Fit grid search\n",
    "best_model = clf.fit(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "079fae53",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'mean_fit_time': array([0.00856683, 0.04491405, 0.04667151, 0.04508078, 0.0516027 ,\n",
       "        0.04677253, 0.05672667, 0.045591  , 0.05495868, 0.04786475,\n",
       "        0.0567843 , 0.04507842, 0.05697658, 0.04800336, 0.0538439 ,\n",
       "        0.0458421 , 0.05170581, 0.04254258, 0.0126024 , 0.01133897,\n",
       "        0.01363463, 0.01582949, 0.01525238, 0.01429677, 0.01297162,\n",
       "        0.01417441, 0.01507125, 0.01686378, 0.04887755, 0.01518228]),\n",
       " 'std_fit_time': array([0.0066089 , 0.00524514, 0.00382878, 0.00872046, 0.00737454,\n",
       "        0.00116615, 0.00730341, 0.00478683, 0.0086961 , 0.00963725,\n",
       "        0.00831757, 0.00479615, 0.00731273, 0.0026454 , 0.00753353,\n",
       "        0.0038575 , 0.00605455, 0.00696722, 0.00690682, 0.00662506,\n",
       "        0.00749616, 0.00404057, 0.00144201, 0.00480595, 0.00761546,\n",
       "        0.00472873, 0.00184831, 0.00797889, 0.0041788 , 0.00581332]),\n",
       " 'mean_score_time': array([0.01211023, 0.00164735, 0.00847194, 0.00771332, 0.00425959,\n",
       "        0.00824516, 0.00315061, 0.0093749 , 0.00472288, 0.00450954,\n",
       "        0.00397179, 0.00674083, 0.00426457, 0.0038336 , 0.00749865,\n",
       "        0.00597415, 0.00940683, 0.00614595, 0.00627673, 0.00566766,\n",
       "        0.00680676, 0.00463936, 0.00509338, 0.00807967, 0.00630498,\n",
       "        0.00317788, 0.00534611, 0.0055258 , 0.0031486 , 0.00705893]),\n",
       " 'std_score_time': array([0.00599256, 0.00329586, 0.00737206, 0.0069992 , 0.00660272,\n",
       "        0.00752445, 0.00630147, 0.00765458, 0.0072145 , 0.00610925,\n",
       "        0.00633397, 0.00736858, 0.00659613, 0.0062633 , 0.00755342,\n",
       "        0.00734976, 0.00755836, 0.00753326, 0.00768761, 0.00700425,\n",
       "        0.00738331, 0.00589579, 0.00785775, 0.00690989, 0.00719667,\n",
       "        0.00635686, 0.00700577, 0.00764282, 0.00519262, 0.00675071]),\n",
       " 'param_classifier': masked_array(data=[LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000)],\n",
       "              mask=[False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'param_classifier__C': masked_array(data=[0.0001, 0.0001, 0.001, 0.001, 0.01, 0.01, 0.1, 0.1,\n",
       "                    1.0, 1.0, 10.0, 10.0, 100.0, 100.0, 1000.0, 1000.0,\n",
       "                    10000.0, 10000.0, 0.0001, 0.001, 0.01, 0.1, 1.0, 10.0,\n",
       "                    100.0, 1000.0, 10000.0, --, --, --],\n",
       "              mask=[False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False,  True,  True,  True],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'param_classifier__penalty': masked_array(data=['l1', 'l2', 'l1', 'l2', 'l1', 'l2', 'l1', 'l2', 'l1',\n",
       "                    'l2', 'l1', 'l2', 'l1', 'l2', 'l1', 'l2', 'l1', 'l2',\n",
       "                    'l2', 'l2', 'l2', 'l2', 'l2', 'l2', 'l2', 'l2', 'l2',\n",
       "                    'none', 'none', --],\n",
       "              mask=[False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False,  True],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'param_classifier__solver': masked_array(data=['saga', 'saga', 'saga', 'saga', 'saga', 'saga', 'saga',\n",
       "                    'saga', 'saga', 'saga', 'saga', 'saga', 'saga', 'saga',\n",
       "                    'saga', 'saga', 'saga', 'saga', 'lbfgs', 'lbfgs',\n",
       "                    'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs',\n",
       "                    'lbfgs', 'lbfgs', 'saga', --],\n",
       "              mask=[False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False,  True],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'params': [{'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 0.0001,\n",
       "   'classifier__penalty': 'l1',\n",
       "   'classifier__solver': 'saga'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 0.0001,\n",
       "   'classifier__penalty': 'l2',\n",
       "   'classifier__solver': 'saga'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 0.001,\n",
       "   'classifier__penalty': 'l1',\n",
       "   'classifier__solver': 'saga'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 0.001,\n",
       "   'classifier__penalty': 'l2',\n",
       "   'classifier__solver': 'saga'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 0.01,\n",
       "   'classifier__penalty': 'l1',\n",
       "   'classifier__solver': 'saga'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 0.01,\n",
       "   'classifier__penalty': 'l2',\n",
       "   'classifier__solver': 'saga'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 0.1,\n",
       "   'classifier__penalty': 'l1',\n",
       "   'classifier__solver': 'saga'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 0.1,\n",
       "   'classifier__penalty': 'l2',\n",
       "   'classifier__solver': 'saga'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 1.0,\n",
       "   'classifier__penalty': 'l1',\n",
       "   'classifier__solver': 'saga'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 1.0,\n",
       "   'classifier__penalty': 'l2',\n",
       "   'classifier__solver': 'saga'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 10.0,\n",
       "   'classifier__penalty': 'l1',\n",
       "   'classifier__solver': 'saga'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 10.0,\n",
       "   'classifier__penalty': 'l2',\n",
       "   'classifier__solver': 'saga'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 100.0,\n",
       "   'classifier__penalty': 'l1',\n",
       "   'classifier__solver': 'saga'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 100.0,\n",
       "   'classifier__penalty': 'l2',\n",
       "   'classifier__solver': 'saga'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 1000.0,\n",
       "   'classifier__penalty': 'l1',\n",
       "   'classifier__solver': 'saga'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 1000.0,\n",
       "   'classifier__penalty': 'l2',\n",
       "   'classifier__solver': 'saga'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 10000.0,\n",
       "   'classifier__penalty': 'l1',\n",
       "   'classifier__solver': 'saga'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 10000.0,\n",
       "   'classifier__penalty': 'l2',\n",
       "   'classifier__solver': 'saga'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 0.0001,\n",
       "   'classifier__penalty': 'l2',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 0.001,\n",
       "   'classifier__penalty': 'l2',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 0.01,\n",
       "   'classifier__penalty': 'l2',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 0.1,\n",
       "   'classifier__penalty': 'l2',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 1.0,\n",
       "   'classifier__penalty': 'l2',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 10.0,\n",
       "   'classifier__penalty': 'l2',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 100.0,\n",
       "   'classifier__penalty': 'l2',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 1000.0,\n",
       "   'classifier__penalty': 'l2',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 10000.0,\n",
       "   'classifier__penalty': 'l2',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__penalty': 'none',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__penalty': 'none',\n",
       "   'classifier__solver': 'saga'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000)}],\n",
       " 'split0_test_accuracy': array([0.605, 0.684, 0.674, 0.732, 0.734, 0.734, 0.732, 0.729, 0.73 ,\n",
       "        0.729, 0.729, 0.729, 0.729, 0.729, 0.729, 0.729, 0.729, 0.729,\n",
       "        0.684, 0.732, 0.734, 0.729, 0.729, 0.729, 0.729, 0.729, 0.729,\n",
       "        0.729, 0.729, 0.729]),\n",
       " 'split1_test_accuracy': array([0.605, 0.697, 0.696, 0.76 , 0.759, 0.762, 0.754, 0.754, 0.754,\n",
       "        0.753, 0.753, 0.753, 0.753, 0.753, 0.753, 0.753, 0.753, 0.753,\n",
       "        0.697, 0.76 , 0.762, 0.754, 0.753, 0.753, 0.753, 0.753, 0.753,\n",
       "        0.753, 0.753, 0.753]),\n",
       " 'split2_test_accuracy': array([0.605, 0.682, 0.682, 0.746, 0.753, 0.752, 0.752, 0.753, 0.753,\n",
       "        0.753, 0.753, 0.753, 0.753, 0.753, 0.753, 0.753, 0.753, 0.753,\n",
       "        0.682, 0.746, 0.752, 0.753, 0.753, 0.753, 0.753, 0.753, 0.753,\n",
       "        0.753, 0.753, 0.753]),\n",
       " 'split3_test_accuracy': array([0.605, 0.685, 0.682, 0.751, 0.75 , 0.752, 0.75 , 0.751, 0.75 ,\n",
       "        0.751, 0.751, 0.752, 0.752, 0.752, 0.752, 0.752, 0.752, 0.752,\n",
       "        0.685, 0.751, 0.752, 0.751, 0.751, 0.752, 0.752, 0.752, 0.752,\n",
       "        0.752, 0.752, 0.751]),\n",
       " 'split4_test_accuracy': array([0.605, 0.685, 0.686, 0.744, 0.752, 0.757, 0.758, 0.757, 0.757,\n",
       "        0.756, 0.756, 0.756, 0.756, 0.756, 0.756, 0.756, 0.756, 0.756,\n",
       "        0.685, 0.744, 0.757, 0.757, 0.756, 0.756, 0.756, 0.756, 0.756,\n",
       "        0.756, 0.756, 0.756]),\n",
       " 'split5_test_accuracy': array([0.605, 0.696, 0.685, 0.752, 0.749, 0.743, 0.744, 0.744, 0.745,\n",
       "        0.745, 0.745, 0.745, 0.745, 0.745, 0.745, 0.745, 0.745, 0.745,\n",
       "        0.696, 0.752, 0.743, 0.744, 0.745, 0.745, 0.745, 0.745, 0.745,\n",
       "        0.745, 0.745, 0.745]),\n",
       " 'split6_test_accuracy': array([0.604, 0.675, 0.682, 0.739, 0.734, 0.731, 0.731, 0.729, 0.729,\n",
       "        0.729, 0.729, 0.729, 0.729, 0.729, 0.729, 0.729, 0.729, 0.729,\n",
       "        0.675, 0.739, 0.731, 0.729, 0.729, 0.729, 0.729, 0.729, 0.729,\n",
       "        0.729, 0.729, 0.729]),\n",
       " 'split7_test_accuracy': array([0.604, 0.699, 0.694, 0.757, 0.767, 0.763, 0.761, 0.762, 0.762,\n",
       "        0.762, 0.762, 0.762, 0.762, 0.762, 0.762, 0.762, 0.762, 0.762,\n",
       "        0.699, 0.757, 0.763, 0.762, 0.762, 0.762, 0.762, 0.762, 0.762,\n",
       "        0.762, 0.762, 0.762]),\n",
       " 'split8_test_accuracy': array([0.604, 0.689, 0.679, 0.704, 0.718, 0.714, 0.717, 0.72 , 0.72 ,\n",
       "        0.72 , 0.72 , 0.72 , 0.72 , 0.72 , 0.72 , 0.72 , 0.72 , 0.72 ,\n",
       "        0.689, 0.704, 0.714, 0.72 , 0.72 , 0.72 , 0.72 , 0.72 , 0.72 ,\n",
       "        0.72 , 0.72 , 0.72 ]),\n",
       " 'split9_test_accuracy': array([0.604, 0.675, 0.672, 0.747, 0.751, 0.748, 0.749, 0.753, 0.751,\n",
       "        0.752, 0.752, 0.752, 0.752, 0.752, 0.752, 0.752, 0.752, 0.752,\n",
       "        0.675, 0.747, 0.748, 0.753, 0.752, 0.752, 0.752, 0.752, 0.752,\n",
       "        0.752, 0.752, 0.752]),\n",
       " 'mean_test_accuracy': array([0.6046, 0.6867, 0.6832, 0.7432, 0.7467, 0.7456, 0.7448, 0.7452,\n",
       "        0.7451, 0.745 , 0.745 , 0.7451, 0.7451, 0.7451, 0.7451, 0.7451,\n",
       "        0.7451, 0.7451, 0.6867, 0.7432, 0.7456, 0.7452, 0.745 , 0.7451,\n",
       "        0.7451, 0.7451, 0.7451, 0.7451, 0.7451, 0.745 ]),\n",
       " 'std_test_accuracy': array([0.0004899 , 0.00811234, 0.00723602, 0.01521052, 0.01346143,\n",
       "        0.01463694, 0.01321212, 0.01347442, 0.01320947, 0.0132665 ,\n",
       "        0.0132665 , 0.01331503, 0.01331503, 0.01331503, 0.01331503,\n",
       "        0.01331503, 0.01331503, 0.01331503, 0.00811234, 0.01521052,\n",
       "        0.01463694, 0.01347442, 0.0132665 , 0.01331503, 0.01331503,\n",
       "        0.01331503, 0.01331503, 0.01331503, 0.01331503, 0.0132665 ]),\n",
       " 'rank_test_accuracy': array([30, 27, 29, 25,  1,  2, 24,  4,  6, 20, 20,  7,  7,  7,  7,  7,  7,\n",
       "         7, 27, 25,  2,  4, 20,  7,  7,  7,  7,  7,  7, 20]),\n",
       " 'split0_test_precision': array([0.        , 0.77622378, 0.70909091, 0.70550162, 0.68481375,\n",
       "        0.68169014, 0.67590028, 0.67127072, 0.67313019, 0.67127072,\n",
       "        0.67127072, 0.67127072, 0.67127072, 0.67127072, 0.67127072,\n",
       "        0.67127072, 0.67127072, 0.67127072, 0.77622378, 0.70550162,\n",
       "        0.68169014, 0.67127072, 0.67127072, 0.67127072, 0.67127072,\n",
       "        0.67127072, 0.67127072, 0.67127072, 0.67127072, 0.67127072]),\n",
       " 'split1_test_precision': array([0.        , 0.81506849, 0.79354839, 0.73413897, 0.7150838 ,\n",
       "        0.71866295, 0.70637119, 0.70523416, 0.70523416, 0.7032967 ,\n",
       "        0.7032967 , 0.7032967 , 0.7032967 , 0.7032967 , 0.7032967 ,\n",
       "        0.7032967 , 0.7032967 , 0.7032967 , 0.81506849, 0.73413897,\n",
       "        0.71866295, 0.70523416, 0.7032967 , 0.7032967 , 0.7032967 ,\n",
       "        0.7032967 , 0.7032967 , 0.7032967 , 0.7032967 , 0.7032967 ]),\n",
       " 'split2_test_precision': array([0.        , 0.75496689, 0.7251462 , 0.72239748, 0.71764706,\n",
       "        0.70940171, 0.7082153 , 0.71142857, 0.71142857, 0.71142857,\n",
       "        0.71142857, 0.71142857, 0.71142857, 0.71142857, 0.71142857,\n",
       "        0.71142857, 0.71142857, 0.71142857, 0.75496689, 0.72239748,\n",
       "        0.70940171, 0.71142857, 0.71142857, 0.71142857, 0.71142857,\n",
       "        0.71142857, 0.71142857, 0.71142857, 0.71142857, 0.71142857]),\n",
       " 'split3_test_precision': array([0.        , 0.75974026, 0.72254335, 0.7122093 , 0.70422535,\n",
       "        0.70360111, 0.70083102, 0.70165746, 0.69972452, 0.70165746,\n",
       "        0.70165746, 0.70247934, 0.70247934, 0.70247934, 0.70247934,\n",
       "        0.70247934, 0.70247934, 0.70247934, 0.75974026, 0.7122093 ,\n",
       "        0.70360111, 0.70165746, 0.70165746, 0.70247934, 0.70247934,\n",
       "        0.70247934, 0.70247934, 0.70247934, 0.70247934, 0.70165746]),\n",
       " 'split4_test_precision': array([0.        , 0.73255814, 0.70351759, 0.69252078, 0.69496021,\n",
       "        0.69791667, 0.6987013 , 0.69791667, 0.69791667, 0.6961039 ,\n",
       "        0.6961039 , 0.6961039 , 0.6961039 , 0.6961039 , 0.6961039 ,\n",
       "        0.6961039 , 0.6961039 , 0.6961039 , 0.73255814, 0.69252078,\n",
       "        0.69791667, 0.69791667, 0.6961039 , 0.6961039 , 0.6961039 ,\n",
       "        0.6961039 , 0.6961039 , 0.6961039 , 0.6961039 , 0.6961039 ]),\n",
       " 'split5_test_precision': array([0.        , 0.7791411 , 0.71978022, 0.71940299, 0.6978022 ,\n",
       "        0.69166667, 0.68834688, 0.6893733 , 0.69021739, 0.69021739,\n",
       "        0.69021739, 0.69021739, 0.69021739, 0.69021739, 0.69021739,\n",
       "        0.69021739, 0.69021739, 0.69021739, 0.7791411 , 0.71940299,\n",
       "        0.69166667, 0.6893733 , 0.69021739, 0.69021739, 0.69021739,\n",
       "        0.69021739, 0.69021739, 0.69021739, 0.69021739, 0.69021739]),\n",
       " 'split6_test_precision': array([0.        , 0.72049689, 0.71195652, 0.70897833, 0.68571429,\n",
       "        0.68091168, 0.67988669, 0.67705382, 0.67705382, 0.67705382,\n",
       "        0.67705382, 0.67705382, 0.67705382, 0.67705382, 0.67705382,\n",
       "        0.67705382, 0.67705382, 0.67705382, 0.72049689, 0.70897833,\n",
       "        0.68091168, 0.67705382, 0.67705382, 0.67705382, 0.67705382,\n",
       "        0.67705382, 0.67705382, 0.67705382, 0.67705382, 0.67705382]),\n",
       " 'split7_test_precision': array([0.        , 0.78443114, 0.75568182, 0.72972973, 0.72207084,\n",
       "        0.71313673, 0.70822281, 0.70899471, 0.70899471, 0.70899471,\n",
       "        0.70899471, 0.70899471, 0.70899471, 0.70899471, 0.70899471,\n",
       "        0.70899471, 0.70899471, 0.70899471, 0.78443114, 0.72972973,\n",
       "        0.71313673, 0.70899471, 0.70899471, 0.70899471, 0.70899471,\n",
       "        0.70899471, 0.70899471, 0.70899471, 0.70899471, 0.70899471]),\n",
       " 'split8_test_precision': array([0.        , 0.74011299, 0.71186441, 0.64450867, 0.65240642,\n",
       "        0.64473684, 0.64907652, 0.65183246, 0.65183246, 0.65183246,\n",
       "        0.65183246, 0.65183246, 0.65183246, 0.65183246, 0.65183246,\n",
       "        0.65183246, 0.65183246, 0.65183246, 0.74011299, 0.64450867,\n",
       "        0.64473684, 0.65183246, 0.65183246, 0.65183246, 0.65183246,\n",
       "        0.65183246, 0.65183246, 0.65183246, 0.65183246, 0.65183246]),\n",
       " 'split9_test_precision': array([0.        , 0.72049689, 0.70238095, 0.72555205, 0.71428571,\n",
       "        0.70338983, 0.70538244, 0.70985915, 0.70704225, 0.70786517,\n",
       "        0.70786517, 0.70786517, 0.70786517, 0.70786517, 0.70786517,\n",
       "        0.70786517, 0.70786517, 0.70786517, 0.72049689, 0.72555205,\n",
       "        0.70338983, 0.70985915, 0.70786517, 0.70786517, 0.70786517,\n",
       "        0.70786517, 0.70786517, 0.70786517, 0.70786517, 0.70786517]),\n",
       " 'mean_test_precision': array([0.        , 0.75832366, 0.72555104, 0.70949399, 0.69890096,\n",
       "        0.69451143, 0.69209344, 0.6924621 , 0.69225747, 0.69197209,\n",
       "        0.69197209, 0.69205428, 0.69205428, 0.69205428, 0.69205428,\n",
       "        0.69205428, 0.69205428, 0.69205428, 0.75832366, 0.70949399,\n",
       "        0.69451143, 0.6924621 , 0.69197209, 0.69205428, 0.69205428,\n",
       "        0.69205428, 0.69205428, 0.69205428, 0.69205428, 0.69197209]),\n",
       " 'std_test_precision': array([0.        , 0.02919906, 0.02690616, 0.02464583, 0.01991482,\n",
       "        0.02039584, 0.01815557, 0.01886781, 0.01832425, 0.01850786,\n",
       "        0.01850786, 0.01855246, 0.01855246, 0.01855246, 0.01855246,\n",
       "        0.01855246, 0.01855246, 0.01855246, 0.02919906, 0.02464583,\n",
       "        0.02039584, 0.01886781, 0.01850786, 0.01855246, 0.01855246,\n",
       "        0.01855246, 0.01855246, 0.01855246, 0.01855246, 0.01850786]),\n",
       " 'rank_test_precision': array([30,  1,  3,  4,  6,  7, 12,  9, 11, 26, 26, 13, 13, 13, 13, 13, 13,\n",
       "        13,  1,  4,  7,  9, 26, 13, 13, 13, 13, 13, 13, 26]),\n",
       " 'split0_test_f1_micro': array([0.605, 0.684, 0.674, 0.732, 0.734, 0.734, 0.732, 0.729, 0.73 ,\n",
       "        0.729, 0.729, 0.729, 0.729, 0.729, 0.729, 0.729, 0.729, 0.729,\n",
       "        0.684, 0.732, 0.734, 0.729, 0.729, 0.729, 0.729, 0.729, 0.729,\n",
       "        0.729, 0.729, 0.729]),\n",
       " 'split1_test_f1_micro': array([0.605, 0.697, 0.696, 0.76 , 0.759, 0.762, 0.754, 0.754, 0.754,\n",
       "        0.753, 0.753, 0.753, 0.753, 0.753, 0.753, 0.753, 0.753, 0.753,\n",
       "        0.697, 0.76 , 0.762, 0.754, 0.753, 0.753, 0.753, 0.753, 0.753,\n",
       "        0.753, 0.753, 0.753]),\n",
       " 'split2_test_f1_micro': array([0.605, 0.682, 0.682, 0.746, 0.753, 0.752, 0.752, 0.753, 0.753,\n",
       "        0.753, 0.753, 0.753, 0.753, 0.753, 0.753, 0.753, 0.753, 0.753,\n",
       "        0.682, 0.746, 0.752, 0.753, 0.753, 0.753, 0.753, 0.753, 0.753,\n",
       "        0.753, 0.753, 0.753]),\n",
       " 'split3_test_f1_micro': array([0.605, 0.685, 0.682, 0.751, 0.75 , 0.752, 0.75 , 0.751, 0.75 ,\n",
       "        0.751, 0.751, 0.752, 0.752, 0.752, 0.752, 0.752, 0.752, 0.752,\n",
       "        0.685, 0.751, 0.752, 0.751, 0.751, 0.752, 0.752, 0.752, 0.752,\n",
       "        0.752, 0.752, 0.751]),\n",
       " 'split4_test_f1_micro': array([0.605, 0.685, 0.686, 0.744, 0.752, 0.757, 0.758, 0.757, 0.757,\n",
       "        0.756, 0.756, 0.756, 0.756, 0.756, 0.756, 0.756, 0.756, 0.756,\n",
       "        0.685, 0.744, 0.757, 0.757, 0.756, 0.756, 0.756, 0.756, 0.756,\n",
       "        0.756, 0.756, 0.756]),\n",
       " 'split5_test_f1_micro': array([0.605, 0.696, 0.685, 0.752, 0.749, 0.743, 0.744, 0.744, 0.745,\n",
       "        0.745, 0.745, 0.745, 0.745, 0.745, 0.745, 0.745, 0.745, 0.745,\n",
       "        0.696, 0.752, 0.743, 0.744, 0.745, 0.745, 0.745, 0.745, 0.745,\n",
       "        0.745, 0.745, 0.745]),\n",
       " 'split6_test_f1_micro': array([0.604, 0.675, 0.682, 0.739, 0.734, 0.731, 0.731, 0.729, 0.729,\n",
       "        0.729, 0.729, 0.729, 0.729, 0.729, 0.729, 0.729, 0.729, 0.729,\n",
       "        0.675, 0.739, 0.731, 0.729, 0.729, 0.729, 0.729, 0.729, 0.729,\n",
       "        0.729, 0.729, 0.729]),\n",
       " 'split7_test_f1_micro': array([0.604, 0.699, 0.694, 0.757, 0.767, 0.763, 0.761, 0.762, 0.762,\n",
       "        0.762, 0.762, 0.762, 0.762, 0.762, 0.762, 0.762, 0.762, 0.762,\n",
       "        0.699, 0.757, 0.763, 0.762, 0.762, 0.762, 0.762, 0.762, 0.762,\n",
       "        0.762, 0.762, 0.762]),\n",
       " 'split8_test_f1_micro': array([0.604, 0.689, 0.679, 0.704, 0.718, 0.714, 0.717, 0.72 , 0.72 ,\n",
       "        0.72 , 0.72 , 0.72 , 0.72 , 0.72 , 0.72 , 0.72 , 0.72 , 0.72 ,\n",
       "        0.689, 0.704, 0.714, 0.72 , 0.72 , 0.72 , 0.72 , 0.72 , 0.72 ,\n",
       "        0.72 , 0.72 , 0.72 ]),\n",
       " 'split9_test_f1_micro': array([0.604, 0.675, 0.672, 0.747, 0.751, 0.748, 0.749, 0.753, 0.751,\n",
       "        0.752, 0.752, 0.752, 0.752, 0.752, 0.752, 0.752, 0.752, 0.752,\n",
       "        0.675, 0.747, 0.748, 0.753, 0.752, 0.752, 0.752, 0.752, 0.752,\n",
       "        0.752, 0.752, 0.752]),\n",
       " 'mean_test_f1_micro': array([0.6046, 0.6867, 0.6832, 0.7432, 0.7467, 0.7456, 0.7448, 0.7452,\n",
       "        0.7451, 0.745 , 0.745 , 0.7451, 0.7451, 0.7451, 0.7451, 0.7451,\n",
       "        0.7451, 0.7451, 0.6867, 0.7432, 0.7456, 0.7452, 0.745 , 0.7451,\n",
       "        0.7451, 0.7451, 0.7451, 0.7451, 0.7451, 0.745 ]),\n",
       " 'std_test_f1_micro': array([0.0004899 , 0.00811234, 0.00723602, 0.01521052, 0.01346143,\n",
       "        0.01463694, 0.01321212, 0.01347442, 0.01320947, 0.0132665 ,\n",
       "        0.0132665 , 0.01331503, 0.01331503, 0.01331503, 0.01331503,\n",
       "        0.01331503, 0.01331503, 0.01331503, 0.00811234, 0.01521052,\n",
       "        0.01463694, 0.01347442, 0.0132665 , 0.01331503, 0.01331503,\n",
       "        0.01331503, 0.01331503, 0.01331503, 0.01331503, 0.0132665 ]),\n",
       " 'rank_test_f1_micro': array([30, 27, 29, 25,  1,  2, 24,  4,  6, 20, 20,  7,  7,  7,  7,  7,  7,\n",
       "         7, 27, 25,  2,  4, 20,  7,  7,  7,  7,  7,  7, 20])}"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_model.cv_results_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "32aa74f3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([30, 27, 29, 25,  1,  2, 24,  4,  6, 20, 20,  7,  7,  7,  7,  7,  7,\n",
       "        7, 27, 25,  2,  4, 20,  7,  7,  7,  7,  7,  7, 20])"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_model.cv_results_['rank_test_accuracy']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "b234d790",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'classifier': LogisticRegression(max_iter=10000),\n",
       " 'classifier__C': 0.0001,\n",
       " 'classifier__penalty': 'l1',\n",
       " 'classifier__solver': 'saga'}"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_model.cv_results_['params'][ np.argmin(best_model.cv_results_['mean_test_accuracy'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "ae51b2e8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>classifier</th>\n",
       "      <th>C</th>\n",
       "      <th>penalty</th>\n",
       "      <th>solver</th>\n",
       "      <th>score_acc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.3954</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.3133</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.3168</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.2568</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0100</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.2533</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0100</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.2544</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.1000</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.2552</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.1000</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.2548</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.2549</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.2550</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>10.0000</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.2550</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>10.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.2549</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>100.0000</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.2549</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>100.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.2549</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>1000.0000</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.2549</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>1000.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.2549</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>10000.0000</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.2549</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>10000.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.2549</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.3133</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.2568</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0100</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.2544</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.1000</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.2548</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.2550</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>10.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.2549</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>100.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.2549</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>1000.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.2549</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>10000.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.2549</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>NaN</td>\n",
       "      <td>none</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.2549</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>NaN</td>\n",
       "      <td>none</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.2549</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.2550</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                            classifier           C penalty solver  score_acc\n",
       "0   LogisticRegression(max_iter=10000)      0.0001      l1   saga     0.3954\n",
       "1   LogisticRegression(max_iter=10000)      0.0001      l2   saga     0.3133\n",
       "2   LogisticRegression(max_iter=10000)      0.0010      l1   saga     0.3168\n",
       "3   LogisticRegression(max_iter=10000)      0.0010      l2   saga     0.2568\n",
       "4   LogisticRegression(max_iter=10000)      0.0100      l1   saga     0.2533\n",
       "5   LogisticRegression(max_iter=10000)      0.0100      l2   saga     0.2544\n",
       "6   LogisticRegression(max_iter=10000)      0.1000      l1   saga     0.2552\n",
       "7   LogisticRegression(max_iter=10000)      0.1000      l2   saga     0.2548\n",
       "8   LogisticRegression(max_iter=10000)      1.0000      l1   saga     0.2549\n",
       "9   LogisticRegression(max_iter=10000)      1.0000      l2   saga     0.2550\n",
       "10  LogisticRegression(max_iter=10000)     10.0000      l1   saga     0.2550\n",
       "11  LogisticRegression(max_iter=10000)     10.0000      l2   saga     0.2549\n",
       "12  LogisticRegression(max_iter=10000)    100.0000      l1   saga     0.2549\n",
       "13  LogisticRegression(max_iter=10000)    100.0000      l2   saga     0.2549\n",
       "14  LogisticRegression(max_iter=10000)   1000.0000      l1   saga     0.2549\n",
       "15  LogisticRegression(max_iter=10000)   1000.0000      l2   saga     0.2549\n",
       "16  LogisticRegression(max_iter=10000)  10000.0000      l1   saga     0.2549\n",
       "17  LogisticRegression(max_iter=10000)  10000.0000      l2   saga     0.2549\n",
       "18  LogisticRegression(max_iter=10000)      0.0001      l2  lbfgs     0.3133\n",
       "19  LogisticRegression(max_iter=10000)      0.0010      l2  lbfgs     0.2568\n",
       "20  LogisticRegression(max_iter=10000)      0.0100      l2  lbfgs     0.2544\n",
       "21  LogisticRegression(max_iter=10000)      0.1000      l2  lbfgs     0.2548\n",
       "22  LogisticRegression(max_iter=10000)      1.0000      l2  lbfgs     0.2550\n",
       "23  LogisticRegression(max_iter=10000)     10.0000      l2  lbfgs     0.2549\n",
       "24  LogisticRegression(max_iter=10000)    100.0000      l2  lbfgs     0.2549\n",
       "25  LogisticRegression(max_iter=10000)   1000.0000      l2  lbfgs     0.2549\n",
       "26  LogisticRegression(max_iter=10000)  10000.0000      l2  lbfgs     0.2549\n",
       "27  LogisticRegression(max_iter=10000)         NaN    none  lbfgs     0.2549\n",
       "28  LogisticRegression(max_iter=10000)         NaN    none   saga     0.2549\n",
       "29  LogisticRegression(max_iter=10000)         NaN     NaN    NaN     0.2550"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results = pd.DataFrame( best_model.cv_results_['params'] )\n",
    "\n",
    "# next grab the score resulting from those parameters, add it to the data\n",
    "# score is accuracy; to display it as misclassification error we use 1 - x\n",
    "results['score_acc'] = 1 - best_model.cv_results_['mean_test_accuracy']\n",
    "\n",
    "# get rid of classifierXX in columns\n",
    "cols = results.columns.to_series().str.split('__').apply(lambda x: x[-1])\n",
    "results.columns = cols\n",
    "\n",
    "# lets show the results for the saga solver across penalty & C values\n",
    "#sns.heatmap( results.query('solver==\"saga\"').pivot('C','penalty','score_acc'),\n",
    "#             annot=True, fmt='.3f')\n",
    "\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "2ed188a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "acc_list.append(results[0:1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "fb96018d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'classifier': LogisticRegression(max_iter=10000),\n",
       " 'classifier__C': 0.0001,\n",
       " 'classifier__penalty': 'l2',\n",
       " 'classifier__solver': 'saga'}"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_model.cv_results_['params'][ np.argmin(best_model.cv_results_['rank_test_precision'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "c299c812",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>classifier</th>\n",
       "      <th>C</th>\n",
       "      <th>penalty</th>\n",
       "      <th>solver</th>\n",
       "      <th>score_precision</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.241676</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.274449</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.290506</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0100</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.301099</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0100</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.305489</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.1000</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.307907</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.1000</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.307538</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.307743</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.308028</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>10.0000</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.308028</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>10.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.307946</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>100.0000</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.307946</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>100.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.307946</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>1000.0000</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.307946</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>1000.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.307946</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>10000.0000</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.307946</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>10000.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.307946</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.241676</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.290506</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0100</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.305489</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.1000</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.307538</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.308028</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>10.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.307946</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>100.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.307946</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>1000.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.307946</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>10000.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.307946</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>NaN</td>\n",
       "      <td>none</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.307946</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>NaN</td>\n",
       "      <td>none</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.307946</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.308028</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                            classifier           C penalty solver  \\\n",
       "0   LogisticRegression(max_iter=10000)      0.0001      l1   saga   \n",
       "1   LogisticRegression(max_iter=10000)      0.0001      l2   saga   \n",
       "2   LogisticRegression(max_iter=10000)      0.0010      l1   saga   \n",
       "3   LogisticRegression(max_iter=10000)      0.0010      l2   saga   \n",
       "4   LogisticRegression(max_iter=10000)      0.0100      l1   saga   \n",
       "5   LogisticRegression(max_iter=10000)      0.0100      l2   saga   \n",
       "6   LogisticRegression(max_iter=10000)      0.1000      l1   saga   \n",
       "7   LogisticRegression(max_iter=10000)      0.1000      l2   saga   \n",
       "8   LogisticRegression(max_iter=10000)      1.0000      l1   saga   \n",
       "9   LogisticRegression(max_iter=10000)      1.0000      l2   saga   \n",
       "10  LogisticRegression(max_iter=10000)     10.0000      l1   saga   \n",
       "11  LogisticRegression(max_iter=10000)     10.0000      l2   saga   \n",
       "12  LogisticRegression(max_iter=10000)    100.0000      l1   saga   \n",
       "13  LogisticRegression(max_iter=10000)    100.0000      l2   saga   \n",
       "14  LogisticRegression(max_iter=10000)   1000.0000      l1   saga   \n",
       "15  LogisticRegression(max_iter=10000)   1000.0000      l2   saga   \n",
       "16  LogisticRegression(max_iter=10000)  10000.0000      l1   saga   \n",
       "17  LogisticRegression(max_iter=10000)  10000.0000      l2   saga   \n",
       "18  LogisticRegression(max_iter=10000)      0.0001      l2  lbfgs   \n",
       "19  LogisticRegression(max_iter=10000)      0.0010      l2  lbfgs   \n",
       "20  LogisticRegression(max_iter=10000)      0.0100      l2  lbfgs   \n",
       "21  LogisticRegression(max_iter=10000)      0.1000      l2  lbfgs   \n",
       "22  LogisticRegression(max_iter=10000)      1.0000      l2  lbfgs   \n",
       "23  LogisticRegression(max_iter=10000)     10.0000      l2  lbfgs   \n",
       "24  LogisticRegression(max_iter=10000)    100.0000      l2  lbfgs   \n",
       "25  LogisticRegression(max_iter=10000)   1000.0000      l2  lbfgs   \n",
       "26  LogisticRegression(max_iter=10000)  10000.0000      l2  lbfgs   \n",
       "27  LogisticRegression(max_iter=10000)         NaN    none  lbfgs   \n",
       "28  LogisticRegression(max_iter=10000)         NaN    none   saga   \n",
       "29  LogisticRegression(max_iter=10000)         NaN     NaN    NaN   \n",
       "\n",
       "    score_precision  \n",
       "0          1.000000  \n",
       "1          0.241676  \n",
       "2          0.274449  \n",
       "3          0.290506  \n",
       "4          0.301099  \n",
       "5          0.305489  \n",
       "6          0.307907  \n",
       "7          0.307538  \n",
       "8          0.307743  \n",
       "9          0.308028  \n",
       "10         0.308028  \n",
       "11         0.307946  \n",
       "12         0.307946  \n",
       "13         0.307946  \n",
       "14         0.307946  \n",
       "15         0.307946  \n",
       "16         0.307946  \n",
       "17         0.307946  \n",
       "18         0.241676  \n",
       "19         0.290506  \n",
       "20         0.305489  \n",
       "21         0.307538  \n",
       "22         0.308028  \n",
       "23         0.307946  \n",
       "24         0.307946  \n",
       "25         0.307946  \n",
       "26         0.307946  \n",
       "27         0.307946  \n",
       "28         0.307946  \n",
       "29         0.308028  "
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results = pd.DataFrame( best_model.cv_results_['params'] )\n",
    "\n",
    "# next grab the score resulting from those parameters, add it to the data\n",
    "# score is accuracy; to display it as misclassification error we use 1 - x\n",
    "results['score_precision'] = 1 - best_model.cv_results_['mean_test_precision']\n",
    "\n",
    "# get rid of classifierXX in columns\n",
    "cols = results.columns.to_series().str.split('__').apply(lambda x: x[-1])\n",
    "results.columns = cols\n",
    "\n",
    "# lets show the results for the saga solver across penalty & C values\n",
    "#sns.heatmap( results.query('solver==\"saga\"').pivot('C','penalty','score_precision'),\n",
    "#             annot=True, fmt='.3f')\n",
    "\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "950c010d",
   "metadata": {},
   "outputs": [],
   "source": [
    "roc_auc_ovr_List.append(results[1:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "a902208d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'classifier': LogisticRegression(max_iter=10000),\n",
       " 'classifier__C': 0.01,\n",
       " 'classifier__penalty': 'l1',\n",
       " 'classifier__solver': 'saga'}"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_model.cv_results_['params'][ np.argmin(best_model.cv_results_['rank_test_f1_micro'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "a3a9405e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>classifier</th>\n",
       "      <th>C</th>\n",
       "      <th>penalty</th>\n",
       "      <th>solver</th>\n",
       "      <th>score_f1_micro</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.3954</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.3133</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.3168</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.2568</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0100</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.2533</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0100</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.2544</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.1000</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.2552</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.1000</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.2548</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.2549</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.2550</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>10.0000</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.2550</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>10.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.2549</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>100.0000</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.2549</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>100.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.2549</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>1000.0000</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.2549</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>1000.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.2549</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>10000.0000</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.2549</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>10000.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.2549</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.3133</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.2568</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0100</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.2544</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.1000</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.2548</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.2550</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>10.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.2549</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>100.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.2549</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>1000.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.2549</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>10000.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.2549</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>NaN</td>\n",
       "      <td>none</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.2549</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>NaN</td>\n",
       "      <td>none</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.2549</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.2550</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                            classifier           C penalty solver  \\\n",
       "0   LogisticRegression(max_iter=10000)      0.0001      l1   saga   \n",
       "1   LogisticRegression(max_iter=10000)      0.0001      l2   saga   \n",
       "2   LogisticRegression(max_iter=10000)      0.0010      l1   saga   \n",
       "3   LogisticRegression(max_iter=10000)      0.0010      l2   saga   \n",
       "4   LogisticRegression(max_iter=10000)      0.0100      l1   saga   \n",
       "5   LogisticRegression(max_iter=10000)      0.0100      l2   saga   \n",
       "6   LogisticRegression(max_iter=10000)      0.1000      l1   saga   \n",
       "7   LogisticRegression(max_iter=10000)      0.1000      l2   saga   \n",
       "8   LogisticRegression(max_iter=10000)      1.0000      l1   saga   \n",
       "9   LogisticRegression(max_iter=10000)      1.0000      l2   saga   \n",
       "10  LogisticRegression(max_iter=10000)     10.0000      l1   saga   \n",
       "11  LogisticRegression(max_iter=10000)     10.0000      l2   saga   \n",
       "12  LogisticRegression(max_iter=10000)    100.0000      l1   saga   \n",
       "13  LogisticRegression(max_iter=10000)    100.0000      l2   saga   \n",
       "14  LogisticRegression(max_iter=10000)   1000.0000      l1   saga   \n",
       "15  LogisticRegression(max_iter=10000)   1000.0000      l2   saga   \n",
       "16  LogisticRegression(max_iter=10000)  10000.0000      l1   saga   \n",
       "17  LogisticRegression(max_iter=10000)  10000.0000      l2   saga   \n",
       "18  LogisticRegression(max_iter=10000)      0.0001      l2  lbfgs   \n",
       "19  LogisticRegression(max_iter=10000)      0.0010      l2  lbfgs   \n",
       "20  LogisticRegression(max_iter=10000)      0.0100      l2  lbfgs   \n",
       "21  LogisticRegression(max_iter=10000)      0.1000      l2  lbfgs   \n",
       "22  LogisticRegression(max_iter=10000)      1.0000      l2  lbfgs   \n",
       "23  LogisticRegression(max_iter=10000)     10.0000      l2  lbfgs   \n",
       "24  LogisticRegression(max_iter=10000)    100.0000      l2  lbfgs   \n",
       "25  LogisticRegression(max_iter=10000)   1000.0000      l2  lbfgs   \n",
       "26  LogisticRegression(max_iter=10000)  10000.0000      l2  lbfgs   \n",
       "27  LogisticRegression(max_iter=10000)         NaN    none  lbfgs   \n",
       "28  LogisticRegression(max_iter=10000)         NaN    none   saga   \n",
       "29  LogisticRegression(max_iter=10000)         NaN     NaN    NaN   \n",
       "\n",
       "    score_f1_micro  \n",
       "0           0.3954  \n",
       "1           0.3133  \n",
       "2           0.3168  \n",
       "3           0.2568  \n",
       "4           0.2533  \n",
       "5           0.2544  \n",
       "6           0.2552  \n",
       "7           0.2548  \n",
       "8           0.2549  \n",
       "9           0.2550  \n",
       "10          0.2550  \n",
       "11          0.2549  \n",
       "12          0.2549  \n",
       "13          0.2549  \n",
       "14          0.2549  \n",
       "15          0.2549  \n",
       "16          0.2549  \n",
       "17          0.2549  \n",
       "18          0.3133  \n",
       "19          0.2568  \n",
       "20          0.2544  \n",
       "21          0.2548  \n",
       "22          0.2550  \n",
       "23          0.2549  \n",
       "24          0.2549  \n",
       "25          0.2549  \n",
       "26          0.2549  \n",
       "27          0.2549  \n",
       "28          0.2549  \n",
       "29          0.2550  "
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results = pd.DataFrame( best_model.cv_results_['params'] )\n",
    "\n",
    "# next grab the score resulting from those parameters, add it to the data\n",
    "# score is accuracy; to display it as misclassification error we use 1 - x\n",
    "results['score_f1_micro'] = 1 - best_model.cv_results_['mean_test_f1_micro']\n",
    "\n",
    "# get rid of classifierXX in columns\n",
    "cols = results.columns.to_series().str.split('__').apply(lambda x: x[-1])\n",
    "results.columns = cols\n",
    "\n",
    "# lets show the results for the saga solver across penalty & C values\n",
    "#sns.heatmap( results.query('solver==\"saga\"').pivot('C','penalty','score_f1_micro'),\n",
    "#             annot=True, fmt='.3f')\n",
    "\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "40b8e54c",
   "metadata": {},
   "outputs": [],
   "source": [
    "f1_micro_List.append(results[4:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e446b35",
   "metadata": {},
   "source": [
    "## Trial 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "6b8df2fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, Y_train, Y_test = train_test_split(X, y, train_size=10000, shuffle=True, random_state = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "8cf50a94",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Guest-All\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1469: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\Guest-All\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1469: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\Guest-All\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1469: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\Guest-All\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1469: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\Guest-All\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1469: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\Guest-All\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1469: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\Guest-All\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1469: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\Guest-All\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1469: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\Guest-All\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1469: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\Guest-All\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1469: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\Guest-All\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1182: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Guest-All\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1182: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Guest-All\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1182: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Guest-All\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1182: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Guest-All\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1182: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Guest-All\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1182: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Guest-All\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1182: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Guest-All\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1182: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Guest-All\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1182: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Guest-All\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1182: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Guest-All\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1182: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Guest-All\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1182: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Guest-All\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1182: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Guest-All\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1182: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Guest-All\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1182: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Guest-All\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1182: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Guest-All\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1182: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Guest-All\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1182: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Guest-All\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1182: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Guest-All\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1182: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Create a pipeline - RF is a stand in, we will populate the classifier part below\n",
    "pipe = Pipeline([('std', StandardScaler()),\n",
    "                 ('classifier', LogisticRegression())])\n",
    "\n",
    "# Create search space of candidate learning algorithms and their hyperparameters\n",
    "# note lbfgs can't do l1, and if you pass penalty='none' it expects no C value\n",
    "search_space = [{'classifier': [LogisticRegression(max_iter=10000)],\n",
    "                 'classifier__solver': ['saga'],\n",
    "                 'classifier__penalty': ['l1', 'l2'],\n",
    "                 'classifier__C': np.logspace(-4, 4, 9)},\n",
    "                {'classifier': [LogisticRegression(max_iter=10000)],\n",
    "                 'classifier__solver': ['lbfgs'],\n",
    "                 'classifier__penalty': ['l2'],\n",
    "                 'classifier__C': np.logspace(-4, 4, 9)},\n",
    "                {'classifier': [LogisticRegression(max_iter=10000)],\n",
    "                 'classifier__solver': ['lbfgs','saga'],\n",
    "                 'classifier__penalty': ['none']},\n",
    "                {'classifier': [LogisticRegression(max_iter=10000)]}\n",
    "                ]\n",
    "\n",
    "# Create grid search\n",
    "clf = GridSearchCV(pipe, search_space, cv=StratifiedKFold(n_splits=10),\n",
    "                   scoring=['accuracy', 'precision', 'f1_micro'], refit=False,\n",
    "                   verbose=0)\n",
    "\n",
    "# Fit grid search\n",
    "best_model = clf.fit(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "a4120924",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'mean_fit_time': array([0.01226552, 0.04000165, 0.05454657, 0.0484731 , 0.05518978,\n",
       "        0.04393113, 0.0578877 , 0.04768958, 0.0621753 , 0.0531491 ,\n",
       "        0.06601825, 0.04763103, 0.06237292, 0.04940424, 0.05939221,\n",
       "        0.04988   , 0.05814936, 0.04569244, 0.01320791, 0.01715674,\n",
       "        0.01999102, 0.01500506, 0.0152216 , 0.01494484, 0.01149192,\n",
       "        0.01627951, 0.01571286, 0.00930028, 0.04674072, 0.01411378]),\n",
       " 'std_fit_time': array([0.00591601, 0.01070684, 0.00752878, 0.00487624, 0.00771433,\n",
       "        0.00742728, 0.00702765, 0.00307363, 0.00902242, 0.0074868 ,\n",
       "        0.00624278, 0.0072178 , 0.00719747, 0.00533581, 0.00548405,\n",
       "        0.00751011, 0.00726871, 0.00468186, 0.00527247, 0.00591886,\n",
       "        0.00771682, 0.00175927, 0.00611885, 0.00214115, 0.00655682,\n",
       "        0.00736244, 0.00015114, 0.00691243, 0.00737582, 0.00471132]),\n",
       " 'mean_score_time': array([0.00788662, 0.00633593, 0.00782175, 0.00649297, 0.00605037,\n",
       "        0.00641944, 0.00496347, 0.00413256, 0.0033463 , 0.00499337,\n",
       "        0.00312774, 0.00887568, 0.00368373, 0.00879939, 0.00820801,\n",
       "        0.00500224, 0.00788388, 0.00770853, 0.00566936, 0.00705154,\n",
       "        0.00352592, 0.0047195 , 0.00483754, 0.00541923, 0.00625451,\n",
       "        0.00413339, 0.00471003, 0.01022694, 0.00710473, 0.00636647]),\n",
       " 'std_score_time': array([0.0046918 , 0.00691434, 0.00782177, 0.00796849, 0.00717725,\n",
       "        0.00744134, 0.00706665, 0.00611947, 0.00617415, 0.00702079,\n",
       "        0.00625666, 0.00747348, 0.00620269, 0.00770505, 0.00707551,\n",
       "        0.00640638, 0.00788482, 0.00771476, 0.00710576, 0.00720454,\n",
       "        0.00710847, 0.00720945, 0.00652703, 0.00699045, 0.00766018,\n",
       "        0.00664056, 0.00719493, 0.00902603, 0.00717724, 0.00858835]),\n",
       " 'param_classifier': masked_array(data=[LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000)],\n",
       "              mask=[False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'param_classifier__C': masked_array(data=[0.0001, 0.0001, 0.001, 0.001, 0.01, 0.01, 0.1, 0.1,\n",
       "                    1.0, 1.0, 10.0, 10.0, 100.0, 100.0, 1000.0, 1000.0,\n",
       "                    10000.0, 10000.0, 0.0001, 0.001, 0.01, 0.1, 1.0, 10.0,\n",
       "                    100.0, 1000.0, 10000.0, --, --, --],\n",
       "              mask=[False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False,  True,  True,  True],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'param_classifier__penalty': masked_array(data=['l1', 'l2', 'l1', 'l2', 'l1', 'l2', 'l1', 'l2', 'l1',\n",
       "                    'l2', 'l1', 'l2', 'l1', 'l2', 'l1', 'l2', 'l1', 'l2',\n",
       "                    'l2', 'l2', 'l2', 'l2', 'l2', 'l2', 'l2', 'l2', 'l2',\n",
       "                    'none', 'none', --],\n",
       "              mask=[False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False,  True],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'param_classifier__solver': masked_array(data=['saga', 'saga', 'saga', 'saga', 'saga', 'saga', 'saga',\n",
       "                    'saga', 'saga', 'saga', 'saga', 'saga', 'saga', 'saga',\n",
       "                    'saga', 'saga', 'saga', 'saga', 'lbfgs', 'lbfgs',\n",
       "                    'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs',\n",
       "                    'lbfgs', 'lbfgs', 'saga', --],\n",
       "              mask=[False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False,  True],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'params': [{'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 0.0001,\n",
       "   'classifier__penalty': 'l1',\n",
       "   'classifier__solver': 'saga'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 0.0001,\n",
       "   'classifier__penalty': 'l2',\n",
       "   'classifier__solver': 'saga'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 0.001,\n",
       "   'classifier__penalty': 'l1',\n",
       "   'classifier__solver': 'saga'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 0.001,\n",
       "   'classifier__penalty': 'l2',\n",
       "   'classifier__solver': 'saga'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 0.01,\n",
       "   'classifier__penalty': 'l1',\n",
       "   'classifier__solver': 'saga'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 0.01,\n",
       "   'classifier__penalty': 'l2',\n",
       "   'classifier__solver': 'saga'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 0.1,\n",
       "   'classifier__penalty': 'l1',\n",
       "   'classifier__solver': 'saga'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 0.1,\n",
       "   'classifier__penalty': 'l2',\n",
       "   'classifier__solver': 'saga'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 1.0,\n",
       "   'classifier__penalty': 'l1',\n",
       "   'classifier__solver': 'saga'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 1.0,\n",
       "   'classifier__penalty': 'l2',\n",
       "   'classifier__solver': 'saga'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 10.0,\n",
       "   'classifier__penalty': 'l1',\n",
       "   'classifier__solver': 'saga'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 10.0,\n",
       "   'classifier__penalty': 'l2',\n",
       "   'classifier__solver': 'saga'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 100.0,\n",
       "   'classifier__penalty': 'l1',\n",
       "   'classifier__solver': 'saga'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 100.0,\n",
       "   'classifier__penalty': 'l2',\n",
       "   'classifier__solver': 'saga'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 1000.0,\n",
       "   'classifier__penalty': 'l1',\n",
       "   'classifier__solver': 'saga'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 1000.0,\n",
       "   'classifier__penalty': 'l2',\n",
       "   'classifier__solver': 'saga'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 10000.0,\n",
       "   'classifier__penalty': 'l1',\n",
       "   'classifier__solver': 'saga'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 10000.0,\n",
       "   'classifier__penalty': 'l2',\n",
       "   'classifier__solver': 'saga'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 0.0001,\n",
       "   'classifier__penalty': 'l2',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 0.001,\n",
       "   'classifier__penalty': 'l2',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 0.01,\n",
       "   'classifier__penalty': 'l2',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 0.1,\n",
       "   'classifier__penalty': 'l2',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 1.0,\n",
       "   'classifier__penalty': 'l2',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 10.0,\n",
       "   'classifier__penalty': 'l2',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 100.0,\n",
       "   'classifier__penalty': 'l2',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 1000.0,\n",
       "   'classifier__penalty': 'l2',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 10000.0,\n",
       "   'classifier__penalty': 'l2',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__penalty': 'none',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__penalty': 'none',\n",
       "   'classifier__solver': 'saga'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000)}],\n",
       " 'split0_test_accuracy': array([0.603, 0.676, 0.673, 0.714, 0.725, 0.724, 0.723, 0.727, 0.728,\n",
       "        0.727, 0.727, 0.727, 0.727, 0.727, 0.727, 0.727, 0.727, 0.727,\n",
       "        0.676, 0.714, 0.724, 0.727, 0.727, 0.727, 0.727, 0.727, 0.727,\n",
       "        0.727, 0.727, 0.727]),\n",
       " 'split1_test_accuracy': array([0.603, 0.669, 0.679, 0.728, 0.73 , 0.731, 0.729, 0.729, 0.729,\n",
       "        0.729, 0.729, 0.729, 0.729, 0.729, 0.729, 0.729, 0.729, 0.729,\n",
       "        0.669, 0.728, 0.731, 0.729, 0.729, 0.729, 0.729, 0.729, 0.729,\n",
       "        0.729, 0.729, 0.729]),\n",
       " 'split2_test_accuracy': array([0.603, 0.703, 0.707, 0.741, 0.747, 0.748, 0.747, 0.746, 0.745,\n",
       "        0.746, 0.745, 0.746, 0.745, 0.746, 0.746, 0.746, 0.745, 0.746,\n",
       "        0.703, 0.741, 0.748, 0.746, 0.746, 0.745, 0.745, 0.745, 0.745,\n",
       "        0.745, 0.746, 0.746]),\n",
       " 'split3_test_accuracy': array([0.603, 0.678, 0.678, 0.72 , 0.714, 0.716, 0.721, 0.717, 0.718,\n",
       "        0.718, 0.718, 0.718, 0.718, 0.718, 0.718, 0.718, 0.718, 0.718,\n",
       "        0.678, 0.72 , 0.716, 0.717, 0.718, 0.718, 0.718, 0.718, 0.718,\n",
       "        0.718, 0.718, 0.718]),\n",
       " 'split4_test_accuracy': array([0.603, 0.678, 0.681, 0.718, 0.729, 0.729, 0.729, 0.73 , 0.731,\n",
       "        0.731, 0.731, 0.731, 0.731, 0.731, 0.731, 0.731, 0.731, 0.731,\n",
       "        0.678, 0.718, 0.729, 0.73 , 0.731, 0.731, 0.731, 0.731, 0.731,\n",
       "        0.731, 0.731, 0.731]),\n",
       " 'split5_test_accuracy': array([0.603, 0.688, 0.689, 0.739, 0.757, 0.754, 0.754, 0.752, 0.752,\n",
       "        0.752, 0.752, 0.752, 0.752, 0.752, 0.752, 0.752, 0.752, 0.752,\n",
       "        0.688, 0.739, 0.754, 0.752, 0.752, 0.752, 0.752, 0.752, 0.752,\n",
       "        0.752, 0.752, 0.752]),\n",
       " 'split6_test_accuracy': array([0.603, 0.698, 0.674, 0.738, 0.741, 0.745, 0.745, 0.746, 0.746,\n",
       "        0.747, 0.747, 0.747, 0.747, 0.747, 0.747, 0.747, 0.747, 0.747,\n",
       "        0.698, 0.738, 0.745, 0.746, 0.747, 0.747, 0.747, 0.747, 0.747,\n",
       "        0.747, 0.747, 0.747]),\n",
       " 'split7_test_accuracy': array([0.603, 0.696, 0.669, 0.738, 0.734, 0.738, 0.741, 0.741, 0.74 ,\n",
       "        0.74 , 0.74 , 0.74 , 0.74 , 0.74 , 0.74 , 0.74 , 0.74 , 0.74 ,\n",
       "        0.696, 0.738, 0.738, 0.741, 0.74 , 0.74 , 0.74 , 0.74 , 0.74 ,\n",
       "        0.74 , 0.74 , 0.74 ]),\n",
       " 'split8_test_accuracy': array([0.603, 0.682, 0.698, 0.753, 0.769, 0.775, 0.778, 0.778, 0.777,\n",
       "        0.777, 0.777, 0.777, 0.777, 0.777, 0.777, 0.777, 0.777, 0.777,\n",
       "        0.682, 0.753, 0.775, 0.778, 0.777, 0.777, 0.777, 0.777, 0.777,\n",
       "        0.777, 0.777, 0.777]),\n",
       " 'split9_test_accuracy': array([0.604, 0.701, 0.696, 0.766, 0.766, 0.774, 0.769, 0.773, 0.773,\n",
       "        0.773, 0.773, 0.773, 0.773, 0.773, 0.773, 0.773, 0.773, 0.773,\n",
       "        0.701, 0.766, 0.774, 0.773, 0.773, 0.773, 0.773, 0.773, 0.773,\n",
       "        0.773, 0.773, 0.773]),\n",
       " 'mean_test_accuracy': array([0.6031, 0.6869, 0.6844, 0.7355, 0.7412, 0.7434, 0.7436, 0.7439,\n",
       "        0.7439, 0.744 , 0.7439, 0.744 , 0.7439, 0.744 , 0.744 , 0.744 ,\n",
       "        0.7439, 0.744 , 0.6869, 0.7355, 0.7434, 0.7439, 0.744 , 0.7439,\n",
       "        0.7439, 0.7439, 0.7439, 0.7439, 0.744 , 0.744 ]),\n",
       " 'std_test_accuracy': array([0.0003    , 0.01134416, 0.0118676 , 0.0153509 , 0.01731935,\n",
       "        0.01894307, 0.01821648, 0.0187534 , 0.01827813, 0.01839021,\n",
       "        0.01838178, 0.01839021, 0.01838178, 0.01839021, 0.01839021,\n",
       "        0.01839021, 0.01838178, 0.01839021, 0.01134416, 0.0153509 ,\n",
       "        0.01894307, 0.0187534 , 0.01839021, 0.01838178, 0.01838178,\n",
       "        0.01838178, 0.01838178, 0.01838178, 0.01839021, 0.01839021]),\n",
       " 'rank_test_accuracy': array([30, 27, 29, 25, 24, 22, 21, 10, 10,  1, 10,  1, 10,  1,  1,  1, 10,\n",
       "         1, 27, 25, 22, 10,  1, 10, 10, 10, 10, 10,  1,  1]),\n",
       " 'split0_test_precision': array([0.        , 0.6972973 , 0.68041237, 0.65994236, 0.66223404,\n",
       "        0.65633075, 0.65384615, 0.65979381, 0.66066838, 0.65979381,\n",
       "        0.65979381, 0.65979381, 0.65979381, 0.65979381, 0.65979381,\n",
       "        0.65979381, 0.65979381, 0.65979381, 0.6972973 , 0.65994236,\n",
       "        0.65633075, 0.65979381, 0.65979381, 0.65979381, 0.65979381,\n",
       "        0.65979381, 0.65979381, 0.65979381, 0.65979381, 0.65979381]),\n",
       " 'split1_test_precision': array([0.        , 0.70625   , 0.71111111, 0.68656716, 0.67688022,\n",
       "        0.67297297, 0.66935484, 0.6684492 , 0.6684492 , 0.6684492 ,\n",
       "        0.6684492 , 0.6684492 , 0.6684492 , 0.6684492 , 0.6684492 ,\n",
       "        0.6684492 , 0.6684492 , 0.6684492 , 0.70625   , 0.68656716,\n",
       "        0.67297297, 0.6684492 , 0.6684492 , 0.6684492 , 0.6684492 ,\n",
       "        0.6684492 , 0.6684492 , 0.6684492 , 0.6684492 , 0.6684492 ]),\n",
       " 'split2_test_precision': array([0.        , 0.79411765, 0.7826087 , 0.72115385, 0.71686747,\n",
       "        0.71386431, 0.71176471, 0.70967742, 0.70882353, 0.70967742,\n",
       "        0.70882353, 0.70967742, 0.70882353, 0.70967742, 0.70967742,\n",
       "        0.70967742, 0.70882353, 0.70967742, 0.79411765, 0.72115385,\n",
       "        0.71386431, 0.70967742, 0.70967742, 0.70882353, 0.70882353,\n",
       "        0.70882353, 0.70882353, 0.70882353, 0.70967742, 0.70967742]),\n",
       " 'split3_test_precision': array([0.        , 0.7245509 , 0.69633508, 0.69055375, 0.66767372,\n",
       "        0.66568915, 0.67151163, 0.66569767, 0.66763848, 0.66763848,\n",
       "        0.66763848, 0.66763848, 0.66763848, 0.66763848, 0.66763848,\n",
       "        0.66763848, 0.66763848, 0.66763848, 0.7245509 , 0.69055375,\n",
       "        0.66568915, 0.66569767, 0.66763848, 0.66763848, 0.66763848,\n",
       "        0.66763848, 0.66763848, 0.66763848, 0.66763848, 0.66763848]),\n",
       " 'split4_test_precision': array([0.        , 0.72189349, 0.72413793, 0.67164179, 0.67696629,\n",
       "        0.67307692, 0.67213115, 0.67302452, 0.67391304, 0.67391304,\n",
       "        0.67391304, 0.67391304, 0.67391304, 0.67391304, 0.67391304,\n",
       "        0.67391304, 0.67391304, 0.67391304, 0.72189349, 0.67164179,\n",
       "        0.67307692, 0.67302452, 0.67391304, 0.67391304, 0.67391304,\n",
       "        0.67391304, 0.67391304, 0.67391304, 0.67391304, 0.67391304]),\n",
       " 'split5_test_precision': array([0.        , 0.77070064, 0.73369565, 0.70238095, 0.71629213,\n",
       "        0.70684932, 0.70572207, 0.70189702, 0.70299728, 0.70299728,\n",
       "        0.70299728, 0.70299728, 0.70299728, 0.70299728, 0.70299728,\n",
       "        0.70299728, 0.70299728, 0.70299728, 0.77070064, 0.70238095,\n",
       "        0.70684932, 0.70189702, 0.70299728, 0.70299728, 0.70299728,\n",
       "        0.70299728, 0.70299728, 0.70299728, 0.70299728, 0.70299728]),\n",
       " 'split6_test_precision': array([0.        , 0.83216783, 0.73509934, 0.72425249, 0.71428571,\n",
       "        0.71130952, 0.71005917, 0.71216617, 0.71216617, 0.71301775,\n",
       "        0.71301775, 0.71301775, 0.71301775, 0.71301775, 0.71301775,\n",
       "        0.71301775, 0.71301775, 0.71301775, 0.83216783, 0.72425249,\n",
       "        0.71130952, 0.71216617, 0.71301775, 0.71301775, 0.71301775,\n",
       "        0.71301775, 0.71301775, 0.71301775, 0.71301775, 0.71301775]),\n",
       " 'split7_test_precision': array([0.        , 0.75977654, 0.67741935, 0.68698061, 0.67101828,\n",
       "        0.67002519, 0.67336683, 0.6725    , 0.67082294, 0.67082294,\n",
       "        0.67082294, 0.67082294, 0.67082294, 0.67082294, 0.67082294,\n",
       "        0.67082294, 0.67082294, 0.67082294, 0.75977654, 0.68698061,\n",
       "        0.67002519, 0.6725    , 0.67082294, 0.67082294, 0.67082294,\n",
       "        0.67082294, 0.67082294, 0.67082294, 0.67082294, 0.67082294]),\n",
       " 'split8_test_precision': array([0.        , 0.73939394, 0.75132275, 0.72058824, 0.72677596,\n",
       "        0.7311828 , 0.73209549, 0.73458445, 0.73262032, 0.73262032,\n",
       "        0.73262032, 0.73262032, 0.73262032, 0.73262032, 0.73262032,\n",
       "        0.73262032, 0.73262032, 0.73262032, 0.73939394, 0.72058824,\n",
       "        0.7311828 , 0.73458445, 0.73262032, 0.73262032, 0.73262032,\n",
       "        0.73262032, 0.73262032, 0.73262032, 0.73262032, 0.73262032]),\n",
       " 'split9_test_precision': array([0.        , 0.78034682, 0.75555556, 0.753125  , 0.72881356,\n",
       "        0.74285714, 0.73239437, 0.73802817, 0.73802817, 0.73802817,\n",
       "        0.73802817, 0.73802817, 0.73802817, 0.73802817, 0.73802817,\n",
       "        0.73802817, 0.73802817, 0.73802817, 0.78034682, 0.753125  ,\n",
       "        0.74285714, 0.73802817, 0.73802817, 0.73802817, 0.73802817,\n",
       "        0.73802817, 0.73802817, 0.73802817, 0.73802817, 0.73802817]),\n",
       " 'mean_test_precision': array([0.        , 0.75264951, 0.72476978, 0.70171862, 0.69578074,\n",
       "        0.69441581, 0.69322464, 0.69358184, 0.69361275, 0.69369584,\n",
       "        0.69361045, 0.69369584, 0.69361045, 0.69369584, 0.69369584,\n",
       "        0.69369584, 0.69361045, 0.69369584, 0.75264951, 0.70171862,\n",
       "        0.69441581, 0.69358184, 0.69369584, 0.69361045, 0.69361045,\n",
       "        0.69361045, 0.69361045, 0.69361045, 0.69369584, 0.69369584]),\n",
       " 'std_test_precision': array([0.        , 0.04039122, 0.03223301, 0.02665693, 0.02549402,\n",
       "        0.02881379, 0.02694381, 0.02783547, 0.02731051, 0.02752451,\n",
       "        0.02747608, 0.02752451, 0.02747608, 0.02752451, 0.02752451,\n",
       "        0.02752451, 0.02747608, 0.02752451, 0.04039122, 0.02665693,\n",
       "        0.02881379, 0.02783547, 0.02752451, 0.02747608, 0.02747608,\n",
       "        0.02747608, 0.02747608, 0.02747608, 0.02752451, 0.02752451]),\n",
       " 'rank_test_precision': array([30,  1,  3,  4,  6,  7, 29, 27, 18,  9, 19,  9, 19,  9,  9,  9, 19,\n",
       "         9,  1,  4,  7, 27,  9, 19, 19, 19, 19, 19,  9,  9]),\n",
       " 'split0_test_f1_micro': array([0.603, 0.676, 0.673, 0.714, 0.725, 0.724, 0.723, 0.727, 0.728,\n",
       "        0.727, 0.727, 0.727, 0.727, 0.727, 0.727, 0.727, 0.727, 0.727,\n",
       "        0.676, 0.714, 0.724, 0.727, 0.727, 0.727, 0.727, 0.727, 0.727,\n",
       "        0.727, 0.727, 0.727]),\n",
       " 'split1_test_f1_micro': array([0.603, 0.669, 0.679, 0.728, 0.73 , 0.731, 0.729, 0.729, 0.729,\n",
       "        0.729, 0.729, 0.729, 0.729, 0.729, 0.729, 0.729, 0.729, 0.729,\n",
       "        0.669, 0.728, 0.731, 0.729, 0.729, 0.729, 0.729, 0.729, 0.729,\n",
       "        0.729, 0.729, 0.729]),\n",
       " 'split2_test_f1_micro': array([0.603, 0.703, 0.707, 0.741, 0.747, 0.748, 0.747, 0.746, 0.745,\n",
       "        0.746, 0.745, 0.746, 0.745, 0.746, 0.746, 0.746, 0.745, 0.746,\n",
       "        0.703, 0.741, 0.748, 0.746, 0.746, 0.745, 0.745, 0.745, 0.745,\n",
       "        0.745, 0.746, 0.746]),\n",
       " 'split3_test_f1_micro': array([0.603, 0.678, 0.678, 0.72 , 0.714, 0.716, 0.721, 0.717, 0.718,\n",
       "        0.718, 0.718, 0.718, 0.718, 0.718, 0.718, 0.718, 0.718, 0.718,\n",
       "        0.678, 0.72 , 0.716, 0.717, 0.718, 0.718, 0.718, 0.718, 0.718,\n",
       "        0.718, 0.718, 0.718]),\n",
       " 'split4_test_f1_micro': array([0.603, 0.678, 0.681, 0.718, 0.729, 0.729, 0.729, 0.73 , 0.731,\n",
       "        0.731, 0.731, 0.731, 0.731, 0.731, 0.731, 0.731, 0.731, 0.731,\n",
       "        0.678, 0.718, 0.729, 0.73 , 0.731, 0.731, 0.731, 0.731, 0.731,\n",
       "        0.731, 0.731, 0.731]),\n",
       " 'split5_test_f1_micro': array([0.603, 0.688, 0.689, 0.739, 0.757, 0.754, 0.754, 0.752, 0.752,\n",
       "        0.752, 0.752, 0.752, 0.752, 0.752, 0.752, 0.752, 0.752, 0.752,\n",
       "        0.688, 0.739, 0.754, 0.752, 0.752, 0.752, 0.752, 0.752, 0.752,\n",
       "        0.752, 0.752, 0.752]),\n",
       " 'split6_test_f1_micro': array([0.603, 0.698, 0.674, 0.738, 0.741, 0.745, 0.745, 0.746, 0.746,\n",
       "        0.747, 0.747, 0.747, 0.747, 0.747, 0.747, 0.747, 0.747, 0.747,\n",
       "        0.698, 0.738, 0.745, 0.746, 0.747, 0.747, 0.747, 0.747, 0.747,\n",
       "        0.747, 0.747, 0.747]),\n",
       " 'split7_test_f1_micro': array([0.603, 0.696, 0.669, 0.738, 0.734, 0.738, 0.741, 0.741, 0.74 ,\n",
       "        0.74 , 0.74 , 0.74 , 0.74 , 0.74 , 0.74 , 0.74 , 0.74 , 0.74 ,\n",
       "        0.696, 0.738, 0.738, 0.741, 0.74 , 0.74 , 0.74 , 0.74 , 0.74 ,\n",
       "        0.74 , 0.74 , 0.74 ]),\n",
       " 'split8_test_f1_micro': array([0.603, 0.682, 0.698, 0.753, 0.769, 0.775, 0.778, 0.778, 0.777,\n",
       "        0.777, 0.777, 0.777, 0.777, 0.777, 0.777, 0.777, 0.777, 0.777,\n",
       "        0.682, 0.753, 0.775, 0.778, 0.777, 0.777, 0.777, 0.777, 0.777,\n",
       "        0.777, 0.777, 0.777]),\n",
       " 'split9_test_f1_micro': array([0.604, 0.701, 0.696, 0.766, 0.766, 0.774, 0.769, 0.773, 0.773,\n",
       "        0.773, 0.773, 0.773, 0.773, 0.773, 0.773, 0.773, 0.773, 0.773,\n",
       "        0.701, 0.766, 0.774, 0.773, 0.773, 0.773, 0.773, 0.773, 0.773,\n",
       "        0.773, 0.773, 0.773]),\n",
       " 'mean_test_f1_micro': array([0.6031, 0.6869, 0.6844, 0.7355, 0.7412, 0.7434, 0.7436, 0.7439,\n",
       "        0.7439, 0.744 , 0.7439, 0.744 , 0.7439, 0.744 , 0.744 , 0.744 ,\n",
       "        0.7439, 0.744 , 0.6869, 0.7355, 0.7434, 0.7439, 0.744 , 0.7439,\n",
       "        0.7439, 0.7439, 0.7439, 0.7439, 0.744 , 0.744 ]),\n",
       " 'std_test_f1_micro': array([0.0003    , 0.01134416, 0.0118676 , 0.0153509 , 0.01731935,\n",
       "        0.01894307, 0.01821648, 0.0187534 , 0.01827813, 0.01839021,\n",
       "        0.01838178, 0.01839021, 0.01838178, 0.01839021, 0.01839021,\n",
       "        0.01839021, 0.01838178, 0.01839021, 0.01134416, 0.0153509 ,\n",
       "        0.01894307, 0.0187534 , 0.01839021, 0.01838178, 0.01838178,\n",
       "        0.01838178, 0.01838178, 0.01838178, 0.01839021, 0.01839021]),\n",
       " 'rank_test_f1_micro': array([30, 27, 29, 25, 24, 22, 21, 10, 10,  1, 10,  1, 10,  1,  1,  1, 10,\n",
       "         1, 27, 25, 22, 10,  1, 10, 10, 10, 10, 10,  1,  1])}"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_model.cv_results_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "6d28a919",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([30, 27, 29, 25, 24, 22, 21, 10, 10,  1, 10,  1, 10,  1,  1,  1, 10,\n",
       "        1, 27, 25, 22, 10,  1, 10, 10, 10, 10, 10,  1,  1])"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_model.cv_results_['rank_test_accuracy']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "0ff3283b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'classifier': LogisticRegression(max_iter=10000),\n",
       " 'classifier__C': 0.0001,\n",
       " 'classifier__penalty': 'l1',\n",
       " 'classifier__solver': 'saga'}"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_model.cv_results_['params'][ np.argmin(best_model.cv_results_['mean_test_accuracy'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "1b20e96a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>classifier</th>\n",
       "      <th>C</th>\n",
       "      <th>penalty</th>\n",
       "      <th>solver</th>\n",
       "      <th>score_acc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.3969</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.3131</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.3156</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.2645</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0100</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.2588</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0100</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.2566</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.1000</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.2564</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.1000</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.2561</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.2561</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.2560</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>10.0000</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.2561</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>10.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.2560</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>100.0000</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.2561</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>100.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.2560</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>1000.0000</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.2560</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>1000.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.2560</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>10000.0000</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.2561</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>10000.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.2560</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.3131</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.2645</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0100</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.2566</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.1000</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.2561</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.2560</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>10.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.2561</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>100.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.2561</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>1000.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.2561</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>10000.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.2561</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>NaN</td>\n",
       "      <td>none</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.2561</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>NaN</td>\n",
       "      <td>none</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.2560</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.2560</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                            classifier           C penalty solver  score_acc\n",
       "0   LogisticRegression(max_iter=10000)      0.0001      l1   saga     0.3969\n",
       "1   LogisticRegression(max_iter=10000)      0.0001      l2   saga     0.3131\n",
       "2   LogisticRegression(max_iter=10000)      0.0010      l1   saga     0.3156\n",
       "3   LogisticRegression(max_iter=10000)      0.0010      l2   saga     0.2645\n",
       "4   LogisticRegression(max_iter=10000)      0.0100      l1   saga     0.2588\n",
       "5   LogisticRegression(max_iter=10000)      0.0100      l2   saga     0.2566\n",
       "6   LogisticRegression(max_iter=10000)      0.1000      l1   saga     0.2564\n",
       "7   LogisticRegression(max_iter=10000)      0.1000      l2   saga     0.2561\n",
       "8   LogisticRegression(max_iter=10000)      1.0000      l1   saga     0.2561\n",
       "9   LogisticRegression(max_iter=10000)      1.0000      l2   saga     0.2560\n",
       "10  LogisticRegression(max_iter=10000)     10.0000      l1   saga     0.2561\n",
       "11  LogisticRegression(max_iter=10000)     10.0000      l2   saga     0.2560\n",
       "12  LogisticRegression(max_iter=10000)    100.0000      l1   saga     0.2561\n",
       "13  LogisticRegression(max_iter=10000)    100.0000      l2   saga     0.2560\n",
       "14  LogisticRegression(max_iter=10000)   1000.0000      l1   saga     0.2560\n",
       "15  LogisticRegression(max_iter=10000)   1000.0000      l2   saga     0.2560\n",
       "16  LogisticRegression(max_iter=10000)  10000.0000      l1   saga     0.2561\n",
       "17  LogisticRegression(max_iter=10000)  10000.0000      l2   saga     0.2560\n",
       "18  LogisticRegression(max_iter=10000)      0.0001      l2  lbfgs     0.3131\n",
       "19  LogisticRegression(max_iter=10000)      0.0010      l2  lbfgs     0.2645\n",
       "20  LogisticRegression(max_iter=10000)      0.0100      l2  lbfgs     0.2566\n",
       "21  LogisticRegression(max_iter=10000)      0.1000      l2  lbfgs     0.2561\n",
       "22  LogisticRegression(max_iter=10000)      1.0000      l2  lbfgs     0.2560\n",
       "23  LogisticRegression(max_iter=10000)     10.0000      l2  lbfgs     0.2561\n",
       "24  LogisticRegression(max_iter=10000)    100.0000      l2  lbfgs     0.2561\n",
       "25  LogisticRegression(max_iter=10000)   1000.0000      l2  lbfgs     0.2561\n",
       "26  LogisticRegression(max_iter=10000)  10000.0000      l2  lbfgs     0.2561\n",
       "27  LogisticRegression(max_iter=10000)         NaN    none  lbfgs     0.2561\n",
       "28  LogisticRegression(max_iter=10000)         NaN    none   saga     0.2560\n",
       "29  LogisticRegression(max_iter=10000)         NaN     NaN    NaN     0.2560"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results = pd.DataFrame( best_model.cv_results_['params'] )\n",
    "\n",
    "# next grab the score resulting from those parameters, add it to the data\n",
    "# score is accuracy; to display it as misclassification error we use 1 - x\n",
    "results['score_acc'] = 1 - best_model.cv_results_['mean_test_accuracy']\n",
    "\n",
    "# get rid of classifierXX in columns\n",
    "cols = results.columns.to_series().str.split('__').apply(lambda x: x[-1])\n",
    "results.columns = cols\n",
    "\n",
    "# lets show the results for the saga solver across penalty & C values\n",
    "#sns.heatmap( results.query('solver==\"saga\"').pivot('C','penalty','score_acc'),\n",
    "#             annot=True, fmt='.3f')\n",
    "\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "a9b8f04c",
   "metadata": {},
   "outputs": [],
   "source": [
    "acc_list.append(results[0:1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "9ff4ec73",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'classifier': LogisticRegression(max_iter=10000),\n",
       " 'classifier__C': 0.0001,\n",
       " 'classifier__penalty': 'l2',\n",
       " 'classifier__solver': 'saga'}"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_model.cv_results_['params'][ np.argmin(best_model.cv_results_['rank_test_precision'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "f5204e89",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>classifier</th>\n",
       "      <th>C</th>\n",
       "      <th>penalty</th>\n",
       "      <th>solver</th>\n",
       "      <th>score_precision</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.247350</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.275230</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.298281</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0100</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.304219</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0100</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.305584</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.1000</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.306775</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.1000</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.306418</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.306387</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.306304</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>10.0000</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.306390</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>10.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.306304</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>100.0000</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.306390</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>100.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.306304</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>1000.0000</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.306304</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>1000.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.306304</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>10000.0000</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.306390</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>10000.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.306304</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.247350</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.298281</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0100</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.305584</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.1000</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.306418</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.306304</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>10.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.306390</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>100.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.306390</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>1000.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.306390</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>10000.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.306390</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>NaN</td>\n",
       "      <td>none</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.306390</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>NaN</td>\n",
       "      <td>none</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.306304</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.306304</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                            classifier           C penalty solver  \\\n",
       "0   LogisticRegression(max_iter=10000)      0.0001      l1   saga   \n",
       "1   LogisticRegression(max_iter=10000)      0.0001      l2   saga   \n",
       "2   LogisticRegression(max_iter=10000)      0.0010      l1   saga   \n",
       "3   LogisticRegression(max_iter=10000)      0.0010      l2   saga   \n",
       "4   LogisticRegression(max_iter=10000)      0.0100      l1   saga   \n",
       "5   LogisticRegression(max_iter=10000)      0.0100      l2   saga   \n",
       "6   LogisticRegression(max_iter=10000)      0.1000      l1   saga   \n",
       "7   LogisticRegression(max_iter=10000)      0.1000      l2   saga   \n",
       "8   LogisticRegression(max_iter=10000)      1.0000      l1   saga   \n",
       "9   LogisticRegression(max_iter=10000)      1.0000      l2   saga   \n",
       "10  LogisticRegression(max_iter=10000)     10.0000      l1   saga   \n",
       "11  LogisticRegression(max_iter=10000)     10.0000      l2   saga   \n",
       "12  LogisticRegression(max_iter=10000)    100.0000      l1   saga   \n",
       "13  LogisticRegression(max_iter=10000)    100.0000      l2   saga   \n",
       "14  LogisticRegression(max_iter=10000)   1000.0000      l1   saga   \n",
       "15  LogisticRegression(max_iter=10000)   1000.0000      l2   saga   \n",
       "16  LogisticRegression(max_iter=10000)  10000.0000      l1   saga   \n",
       "17  LogisticRegression(max_iter=10000)  10000.0000      l2   saga   \n",
       "18  LogisticRegression(max_iter=10000)      0.0001      l2  lbfgs   \n",
       "19  LogisticRegression(max_iter=10000)      0.0010      l2  lbfgs   \n",
       "20  LogisticRegression(max_iter=10000)      0.0100      l2  lbfgs   \n",
       "21  LogisticRegression(max_iter=10000)      0.1000      l2  lbfgs   \n",
       "22  LogisticRegression(max_iter=10000)      1.0000      l2  lbfgs   \n",
       "23  LogisticRegression(max_iter=10000)     10.0000      l2  lbfgs   \n",
       "24  LogisticRegression(max_iter=10000)    100.0000      l2  lbfgs   \n",
       "25  LogisticRegression(max_iter=10000)   1000.0000      l2  lbfgs   \n",
       "26  LogisticRegression(max_iter=10000)  10000.0000      l2  lbfgs   \n",
       "27  LogisticRegression(max_iter=10000)         NaN    none  lbfgs   \n",
       "28  LogisticRegression(max_iter=10000)         NaN    none   saga   \n",
       "29  LogisticRegression(max_iter=10000)         NaN     NaN    NaN   \n",
       "\n",
       "    score_precision  \n",
       "0          1.000000  \n",
       "1          0.247350  \n",
       "2          0.275230  \n",
       "3          0.298281  \n",
       "4          0.304219  \n",
       "5          0.305584  \n",
       "6          0.306775  \n",
       "7          0.306418  \n",
       "8          0.306387  \n",
       "9          0.306304  \n",
       "10         0.306390  \n",
       "11         0.306304  \n",
       "12         0.306390  \n",
       "13         0.306304  \n",
       "14         0.306304  \n",
       "15         0.306304  \n",
       "16         0.306390  \n",
       "17         0.306304  \n",
       "18         0.247350  \n",
       "19         0.298281  \n",
       "20         0.305584  \n",
       "21         0.306418  \n",
       "22         0.306304  \n",
       "23         0.306390  \n",
       "24         0.306390  \n",
       "25         0.306390  \n",
       "26         0.306390  \n",
       "27         0.306390  \n",
       "28         0.306304  \n",
       "29         0.306304  "
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results = pd.DataFrame( best_model.cv_results_['params'] )\n",
    "\n",
    "# next grab the score resulting from those parameters, add it to the data\n",
    "# score is accuracy; to display it as misclassification error we use 1 - x\n",
    "results['score_precision'] = 1 - best_model.cv_results_['mean_test_precision']\n",
    "\n",
    "# get rid of classifierXX in columns\n",
    "cols = results.columns.to_series().str.split('__').apply(lambda x: x[-1])\n",
    "results.columns = cols\n",
    "\n",
    "# lets show the results for the saga solver across penalty & C values\n",
    "#sns.heatmap( results.query('solver==\"saga\"').pivot('C','penalty','score_precision'),\n",
    "#             annot=True, fmt='.3f')\n",
    "\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "43b18550",
   "metadata": {},
   "outputs": [],
   "source": [
    "roc_auc_ovr_List.append(results[1:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "24071971",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'classifier': LogisticRegression(max_iter=10000),\n",
       " 'classifier__C': 1.0,\n",
       " 'classifier__penalty': 'l2',\n",
       " 'classifier__solver': 'saga'}"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_model.cv_results_['params'][ np.argmin(best_model.cv_results_['rank_test_f1_micro'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "87b8ca97",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>classifier</th>\n",
       "      <th>C</th>\n",
       "      <th>penalty</th>\n",
       "      <th>solver</th>\n",
       "      <th>score_f1_micro</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.3969</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.3131</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.3156</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.2645</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0100</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.2588</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0100</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.2566</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.1000</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.2564</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.1000</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.2561</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.2561</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.2560</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>10.0000</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.2561</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>10.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.2560</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>100.0000</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.2561</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>100.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.2560</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>1000.0000</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.2560</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>1000.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.2560</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>10000.0000</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.2561</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>10000.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.2560</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.3131</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.2645</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0100</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.2566</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.1000</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.2561</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.2560</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>10.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.2561</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>100.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.2561</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>1000.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.2561</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>10000.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.2561</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>NaN</td>\n",
       "      <td>none</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.2561</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>NaN</td>\n",
       "      <td>none</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.2560</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.2560</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                            classifier           C penalty solver  \\\n",
       "0   LogisticRegression(max_iter=10000)      0.0001      l1   saga   \n",
       "1   LogisticRegression(max_iter=10000)      0.0001      l2   saga   \n",
       "2   LogisticRegression(max_iter=10000)      0.0010      l1   saga   \n",
       "3   LogisticRegression(max_iter=10000)      0.0010      l2   saga   \n",
       "4   LogisticRegression(max_iter=10000)      0.0100      l1   saga   \n",
       "5   LogisticRegression(max_iter=10000)      0.0100      l2   saga   \n",
       "6   LogisticRegression(max_iter=10000)      0.1000      l1   saga   \n",
       "7   LogisticRegression(max_iter=10000)      0.1000      l2   saga   \n",
       "8   LogisticRegression(max_iter=10000)      1.0000      l1   saga   \n",
       "9   LogisticRegression(max_iter=10000)      1.0000      l2   saga   \n",
       "10  LogisticRegression(max_iter=10000)     10.0000      l1   saga   \n",
       "11  LogisticRegression(max_iter=10000)     10.0000      l2   saga   \n",
       "12  LogisticRegression(max_iter=10000)    100.0000      l1   saga   \n",
       "13  LogisticRegression(max_iter=10000)    100.0000      l2   saga   \n",
       "14  LogisticRegression(max_iter=10000)   1000.0000      l1   saga   \n",
       "15  LogisticRegression(max_iter=10000)   1000.0000      l2   saga   \n",
       "16  LogisticRegression(max_iter=10000)  10000.0000      l1   saga   \n",
       "17  LogisticRegression(max_iter=10000)  10000.0000      l2   saga   \n",
       "18  LogisticRegression(max_iter=10000)      0.0001      l2  lbfgs   \n",
       "19  LogisticRegression(max_iter=10000)      0.0010      l2  lbfgs   \n",
       "20  LogisticRegression(max_iter=10000)      0.0100      l2  lbfgs   \n",
       "21  LogisticRegression(max_iter=10000)      0.1000      l2  lbfgs   \n",
       "22  LogisticRegression(max_iter=10000)      1.0000      l2  lbfgs   \n",
       "23  LogisticRegression(max_iter=10000)     10.0000      l2  lbfgs   \n",
       "24  LogisticRegression(max_iter=10000)    100.0000      l2  lbfgs   \n",
       "25  LogisticRegression(max_iter=10000)   1000.0000      l2  lbfgs   \n",
       "26  LogisticRegression(max_iter=10000)  10000.0000      l2  lbfgs   \n",
       "27  LogisticRegression(max_iter=10000)         NaN    none  lbfgs   \n",
       "28  LogisticRegression(max_iter=10000)         NaN    none   saga   \n",
       "29  LogisticRegression(max_iter=10000)         NaN     NaN    NaN   \n",
       "\n",
       "    score_f1_micro  \n",
       "0           0.3969  \n",
       "1           0.3131  \n",
       "2           0.3156  \n",
       "3           0.2645  \n",
       "4           0.2588  \n",
       "5           0.2566  \n",
       "6           0.2564  \n",
       "7           0.2561  \n",
       "8           0.2561  \n",
       "9           0.2560  \n",
       "10          0.2561  \n",
       "11          0.2560  \n",
       "12          0.2561  \n",
       "13          0.2560  \n",
       "14          0.2560  \n",
       "15          0.2560  \n",
       "16          0.2561  \n",
       "17          0.2560  \n",
       "18          0.3131  \n",
       "19          0.2645  \n",
       "20          0.2566  \n",
       "21          0.2561  \n",
       "22          0.2560  \n",
       "23          0.2561  \n",
       "24          0.2561  \n",
       "25          0.2561  \n",
       "26          0.2561  \n",
       "27          0.2561  \n",
       "28          0.2560  \n",
       "29          0.2560  "
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results = pd.DataFrame( best_model.cv_results_['params'] )\n",
    "\n",
    "# next grab the score resulting from those parameters, add it to the data\n",
    "# score is accuracy; to display it as misclassification error we use 1 - x\n",
    "results['score_f1_micro'] = 1 - best_model.cv_results_['mean_test_f1_micro']\n",
    "\n",
    "# get rid of classifierXX in columns\n",
    "cols = results.columns.to_series().str.split('__').apply(lambda x: x[-1])\n",
    "results.columns = cols\n",
    "\n",
    "# lets show the results for the saga solver across penalty & C values\n",
    "#sns.heatmap( results.query('solver==\"saga\"').pivot('C','penalty','score_f1_micro'),\n",
    "#             annot=True, fmt='.3f')\n",
    "\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "39fb7b98",
   "metadata": {},
   "outputs": [],
   "source": [
    "f1_micro_List.append(results[9:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6491ce89",
   "metadata": {},
   "source": [
    "## Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "id": "f82dc269",
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy = [x['score_acc'][0] for x in acc_list]\n",
    "accuracy_c = [x['C'][0] for x in acc_list]\n",
    "accuracy_penalty = [x['penalty'][0] for x in acc_list]\n",
    "roc = [x['score_precision'].reset_index(drop=True)[0] for x in roc_auc_ovr_List]\n",
    "roc_c = [x['C'].reset_index(drop=True)[0] for x in roc_auc_ovr_List]\n",
    "roc_penalty = [x['penalty'].reset_index(drop=True)[0] for x in roc_auc_ovr_List]\n",
    "f1 = [x['score_f1_micro'].reset_index(drop=True)[0] for x in f1_micro_List]\n",
    "f1_c = [x['C'].reset_index(drop=True)[0] for x in f1_micro_List]\n",
    "f1_penalty = [x['penalty'].reset_index(drop=True)[0] for x in f1_micro_List]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "id": "5d58acc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {'Accuracy': accuracy, 'Accuracy C': accuracy_c, 'Accuracy Penalty': accuracy_penalty,\n",
    "        'Roc_auc_ovr': roc, 'Roc_auc_ovr C': roc_c, 'Roc_auc_ovr Penalty': roc_penalty,\n",
    "        'F1_micro':f1, 'F1_micro C': f1_c, 'F1_micro Penalty': f1_penalty}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "id": "27c29018",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Accuracy C</th>\n",
       "      <th>Accuracy Penalty</th>\n",
       "      <th>Roc_auc_ovr</th>\n",
       "      <th>Roc_auc_ovr C</th>\n",
       "      <th>Roc_auc_ovr Penalty</th>\n",
       "      <th>F1_micro</th>\n",
       "      <th>F1_micro C</th>\n",
       "      <th>F1_micro Penalty</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.3961</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>l1</td>\n",
       "      <td>0.245991</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>l2</td>\n",
       "      <td>0.2586</td>\n",
       "      <td>1.00</td>\n",
       "      <td>l1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.3996</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>l1</td>\n",
       "      <td>0.245796</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>l2</td>\n",
       "      <td>0.2586</td>\n",
       "      <td>0.01</td>\n",
       "      <td>l2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.3989</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>l1</td>\n",
       "      <td>0.248720</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>l2</td>\n",
       "      <td>0.2539</td>\n",
       "      <td>0.01</td>\n",
       "      <td>l2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.3989</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>l1</td>\n",
       "      <td>0.236486</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>l2</td>\n",
       "      <td>0.2567</td>\n",
       "      <td>1.00</td>\n",
       "      <td>l2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.4021</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>l1</td>\n",
       "      <td>0.243476</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>l2</td>\n",
       "      <td>0.2532</td>\n",
       "      <td>0.10</td>\n",
       "      <td>l1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.3997</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>l1</td>\n",
       "      <td>0.221245</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>l2</td>\n",
       "      <td>0.2460</td>\n",
       "      <td>0.10</td>\n",
       "      <td>l1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.4034</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>l1</td>\n",
       "      <td>0.238699</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>l2</td>\n",
       "      <td>0.2520</td>\n",
       "      <td>1.00</td>\n",
       "      <td>l1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.4069</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>l1</td>\n",
       "      <td>0.235141</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>l2</td>\n",
       "      <td>0.2453</td>\n",
       "      <td>0.10</td>\n",
       "      <td>l1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.3954</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>l1</td>\n",
       "      <td>0.241676</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>l2</td>\n",
       "      <td>0.2533</td>\n",
       "      <td>0.01</td>\n",
       "      <td>l1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.3969</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>l1</td>\n",
       "      <td>0.247350</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>l2</td>\n",
       "      <td>0.2560</td>\n",
       "      <td>1.00</td>\n",
       "      <td>l2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Accuracy  Accuracy C Accuracy Penalty  Roc_auc_ovr  Roc_auc_ovr C  \\\n",
       "0    0.3961      0.0001               l1     0.245991         0.0001   \n",
       "1    0.3996      0.0001               l1     0.245796         0.0001   \n",
       "2    0.3989      0.0001               l1     0.248720         0.0001   \n",
       "3    0.3989      0.0001               l1     0.236486         0.0001   \n",
       "4    0.4021      0.0001               l1     0.243476         0.0001   \n",
       "5    0.3997      0.0001               l1     0.221245         0.0001   \n",
       "6    0.4034      0.0001               l1     0.238699         0.0001   \n",
       "7    0.4069      0.0001               l1     0.235141         0.0001   \n",
       "8    0.3954      0.0001               l1     0.241676         0.0001   \n",
       "9    0.3969      0.0001               l1     0.247350         0.0001   \n",
       "\n",
       "  Roc_auc_ovr Penalty  F1_micro  F1_micro C F1_micro Penalty  \n",
       "0                  l2    0.2586        1.00               l1  \n",
       "1                  l2    0.2586        0.01               l2  \n",
       "2                  l2    0.2539        0.01               l2  \n",
       "3                  l2    0.2567        1.00               l2  \n",
       "4                  l2    0.2532        0.10               l1  \n",
       "5                  l2    0.2460        0.10               l1  \n",
       "6                  l2    0.2520        1.00               l1  \n",
       "7                  l2    0.2453        0.10               l1  \n",
       "8                  l2    0.2533        0.01               l1  \n",
       "9                  l2    0.2560        1.00               l2  "
      ]
     },
     "execution_count": 195,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainingResults  = pd.DataFrame(data = data)\n",
    "pd.options.display.max_colwidth = 100\n",
    "trainingResults"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "id": "21d2830e",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainingResults.to_csv('LR_trainingResults.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
