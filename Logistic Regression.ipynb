{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c5923a4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.preprocessing import MinMaxScaler, MaxAbsScaler, StandardScaler \n",
    "#from sklearn.preprocessing import RobustScaler, Normalizer, QuantileTransformer, PowerTransformer\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "#from sklearn import svm, datasets\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn import preprocessing\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8f9a1a2a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>HighBP</th>\n",
       "      <th>HighChol</th>\n",
       "      <th>CholCheck</th>\n",
       "      <th>BMI</th>\n",
       "      <th>Smoker</th>\n",
       "      <th>Stroke</th>\n",
       "      <th>HeartDiseaseorAttack</th>\n",
       "      <th>PhysActivity</th>\n",
       "      <th>Fruits</th>\n",
       "      <th>Veggies</th>\n",
       "      <th>...</th>\n",
       "      <th>NoDocbcCost</th>\n",
       "      <th>GenHlth</th>\n",
       "      <th>MentHlth</th>\n",
       "      <th>PhysHlth</th>\n",
       "      <th>DiffWalk</th>\n",
       "      <th>Sex</th>\n",
       "      <th>Age</th>\n",
       "      <th>Education</th>\n",
       "      <th>Income</th>\n",
       "      <th>Diabetes_binary</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>23</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>11</td>\n",
       "      <td>5</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>19</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>26</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>22</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>12</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>22</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>6</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>253659</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>37</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>253668</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>29</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>3</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>253670</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>25</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>15</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>13</td>\n",
       "      <td>6</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>253676</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>18</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>11</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>253679</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>25</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>6</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>88365 rows Ã— 22 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        HighBP  HighChol  CholCheck  BMI  Smoker  Stroke  \\\n",
       "0            0         0          1   23       0       0   \n",
       "1            1         0          1   19       0       0   \n",
       "2            0         0          1   26       1       0   \n",
       "3            0         1          1   22       0       0   \n",
       "4            0         0          1   22       0       0   \n",
       "...        ...       ...        ...  ...     ...     ...   \n",
       "253659       0         1          1   37       0       0   \n",
       "253668       0         1          1   29       1       0   \n",
       "253670       1         1          1   25       0       0   \n",
       "253676       1         1          1   18       0       0   \n",
       "253679       1         1          1   25       0       0   \n",
       "\n",
       "        HeartDiseaseorAttack  PhysActivity  Fruits  Veggies  ...  NoDocbcCost  \\\n",
       "0                          0             1       0        0  ...            0   \n",
       "1                          0             0       1        1  ...            0   \n",
       "2                          0             1       1        1  ...            0   \n",
       "3                          0             1       1        1  ...            0   \n",
       "4                          0             0       1        1  ...            0   \n",
       "...                      ...           ...     ...      ...  ...          ...   \n",
       "253659                     0             0       0        1  ...            0   \n",
       "253668                     1             0       1        1  ...            0   \n",
       "253670                     1             0       1        0  ...            0   \n",
       "253676                     0             0       0        0  ...            0   \n",
       "253679                     1             1       1        0  ...            0   \n",
       "\n",
       "        GenHlth  MentHlth  PhysHlth  DiffWalk  Sex  Age  Education  Income  \\\n",
       "0             1         0         0         0    0   11          5       7   \n",
       "1             3         0         0         0    0    6          6       8   \n",
       "2             2         0         0         0    0    1          4       4   \n",
       "3             1         0         0         0    1   12          4       2   \n",
       "4             1         0         0         0    0    4          6       8   \n",
       "...         ...       ...       ...       ...  ...  ...        ...     ...   \n",
       "253659        4         0         0         0    0    6          4       1   \n",
       "253668        2         0         0         1    1   10          3       6   \n",
       "253670        5        15         0         1    0   13          6       4   \n",
       "253676        4         0         0         1    0   11          2       4   \n",
       "253679        2         0         0         0    0    9          6       2   \n",
       "\n",
       "        Diabetes_binary  \n",
       "0                     0  \n",
       "1                     0  \n",
       "2                     0  \n",
       "3                     0  \n",
       "4                     0  \n",
       "...                 ...  \n",
       "253659                1  \n",
       "253668                1  \n",
       "253670                1  \n",
       "253676                1  \n",
       "253679                1  \n",
       "\n",
       "[88365 rows x 22 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('Preprocessed_data.csv', index_col = 0)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "185b32db",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.iloc[:,0:-1]\n",
    "y = df.loc[:, 'Diabetes_binary']\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, y, test_size=0.2, shuffle=True, random_state = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9e763e27",
   "metadata": {},
   "outputs": [],
   "source": [
    "acc_list = []\n",
    "roc_auc_ovr_List = []\n",
    "f1_micro_List = []"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62155024",
   "metadata": {},
   "source": [
    "## Trial 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cc362efc",
   "metadata": {},
   "outputs": [],
   "source": [
    "start = datetime.now()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a65d5cf2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Guest-All\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\Guest-All\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\Guest-All\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\Guest-All\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\Guest-All\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\Guest-All\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\Guest-All\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\Guest-All\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\Guest-All\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\Guest-All\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\Guest-All\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1183: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Guest-All\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1183: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Guest-All\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1183: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Guest-All\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1183: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Guest-All\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1183: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Guest-All\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1183: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Guest-All\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1183: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Guest-All\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1183: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Guest-All\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1183: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Guest-All\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1183: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Guest-All\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1183: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Guest-All\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1183: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Guest-All\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1183: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Guest-All\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1183: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Guest-All\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1183: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Guest-All\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1183: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Guest-All\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1183: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Guest-All\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1183: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Guest-All\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1183: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Guest-All\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1183: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Create a pipeline - RF is a stand in, we will populate the classifier part below\n",
    "pipe = Pipeline([('std', StandardScaler()),\n",
    "                 ('classifier', LogisticRegression())])\n",
    "\n",
    "# Create search space of candidate learning algorithms and their hyperparameters\n",
    "# note lbfgs can't do l1, and if you pass penalty='none' it expects no C value\n",
    "search_space = [{'classifier': [LogisticRegression(max_iter=10000)],\n",
    "                 'classifier__solver': ['saga'],\n",
    "                 'classifier__penalty': ['l1', 'l2'],\n",
    "                 'classifier__C': np.logspace(-4, 4, 9)},\n",
    "                {'classifier': [LogisticRegression(max_iter=10000)],\n",
    "                 'classifier__solver': ['lbfgs'],\n",
    "                 'classifier__penalty': ['l2'],\n",
    "                 'classifier__C': np.logspace(-4, 4, 9)},\n",
    "                {'classifier': [LogisticRegression(max_iter=10000)],\n",
    "                 'classifier__solver': ['lbfgs','saga'],\n",
    "                 'classifier__penalty': ['none']},\n",
    "                {'classifier': [LogisticRegression(max_iter=10000)]}\n",
    "                ]\n",
    "\n",
    "# Create grid search\n",
    "clf = GridSearchCV(pipe, search_space, cv=StratifiedKFold(n_splits=10),\n",
    "                   scoring=['accuracy', 'precision', 'f1_micro'], refit=False,\n",
    "                   verbose=0, penalty = )\n",
    "\n",
    "# Fit grid search\n",
    "best_model = clf.fit(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8390b96b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'mean_fit_time': array([0.65914841, 0.65778089, 0.5563247 , 0.8201467 , 0.85659654,\n",
       "        0.61211035, 0.84067125, 0.88029273, 0.83573036, 0.85770466,\n",
       "        0.73098173, 0.89634936, 0.98322189, 0.71891797, 0.71105556,\n",
       "        0.75074055, 0.86792619, 0.770857  , 0.09161048, 0.10807879,\n",
       "        0.10175478, 0.10385723, 0.10838196, 0.10472271, 0.10932174,\n",
       "        0.10816913, 0.10585136, 0.10269229, 0.8219702 , 0.11308763]),\n",
       " 'std_fit_time': array([0.14267239, 0.14577944, 0.0861196 , 0.20688543, 0.26137992,\n",
       "        0.22976913, 0.30814208, 0.26009263, 0.29695248, 0.26156124,\n",
       "        0.17729057, 0.28477449, 0.28926526, 0.28346317, 0.1607101 ,\n",
       "        0.07805331, 0.0922573 , 0.02717971, 0.00471796, 0.00423326,\n",
       "        0.0045063 , 0.00572735, 0.00066364, 0.00278133, 0.009425  ,\n",
       "        0.00071958, 0.00627034, 0.00563807, 0.04701035, 0.00360152]),\n",
       " 'mean_score_time': array([0.01089888, 0.01220818, 0.00938954, 0.01338258, 0.01389461,\n",
       "        0.01270204, 0.01507306, 0.01168995, 0.01257172, 0.01759176,\n",
       "        0.01226838, 0.0140913 , 0.01955795, 0.0114959 , 0.01157577,\n",
       "        0.01245995, 0.01274402, 0.0132087 , 0.01178708, 0.01399367,\n",
       "        0.01356046, 0.01406138, 0.01469533, 0.01338303, 0.013608  ,\n",
       "        0.01471696, 0.01372061, 0.01245036, 0.01346233, 0.01500852]),\n",
       " 'std_score_time': array([0.00200475, 0.00199328, 0.00079642, 0.00137604, 0.00699048,\n",
       "        0.01069776, 0.00756134, 0.00193325, 0.00555333, 0.01134482,\n",
       "        0.00643126, 0.00468844, 0.01743653, 0.00344147, 0.00251744,\n",
       "        0.00120849, 0.00115191, 0.00086036, 0.00039974, 0.00270726,\n",
       "        0.00167378, 0.00139119, 0.00259961, 0.00286558, 0.00196378,\n",
       "        0.00233862, 0.00353465, 0.00123419, 0.00123444, 0.00183374]),\n",
       " 'param_classifier': masked_array(data=[LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000)],\n",
       "              mask=[False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'param_classifier__C': masked_array(data=[0.0001, 0.0001, 0.001, 0.001, 0.01, 0.01, 0.1, 0.1,\n",
       "                    1.0, 1.0, 10.0, 10.0, 100.0, 100.0, 1000.0, 1000.0,\n",
       "                    10000.0, 10000.0, 0.0001, 0.001, 0.01, 0.1, 1.0, 10.0,\n",
       "                    100.0, 1000.0, 10000.0, --, --, --],\n",
       "              mask=[False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False,  True,  True,  True],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'param_classifier__penalty': masked_array(data=['l1', 'l2', 'l1', 'l2', 'l1', 'l2', 'l1', 'l2', 'l1',\n",
       "                    'l2', 'l1', 'l2', 'l1', 'l2', 'l1', 'l2', 'l1', 'l2',\n",
       "                    'l2', 'l2', 'l2', 'l2', 'l2', 'l2', 'l2', 'l2', 'l2',\n",
       "                    'none', 'none', --],\n",
       "              mask=[False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False,  True],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'param_classifier__solver': masked_array(data=['saga', 'saga', 'saga', 'saga', 'saga', 'saga', 'saga',\n",
       "                    'saga', 'saga', 'saga', 'saga', 'saga', 'saga', 'saga',\n",
       "                    'saga', 'saga', 'saga', 'saga', 'lbfgs', 'lbfgs',\n",
       "                    'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs',\n",
       "                    'lbfgs', 'lbfgs', 'saga', --],\n",
       "              mask=[False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False,  True],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'params': [{'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 0.0001,\n",
       "   'classifier__penalty': 'l1',\n",
       "   'classifier__solver': 'saga'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 0.0001,\n",
       "   'classifier__penalty': 'l2',\n",
       "   'classifier__solver': 'saga'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 0.001,\n",
       "   'classifier__penalty': 'l1',\n",
       "   'classifier__solver': 'saga'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 0.001,\n",
       "   'classifier__penalty': 'l2',\n",
       "   'classifier__solver': 'saga'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 0.01,\n",
       "   'classifier__penalty': 'l1',\n",
       "   'classifier__solver': 'saga'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 0.01,\n",
       "   'classifier__penalty': 'l2',\n",
       "   'classifier__solver': 'saga'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 0.1,\n",
       "   'classifier__penalty': 'l1',\n",
       "   'classifier__solver': 'saga'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 0.1,\n",
       "   'classifier__penalty': 'l2',\n",
       "   'classifier__solver': 'saga'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 1.0,\n",
       "   'classifier__penalty': 'l1',\n",
       "   'classifier__solver': 'saga'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 1.0,\n",
       "   'classifier__penalty': 'l2',\n",
       "   'classifier__solver': 'saga'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 10.0,\n",
       "   'classifier__penalty': 'l1',\n",
       "   'classifier__solver': 'saga'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 10.0,\n",
       "   'classifier__penalty': 'l2',\n",
       "   'classifier__solver': 'saga'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 100.0,\n",
       "   'classifier__penalty': 'l1',\n",
       "   'classifier__solver': 'saga'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 100.0,\n",
       "   'classifier__penalty': 'l2',\n",
       "   'classifier__solver': 'saga'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 1000.0,\n",
       "   'classifier__penalty': 'l1',\n",
       "   'classifier__solver': 'saga'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 1000.0,\n",
       "   'classifier__penalty': 'l2',\n",
       "   'classifier__solver': 'saga'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 10000.0,\n",
       "   'classifier__penalty': 'l1',\n",
       "   'classifier__solver': 'saga'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 10000.0,\n",
       "   'classifier__penalty': 'l2',\n",
       "   'classifier__solver': 'saga'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 0.0001,\n",
       "   'classifier__penalty': 'l2',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 0.001,\n",
       "   'classifier__penalty': 'l2',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 0.01,\n",
       "   'classifier__penalty': 'l2',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 0.1,\n",
       "   'classifier__penalty': 'l2',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 1.0,\n",
       "   'classifier__penalty': 'l2',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 10.0,\n",
       "   'classifier__penalty': 'l2',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 100.0,\n",
       "   'classifier__penalty': 'l2',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 1000.0,\n",
       "   'classifier__penalty': 'l2',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 10000.0,\n",
       "   'classifier__penalty': 'l2',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__penalty': 'none',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__penalty': 'none',\n",
       "   'classifier__solver': 'saga'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000)}],\n",
       " 'split0_test_accuracy': array([0.60056577, 0.73776521, 0.74087694, 0.74328147, 0.74356436,\n",
       "        0.74328147, 0.74243281, 0.7427157 , 0.7427157 , 0.7427157 ,\n",
       "        0.7427157 , 0.7427157 , 0.7427157 , 0.7427157 , 0.7427157 ,\n",
       "        0.7427157 , 0.7427157 , 0.7427157 , 0.73776521, 0.74328147,\n",
       "        0.74328147, 0.7427157 , 0.74257426, 0.74257426, 0.74257426,\n",
       "        0.74257426, 0.74257426, 0.74257426, 0.7427157 , 0.74257426]),\n",
       " 'split1_test_accuracy': array([0.60056577, 0.73776521, 0.74314003, 0.7446959 , 0.74540311,\n",
       "        0.74441301, 0.7437058 , 0.74413013, 0.74427157, 0.74427157,\n",
       "        0.74427157, 0.74427157, 0.74427157, 0.74427157, 0.74427157,\n",
       "        0.74427157, 0.74427157, 0.74427157, 0.73776521, 0.7446959 ,\n",
       "        0.74441301, 0.74413013, 0.74427157, 0.74427157, 0.74427157,\n",
       "        0.74427157, 0.74427157, 0.74427157, 0.74427157, 0.74427157]),\n",
       " 'split2_test_accuracy': array([0.60065073, 0.73758665, 0.73857688, 0.74522563, 0.74381101,\n",
       "        0.7453671 , 0.74451832, 0.74494271, 0.74480124, 0.74494271,\n",
       "        0.74494271, 0.74480124, 0.74494271, 0.74494271, 0.74494271,\n",
       "        0.74494271, 0.74480124, 0.74480124, 0.73758665, 0.74522563,\n",
       "        0.7453671 , 0.74494271, 0.74494271, 0.74494271, 0.74494271,\n",
       "        0.74494271, 0.74494271, 0.74494271, 0.74494271, 0.74494271]),\n",
       " 'split3_test_accuracy': array([0.60050927, 0.73970859, 0.74282077, 0.7464988 , 0.74310369,\n",
       "        0.74480124, 0.74465978, 0.74465978, 0.74465978, 0.74480124,\n",
       "        0.74480124, 0.74480124, 0.74480124, 0.74480124, 0.74480124,\n",
       "        0.74480124, 0.74480124, 0.74480124, 0.73970859, 0.7464988 ,\n",
       "        0.74480124, 0.74465978, 0.74465978, 0.74480124, 0.74480124,\n",
       "        0.74480124, 0.74480124, 0.74480124, 0.74480124, 0.74465978]),\n",
       " 'split4_test_accuracy': array([0.60050927, 0.74013297, 0.75060122, 0.75187438, 0.75244023,\n",
       "        0.75258169, 0.75286462, 0.75258169, 0.75272316, 0.75272316,\n",
       "        0.75272316, 0.75272316, 0.75272316, 0.75272316, 0.75272316,\n",
       "        0.75272316, 0.75272316, 0.75272316, 0.74013297, 0.75187438,\n",
       "        0.75258169, 0.75258169, 0.75272316, 0.75272316, 0.75272316,\n",
       "        0.75272316, 0.75272316, 0.75272316, 0.75272316, 0.75272316]),\n",
       " 'split5_test_accuracy': array([0.60050927, 0.73744518, 0.74522563, 0.74720611, 0.74748904,\n",
       "        0.7476305 , 0.74706465, 0.74720611, 0.74706465, 0.74720611,\n",
       "        0.74706465, 0.74720611, 0.74720611, 0.74720611, 0.74720611,\n",
       "        0.74720611, 0.74720611, 0.74720611, 0.73744518, 0.74720611,\n",
       "        0.74748904, 0.74720611, 0.74720611, 0.74706465, 0.74706465,\n",
       "        0.74706465, 0.74706465, 0.74706465, 0.74720611, 0.74720611]),\n",
       " 'split6_test_accuracy': array([0.60050927, 0.75074268, 0.76022068, 0.76191823, 0.76177677,\n",
       "        0.76220116, 0.76234262, 0.76248409, 0.76234262, 0.76248409,\n",
       "        0.76248409, 0.76248409, 0.76248409, 0.76248409, 0.76248409,\n",
       "        0.76248409, 0.76248409, 0.76248409, 0.75074268, 0.76191823,\n",
       "        0.76220116, 0.76248409, 0.76248409, 0.76248409, 0.76248409,\n",
       "        0.76248409, 0.76248409, 0.76248409, 0.76248409, 0.76248409]),\n",
       " 'split7_test_accuracy': array([0.60050927, 0.73122082, 0.734333  , 0.73687933, 0.73758665,\n",
       "        0.73588909, 0.73560617, 0.73532324, 0.73532324, 0.73532324,\n",
       "        0.73532324, 0.73532324, 0.73532324, 0.73532324, 0.73532324,\n",
       "        0.73532324, 0.73532324, 0.73532324, 0.73122082, 0.73687933,\n",
       "        0.73588909, 0.73532324, 0.73532324, 0.73532324, 0.73532324,\n",
       "        0.73532324, 0.73532324, 0.73532324, 0.73532324, 0.73532324]),\n",
       " 'split8_test_accuracy': array([0.60050927, 0.73772811, 0.74197199, 0.7426793 , 0.74381101,\n",
       "        0.74352808, 0.74395247, 0.74366954, 0.74366954, 0.74366954,\n",
       "        0.74366954, 0.74366954, 0.74366954, 0.74366954, 0.74366954,\n",
       "        0.74366954, 0.74366954, 0.74366954, 0.73772811, 0.7426793 ,\n",
       "        0.74352808, 0.74366954, 0.74366954, 0.74366954, 0.74366954,\n",
       "        0.74366954, 0.74366954, 0.74366954, 0.74366954, 0.74366954]),\n",
       " 'split9_test_accuracy': array([0.60050927, 0.73900127, 0.74522563, 0.74621587, 0.74748904,\n",
       "        0.74734757, 0.74847928, 0.74805489, 0.74819635, 0.74805489,\n",
       "        0.74805489, 0.74805489, 0.74805489, 0.74805489, 0.74805489,\n",
       "        0.74805489, 0.74805489, 0.74805489, 0.73900127, 0.74621587,\n",
       "        0.74734757, 0.74805489, 0.74819635, 0.74805489, 0.74805489,\n",
       "        0.74805489, 0.74805489, 0.74805489, 0.74805489, 0.74819635]),\n",
       " 'mean_test_accuracy': array([0.60053471, 0.73890967, 0.74429928, 0.7466475 , 0.74664749,\n",
       "        0.74670409, 0.74656265, 0.74657679, 0.74657679, 0.74661922,\n",
       "        0.74660508, 0.74660508, 0.74661922, 0.74661922, 0.74661922,\n",
       "        0.74661922, 0.74660508, 0.74660508, 0.73890967, 0.7466475 ,\n",
       "        0.74668995, 0.74657679, 0.74660508, 0.74659093, 0.74659093,\n",
       "        0.74659093, 0.74659093, 0.74659093, 0.74661922, 0.74660508]),\n",
       " 'std_test_accuracy': array([4.46337344e-05, 4.57375566e-03, 6.69068531e-03, 6.23987474e-03,\n",
       "        6.21023593e-03, 6.52227447e-03, 6.72698078e-03, 6.73806711e-03,\n",
       "        6.71821590e-03, 6.74182432e-03, 6.74072635e-03, 6.74547471e-03,\n",
       "        6.74182432e-03, 6.74182432e-03, 6.74182432e-03, 6.74182432e-03,\n",
       "        6.74547471e-03, 6.74547471e-03, 4.57375566e-03, 6.23987474e-03,\n",
       "        6.52040298e-03, 6.73806711e-03, 6.75725365e-03, 6.74901601e-03,\n",
       "        6.74901601e-03, 6.74901601e-03, 6.74901601e-03, 6.74901601e-03,\n",
       "        6.74182432e-03, 6.75725365e-03]),\n",
       " 'rank_test_accuracy': array([30, 28, 27,  3,  5,  1, 26, 23, 25,  6, 17, 14,  6,  6,  6,  6, 14,\n",
       "        14, 28,  3,  2, 23, 12, 18, 18, 18, 18, 18,  6, 12]),\n",
       " 'split0_test_precision': array([0.        , 0.70815451, 0.6948154 , 0.69102613, 0.68968105,\n",
       "        0.68873924, 0.68719672, 0.68771012, 0.68771012, 0.68771012,\n",
       "        0.68771012, 0.68771012, 0.68771012, 0.68771012, 0.68771012,\n",
       "        0.68771012, 0.68771012, 0.68771012, 0.70815451, 0.69102613,\n",
       "        0.68873924, 0.68771012, 0.68745332, 0.68745332, 0.68745332,\n",
       "        0.68745332, 0.68745332, 0.68745332, 0.68771012, 0.68745332]),\n",
       " 'split1_test_precision': array([0.        , 0.71403354, 0.69858156, 0.69725126, 0.69527079,\n",
       "        0.69445507, 0.69342508, 0.69377625, 0.69389313, 0.69389313,\n",
       "        0.69389313, 0.69389313, 0.69389313, 0.69389313, 0.69389313,\n",
       "        0.69389313, 0.69389313, 0.69389313, 0.71403354, 0.69725126,\n",
       "        0.69445507, 0.69377625, 0.69389313, 0.69389313, 0.69389313,\n",
       "        0.69389313, 0.69389313, 0.69389313, 0.69389313, 0.69389313]),\n",
       " 'split2_test_precision': array([0.        , 0.70666097, 0.68946755, 0.69429658, 0.69051205,\n",
       "        0.69193246, 0.69109357, 0.69129782, 0.69132557, 0.69144144,\n",
       "        0.69144144, 0.69132557, 0.69144144, 0.69144144, 0.69144144,\n",
       "        0.69144144, 0.69132557, 0.69132557, 0.70666097, 0.69429658,\n",
       "        0.69193246, 0.69129782, 0.69144144, 0.69144144, 0.69144144,\n",
       "        0.69144144, 0.69144144, 0.69144144, 0.69144144, 0.69144144]),\n",
       " 'split3_test_precision': array([0.        , 0.71616872, 0.69865719, 0.6996904 , 0.69251337,\n",
       "        0.69362187, 0.69335863, 0.69306556, 0.69306556, 0.69318182,\n",
       "        0.69318182, 0.69318182, 0.69318182, 0.69318182, 0.69318182,\n",
       "        0.69318182, 0.69318182, 0.69318182, 0.71616872, 0.6996904 ,\n",
       "        0.69362187, 0.69306556, 0.69306556, 0.69318182, 0.69318182,\n",
       "        0.69318182, 0.69318182, 0.69318182, 0.69318182, 0.69306556]),\n",
       " 'split4_test_precision': array([0.        , 0.71428571, 0.70666147, 0.70576923, 0.70402736,\n",
       "        0.70336739, 0.70359168, 0.70306007, 0.70332577, 0.70332577,\n",
       "        0.70332577, 0.70332577, 0.70332577, 0.70332577, 0.70332577,\n",
       "        0.70332577, 0.70332577, 0.70332577, 0.71428571, 0.70576923,\n",
       "        0.70336739, 0.70306007, 0.70332577, 0.70332577, 0.70332577,\n",
       "        0.70332577, 0.70332577, 0.70332577, 0.70332577, 0.70332577]),\n",
       " 'split5_test_precision': array([0.        , 0.70826162, 0.69957082, 0.69812763, 0.69670579,\n",
       "        0.69548872, 0.69503012, 0.69485156, 0.69473684, 0.69485156,\n",
       "        0.69473684, 0.69485156, 0.69485156, 0.69485156, 0.69485156,\n",
       "        0.69485156, 0.69485156, 0.69485156, 0.70826162, 0.69812763,\n",
       "        0.6953742 , 0.69485156, 0.69485156, 0.69473684, 0.69473684,\n",
       "        0.69473684, 0.69473684, 0.69473684, 0.69485156, 0.69485156]),\n",
       " 'split6_test_precision': array([0.        , 0.73208042, 0.72589036, 0.72069632, 0.71722561,\n",
       "        0.71688805, 0.71650265, 0.71677395, 0.71650265, 0.71677395,\n",
       "        0.71677395, 0.71677395, 0.71677395, 0.71677395, 0.71677395,\n",
       "        0.71677395, 0.71677395, 0.71677395, 0.73208042, 0.72069632,\n",
       "        0.71688805, 0.71677395, 0.71677395, 0.71677395, 0.71677395,\n",
       "        0.71677395, 0.71677395, 0.71677395, 0.71677395, 0.71677395]),\n",
       " 'split7_test_precision': array([0.        , 0.69477234, 0.68276662, 0.68188679, 0.67991088,\n",
       "        0.67807964, 0.67717996, 0.67694022, 0.67694022, 0.67694022,\n",
       "        0.67694022, 0.67694022, 0.67694022, 0.67694022, 0.67694022,\n",
       "        0.67694022, 0.67694022, 0.67694022, 0.69477234, 0.68188679,\n",
       "        0.67807964, 0.67694022, 0.67694022, 0.67694022, 0.67694022,\n",
       "        0.67694022, 0.67694022, 0.67694022, 0.67694022, 0.67694022]),\n",
       " 'split8_test_precision': array([0.        , 0.71253287, 0.69952115, 0.69469198, 0.69265881,\n",
       "        0.69213227, 0.69233687, 0.69195751, 0.69195751, 0.69195751,\n",
       "        0.69195751, 0.69195751, 0.69195751, 0.69195751, 0.69195751,\n",
       "        0.69195751, 0.69195751, 0.69195751, 0.71253287, 0.69469198,\n",
       "        0.69213227, 0.69195751, 0.69195751, 0.69195751, 0.69195751,\n",
       "        0.69195751, 0.69195751, 0.69195751, 0.69195751, 0.69195751]),\n",
       " 'split9_test_precision': array([0.        , 0.71328976, 0.70193447, 0.7000777 , 0.69820679,\n",
       "        0.69748858, 0.69885932, 0.6983644 , 0.69847909, 0.6983644 ,\n",
       "        0.69821361, 0.69821361, 0.69821361, 0.69821361, 0.69821361,\n",
       "        0.69821361, 0.69821361, 0.69821361, 0.71328976, 0.7000777 ,\n",
       "        0.69748858, 0.6983644 , 0.69847909, 0.69821361, 0.69821361,\n",
       "        0.69821361, 0.69821361, 0.69821361, 0.69821361, 0.69847909]),\n",
       " 'mean_test_precision': array([0.        , 0.71202405, 0.69978666, 0.6983514 , 0.69567125,\n",
       "        0.69521933, 0.69485746, 0.69477975, 0.69479365, 0.69484399,\n",
       "        0.69481744, 0.69481733, 0.69482891, 0.69482891, 0.69482891,\n",
       "        0.69482891, 0.69481733, 0.69481733, 0.71202405, 0.6983514 ,\n",
       "        0.69520788, 0.69477975, 0.69481815, 0.69479176, 0.69479176,\n",
       "        0.69479176, 0.69479176, 0.69479176, 0.69482891, 0.69481815]),\n",
       " 'std_test_precision': array([0.        , 0.00886546, 0.01075641, 0.00956519, 0.00933815,\n",
       "        0.00948881, 0.00978184, 0.00978482, 0.00974912, 0.00979934,\n",
       "        0.00979406, 0.00979809, 0.00979402, 0.00979402, 0.00979402,\n",
       "        0.00979402, 0.00979809, 0.00979809, 0.00886546, 0.00956519,\n",
       "        0.00948855, 0.00978482, 0.00982453, 0.00981298, 0.00981298,\n",
       "        0.00981298, 0.00981298, 0.00981298, 0.00979402, 0.00982453]),\n",
       " 'rank_test_precision': array([30,  1,  3,  4,  6,  7,  9, 28, 22, 10, 18, 19, 11, 11, 11, 11, 19,\n",
       "        19,  1,  4,  8, 28, 16, 23, 23, 23, 23, 23, 11, 16]),\n",
       " 'split0_test_f1_micro': array([0.60056577, 0.73776521, 0.74087694, 0.74328147, 0.74356436,\n",
       "        0.74328147, 0.74243281, 0.7427157 , 0.7427157 , 0.7427157 ,\n",
       "        0.7427157 , 0.7427157 , 0.7427157 , 0.7427157 , 0.7427157 ,\n",
       "        0.7427157 , 0.7427157 , 0.7427157 , 0.73776521, 0.74328147,\n",
       "        0.74328147, 0.7427157 , 0.74257426, 0.74257426, 0.74257426,\n",
       "        0.74257426, 0.74257426, 0.74257426, 0.7427157 , 0.74257426]),\n",
       " 'split1_test_f1_micro': array([0.60056577, 0.73776521, 0.74314003, 0.7446959 , 0.74540311,\n",
       "        0.74441301, 0.7437058 , 0.74413013, 0.74427157, 0.74427157,\n",
       "        0.74427157, 0.74427157, 0.74427157, 0.74427157, 0.74427157,\n",
       "        0.74427157, 0.74427157, 0.74427157, 0.73776521, 0.7446959 ,\n",
       "        0.74441301, 0.74413013, 0.74427157, 0.74427157, 0.74427157,\n",
       "        0.74427157, 0.74427157, 0.74427157, 0.74427157, 0.74427157]),\n",
       " 'split2_test_f1_micro': array([0.60065073, 0.73758665, 0.73857688, 0.74522563, 0.74381101,\n",
       "        0.7453671 , 0.74451832, 0.74494271, 0.74480124, 0.74494271,\n",
       "        0.74494271, 0.74480124, 0.74494271, 0.74494271, 0.74494271,\n",
       "        0.74494271, 0.74480124, 0.74480124, 0.73758665, 0.74522563,\n",
       "        0.7453671 , 0.74494271, 0.74494271, 0.74494271, 0.74494271,\n",
       "        0.74494271, 0.74494271, 0.74494271, 0.74494271, 0.74494271]),\n",
       " 'split3_test_f1_micro': array([0.60050927, 0.73970859, 0.74282077, 0.7464988 , 0.74310369,\n",
       "        0.74480124, 0.74465978, 0.74465978, 0.74465978, 0.74480124,\n",
       "        0.74480124, 0.74480124, 0.74480124, 0.74480124, 0.74480124,\n",
       "        0.74480124, 0.74480124, 0.74480124, 0.73970859, 0.7464988 ,\n",
       "        0.74480124, 0.74465978, 0.74465978, 0.74480124, 0.74480124,\n",
       "        0.74480124, 0.74480124, 0.74480124, 0.74480124, 0.74465978]),\n",
       " 'split4_test_f1_micro': array([0.60050927, 0.74013297, 0.75060122, 0.75187438, 0.75244023,\n",
       "        0.75258169, 0.75286462, 0.75258169, 0.75272316, 0.75272316,\n",
       "        0.75272316, 0.75272316, 0.75272316, 0.75272316, 0.75272316,\n",
       "        0.75272316, 0.75272316, 0.75272316, 0.74013297, 0.75187438,\n",
       "        0.75258169, 0.75258169, 0.75272316, 0.75272316, 0.75272316,\n",
       "        0.75272316, 0.75272316, 0.75272316, 0.75272316, 0.75272316]),\n",
       " 'split5_test_f1_micro': array([0.60050927, 0.73744518, 0.74522563, 0.74720611, 0.74748904,\n",
       "        0.7476305 , 0.74706465, 0.74720611, 0.74706465, 0.74720611,\n",
       "        0.74706465, 0.74720611, 0.74720611, 0.74720611, 0.74720611,\n",
       "        0.74720611, 0.74720611, 0.74720611, 0.73744518, 0.74720611,\n",
       "        0.74748904, 0.74720611, 0.74720611, 0.74706465, 0.74706465,\n",
       "        0.74706465, 0.74706465, 0.74706465, 0.74720611, 0.74720611]),\n",
       " 'split6_test_f1_micro': array([0.60050927, 0.75074268, 0.76022068, 0.76191823, 0.76177677,\n",
       "        0.76220116, 0.76234262, 0.76248409, 0.76234262, 0.76248409,\n",
       "        0.76248409, 0.76248409, 0.76248409, 0.76248409, 0.76248409,\n",
       "        0.76248409, 0.76248409, 0.76248409, 0.75074268, 0.76191823,\n",
       "        0.76220116, 0.76248409, 0.76248409, 0.76248409, 0.76248409,\n",
       "        0.76248409, 0.76248409, 0.76248409, 0.76248409, 0.76248409]),\n",
       " 'split7_test_f1_micro': array([0.60050927, 0.73122082, 0.734333  , 0.73687933, 0.73758665,\n",
       "        0.73588909, 0.73560617, 0.73532324, 0.73532324, 0.73532324,\n",
       "        0.73532324, 0.73532324, 0.73532324, 0.73532324, 0.73532324,\n",
       "        0.73532324, 0.73532324, 0.73532324, 0.73122082, 0.73687933,\n",
       "        0.73588909, 0.73532324, 0.73532324, 0.73532324, 0.73532324,\n",
       "        0.73532324, 0.73532324, 0.73532324, 0.73532324, 0.73532324]),\n",
       " 'split8_test_f1_micro': array([0.60050927, 0.73772811, 0.74197199, 0.7426793 , 0.74381101,\n",
       "        0.74352808, 0.74395247, 0.74366954, 0.74366954, 0.74366954,\n",
       "        0.74366954, 0.74366954, 0.74366954, 0.74366954, 0.74366954,\n",
       "        0.74366954, 0.74366954, 0.74366954, 0.73772811, 0.7426793 ,\n",
       "        0.74352808, 0.74366954, 0.74366954, 0.74366954, 0.74366954,\n",
       "        0.74366954, 0.74366954, 0.74366954, 0.74366954, 0.74366954]),\n",
       " 'split9_test_f1_micro': array([0.60050927, 0.73900127, 0.74522563, 0.74621587, 0.74748904,\n",
       "        0.74734757, 0.74847928, 0.74805489, 0.74819635, 0.74805489,\n",
       "        0.74805489, 0.74805489, 0.74805489, 0.74805489, 0.74805489,\n",
       "        0.74805489, 0.74805489, 0.74805489, 0.73900127, 0.74621587,\n",
       "        0.74734757, 0.74805489, 0.74819635, 0.74805489, 0.74805489,\n",
       "        0.74805489, 0.74805489, 0.74805489, 0.74805489, 0.74819635]),\n",
       " 'mean_test_f1_micro': array([0.60053471, 0.73890967, 0.74429928, 0.7466475 , 0.74664749,\n",
       "        0.74670409, 0.74656265, 0.74657679, 0.74657679, 0.74661922,\n",
       "        0.74660508, 0.74660508, 0.74661922, 0.74661922, 0.74661922,\n",
       "        0.74661922, 0.74660508, 0.74660508, 0.73890967, 0.7466475 ,\n",
       "        0.74668995, 0.74657679, 0.74660508, 0.74659093, 0.74659093,\n",
       "        0.74659093, 0.74659093, 0.74659093, 0.74661922, 0.74660508]),\n",
       " 'std_test_f1_micro': array([4.46337344e-05, 4.57375566e-03, 6.69068531e-03, 6.23987474e-03,\n",
       "        6.21023593e-03, 6.52227447e-03, 6.72698078e-03, 6.73806711e-03,\n",
       "        6.71821590e-03, 6.74182432e-03, 6.74072635e-03, 6.74547471e-03,\n",
       "        6.74182432e-03, 6.74182432e-03, 6.74182432e-03, 6.74182432e-03,\n",
       "        6.74547471e-03, 6.74547471e-03, 4.57375566e-03, 6.23987474e-03,\n",
       "        6.52040298e-03, 6.73806711e-03, 6.75725365e-03, 6.74901601e-03,\n",
       "        6.74901601e-03, 6.74901601e-03, 6.74901601e-03, 6.74901601e-03,\n",
       "        6.74182432e-03, 6.75725365e-03]),\n",
       " 'rank_test_f1_micro': array([30, 28, 27,  3,  5,  1, 26, 23, 25,  6, 14, 15,  6,  6,  6,  6, 15,\n",
       "        15, 28,  3,  2, 23, 12, 18, 18, 18, 18, 18,  6, 12])}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_model.cv_results_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a4578782",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([30, 28, 27,  3,  5,  1, 26, 23, 25,  6, 17, 14,  6,  6,  6,  6, 14,\n",
       "       14, 28,  3,  2, 23, 12, 18, 18, 18, 18, 18,  6, 12])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_model.cv_results_['rank_test_accuracy']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "29844e0d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'classifier': LogisticRegression(max_iter=10000),\n",
       " 'classifier__C': 0.0001,\n",
       " 'classifier__penalty': 'l1',\n",
       " 'classifier__solver': 'saga'}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_model.cv_results_['params'][ np.argmin(best_model.cv_results_['mean_test_accuracy'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "da854c46",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>classifier</th>\n",
       "      <th>C</th>\n",
       "      <th>penalty</th>\n",
       "      <th>solver</th>\n",
       "      <th>score_acc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.600535</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.738910</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.744299</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.746648</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0100</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.746647</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0100</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.746704</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.1000</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.746563</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.1000</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.746577</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.746577</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.746619</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>10.0000</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.746605</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>10.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.746605</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>100.0000</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.746619</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>100.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.746619</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>1000.0000</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.746619</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>1000.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.746619</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>10000.0000</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.746605</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>10000.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.746605</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.738910</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.746648</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0100</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.746690</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.1000</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.746577</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.746605</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>10.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.746591</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>100.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.746591</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>1000.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.746591</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>10000.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.746591</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>NaN</td>\n",
       "      <td>none</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.746591</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>NaN</td>\n",
       "      <td>none</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.746619</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.746605</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                            classifier           C penalty solver  score_acc\n",
       "0   LogisticRegression(max_iter=10000)      0.0001      l1   saga   0.600535\n",
       "1   LogisticRegression(max_iter=10000)      0.0001      l2   saga   0.738910\n",
       "2   LogisticRegression(max_iter=10000)      0.0010      l1   saga   0.744299\n",
       "3   LogisticRegression(max_iter=10000)      0.0010      l2   saga   0.746648\n",
       "4   LogisticRegression(max_iter=10000)      0.0100      l1   saga   0.746647\n",
       "5   LogisticRegression(max_iter=10000)      0.0100      l2   saga   0.746704\n",
       "6   LogisticRegression(max_iter=10000)      0.1000      l1   saga   0.746563\n",
       "7   LogisticRegression(max_iter=10000)      0.1000      l2   saga   0.746577\n",
       "8   LogisticRegression(max_iter=10000)      1.0000      l1   saga   0.746577\n",
       "9   LogisticRegression(max_iter=10000)      1.0000      l2   saga   0.746619\n",
       "10  LogisticRegression(max_iter=10000)     10.0000      l1   saga   0.746605\n",
       "11  LogisticRegression(max_iter=10000)     10.0000      l2   saga   0.746605\n",
       "12  LogisticRegression(max_iter=10000)    100.0000      l1   saga   0.746619\n",
       "13  LogisticRegression(max_iter=10000)    100.0000      l2   saga   0.746619\n",
       "14  LogisticRegression(max_iter=10000)   1000.0000      l1   saga   0.746619\n",
       "15  LogisticRegression(max_iter=10000)   1000.0000      l2   saga   0.746619\n",
       "16  LogisticRegression(max_iter=10000)  10000.0000      l1   saga   0.746605\n",
       "17  LogisticRegression(max_iter=10000)  10000.0000      l2   saga   0.746605\n",
       "18  LogisticRegression(max_iter=10000)      0.0001      l2  lbfgs   0.738910\n",
       "19  LogisticRegression(max_iter=10000)      0.0010      l2  lbfgs   0.746648\n",
       "20  LogisticRegression(max_iter=10000)      0.0100      l2  lbfgs   0.746690\n",
       "21  LogisticRegression(max_iter=10000)      0.1000      l2  lbfgs   0.746577\n",
       "22  LogisticRegression(max_iter=10000)      1.0000      l2  lbfgs   0.746605\n",
       "23  LogisticRegression(max_iter=10000)     10.0000      l2  lbfgs   0.746591\n",
       "24  LogisticRegression(max_iter=10000)    100.0000      l2  lbfgs   0.746591\n",
       "25  LogisticRegression(max_iter=10000)   1000.0000      l2  lbfgs   0.746591\n",
       "26  LogisticRegression(max_iter=10000)  10000.0000      l2  lbfgs   0.746591\n",
       "27  LogisticRegression(max_iter=10000)         NaN    none  lbfgs   0.746591\n",
       "28  LogisticRegression(max_iter=10000)         NaN    none   saga   0.746619\n",
       "29  LogisticRegression(max_iter=10000)         NaN     NaN    NaN   0.746605"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results = pd.DataFrame( best_model.cv_results_['params'] )\n",
    "\n",
    "results['score_acc'] = best_model.cv_results_['mean_test_accuracy']\n",
    "cols = results.columns.to_series().str.split('__').apply(lambda x: x[-1])\n",
    "results.columns = cols\n",
    "\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cec21d10",
   "metadata": {},
   "outputs": [],
   "source": [
    "acc_list.append(results[0:1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8e72c958",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'classifier': LogisticRegression(max_iter=10000),\n",
       " 'classifier__C': 0.0001,\n",
       " 'classifier__penalty': 'l2',\n",
       " 'classifier__solver': 'saga'}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_model.cv_results_['params'][ np.argmin(best_model.cv_results_['rank_test_precision'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "426600e6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>classifier</th>\n",
       "      <th>C</th>\n",
       "      <th>penalty</th>\n",
       "      <th>solver</th>\n",
       "      <th>score_precision</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.712024</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.699787</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.698351</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0100</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.695671</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0100</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.695219</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.1000</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.694857</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.1000</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.694780</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.694794</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.694844</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>10.0000</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.694817</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>10.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.694817</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>100.0000</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.694829</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>100.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.694829</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>1000.0000</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.694829</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>1000.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.694829</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>10000.0000</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.694817</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>10000.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.694817</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.712024</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.698351</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0100</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.695208</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.1000</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.694780</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.694818</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>10.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.694792</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>100.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.694792</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>1000.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.694792</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>10000.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.694792</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>NaN</td>\n",
       "      <td>none</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.694792</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>NaN</td>\n",
       "      <td>none</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.694829</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.694818</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                            classifier           C penalty solver  \\\n",
       "0   LogisticRegression(max_iter=10000)      0.0001      l1   saga   \n",
       "1   LogisticRegression(max_iter=10000)      0.0001      l2   saga   \n",
       "2   LogisticRegression(max_iter=10000)      0.0010      l1   saga   \n",
       "3   LogisticRegression(max_iter=10000)      0.0010      l2   saga   \n",
       "4   LogisticRegression(max_iter=10000)      0.0100      l1   saga   \n",
       "5   LogisticRegression(max_iter=10000)      0.0100      l2   saga   \n",
       "6   LogisticRegression(max_iter=10000)      0.1000      l1   saga   \n",
       "7   LogisticRegression(max_iter=10000)      0.1000      l2   saga   \n",
       "8   LogisticRegression(max_iter=10000)      1.0000      l1   saga   \n",
       "9   LogisticRegression(max_iter=10000)      1.0000      l2   saga   \n",
       "10  LogisticRegression(max_iter=10000)     10.0000      l1   saga   \n",
       "11  LogisticRegression(max_iter=10000)     10.0000      l2   saga   \n",
       "12  LogisticRegression(max_iter=10000)    100.0000      l1   saga   \n",
       "13  LogisticRegression(max_iter=10000)    100.0000      l2   saga   \n",
       "14  LogisticRegression(max_iter=10000)   1000.0000      l1   saga   \n",
       "15  LogisticRegression(max_iter=10000)   1000.0000      l2   saga   \n",
       "16  LogisticRegression(max_iter=10000)  10000.0000      l1   saga   \n",
       "17  LogisticRegression(max_iter=10000)  10000.0000      l2   saga   \n",
       "18  LogisticRegression(max_iter=10000)      0.0001      l2  lbfgs   \n",
       "19  LogisticRegression(max_iter=10000)      0.0010      l2  lbfgs   \n",
       "20  LogisticRegression(max_iter=10000)      0.0100      l2  lbfgs   \n",
       "21  LogisticRegression(max_iter=10000)      0.1000      l2  lbfgs   \n",
       "22  LogisticRegression(max_iter=10000)      1.0000      l2  lbfgs   \n",
       "23  LogisticRegression(max_iter=10000)     10.0000      l2  lbfgs   \n",
       "24  LogisticRegression(max_iter=10000)    100.0000      l2  lbfgs   \n",
       "25  LogisticRegression(max_iter=10000)   1000.0000      l2  lbfgs   \n",
       "26  LogisticRegression(max_iter=10000)  10000.0000      l2  lbfgs   \n",
       "27  LogisticRegression(max_iter=10000)         NaN    none  lbfgs   \n",
       "28  LogisticRegression(max_iter=10000)         NaN    none   saga   \n",
       "29  LogisticRegression(max_iter=10000)         NaN     NaN    NaN   \n",
       "\n",
       "    score_precision  \n",
       "0          0.000000  \n",
       "1          0.712024  \n",
       "2          0.699787  \n",
       "3          0.698351  \n",
       "4          0.695671  \n",
       "5          0.695219  \n",
       "6          0.694857  \n",
       "7          0.694780  \n",
       "8          0.694794  \n",
       "9          0.694844  \n",
       "10         0.694817  \n",
       "11         0.694817  \n",
       "12         0.694829  \n",
       "13         0.694829  \n",
       "14         0.694829  \n",
       "15         0.694829  \n",
       "16         0.694817  \n",
       "17         0.694817  \n",
       "18         0.712024  \n",
       "19         0.698351  \n",
       "20         0.695208  \n",
       "21         0.694780  \n",
       "22         0.694818  \n",
       "23         0.694792  \n",
       "24         0.694792  \n",
       "25         0.694792  \n",
       "26         0.694792  \n",
       "27         0.694792  \n",
       "28         0.694829  \n",
       "29         0.694818  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results = pd.DataFrame( best_model.cv_results_['params'] )\n",
    "\n",
    "results['score_precision'] = best_model.cv_results_['mean_test_precision']\n",
    "cols = results.columns.to_series().str.split('__').apply(lambda x: x[-1])\n",
    "results.columns = cols\n",
    "\n",
    "\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "46a6f899",
   "metadata": {},
   "outputs": [],
   "source": [
    "roc_auc_ovr_List.append(results[1:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6ceb54de",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'classifier': LogisticRegression(max_iter=10000),\n",
       " 'classifier__C': 0.01,\n",
       " 'classifier__penalty': 'l2',\n",
       " 'classifier__solver': 'saga'}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_model.cv_results_['params'][ np.argmin(best_model.cv_results_['rank_test_f1_micro'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "886a47b2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>classifier</th>\n",
       "      <th>C</th>\n",
       "      <th>penalty</th>\n",
       "      <th>solver</th>\n",
       "      <th>score_f1_micro</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.600535</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.738910</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.744299</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.746648</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0100</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.746647</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0100</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.746704</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.1000</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.746563</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.1000</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.746577</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.746577</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.746619</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>10.0000</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.746605</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>10.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.746605</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>100.0000</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.746619</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>100.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.746619</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>1000.0000</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.746619</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>1000.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.746619</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>10000.0000</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.746605</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>10000.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.746605</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.738910</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.746648</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0100</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.746690</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.1000</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.746577</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.746605</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>10.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.746591</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>100.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.746591</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>1000.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.746591</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>10000.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.746591</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>NaN</td>\n",
       "      <td>none</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.746591</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>NaN</td>\n",
       "      <td>none</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.746619</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.746605</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                            classifier           C penalty solver  \\\n",
       "0   LogisticRegression(max_iter=10000)      0.0001      l1   saga   \n",
       "1   LogisticRegression(max_iter=10000)      0.0001      l2   saga   \n",
       "2   LogisticRegression(max_iter=10000)      0.0010      l1   saga   \n",
       "3   LogisticRegression(max_iter=10000)      0.0010      l2   saga   \n",
       "4   LogisticRegression(max_iter=10000)      0.0100      l1   saga   \n",
       "5   LogisticRegression(max_iter=10000)      0.0100      l2   saga   \n",
       "6   LogisticRegression(max_iter=10000)      0.1000      l1   saga   \n",
       "7   LogisticRegression(max_iter=10000)      0.1000      l2   saga   \n",
       "8   LogisticRegression(max_iter=10000)      1.0000      l1   saga   \n",
       "9   LogisticRegression(max_iter=10000)      1.0000      l2   saga   \n",
       "10  LogisticRegression(max_iter=10000)     10.0000      l1   saga   \n",
       "11  LogisticRegression(max_iter=10000)     10.0000      l2   saga   \n",
       "12  LogisticRegression(max_iter=10000)    100.0000      l1   saga   \n",
       "13  LogisticRegression(max_iter=10000)    100.0000      l2   saga   \n",
       "14  LogisticRegression(max_iter=10000)   1000.0000      l1   saga   \n",
       "15  LogisticRegression(max_iter=10000)   1000.0000      l2   saga   \n",
       "16  LogisticRegression(max_iter=10000)  10000.0000      l1   saga   \n",
       "17  LogisticRegression(max_iter=10000)  10000.0000      l2   saga   \n",
       "18  LogisticRegression(max_iter=10000)      0.0001      l2  lbfgs   \n",
       "19  LogisticRegression(max_iter=10000)      0.0010      l2  lbfgs   \n",
       "20  LogisticRegression(max_iter=10000)      0.0100      l2  lbfgs   \n",
       "21  LogisticRegression(max_iter=10000)      0.1000      l2  lbfgs   \n",
       "22  LogisticRegression(max_iter=10000)      1.0000      l2  lbfgs   \n",
       "23  LogisticRegression(max_iter=10000)     10.0000      l2  lbfgs   \n",
       "24  LogisticRegression(max_iter=10000)    100.0000      l2  lbfgs   \n",
       "25  LogisticRegression(max_iter=10000)   1000.0000      l2  lbfgs   \n",
       "26  LogisticRegression(max_iter=10000)  10000.0000      l2  lbfgs   \n",
       "27  LogisticRegression(max_iter=10000)         NaN    none  lbfgs   \n",
       "28  LogisticRegression(max_iter=10000)         NaN    none   saga   \n",
       "29  LogisticRegression(max_iter=10000)         NaN     NaN    NaN   \n",
       "\n",
       "    score_f1_micro  \n",
       "0         0.600535  \n",
       "1         0.738910  \n",
       "2         0.744299  \n",
       "3         0.746648  \n",
       "4         0.746647  \n",
       "5         0.746704  \n",
       "6         0.746563  \n",
       "7         0.746577  \n",
       "8         0.746577  \n",
       "9         0.746619  \n",
       "10        0.746605  \n",
       "11        0.746605  \n",
       "12        0.746619  \n",
       "13        0.746619  \n",
       "14        0.746619  \n",
       "15        0.746619  \n",
       "16        0.746605  \n",
       "17        0.746605  \n",
       "18        0.738910  \n",
       "19        0.746648  \n",
       "20        0.746690  \n",
       "21        0.746577  \n",
       "22        0.746605  \n",
       "23        0.746591  \n",
       "24        0.746591  \n",
       "25        0.746591  \n",
       "26        0.746591  \n",
       "27        0.746591  \n",
       "28        0.746619  \n",
       "29        0.746605  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results = pd.DataFrame( best_model.cv_results_['params'] )\n",
    "\n",
    "\n",
    "results['score_f1_micro'] = best_model.cv_results_['mean_test_f1_micro']\n",
    "cols = results.columns.to_series().str.split('__').apply(lambda x: x[-1])\n",
    "results.columns = cols\n",
    "\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "943aea19",
   "metadata": {},
   "outputs": [],
   "source": [
    "f1_micro_List.append(results[5:6])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02458d54",
   "metadata": {},
   "source": [
    "## Trial 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7bf26ccd",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, Y_train, Y_test = train_test_split(X, y, test_size=0.2, shuffle=True, random_state = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4628ee31",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Guest-All\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\Guest-All\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\Guest-All\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\Guest-All\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\Guest-All\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\Guest-All\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\Guest-All\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\Guest-All\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\Guest-All\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\Guest-All\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\Guest-All\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1183: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Guest-All\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1183: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Guest-All\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1183: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Guest-All\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1183: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Guest-All\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1183: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Guest-All\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1183: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Guest-All\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1183: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Guest-All\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1183: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Guest-All\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1183: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Guest-All\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1183: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Guest-All\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1183: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Guest-All\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1183: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Guest-All\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1183: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Guest-All\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1183: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Guest-All\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1183: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Guest-All\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1183: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Guest-All\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1183: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Guest-All\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1183: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Guest-All\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1183: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Guest-All\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1183: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Create a pipeline - RF is a stand in, we will populate the classifier part below\n",
    "pipe = Pipeline([('std', StandardScaler()),\n",
    "                 ('classifier', LogisticRegression())])\n",
    "\n",
    "# Create search space of candidate learning algorithms and their hyperparameters\n",
    "# note lbfgs can't do l1, and if you pass penalty='none' it expects no C value\n",
    "search_space = [{'classifier': [LogisticRegression(max_iter=10000)],\n",
    "                 'classifier__solver': ['saga'],\n",
    "                 'classifier__penalty': ['l1', 'l2'],\n",
    "                 'classifier__C': np.logspace(-4, 4, 9)},\n",
    "                {'classifier': [LogisticRegression(max_iter=10000)],\n",
    "                 'classifier__solver': ['lbfgs'],\n",
    "                 'classifier__penalty': ['l2'],\n",
    "                 'classifier__C': np.logspace(-4, 4, 9)},\n",
    "                {'classifier': [LogisticRegression(max_iter=10000)],\n",
    "                 'classifier__solver': ['lbfgs','saga'],\n",
    "                 'classifier__penalty': ['none']},\n",
    "                {'classifier': [LogisticRegression(max_iter=10000)]}\n",
    "                ]\n",
    "\n",
    "# Create grid search\n",
    "clf = GridSearchCV(pipe, search_space, cv=StratifiedKFold(n_splits=10),\n",
    "                   scoring=['accuracy', 'precision', 'f1_micro'], refit=False,\n",
    "                   verbose=0)\n",
    "\n",
    "# Fit grid search\n",
    "best_model = clf.fit(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2bf2b4d0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'mean_fit_time': array([0.98096454, 0.74885468, 1.06254656, 0.73179348, 0.87304895,\n",
       "        0.79987073, 0.89625387, 0.68071721, 0.58893275, 0.62166436,\n",
       "        0.55435581, 0.46965127, 0.76578488, 0.46524472, 0.56083415,\n",
       "        0.4551805 , 0.53803239, 0.56402125, 0.08131628, 0.09206002,\n",
       "        0.09261935, 0.09434807, 0.09456151, 0.08888946, 0.08935623,\n",
       "        0.09464142, 0.09537334, 0.09137297, 0.46612024, 0.09281585]),\n",
       " 'std_fit_time': array([0.2530577 , 0.21492931, 0.21009976, 0.26251785, 0.2686503 ,\n",
       "        0.25128728, 0.22592188, 0.25689996, 0.1213885 , 0.14629934,\n",
       "        0.037988  , 0.02467861, 0.16026908, 0.02075705, 0.02352041,\n",
       "        0.02240403, 0.0248996 , 0.14583097, 0.00087406, 0.00404896,\n",
       "        0.00937839, 0.00234774, 0.00209957, 0.0021123 , 0.0087712 ,\n",
       "        0.00272986, 0.00196472, 0.00229208, 0.02875863, 0.00227822]),\n",
       " 'mean_score_time': array([0.02022383, 0.01636581, 0.01615379, 0.01436884, 0.0155673 ,\n",
       "        0.01577425, 0.01270113, 0.01619236, 0.00986848, 0.01091905,\n",
       "        0.00880094, 0.00926166, 0.01156137, 0.00897586, 0.00921504,\n",
       "        0.00875046, 0.00895402, 0.01072698, 0.00902021, 0.00872233,\n",
       "        0.00927575, 0.00891387, 0.00878775, 0.00878868, 0.00939989,\n",
       "        0.00898564, 0.00908821, 0.00890114, 0.00901854, 0.00857489]),\n",
       " 'std_score_time': array([0.01100051, 0.00832137, 0.00796381, 0.01051534, 0.00842067,\n",
       "        0.00827853, 0.00268441, 0.01043845, 0.00231024, 0.00185073,\n",
       "        0.00040146, 0.00123649, 0.00207131, 0.00051257, 0.00047474,\n",
       "        0.00040674, 0.00058205, 0.00253549, 0.0005749 , 0.0005862 ,\n",
       "        0.00100096, 0.00028229, 0.00056604, 0.00039737, 0.00066063,\n",
       "        0.00044906, 0.00054167, 0.00030035, 0.00036604, 0.00047637]),\n",
       " 'param_classifier': masked_array(data=[LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000)],\n",
       "              mask=[False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'param_classifier__C': masked_array(data=[0.0001, 0.0001, 0.001, 0.001, 0.01, 0.01, 0.1, 0.1,\n",
       "                    1.0, 1.0, 10.0, 10.0, 100.0, 100.0, 1000.0, 1000.0,\n",
       "                    10000.0, 10000.0, 0.0001, 0.001, 0.01, 0.1, 1.0, 10.0,\n",
       "                    100.0, 1000.0, 10000.0, --, --, --],\n",
       "              mask=[False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False,  True,  True,  True],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'param_classifier__penalty': masked_array(data=['l1', 'l2', 'l1', 'l2', 'l1', 'l2', 'l1', 'l2', 'l1',\n",
       "                    'l2', 'l1', 'l2', 'l1', 'l2', 'l1', 'l2', 'l1', 'l2',\n",
       "                    'l2', 'l2', 'l2', 'l2', 'l2', 'l2', 'l2', 'l2', 'l2',\n",
       "                    'none', 'none', --],\n",
       "              mask=[False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False,  True],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'param_classifier__solver': masked_array(data=['saga', 'saga', 'saga', 'saga', 'saga', 'saga', 'saga',\n",
       "                    'saga', 'saga', 'saga', 'saga', 'saga', 'saga', 'saga',\n",
       "                    'saga', 'saga', 'saga', 'saga', 'lbfgs', 'lbfgs',\n",
       "                    'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs',\n",
       "                    'lbfgs', 'lbfgs', 'saga', --],\n",
       "              mask=[False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False,  True],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'params': [{'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 0.0001,\n",
       "   'classifier__penalty': 'l1',\n",
       "   'classifier__solver': 'saga'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 0.0001,\n",
       "   'classifier__penalty': 'l2',\n",
       "   'classifier__solver': 'saga'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 0.001,\n",
       "   'classifier__penalty': 'l1',\n",
       "   'classifier__solver': 'saga'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 0.001,\n",
       "   'classifier__penalty': 'l2',\n",
       "   'classifier__solver': 'saga'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 0.01,\n",
       "   'classifier__penalty': 'l1',\n",
       "   'classifier__solver': 'saga'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 0.01,\n",
       "   'classifier__penalty': 'l2',\n",
       "   'classifier__solver': 'saga'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 0.1,\n",
       "   'classifier__penalty': 'l1',\n",
       "   'classifier__solver': 'saga'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 0.1,\n",
       "   'classifier__penalty': 'l2',\n",
       "   'classifier__solver': 'saga'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 1.0,\n",
       "   'classifier__penalty': 'l1',\n",
       "   'classifier__solver': 'saga'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 1.0,\n",
       "   'classifier__penalty': 'l2',\n",
       "   'classifier__solver': 'saga'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 10.0,\n",
       "   'classifier__penalty': 'l1',\n",
       "   'classifier__solver': 'saga'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 10.0,\n",
       "   'classifier__penalty': 'l2',\n",
       "   'classifier__solver': 'saga'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 100.0,\n",
       "   'classifier__penalty': 'l1',\n",
       "   'classifier__solver': 'saga'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 100.0,\n",
       "   'classifier__penalty': 'l2',\n",
       "   'classifier__solver': 'saga'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 1000.0,\n",
       "   'classifier__penalty': 'l1',\n",
       "   'classifier__solver': 'saga'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 1000.0,\n",
       "   'classifier__penalty': 'l2',\n",
       "   'classifier__solver': 'saga'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 10000.0,\n",
       "   'classifier__penalty': 'l1',\n",
       "   'classifier__solver': 'saga'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 10000.0,\n",
       "   'classifier__penalty': 'l2',\n",
       "   'classifier__solver': 'saga'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 0.0001,\n",
       "   'classifier__penalty': 'l2',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 0.001,\n",
       "   'classifier__penalty': 'l2',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 0.01,\n",
       "   'classifier__penalty': 'l2',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 0.1,\n",
       "   'classifier__penalty': 'l2',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 1.0,\n",
       "   'classifier__penalty': 'l2',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 10.0,\n",
       "   'classifier__penalty': 'l2',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 100.0,\n",
       "   'classifier__penalty': 'l2',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 1000.0,\n",
       "   'classifier__penalty': 'l2',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 10000.0,\n",
       "   'classifier__penalty': 'l2',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__penalty': 'none',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__penalty': 'none',\n",
       "   'classifier__solver': 'saga'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000)}],\n",
       " 'split0_test_accuracy': array([0.59915134, 0.74045262, 0.74964639, 0.75007072, 0.74865629,\n",
       "        0.75035361, 0.75035361, 0.75007072, 0.75021216, 0.75007072,\n",
       "        0.74992928, 0.74992928, 0.74992928, 0.74992928, 0.74992928,\n",
       "        0.75007072, 0.74992928, 0.74992928, 0.74045262, 0.75007072,\n",
       "        0.75035361, 0.75007072, 0.75007072, 0.74992928, 0.74992928,\n",
       "        0.74992928, 0.74992928, 0.74992928, 0.74992928, 0.75007072]),\n",
       " 'split1_test_accuracy': array([0.59915134, 0.73592645, 0.7446959 , 0.745686  , 0.74639321,\n",
       "        0.74724187, 0.74695898, 0.7466761 , 0.74639321, 0.74639321,\n",
       "        0.74653465, 0.74653465, 0.74653465, 0.74653465, 0.74653465,\n",
       "        0.74653465, 0.74653465, 0.74639321, 0.73592645, 0.745686  ,\n",
       "        0.74724187, 0.74695898, 0.74653465, 0.74653465, 0.74653465,\n",
       "        0.74653465, 0.74653465, 0.74653465, 0.74653465, 0.74653465]),\n",
       " 'split2_test_accuracy': array([0.5992361 , 0.7404159 , 0.74253784, 0.74720611, 0.74862074,\n",
       "        0.74904513, 0.74961098, 0.74975244, 0.7498939 , 0.7498939 ,\n",
       "        0.7498939 , 0.7498939 , 0.7498939 , 0.7498939 , 0.7498939 ,\n",
       "        0.7498939 , 0.7498939 , 0.7498939 , 0.7404159 , 0.74720611,\n",
       "        0.74904513, 0.74975244, 0.7498939 , 0.7498939 , 0.7498939 ,\n",
       "        0.7498939 , 0.7498939 , 0.7498939 , 0.7498939 , 0.7498939 ]),\n",
       " 'split3_test_accuracy': array([0.59909464, 0.73560617, 0.74423539, 0.74409393, 0.74522563,\n",
       "        0.74494271, 0.74494271, 0.74522563, 0.7453671 , 0.7453671 ,\n",
       "        0.7453671 , 0.7453671 , 0.7453671 , 0.7453671 , 0.7453671 ,\n",
       "        0.7453671 , 0.7453671 , 0.7453671 , 0.73560617, 0.74409393,\n",
       "        0.74494271, 0.7453671 , 0.7453671 , 0.7453671 , 0.7453671 ,\n",
       "        0.7453671 , 0.7453671 , 0.7453671 , 0.7453671 , 0.7453671 ]),\n",
       " 'split4_test_accuracy': array([0.59909464, 0.73871835, 0.73900127, 0.74451832, 0.74352808,\n",
       "        0.74282077, 0.74338662, 0.74338662, 0.74352808, 0.74338662,\n",
       "        0.74352808, 0.74352808, 0.74352808, 0.74352808, 0.74352808,\n",
       "        0.74352808, 0.74352808, 0.74352808, 0.73871835, 0.74451832,\n",
       "        0.74282077, 0.74324515, 0.74338662, 0.74338662, 0.74352808,\n",
       "        0.74352808, 0.74352808, 0.74352808, 0.74352808, 0.74338662]),\n",
       " 'split5_test_accuracy': array([0.59909464, 0.74607441, 0.74946951, 0.75328901, 0.75215731,\n",
       "        0.75314755, 0.75286462, 0.75314755, 0.75357193, 0.75357193,\n",
       "        0.75357193, 0.75357193, 0.75357193, 0.75357193, 0.75357193,\n",
       "        0.75357193, 0.75357193, 0.75357193, 0.74593295, 0.75328901,\n",
       "        0.75314755, 0.75314755, 0.75357193, 0.75357193, 0.75357193,\n",
       "        0.75357193, 0.75357193, 0.75357193, 0.75357193, 0.75357193]),\n",
       " 'split6_test_accuracy': array([0.59909464, 0.74480124, 0.74310369, 0.74862074, 0.74706465,\n",
       "        0.74748904, 0.74833781, 0.74819635, 0.74819635, 0.74819635,\n",
       "        0.74819635, 0.74819635, 0.74819635, 0.74819635, 0.74819635,\n",
       "        0.74819635, 0.74819635, 0.74819635, 0.74480124, 0.74862074,\n",
       "        0.74748904, 0.74819635, 0.74819635, 0.74819635, 0.74819635,\n",
       "        0.74819635, 0.74819635, 0.74819635, 0.74819635, 0.74819635]),\n",
       " 'split7_test_accuracy': array([0.59909464, 0.73263545, 0.73970859, 0.74112321, 0.74055736,\n",
       "        0.7415476 , 0.74140614, 0.74183053, 0.74197199, 0.74197199,\n",
       "        0.74211345, 0.74211345, 0.74211345, 0.74211345, 0.74211345,\n",
       "        0.74211345, 0.74211345, 0.74211345, 0.73263545, 0.74112321,\n",
       "        0.7415476 , 0.74183053, 0.74197199, 0.74211345, 0.74211345,\n",
       "        0.74211345, 0.74211345, 0.74211345, 0.74211345, 0.74197199]),\n",
       " 'split8_test_accuracy': array([0.59909464, 0.73687933, 0.74324515, 0.74465978, 0.74522563,\n",
       "        0.74550856, 0.7453671 , 0.74550856, 0.74565002, 0.74565002,\n",
       "        0.74565002, 0.74565002, 0.74565002, 0.74565002, 0.74565002,\n",
       "        0.74565002, 0.74565002, 0.74565002, 0.73687933, 0.74465978,\n",
       "        0.74550856, 0.74550856, 0.74565002, 0.74565002, 0.74565002,\n",
       "        0.74565002, 0.74565002, 0.74565002, 0.74565002, 0.74565002]),\n",
       " 'split9_test_accuracy': array([0.59909464, 0.73857688, 0.74084029, 0.74338662, 0.74352808,\n",
       "        0.74508417, 0.74607441, 0.74550856, 0.74550856, 0.74550856,\n",
       "        0.74550856, 0.74550856, 0.74550856, 0.74550856, 0.74550856,\n",
       "        0.74550856, 0.74550856, 0.74550856, 0.73857688, 0.74338662,\n",
       "        0.74508417, 0.74550856, 0.74550856, 0.74550856, 0.74550856,\n",
       "        0.74550856, 0.74550856, 0.74550856, 0.74550856, 0.74550856]),\n",
       " 'mean_test_accuracy': array([0.59912013, 0.73900868, 0.7436484 , 0.74626544, 0.7460957 ,\n",
       "        0.7467181 , 0.7469303 , 0.7469303 , 0.74702933, 0.74700104,\n",
       "        0.74702933, 0.74702933, 0.74702933, 0.74702933, 0.74702933,\n",
       "        0.74704348, 0.74702933, 0.74701519, 0.73899453, 0.74626544,\n",
       "        0.7467181 , 0.74695859, 0.74701518, 0.74701519, 0.74702933,\n",
       "        0.74702933, 0.74702933, 0.74702933, 0.74702933, 0.74701518]),\n",
       " 'std_test_accuracy': array([4.46616433e-05, 3.92484025e-03, 3.43510078e-03, 3.40080651e-03,\n",
       "        3.09960415e-03, 3.31732419e-03, 3.26042707e-03, 3.21966330e-03,\n",
       "        3.28210533e-03, 3.28396687e-03, 3.23157302e-03, 3.23157302e-03,\n",
       "        3.23157302e-03, 3.23157302e-03, 3.23157302e-03, 3.24451845e-03,\n",
       "        3.23157302e-03, 3.23401585e-03, 3.89952107e-03, 3.40080651e-03,\n",
       "        3.31732419e-03, 3.22724098e-03, 3.28162221e-03, 3.24714099e-03,\n",
       "        3.23157302e-03, 3.23157302e-03, 3.23157302e-03, 3.23157302e-03,\n",
       "        3.23157302e-03, 3.28162221e-03]),\n",
       " 'rank_test_accuracy': array([30, 28, 27, 24, 26, 22, 21, 20, 13, 18,  2,  2,  2,  2,  2,  1,  2,\n",
       "        14, 29, 24, 22, 19, 16, 15,  2,  2,  2,  2,  2, 16]),\n",
       " 'split0_test_precision': array([0.        , 0.71120507, 0.70508867, 0.69854857, 0.69451601,\n",
       "        0.69614679, 0.69600293, 0.69563623, 0.69589142, 0.69563623,\n",
       "        0.69552458, 0.69552458, 0.69552458, 0.69552458, 0.69552458,\n",
       "        0.69563623, 0.69552458, 0.69552458, 0.71120507, 0.69854857,\n",
       "        0.69614679, 0.69563623, 0.69563623, 0.69552458, 0.69552458,\n",
       "        0.69552458, 0.69552458, 0.69552458, 0.69552458, 0.69563623]),\n",
       " 'split1_test_precision': array([0.        , 0.70759983, 0.69996113, 0.69801223, 0.69530957,\n",
       "        0.69628796, 0.6953271 , 0.69495327, 0.69443407, 0.69443407,\n",
       "        0.69469357, 0.69469357, 0.69469357, 0.69469357, 0.69469357,\n",
       "        0.69469357, 0.69469357, 0.69443407, 0.70759983, 0.69801223,\n",
       "        0.69628796, 0.69547325, 0.69469357, 0.69469357, 0.69469357,\n",
       "        0.69469357, 0.69469357, 0.69469357, 0.69469357, 0.69469357]),\n",
       " 'split2_test_precision': array([0.        , 0.71695652, 0.69996052, 0.70239938, 0.70076046,\n",
       "        0.70171429, 0.70170778, 0.70182094, 0.70193402, 0.70193402,\n",
       "        0.70193402, 0.70193402, 0.70193402, 0.70193402, 0.70193402,\n",
       "        0.70193402, 0.70193402, 0.70193402, 0.71695652, 0.70239938,\n",
       "        0.70171429, 0.70182094, 0.70193402, 0.70193402, 0.70193402,\n",
       "        0.70193402, 0.70193402, 0.70193402, 0.70193402, 0.70193402]),\n",
       " 'split3_test_precision': array([0.        , 0.70593257, 0.69883721, 0.69508946, 0.69366329,\n",
       "        0.69256631, 0.69242255, 0.69293986, 0.69291045, 0.69291045,\n",
       "        0.69291045, 0.69291045, 0.69291045, 0.69291045, 0.69291045,\n",
       "        0.69291045, 0.69291045, 0.69291045, 0.70593257, 0.69508946,\n",
       "        0.69256631, 0.69305452, 0.69291045, 0.69291045, 0.69291045,\n",
       "        0.69291045, 0.69291045, 0.69291045, 0.69291045, 0.69291045]),\n",
       " 'split4_test_precision': array([0.        , 0.71540812, 0.69599683, 0.69738863, 0.69447619,\n",
       "        0.69257013, 0.69318182, 0.69288956, 0.69315172, 0.69288956,\n",
       "        0.69315172, 0.69315172, 0.69315172, 0.69315172, 0.69315172,\n",
       "        0.69315172, 0.69315172, 0.69315172, 0.71540812, 0.69738863,\n",
       "        0.69257013, 0.69277336, 0.69288956, 0.69288956, 0.69315172,\n",
       "        0.69315172, 0.69315172, 0.69315172, 0.69315172, 0.69288956]),\n",
       " 'split5_test_precision': array([0.        , 0.72229354, 0.70933438, 0.70865237, 0.70415094,\n",
       "        0.70554926, 0.70501697, 0.70523935, 0.70557229, 0.70557229,\n",
       "        0.70557229, 0.70557229, 0.70557229, 0.70557229, 0.70557229,\n",
       "        0.70557229, 0.70557229, 0.70557229, 0.7219846 , 0.70865237,\n",
       "        0.70554926, 0.70523935, 0.70557229, 0.70557229, 0.70557229,\n",
       "        0.70557229, 0.70557229, 0.70557229, 0.70557229, 0.70557229]),\n",
       " 'split6_test_precision': array([0.        , 0.72275087, 0.70055162, 0.70319108, 0.69916222,\n",
       "        0.69935386, 0.70003792, 0.69977255, 0.69977255, 0.69977255,\n",
       "        0.69977255, 0.69977255, 0.69977255, 0.69977255, 0.69977255,\n",
       "        0.69977255, 0.69977255, 0.69977255, 0.72275087, 0.70319108,\n",
       "        0.69935386, 0.69977255, 0.69977255, 0.69977255, 0.69977255,\n",
       "        0.69977255, 0.69977255, 0.69977255, 0.69977255, 0.69977255]),\n",
       " 'split7_test_precision': array([0.        , 0.69782062, 0.69086022, 0.68843844, 0.68477458,\n",
       "        0.68558791, 0.6853353 , 0.68568274, 0.68579838, 0.68579838,\n",
       "        0.68591394, 0.68591394, 0.68591394, 0.68591394, 0.68591394,\n",
       "        0.68591394, 0.68591394, 0.68591394, 0.69782062, 0.68843844,\n",
       "        0.68558791, 0.68568274, 0.68579838, 0.68591394, 0.68591394,\n",
       "        0.68591394, 0.68591394, 0.68591394, 0.68591394, 0.68579838]),\n",
       " 'split8_test_precision': array([0.        , 0.70937231, 0.69558541, 0.69363944, 0.69136717,\n",
       "        0.69202226, 0.69133975, 0.69159571, 0.69170984, 0.69170984,\n",
       "        0.69170984, 0.69170984, 0.69170984, 0.69170984, 0.69170984,\n",
       "        0.69170984, 0.69170984, 0.69170984, 0.70937231, 0.69363944,\n",
       "        0.69202226, 0.69159571, 0.69170984, 0.69170984, 0.69170984,\n",
       "        0.69170984, 0.69170984, 0.69170984, 0.69170984, 0.69170984]),\n",
       " 'split9_test_precision': array([0.        , 0.71397569, 0.69479005, 0.693769  , 0.69155722,\n",
       "        0.69311377, 0.69420561, 0.69345794, 0.69345794, 0.69345794,\n",
       "        0.69345794, 0.69345794, 0.69345794, 0.69345794, 0.69345794,\n",
       "        0.69345794, 0.69345794, 0.69345794, 0.71397569, 0.693769  ,\n",
       "        0.69311377, 0.69345794, 0.69345794, 0.69345794, 0.69345794,\n",
       "        0.69345794, 0.69345794, 0.69345794, 0.69345794, 0.69345794]),\n",
       " 'mean_test_precision': array([0.        , 0.71233151, 0.6990966 , 0.69791286, 0.69497377,\n",
       "        0.69549125, 0.69545777, 0.69539882, 0.69546327, 0.69541153,\n",
       "        0.69546409, 0.69546409, 0.69546409, 0.69546409, 0.69546409,\n",
       "        0.69547526, 0.69546409, 0.69543814, 0.71230062, 0.69791286,\n",
       "        0.69549125, 0.69545066, 0.69543748, 0.69543787, 0.69546409,\n",
       "        0.69546409, 0.69546409, 0.69546409, 0.69546409, 0.69543748]),\n",
       " 'std_test_precision': array([0.        , 0.0072477 , 0.00502709, 0.00545146, 0.00515985,\n",
       "        0.00537331, 0.00535832, 0.00531451, 0.00535962, 0.00536988,\n",
       "        0.00533281, 0.00533281, 0.00533281, 0.00533281, 0.00533281,\n",
       "        0.00533304, 0.00533281, 0.00533712, 0.00720571, 0.00545146,\n",
       "        0.00537331, 0.00531287, 0.00536571, 0.00534474, 0.00533281,\n",
       "        0.00533281, 0.00533281, 0.00533281, 0.00533281, 0.00536571]),\n",
       " 'rank_test_precision': array([30,  1,  3,  4, 29,  6, 21, 28, 20, 27,  9,  9,  9,  9,  9,  8,  9,\n",
       "        23,  2,  4,  6, 22, 25, 24,  9,  9,  9,  9,  9, 25]),\n",
       " 'split0_test_f1_micro': array([0.59915134, 0.74045262, 0.74964639, 0.75007072, 0.74865629,\n",
       "        0.75035361, 0.75035361, 0.75007072, 0.75021216, 0.75007072,\n",
       "        0.74992928, 0.74992928, 0.74992928, 0.74992928, 0.74992928,\n",
       "        0.75007072, 0.74992928, 0.74992928, 0.74045262, 0.75007072,\n",
       "        0.75035361, 0.75007072, 0.75007072, 0.74992928, 0.74992928,\n",
       "        0.74992928, 0.74992928, 0.74992928, 0.74992928, 0.75007072]),\n",
       " 'split1_test_f1_micro': array([0.59915134, 0.73592645, 0.7446959 , 0.745686  , 0.74639321,\n",
       "        0.74724187, 0.74695898, 0.7466761 , 0.74639321, 0.74639321,\n",
       "        0.74653465, 0.74653465, 0.74653465, 0.74653465, 0.74653465,\n",
       "        0.74653465, 0.74653465, 0.74639321, 0.73592645, 0.745686  ,\n",
       "        0.74724187, 0.74695898, 0.74653465, 0.74653465, 0.74653465,\n",
       "        0.74653465, 0.74653465, 0.74653465, 0.74653465, 0.74653465]),\n",
       " 'split2_test_f1_micro': array([0.5992361 , 0.7404159 , 0.74253784, 0.74720611, 0.74862074,\n",
       "        0.74904513, 0.74961098, 0.74975244, 0.7498939 , 0.7498939 ,\n",
       "        0.7498939 , 0.7498939 , 0.7498939 , 0.7498939 , 0.7498939 ,\n",
       "        0.7498939 , 0.7498939 , 0.7498939 , 0.7404159 , 0.74720611,\n",
       "        0.74904513, 0.74975244, 0.7498939 , 0.7498939 , 0.7498939 ,\n",
       "        0.7498939 , 0.7498939 , 0.7498939 , 0.7498939 , 0.7498939 ]),\n",
       " 'split3_test_f1_micro': array([0.59909464, 0.73560617, 0.74423539, 0.74409393, 0.74522563,\n",
       "        0.74494271, 0.74494271, 0.74522563, 0.7453671 , 0.7453671 ,\n",
       "        0.7453671 , 0.7453671 , 0.7453671 , 0.7453671 , 0.7453671 ,\n",
       "        0.7453671 , 0.7453671 , 0.7453671 , 0.73560617, 0.74409393,\n",
       "        0.74494271, 0.7453671 , 0.7453671 , 0.7453671 , 0.7453671 ,\n",
       "        0.7453671 , 0.7453671 , 0.7453671 , 0.7453671 , 0.7453671 ]),\n",
       " 'split4_test_f1_micro': array([0.59909464, 0.73871835, 0.73900127, 0.74451832, 0.74352808,\n",
       "        0.74282077, 0.74338662, 0.74338662, 0.74352808, 0.74338662,\n",
       "        0.74352808, 0.74352808, 0.74352808, 0.74352808, 0.74352808,\n",
       "        0.74352808, 0.74352808, 0.74352808, 0.73871835, 0.74451832,\n",
       "        0.74282077, 0.74324515, 0.74338662, 0.74338662, 0.74352808,\n",
       "        0.74352808, 0.74352808, 0.74352808, 0.74352808, 0.74338662]),\n",
       " 'split5_test_f1_micro': array([0.59909464, 0.74607441, 0.74946951, 0.75328901, 0.75215731,\n",
       "        0.75314755, 0.75286462, 0.75314755, 0.75357193, 0.75357193,\n",
       "        0.75357193, 0.75357193, 0.75357193, 0.75357193, 0.75357193,\n",
       "        0.75357193, 0.75357193, 0.75357193, 0.74593295, 0.75328901,\n",
       "        0.75314755, 0.75314755, 0.75357193, 0.75357193, 0.75357193,\n",
       "        0.75357193, 0.75357193, 0.75357193, 0.75357193, 0.75357193]),\n",
       " 'split6_test_f1_micro': array([0.59909464, 0.74480124, 0.74310369, 0.74862074, 0.74706465,\n",
       "        0.74748904, 0.74833781, 0.74819635, 0.74819635, 0.74819635,\n",
       "        0.74819635, 0.74819635, 0.74819635, 0.74819635, 0.74819635,\n",
       "        0.74819635, 0.74819635, 0.74819635, 0.74480124, 0.74862074,\n",
       "        0.74748904, 0.74819635, 0.74819635, 0.74819635, 0.74819635,\n",
       "        0.74819635, 0.74819635, 0.74819635, 0.74819635, 0.74819635]),\n",
       " 'split7_test_f1_micro': array([0.59909464, 0.73263545, 0.73970859, 0.74112321, 0.74055736,\n",
       "        0.7415476 , 0.74140614, 0.74183053, 0.74197199, 0.74197199,\n",
       "        0.74211345, 0.74211345, 0.74211345, 0.74211345, 0.74211345,\n",
       "        0.74211345, 0.74211345, 0.74211345, 0.73263545, 0.74112321,\n",
       "        0.7415476 , 0.74183053, 0.74197199, 0.74211345, 0.74211345,\n",
       "        0.74211345, 0.74211345, 0.74211345, 0.74211345, 0.74197199]),\n",
       " 'split8_test_f1_micro': array([0.59909464, 0.73687933, 0.74324515, 0.74465978, 0.74522563,\n",
       "        0.74550856, 0.7453671 , 0.74550856, 0.74565002, 0.74565002,\n",
       "        0.74565002, 0.74565002, 0.74565002, 0.74565002, 0.74565002,\n",
       "        0.74565002, 0.74565002, 0.74565002, 0.73687933, 0.74465978,\n",
       "        0.74550856, 0.74550856, 0.74565002, 0.74565002, 0.74565002,\n",
       "        0.74565002, 0.74565002, 0.74565002, 0.74565002, 0.74565002]),\n",
       " 'split9_test_f1_micro': array([0.59909464, 0.73857688, 0.74084029, 0.74338662, 0.74352808,\n",
       "        0.74508417, 0.74607441, 0.74550856, 0.74550856, 0.74550856,\n",
       "        0.74550856, 0.74550856, 0.74550856, 0.74550856, 0.74550856,\n",
       "        0.74550856, 0.74550856, 0.74550856, 0.73857688, 0.74338662,\n",
       "        0.74508417, 0.74550856, 0.74550856, 0.74550856, 0.74550856,\n",
       "        0.74550856, 0.74550856, 0.74550856, 0.74550856, 0.74550856]),\n",
       " 'mean_test_f1_micro': array([0.59912013, 0.73900868, 0.7436484 , 0.74626544, 0.7460957 ,\n",
       "        0.7467181 , 0.7469303 , 0.7469303 , 0.74702933, 0.74700104,\n",
       "        0.74702933, 0.74702933, 0.74702933, 0.74702933, 0.74702933,\n",
       "        0.74704348, 0.74702933, 0.74701519, 0.73899453, 0.74626544,\n",
       "        0.7467181 , 0.74695859, 0.74701518, 0.74701519, 0.74702933,\n",
       "        0.74702933, 0.74702933, 0.74702933, 0.74702933, 0.74701518]),\n",
       " 'std_test_f1_micro': array([4.46616433e-05, 3.92484025e-03, 3.43510078e-03, 3.40080651e-03,\n",
       "        3.09960415e-03, 3.31732419e-03, 3.26042707e-03, 3.21966330e-03,\n",
       "        3.28210533e-03, 3.28396687e-03, 3.23157302e-03, 3.23157302e-03,\n",
       "        3.23157302e-03, 3.23157302e-03, 3.23157302e-03, 3.24451845e-03,\n",
       "        3.23157302e-03, 3.23401585e-03, 3.89952107e-03, 3.40080651e-03,\n",
       "        3.31732419e-03, 3.22724098e-03, 3.28162221e-03, 3.24714099e-03,\n",
       "        3.23157302e-03, 3.23157302e-03, 3.23157302e-03, 3.23157302e-03,\n",
       "        3.23157302e-03, 3.28162221e-03]),\n",
       " 'rank_test_f1_micro': array([30, 28, 27, 24, 26, 22, 21, 20, 13, 18,  2,  2,  2,  2,  2,  1,  2,\n",
       "        14, 29, 24, 22, 19, 16, 15,  2,  2,  2,  2,  2, 16])}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_model.cv_results_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6e3313e2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([30, 28, 27, 24, 26, 22, 21, 20, 13, 18,  2,  2,  2,  2,  2,  1,  2,\n",
       "       14, 29, 24, 22, 19, 16, 15,  2,  2,  2,  2,  2, 16])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_model.cv_results_['rank_test_accuracy']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "3d08f33e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'classifier': LogisticRegression(max_iter=10000),\n",
       " 'classifier__C': 0.0001,\n",
       " 'classifier__penalty': 'l1',\n",
       " 'classifier__solver': 'saga'}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_model.cv_results_['params'][ np.argmin(best_model.cv_results_['mean_test_accuracy'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c8da90d4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>classifier</th>\n",
       "      <th>C</th>\n",
       "      <th>penalty</th>\n",
       "      <th>solver</th>\n",
       "      <th>score_acc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.599120</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.739009</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.743648</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.746265</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0100</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.746096</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0100</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.746718</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.1000</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.746930</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.1000</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.746930</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.747029</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.747001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>10.0000</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.747029</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>10.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.747029</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>100.0000</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.747029</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>100.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.747029</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>1000.0000</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.747029</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>1000.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.747043</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>10000.0000</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.747029</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>10000.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.747015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.738995</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.746265</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0100</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.746718</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.1000</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.746959</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.747015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>10.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.747015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>100.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.747029</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>1000.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.747029</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>10000.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.747029</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>NaN</td>\n",
       "      <td>none</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.747029</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>NaN</td>\n",
       "      <td>none</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.747029</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.747015</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                            classifier           C penalty solver  score_acc\n",
       "0   LogisticRegression(max_iter=10000)      0.0001      l1   saga   0.599120\n",
       "1   LogisticRegression(max_iter=10000)      0.0001      l2   saga   0.739009\n",
       "2   LogisticRegression(max_iter=10000)      0.0010      l1   saga   0.743648\n",
       "3   LogisticRegression(max_iter=10000)      0.0010      l2   saga   0.746265\n",
       "4   LogisticRegression(max_iter=10000)      0.0100      l1   saga   0.746096\n",
       "5   LogisticRegression(max_iter=10000)      0.0100      l2   saga   0.746718\n",
       "6   LogisticRegression(max_iter=10000)      0.1000      l1   saga   0.746930\n",
       "7   LogisticRegression(max_iter=10000)      0.1000      l2   saga   0.746930\n",
       "8   LogisticRegression(max_iter=10000)      1.0000      l1   saga   0.747029\n",
       "9   LogisticRegression(max_iter=10000)      1.0000      l2   saga   0.747001\n",
       "10  LogisticRegression(max_iter=10000)     10.0000      l1   saga   0.747029\n",
       "11  LogisticRegression(max_iter=10000)     10.0000      l2   saga   0.747029\n",
       "12  LogisticRegression(max_iter=10000)    100.0000      l1   saga   0.747029\n",
       "13  LogisticRegression(max_iter=10000)    100.0000      l2   saga   0.747029\n",
       "14  LogisticRegression(max_iter=10000)   1000.0000      l1   saga   0.747029\n",
       "15  LogisticRegression(max_iter=10000)   1000.0000      l2   saga   0.747043\n",
       "16  LogisticRegression(max_iter=10000)  10000.0000      l1   saga   0.747029\n",
       "17  LogisticRegression(max_iter=10000)  10000.0000      l2   saga   0.747015\n",
       "18  LogisticRegression(max_iter=10000)      0.0001      l2  lbfgs   0.738995\n",
       "19  LogisticRegression(max_iter=10000)      0.0010      l2  lbfgs   0.746265\n",
       "20  LogisticRegression(max_iter=10000)      0.0100      l2  lbfgs   0.746718\n",
       "21  LogisticRegression(max_iter=10000)      0.1000      l2  lbfgs   0.746959\n",
       "22  LogisticRegression(max_iter=10000)      1.0000      l2  lbfgs   0.747015\n",
       "23  LogisticRegression(max_iter=10000)     10.0000      l2  lbfgs   0.747015\n",
       "24  LogisticRegression(max_iter=10000)    100.0000      l2  lbfgs   0.747029\n",
       "25  LogisticRegression(max_iter=10000)   1000.0000      l2  lbfgs   0.747029\n",
       "26  LogisticRegression(max_iter=10000)  10000.0000      l2  lbfgs   0.747029\n",
       "27  LogisticRegression(max_iter=10000)         NaN    none  lbfgs   0.747029\n",
       "28  LogisticRegression(max_iter=10000)         NaN    none   saga   0.747029\n",
       "29  LogisticRegression(max_iter=10000)         NaN     NaN    NaN   0.747015"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results = pd.DataFrame( best_model.cv_results_['params'] )\n",
    "\n",
    "results['score_acc'] = best_model.cv_results_['mean_test_accuracy']\n",
    "cols = results.columns.to_series().str.split('__').apply(lambda x: x[-1])\n",
    "results.columns = cols\n",
    "\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "eb7e7292",
   "metadata": {},
   "outputs": [],
   "source": [
    "acc_list.append(results[0:1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ce1a13cb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'classifier': LogisticRegression(max_iter=10000),\n",
       " 'classifier__C': 0.0001,\n",
       " 'classifier__penalty': 'l2',\n",
       " 'classifier__solver': 'saga'}"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_model.cv_results_['params'][ np.argmin(best_model.cv_results_['rank_test_precision'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "e6654f1a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>classifier</th>\n",
       "      <th>C</th>\n",
       "      <th>penalty</th>\n",
       "      <th>solver</th>\n",
       "      <th>score_precision</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.712332</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.699097</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.697913</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0100</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.694974</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0100</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.695491</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.1000</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.695458</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.1000</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.695399</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.695463</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.695412</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>10.0000</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.695464</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>10.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.695464</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>100.0000</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.695464</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>100.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.695464</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>1000.0000</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.695464</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>1000.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.695475</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>10000.0000</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.695464</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>10000.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.695438</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.712301</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.697913</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0100</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.695491</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.1000</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.695451</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.695437</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>10.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.695438</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>100.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.695464</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>1000.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.695464</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>10000.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.695464</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>NaN</td>\n",
       "      <td>none</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.695464</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>NaN</td>\n",
       "      <td>none</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.695464</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.695437</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                            classifier           C penalty solver  \\\n",
       "0   LogisticRegression(max_iter=10000)      0.0001      l1   saga   \n",
       "1   LogisticRegression(max_iter=10000)      0.0001      l2   saga   \n",
       "2   LogisticRegression(max_iter=10000)      0.0010      l1   saga   \n",
       "3   LogisticRegression(max_iter=10000)      0.0010      l2   saga   \n",
       "4   LogisticRegression(max_iter=10000)      0.0100      l1   saga   \n",
       "5   LogisticRegression(max_iter=10000)      0.0100      l2   saga   \n",
       "6   LogisticRegression(max_iter=10000)      0.1000      l1   saga   \n",
       "7   LogisticRegression(max_iter=10000)      0.1000      l2   saga   \n",
       "8   LogisticRegression(max_iter=10000)      1.0000      l1   saga   \n",
       "9   LogisticRegression(max_iter=10000)      1.0000      l2   saga   \n",
       "10  LogisticRegression(max_iter=10000)     10.0000      l1   saga   \n",
       "11  LogisticRegression(max_iter=10000)     10.0000      l2   saga   \n",
       "12  LogisticRegression(max_iter=10000)    100.0000      l1   saga   \n",
       "13  LogisticRegression(max_iter=10000)    100.0000      l2   saga   \n",
       "14  LogisticRegression(max_iter=10000)   1000.0000      l1   saga   \n",
       "15  LogisticRegression(max_iter=10000)   1000.0000      l2   saga   \n",
       "16  LogisticRegression(max_iter=10000)  10000.0000      l1   saga   \n",
       "17  LogisticRegression(max_iter=10000)  10000.0000      l2   saga   \n",
       "18  LogisticRegression(max_iter=10000)      0.0001      l2  lbfgs   \n",
       "19  LogisticRegression(max_iter=10000)      0.0010      l2  lbfgs   \n",
       "20  LogisticRegression(max_iter=10000)      0.0100      l2  lbfgs   \n",
       "21  LogisticRegression(max_iter=10000)      0.1000      l2  lbfgs   \n",
       "22  LogisticRegression(max_iter=10000)      1.0000      l2  lbfgs   \n",
       "23  LogisticRegression(max_iter=10000)     10.0000      l2  lbfgs   \n",
       "24  LogisticRegression(max_iter=10000)    100.0000      l2  lbfgs   \n",
       "25  LogisticRegression(max_iter=10000)   1000.0000      l2  lbfgs   \n",
       "26  LogisticRegression(max_iter=10000)  10000.0000      l2  lbfgs   \n",
       "27  LogisticRegression(max_iter=10000)         NaN    none  lbfgs   \n",
       "28  LogisticRegression(max_iter=10000)         NaN    none   saga   \n",
       "29  LogisticRegression(max_iter=10000)         NaN     NaN    NaN   \n",
       "\n",
       "    score_precision  \n",
       "0          0.000000  \n",
       "1          0.712332  \n",
       "2          0.699097  \n",
       "3          0.697913  \n",
       "4          0.694974  \n",
       "5          0.695491  \n",
       "6          0.695458  \n",
       "7          0.695399  \n",
       "8          0.695463  \n",
       "9          0.695412  \n",
       "10         0.695464  \n",
       "11         0.695464  \n",
       "12         0.695464  \n",
       "13         0.695464  \n",
       "14         0.695464  \n",
       "15         0.695475  \n",
       "16         0.695464  \n",
       "17         0.695438  \n",
       "18         0.712301  \n",
       "19         0.697913  \n",
       "20         0.695491  \n",
       "21         0.695451  \n",
       "22         0.695437  \n",
       "23         0.695438  \n",
       "24         0.695464  \n",
       "25         0.695464  \n",
       "26         0.695464  \n",
       "27         0.695464  \n",
       "28         0.695464  \n",
       "29         0.695437  "
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results = pd.DataFrame( best_model.cv_results_['params'] )\n",
    "\n",
    "results['score_precision'] = best_model.cv_results_['mean_test_precision']\n",
    "cols = results.columns.to_series().str.split('__').apply(lambda x: x[-1])\n",
    "results.columns = cols\n",
    "\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "71d804bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "roc_auc_ovr_List.append(results[1:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "c8742a4b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'classifier': LogisticRegression(max_iter=10000),\n",
       " 'classifier__C': 1000.0,\n",
       " 'classifier__penalty': 'l2',\n",
       " 'classifier__solver': 'saga'}"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_model.cv_results_['params'][ np.argmin(best_model.cv_results_['rank_test_f1_micro'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "4bf5cd96",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>classifier</th>\n",
       "      <th>C</th>\n",
       "      <th>penalty</th>\n",
       "      <th>solver</th>\n",
       "      <th>score_f1_micro</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.599120</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.739009</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.743648</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.746265</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0100</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.746096</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0100</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.746718</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.1000</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.746930</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.1000</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.746930</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.747029</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.747001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>10.0000</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.747029</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>10.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.747029</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>100.0000</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.747029</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>100.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.747029</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>1000.0000</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.747029</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>1000.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.747043</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>10000.0000</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.747029</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>10000.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.747015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.738995</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.746265</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0100</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.746718</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.1000</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.746959</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.747015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>10.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.747015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>100.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.747029</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>1000.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.747029</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>10000.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.747029</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>NaN</td>\n",
       "      <td>none</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.747029</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>NaN</td>\n",
       "      <td>none</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.747029</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.747015</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                            classifier           C penalty solver  \\\n",
       "0   LogisticRegression(max_iter=10000)      0.0001      l1   saga   \n",
       "1   LogisticRegression(max_iter=10000)      0.0001      l2   saga   \n",
       "2   LogisticRegression(max_iter=10000)      0.0010      l1   saga   \n",
       "3   LogisticRegression(max_iter=10000)      0.0010      l2   saga   \n",
       "4   LogisticRegression(max_iter=10000)      0.0100      l1   saga   \n",
       "5   LogisticRegression(max_iter=10000)      0.0100      l2   saga   \n",
       "6   LogisticRegression(max_iter=10000)      0.1000      l1   saga   \n",
       "7   LogisticRegression(max_iter=10000)      0.1000      l2   saga   \n",
       "8   LogisticRegression(max_iter=10000)      1.0000      l1   saga   \n",
       "9   LogisticRegression(max_iter=10000)      1.0000      l2   saga   \n",
       "10  LogisticRegression(max_iter=10000)     10.0000      l1   saga   \n",
       "11  LogisticRegression(max_iter=10000)     10.0000      l2   saga   \n",
       "12  LogisticRegression(max_iter=10000)    100.0000      l1   saga   \n",
       "13  LogisticRegression(max_iter=10000)    100.0000      l2   saga   \n",
       "14  LogisticRegression(max_iter=10000)   1000.0000      l1   saga   \n",
       "15  LogisticRegression(max_iter=10000)   1000.0000      l2   saga   \n",
       "16  LogisticRegression(max_iter=10000)  10000.0000      l1   saga   \n",
       "17  LogisticRegression(max_iter=10000)  10000.0000      l2   saga   \n",
       "18  LogisticRegression(max_iter=10000)      0.0001      l2  lbfgs   \n",
       "19  LogisticRegression(max_iter=10000)      0.0010      l2  lbfgs   \n",
       "20  LogisticRegression(max_iter=10000)      0.0100      l2  lbfgs   \n",
       "21  LogisticRegression(max_iter=10000)      0.1000      l2  lbfgs   \n",
       "22  LogisticRegression(max_iter=10000)      1.0000      l2  lbfgs   \n",
       "23  LogisticRegression(max_iter=10000)     10.0000      l2  lbfgs   \n",
       "24  LogisticRegression(max_iter=10000)    100.0000      l2  lbfgs   \n",
       "25  LogisticRegression(max_iter=10000)   1000.0000      l2  lbfgs   \n",
       "26  LogisticRegression(max_iter=10000)  10000.0000      l2  lbfgs   \n",
       "27  LogisticRegression(max_iter=10000)         NaN    none  lbfgs   \n",
       "28  LogisticRegression(max_iter=10000)         NaN    none   saga   \n",
       "29  LogisticRegression(max_iter=10000)         NaN     NaN    NaN   \n",
       "\n",
       "    score_f1_micro  \n",
       "0         0.599120  \n",
       "1         0.739009  \n",
       "2         0.743648  \n",
       "3         0.746265  \n",
       "4         0.746096  \n",
       "5         0.746718  \n",
       "6         0.746930  \n",
       "7         0.746930  \n",
       "8         0.747029  \n",
       "9         0.747001  \n",
       "10        0.747029  \n",
       "11        0.747029  \n",
       "12        0.747029  \n",
       "13        0.747029  \n",
       "14        0.747029  \n",
       "15        0.747043  \n",
       "16        0.747029  \n",
       "17        0.747015  \n",
       "18        0.738995  \n",
       "19        0.746265  \n",
       "20        0.746718  \n",
       "21        0.746959  \n",
       "22        0.747015  \n",
       "23        0.747015  \n",
       "24        0.747029  \n",
       "25        0.747029  \n",
       "26        0.747029  \n",
       "27        0.747029  \n",
       "28        0.747029  \n",
       "29        0.747015  "
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results = pd.DataFrame( best_model.cv_results_['params'] )\n",
    "\n",
    "results['score_f1_micro'] = best_model.cv_results_['mean_test_f1_micro']\n",
    "cols = results.columns.to_series().str.split('__').apply(lambda x: x[-1])\n",
    "results.columns = cols\n",
    "\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "8cebbf79",
   "metadata": {},
   "outputs": [],
   "source": [
    "f1_micro_List.append(results[15:16])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b315f477",
   "metadata": {},
   "source": [
    "## Trial 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "8e5bcf1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, Y_train, Y_test = train_test_split(X, y, test_size=0.2, shuffle=True, random_state = 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "e91ba61f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Guest-All\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\Guest-All\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\Guest-All\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\Guest-All\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\Guest-All\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\Guest-All\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\Guest-All\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\Guest-All\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\Guest-All\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\Guest-All\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\Guest-All\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1183: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Guest-All\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1183: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Guest-All\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1183: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Guest-All\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1183: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Guest-All\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1183: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Guest-All\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1183: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Guest-All\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1183: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Guest-All\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1183: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Guest-All\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1183: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Guest-All\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1183: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Guest-All\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1183: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Guest-All\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1183: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Guest-All\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1183: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Guest-All\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1183: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Guest-All\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1183: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Guest-All\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1183: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Guest-All\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1183: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Guest-All\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1183: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Guest-All\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1183: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Guest-All\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1183: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Create a pipeline - RF is a stand in, we will populate the classifier part below\n",
    "pipe = Pipeline([('std', StandardScaler()),\n",
    "                 ('classifier', LogisticRegression())])\n",
    "\n",
    "# Create search space of candidate learning algorithms and their hyperparameters\n",
    "# note lbfgs can't do l1, and if you pass penalty='none' it expects no C value\n",
    "search_space = [{'classifier': [LogisticRegression(max_iter=10000)],\n",
    "                 'classifier__solver': ['saga'],\n",
    "                 'classifier__penalty': ['l1', 'l2'],\n",
    "                 'classifier__C': np.logspace(-4, 4, 9)},\n",
    "                {'classifier': [LogisticRegression(max_iter=10000)],\n",
    "                 'classifier__solver': ['lbfgs'],\n",
    "                 'classifier__penalty': ['l2'],\n",
    "                 'classifier__C': np.logspace(-4, 4, 9)},\n",
    "                {'classifier': [LogisticRegression(max_iter=10000)],\n",
    "                 'classifier__solver': ['lbfgs','saga'],\n",
    "                 'classifier__penalty': ['none']},\n",
    "                {'classifier': [LogisticRegression(max_iter=10000)]}\n",
    "                ]\n",
    "\n",
    "# Create grid search\n",
    "clf = GridSearchCV(pipe, search_space, cv=StratifiedKFold(n_splits=10),\n",
    "                   scoring=['accuracy', 'precision', 'f1_micro'], refit=False,\n",
    "                   verbose=0)\n",
    "\n",
    "# Fit grid search\n",
    "best_model = clf.fit(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "36bb3f82",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'mean_fit_time': array([0.75290251, 0.66871369, 0.76771591, 0.65874445, 0.81238613,\n",
       "        0.71601481, 0.89895306, 0.86977723, 1.02176297, 0.87500408,\n",
       "        0.96130064, 0.88397105, 0.96545217, 0.90873976, 0.9930603 ,\n",
       "        0.92028687, 0.86143777, 0.70369284, 0.09372852, 0.10877235,\n",
       "        0.10553417, 0.11161778, 0.10928605, 0.10749404, 0.10272226,\n",
       "        0.10854416, 0.10725744, 0.10935886, 0.71608827, 0.10303392]),\n",
       " 'std_fit_time': array([0.1164699 , 0.03797113, 0.04754982, 0.06160515, 0.0830729 ,\n",
       "        0.05015624, 0.20079105, 0.14390754, 0.21167597, 0.17702368,\n",
       "        0.23219803, 0.2168387 , 0.2156959 , 0.18614958, 0.21113671,\n",
       "        0.22258151, 0.07744749, 0.04635152, 0.00919033, 0.00509653,\n",
       "        0.0055913 , 0.01758602, 0.00523933, 0.0053457 , 0.0047531 ,\n",
       "        0.01160619, 0.0064342 , 0.00523853, 0.04525995, 0.00603959]),\n",
       " 'mean_score_time': array([0.01372344, 0.01285002, 0.01133633, 0.01251369, 0.01313908,\n",
       "        0.01365538, 0.01288407, 0.01837037, 0.01400797, 0.01852844,\n",
       "        0.01590235, 0.01407974, 0.01475282, 0.0180011 , 0.01642089,\n",
       "        0.01448698, 0.0119451 , 0.01258516, 0.01546886, 0.01527083,\n",
       "        0.01361372, 0.01206923, 0.01526363, 0.01446974, 0.01441865,\n",
       "        0.01531444, 0.01263547, 0.01216779, 0.0118067 , 0.01123528]),\n",
       " 'std_score_time': array([0.00253251, 0.00396379, 0.00387429, 0.00402281, 0.00363509,\n",
       "        0.00366957, 0.00388703, 0.01249608, 0.00491341, 0.00959939,\n",
       "        0.00925352, 0.00489679, 0.00681953, 0.00539892, 0.00583936,\n",
       "        0.00479858, 0.0040141 , 0.00373595, 0.00251737, 0.00242428,\n",
       "        0.00512463, 0.00406096, 0.00242581, 0.00256625, 0.00308977,\n",
       "        0.00247436, 0.00366695, 0.00386246, 0.00375944, 0.00390904]),\n",
       " 'param_classifier': masked_array(data=[LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000)],\n",
       "              mask=[False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'param_classifier__C': masked_array(data=[0.0001, 0.0001, 0.001, 0.001, 0.01, 0.01, 0.1, 0.1,\n",
       "                    1.0, 1.0, 10.0, 10.0, 100.0, 100.0, 1000.0, 1000.0,\n",
       "                    10000.0, 10000.0, 0.0001, 0.001, 0.01, 0.1, 1.0, 10.0,\n",
       "                    100.0, 1000.0, 10000.0, --, --, --],\n",
       "              mask=[False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False,  True,  True,  True],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'param_classifier__penalty': masked_array(data=['l1', 'l2', 'l1', 'l2', 'l1', 'l2', 'l1', 'l2', 'l1',\n",
       "                    'l2', 'l1', 'l2', 'l1', 'l2', 'l1', 'l2', 'l1', 'l2',\n",
       "                    'l2', 'l2', 'l2', 'l2', 'l2', 'l2', 'l2', 'l2', 'l2',\n",
       "                    'none', 'none', --],\n",
       "              mask=[False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False,  True],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'param_classifier__solver': masked_array(data=['saga', 'saga', 'saga', 'saga', 'saga', 'saga', 'saga',\n",
       "                    'saga', 'saga', 'saga', 'saga', 'saga', 'saga', 'saga',\n",
       "                    'saga', 'saga', 'saga', 'saga', 'lbfgs', 'lbfgs',\n",
       "                    'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs',\n",
       "                    'lbfgs', 'lbfgs', 'saga', --],\n",
       "              mask=[False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False,  True],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'params': [{'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 0.0001,\n",
       "   'classifier__penalty': 'l1',\n",
       "   'classifier__solver': 'saga'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 0.0001,\n",
       "   'classifier__penalty': 'l2',\n",
       "   'classifier__solver': 'saga'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 0.001,\n",
       "   'classifier__penalty': 'l1',\n",
       "   'classifier__solver': 'saga'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 0.001,\n",
       "   'classifier__penalty': 'l2',\n",
       "   'classifier__solver': 'saga'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 0.01,\n",
       "   'classifier__penalty': 'l1',\n",
       "   'classifier__solver': 'saga'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 0.01,\n",
       "   'classifier__penalty': 'l2',\n",
       "   'classifier__solver': 'saga'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 0.1,\n",
       "   'classifier__penalty': 'l1',\n",
       "   'classifier__solver': 'saga'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 0.1,\n",
       "   'classifier__penalty': 'l2',\n",
       "   'classifier__solver': 'saga'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 1.0,\n",
       "   'classifier__penalty': 'l1',\n",
       "   'classifier__solver': 'saga'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 1.0,\n",
       "   'classifier__penalty': 'l2',\n",
       "   'classifier__solver': 'saga'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 10.0,\n",
       "   'classifier__penalty': 'l1',\n",
       "   'classifier__solver': 'saga'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 10.0,\n",
       "   'classifier__penalty': 'l2',\n",
       "   'classifier__solver': 'saga'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 100.0,\n",
       "   'classifier__penalty': 'l1',\n",
       "   'classifier__solver': 'saga'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 100.0,\n",
       "   'classifier__penalty': 'l2',\n",
       "   'classifier__solver': 'saga'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 1000.0,\n",
       "   'classifier__penalty': 'l1',\n",
       "   'classifier__solver': 'saga'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 1000.0,\n",
       "   'classifier__penalty': 'l2',\n",
       "   'classifier__solver': 'saga'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 10000.0,\n",
       "   'classifier__penalty': 'l1',\n",
       "   'classifier__solver': 'saga'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 10000.0,\n",
       "   'classifier__penalty': 'l2',\n",
       "   'classifier__solver': 'saga'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 0.0001,\n",
       "   'classifier__penalty': 'l2',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 0.001,\n",
       "   'classifier__penalty': 'l2',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 0.01,\n",
       "   'classifier__penalty': 'l2',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 0.1,\n",
       "   'classifier__penalty': 'l2',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 1.0,\n",
       "   'classifier__penalty': 'l2',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 10.0,\n",
       "   'classifier__penalty': 'l2',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 100.0,\n",
       "   'classifier__penalty': 'l2',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 1000.0,\n",
       "   'classifier__penalty': 'l2',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 10000.0,\n",
       "   'classifier__penalty': 'l2',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__penalty': 'none',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__penalty': 'none',\n",
       "   'classifier__solver': 'saga'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000)}],\n",
       " 'split0_test_accuracy': array([0.59957567, 0.73224894, 0.74540311, 0.74214993, 0.74497878,\n",
       "        0.74497878, 0.74582744, 0.74582744, 0.74596888, 0.74582744,\n",
       "        0.74582744, 0.74582744, 0.74582744, 0.74582744, 0.74582744,\n",
       "        0.74582744, 0.74582744, 0.74582744, 0.73224894, 0.74214993,\n",
       "        0.74497878, 0.74582744, 0.74596888, 0.74596888, 0.74596888,\n",
       "        0.74596888, 0.74596888, 0.74596888, 0.74582744, 0.74596888]),\n",
       " 'split1_test_accuracy': array([0.59957567, 0.74483734, 0.74483734, 0.74851485, 0.74809052,\n",
       "        0.74908062, 0.74922207, 0.74936351, 0.74908062, 0.74908062,\n",
       "        0.74908062, 0.74908062, 0.74908062, 0.74908062, 0.74908062,\n",
       "        0.74908062, 0.74908062, 0.74908062, 0.74483734, 0.74851485,\n",
       "        0.74908062, 0.74936351, 0.74908062, 0.74908062, 0.74908062,\n",
       "        0.74908062, 0.74908062, 0.74908062, 0.74908062, 0.74908062]),\n",
       " 'split2_test_accuracy': array([0.59951903, 0.74211345, 0.74593295, 0.74621587, 0.74579148,\n",
       "        0.74579148, 0.7464988 , 0.74635733, 0.74635733, 0.74635733,\n",
       "        0.74635733, 0.74635733, 0.74635733, 0.74635733, 0.74635733,\n",
       "        0.74635733, 0.74635733, 0.74635733, 0.74211345, 0.74621587,\n",
       "        0.74579148, 0.74635733, 0.74635733, 0.74635733, 0.74635733,\n",
       "        0.74635733, 0.74635733, 0.74635733, 0.74635733, 0.74635733]),\n",
       " 'split3_test_accuracy': array([0.59951903, 0.73673787, 0.74282077, 0.74366954, 0.74550856,\n",
       "        0.74607441, 0.74607441, 0.74607441, 0.74607441, 0.74607441,\n",
       "        0.74607441, 0.74607441, 0.74607441, 0.74607441, 0.74607441,\n",
       "        0.74607441, 0.74607441, 0.74607441, 0.73673787, 0.74381101,\n",
       "        0.74607441, 0.74607441, 0.74607441, 0.74607441, 0.74607441,\n",
       "        0.74607441, 0.74607441, 0.74607441, 0.74607441, 0.74607441]),\n",
       " 'split4_test_accuracy': array([0.59951903, 0.73574763, 0.73885981, 0.74140614, 0.74098175,\n",
       "        0.73970859, 0.74069883, 0.73999151, 0.73999151, 0.73985005,\n",
       "        0.73985005, 0.73985005, 0.73985005, 0.73985005, 0.73985005,\n",
       "        0.73985005, 0.73985005, 0.73985005, 0.73574763, 0.74140614,\n",
       "        0.73970859, 0.73999151, 0.73985005, 0.73985005, 0.73985005,\n",
       "        0.73985005, 0.73985005, 0.73985005, 0.73985005, 0.73985005]),\n",
       " 'split5_test_accuracy': array([0.59966049, 0.74112321, 0.74932805, 0.7487622 , 0.7510256 ,\n",
       "        0.75045975, 0.75144999, 0.75159146, 0.75144999, 0.75144999,\n",
       "        0.75144999, 0.75144999, 0.75144999, 0.75144999, 0.75144999,\n",
       "        0.75144999, 0.75144999, 0.75144999, 0.74112321, 0.7487622 ,\n",
       "        0.75045975, 0.75159146, 0.75144999, 0.75144999, 0.75144999,\n",
       "        0.75144999, 0.75144999, 0.75144999, 0.75144999, 0.75144999]),\n",
       " 'split6_test_accuracy': array([0.59966049, 0.73475739, 0.74027444, 0.74352808, 0.74352808,\n",
       "        0.74437686, 0.74550856, 0.74508417, 0.74550856, 0.7453671 ,\n",
       "        0.7453671 , 0.7453671 , 0.7453671 , 0.7453671 , 0.7453671 ,\n",
       "        0.7453671 , 0.7453671 , 0.7453671 , 0.73475739, 0.74352808,\n",
       "        0.74423539, 0.74508417, 0.7453671 , 0.7453671 , 0.7453671 ,\n",
       "        0.7453671 , 0.7453671 , 0.7453671 , 0.7453671 , 0.7453671 ]),\n",
       " 'split7_test_accuracy': array([0.59966049, 0.73942566, 0.74437686, 0.74862074, 0.74777196,\n",
       "        0.74847928, 0.74777196, 0.7476305 , 0.74734757, 0.74734757,\n",
       "        0.74734757, 0.74734757, 0.74734757, 0.74734757, 0.74734757,\n",
       "        0.74734757, 0.74734757, 0.74734757, 0.73942566, 0.74862074,\n",
       "        0.74847928, 0.7476305 , 0.74734757, 0.74734757, 0.74734757,\n",
       "        0.74734757, 0.74734757, 0.74734757, 0.74734757, 0.74734757]),\n",
       " 'split8_test_accuracy': array([0.59966049, 0.74253784, 0.74748904, 0.75074268, 0.74932805,\n",
       "        0.75159146, 0.7510256 , 0.75116707, 0.75116707, 0.75130853,\n",
       "        0.75130853, 0.75130853, 0.75130853, 0.75130853, 0.75130853,\n",
       "        0.75130853, 0.75130853, 0.75130853, 0.74253784, 0.75074268,\n",
       "        0.75159146, 0.75116707, 0.75130853, 0.75130853, 0.75130853,\n",
       "        0.75130853, 0.75130853, 0.75130853, 0.75130853, 0.75130853]),\n",
       " 'split9_test_accuracy': array([0.59966049, 0.73645494, 0.74211345, 0.74480124, 0.74381101,\n",
       "        0.74494271, 0.74437686, 0.74423539, 0.74451832, 0.74451832,\n",
       "        0.74451832, 0.74451832, 0.74451832, 0.74451832, 0.74451832,\n",
       "        0.74437686, 0.74451832, 0.74451832, 0.73645494, 0.74480124,\n",
       "        0.74494271, 0.74423539, 0.74451832, 0.74451832, 0.74451832,\n",
       "        0.74451832, 0.74451832, 0.74451832, 0.74451832, 0.74451832]),\n",
       " 'mean_test_accuracy': array([0.59960109, 0.73859843, 0.74414358, 0.74584113, 0.74608158,\n",
       "        0.74654839, 0.74684545, 0.74673228, 0.74674643, 0.74671814,\n",
       "        0.74671814, 0.74671814, 0.74671814, 0.74671814, 0.74671814,\n",
       "        0.74670399, 0.74671814, 0.74671814, 0.73859843, 0.74585527,\n",
       "        0.74653425, 0.74673228, 0.74673228, 0.74673228, 0.74673228,\n",
       "        0.74673228, 0.74673228, 0.74673228, 0.74671814, 0.74673228]),\n",
       " 'std_test_accuracy': array([6.25593973e-05, 3.80933182e-03, 3.03569661e-03, 3.03339310e-03,\n",
       "        2.84936828e-03, 3.28952441e-03, 3.04295037e-03, 3.26787170e-03,\n",
       "        3.17503813e-03, 3.23438925e-03, 3.23438925e-03, 3.23438925e-03,\n",
       "        3.23438925e-03, 3.23438925e-03, 3.23438925e-03, 3.24427393e-03,\n",
       "        3.23438925e-03, 3.23438925e-03, 3.80933182e-03, 3.02354677e-03,\n",
       "        3.29912264e-03, 3.26787170e-03, 3.23077048e-03, 3.23077048e-03,\n",
       "        3.23077048e-03, 3.23077048e-03, 3.23077048e-03, 3.23077048e-03,\n",
       "        3.23438925e-03, 3.23077048e-03]),\n",
       " 'rank_test_accuracy': array([30, 28, 27, 26, 24, 22,  1, 10,  2, 12, 12, 12, 12, 12, 12, 21, 12,\n",
       "        12, 28, 25, 23, 10,  3,  3,  3,  3,  3,  3, 12,  3]),\n",
       " 'split0_test_precision': array([0.        , 0.70285467, 0.70271333, 0.69489559, 0.69528875,\n",
       "        0.69543726, 0.69583333, 0.69583333, 0.6959485 , 0.69568509,\n",
       "        0.69568509, 0.69568509, 0.69568509, 0.69568509, 0.69568509,\n",
       "        0.69568509, 0.69568509, 0.69568509, 0.70285467, 0.69489559,\n",
       "        0.69543726, 0.69583333, 0.6959485 , 0.6959485 , 0.6959485 ,\n",
       "        0.6959485 , 0.6959485 , 0.6959485 , 0.69568509, 0.6959485 ]),\n",
       " 'split1_test_precision': array([0.        , 0.72316384, 0.70288424, 0.70383275, 0.70099541,\n",
       "        0.70241287, 0.70221713, 0.70248566, 0.70210325, 0.70210325,\n",
       "        0.70210325, 0.70210325, 0.70210325, 0.70210325, 0.70210325,\n",
       "        0.70210325, 0.70210325, 0.70210325, 0.72316384, 0.70383275,\n",
       "        0.70241287, 0.70248566, 0.70210325, 0.70210325, 0.70210325,\n",
       "        0.70210325, 0.70210325, 0.70210325, 0.70210325, 0.70210325]),\n",
       " 'split2_test_precision': array([0.        , 0.71265823, 0.6975945 , 0.6944132 , 0.69091581,\n",
       "        0.69105691, 0.69162671, 0.69151292, 0.69137168, 0.69137168,\n",
       "        0.69137168, 0.69137168, 0.69137168, 0.69137168, 0.69137168,\n",
       "        0.69137168, 0.69137168, 0.69137168, 0.71265823, 0.6944132 ,\n",
       "        0.69105691, 0.69151292, 0.69137168, 0.69137168, 0.69137168,\n",
       "        0.69137168, 0.69137168, 0.69137168, 0.69137168, 0.69137168]),\n",
       " 'split3_test_precision': array([0.        , 0.71216098, 0.70123162, 0.69528555, 0.69575114,\n",
       "        0.69591528, 0.69591528, 0.69591528, 0.69591528, 0.69591528,\n",
       "        0.69591528, 0.69591528, 0.69591528, 0.69591528, 0.69591528,\n",
       "        0.69591528, 0.69591528, 0.69591528, 0.71216098, 0.6954023 ,\n",
       "        0.69591528, 0.69591528, 0.69591528, 0.69591528, 0.69591528,\n",
       "        0.69591528, 0.69591528, 0.69591528, 0.69591528, 0.69591528]),\n",
       " 'split4_test_precision': array([0.        , 0.7058572 , 0.6929103 , 0.69133918, 0.68867925,\n",
       "        0.6869106 , 0.68815988, 0.68672433, 0.68686489, 0.68660647,\n",
       "        0.68660647, 0.68660647, 0.68660647, 0.68660647, 0.68660647,\n",
       "        0.68660647, 0.68660647, 0.68660647, 0.7058572 , 0.69133918,\n",
       "        0.6869106 , 0.68672433, 0.68660647, 0.68660647, 0.68660647,\n",
       "        0.68660647, 0.68660647, 0.68660647, 0.68660647, 0.68660647]),\n",
       " 'split5_test_precision': array([0.        , 0.71588946, 0.70761381, 0.70300462, 0.70311314,\n",
       "        0.70143613, 0.70237646, 0.70248869, 0.7022239 , 0.7022239 ,\n",
       "        0.7022239 , 0.7022239 , 0.7022239 , 0.7022239 , 0.7022239 ,\n",
       "        0.7022239 , 0.7022239 , 0.7022239 , 0.71588946, 0.70300462,\n",
       "        0.70143613, 0.70248869, 0.7022239 , 0.7022239 , 0.7022239 ,\n",
       "        0.7022239 , 0.7022239 , 0.7022239 , 0.7022239 , 0.7022239 ]),\n",
       " 'split6_test_precision': array([0.        , 0.70897155, 0.69582348, 0.69415808, 0.69224953,\n",
       "        0.69207661, 0.69299888, 0.69222139, 0.69271028, 0.69230769,\n",
       "        0.69230769, 0.69230769, 0.69230769, 0.69230769, 0.69230769,\n",
       "        0.69230769, 0.69230769, 0.69230769, 0.70897155, 0.69415808,\n",
       "        0.69181682, 0.69222139, 0.69230769, 0.69230769, 0.69230769,\n",
       "        0.69230769, 0.69230769, 0.69230769, 0.69230769, 0.69230769]),\n",
       " 'split7_test_precision': array([0.        , 0.71348315, 0.70161608, 0.70057143, 0.69614088,\n",
       "        0.69729932, 0.69614088, 0.69588015, 0.69535928, 0.69535928,\n",
       "        0.69535928, 0.69535928, 0.69535928, 0.69535928, 0.69535928,\n",
       "        0.69535928, 0.69535928, 0.69535928, 0.71348315, 0.70057143,\n",
       "        0.69729932, 0.69588015, 0.69535928, 0.69535928, 0.69535928,\n",
       "        0.69535928, 0.69535928, 0.69535928, 0.69535928, 0.69535928]),\n",
       " 'split8_test_precision': array([0.        , 0.71362098, 0.69859369, 0.70150943, 0.69592593,\n",
       "        0.69859467, 0.69785503, 0.69811321, 0.69811321, 0.69822485,\n",
       "        0.69822485, 0.69822485, 0.69822485, 0.69822485, 0.69822485,\n",
       "        0.69822485, 0.69822485, 0.69822485, 0.71362098, 0.70150943,\n",
       "        0.69859467, 0.69811321, 0.69822485, 0.69822485, 0.69822485,\n",
       "        0.69822485, 0.69822485, 0.69822485, 0.69822485, 0.69822485]),\n",
       " 'split9_test_precision': array([0.        , 0.70939801, 0.69940594, 0.69852941, 0.6945399 ,\n",
       "        0.6960672 , 0.69500572, 0.69444444, 0.69482496, 0.69482496,\n",
       "        0.69482496, 0.69482496, 0.69482496, 0.69482496, 0.69482496,\n",
       "        0.69470879, 0.69482496, 0.69482496, 0.70939801, 0.69852941,\n",
       "        0.6960672 , 0.69444444, 0.69482496, 0.69482496, 0.69482496,\n",
       "        0.69482496, 0.69482496, 0.69482496, 0.69482496, 0.69482496]),\n",
       " 'mean_test_precision': array([0.        , 0.71180581, 0.7000387 , 0.69775392, 0.69535997,\n",
       "        0.69572069, 0.69581293, 0.69556194, 0.69554352, 0.69546225,\n",
       "        0.69546225, 0.69546225, 0.69546225, 0.69546225, 0.69546225,\n",
       "        0.69545063, 0.69546225, 0.69546225, 0.71180581, 0.6977656 ,\n",
       "        0.69569471, 0.69556194, 0.69548859, 0.69548859, 0.69548859,\n",
       "        0.69548859, 0.69548859, 0.69548859, 0.69546225, 0.69548859]),\n",
       " 'std_test_precision': array([0.        , 0.00531019, 0.00391904, 0.0040825 , 0.00409497,\n",
       "        0.0044843 , 0.00415988, 0.00457856, 0.00442565, 0.00450816,\n",
       "        0.00450816, 0.00450816, 0.00450816, 0.00450816, 0.00450816,\n",
       "        0.00450994, 0.00450816, 0.00450816, 0.00531019, 0.00407558,\n",
       "        0.00450604, 0.00457856, 0.00451016, 0.00451016, 0.00451016,\n",
       "        0.00451016, 0.00451016, 0.00451016, 0.00450816, 0.00451016]),\n",
       " 'rank_test_precision': array([30,  1,  3,  5, 29,  7,  6,  9, 11, 19, 19, 19, 19, 19, 19, 28, 19,\n",
       "        19,  1,  4,  8,  9, 12, 12, 12, 12, 12, 12, 19, 12]),\n",
       " 'split0_test_f1_micro': array([0.59957567, 0.73224894, 0.74540311, 0.74214993, 0.74497878,\n",
       "        0.74497878, 0.74582744, 0.74582744, 0.74596888, 0.74582744,\n",
       "        0.74582744, 0.74582744, 0.74582744, 0.74582744, 0.74582744,\n",
       "        0.74582744, 0.74582744, 0.74582744, 0.73224894, 0.74214993,\n",
       "        0.74497878, 0.74582744, 0.74596888, 0.74596888, 0.74596888,\n",
       "        0.74596888, 0.74596888, 0.74596888, 0.74582744, 0.74596888]),\n",
       " 'split1_test_f1_micro': array([0.59957567, 0.74483734, 0.74483734, 0.74851485, 0.74809052,\n",
       "        0.74908062, 0.74922207, 0.74936351, 0.74908062, 0.74908062,\n",
       "        0.74908062, 0.74908062, 0.74908062, 0.74908062, 0.74908062,\n",
       "        0.74908062, 0.74908062, 0.74908062, 0.74483734, 0.74851485,\n",
       "        0.74908062, 0.74936351, 0.74908062, 0.74908062, 0.74908062,\n",
       "        0.74908062, 0.74908062, 0.74908062, 0.74908062, 0.74908062]),\n",
       " 'split2_test_f1_micro': array([0.59951903, 0.74211345, 0.74593295, 0.74621587, 0.74579148,\n",
       "        0.74579148, 0.7464988 , 0.74635733, 0.74635733, 0.74635733,\n",
       "        0.74635733, 0.74635733, 0.74635733, 0.74635733, 0.74635733,\n",
       "        0.74635733, 0.74635733, 0.74635733, 0.74211345, 0.74621587,\n",
       "        0.74579148, 0.74635733, 0.74635733, 0.74635733, 0.74635733,\n",
       "        0.74635733, 0.74635733, 0.74635733, 0.74635733, 0.74635733]),\n",
       " 'split3_test_f1_micro': array([0.59951903, 0.73673787, 0.74282077, 0.74366954, 0.74550856,\n",
       "        0.74607441, 0.74607441, 0.74607441, 0.74607441, 0.74607441,\n",
       "        0.74607441, 0.74607441, 0.74607441, 0.74607441, 0.74607441,\n",
       "        0.74607441, 0.74607441, 0.74607441, 0.73673787, 0.74381101,\n",
       "        0.74607441, 0.74607441, 0.74607441, 0.74607441, 0.74607441,\n",
       "        0.74607441, 0.74607441, 0.74607441, 0.74607441, 0.74607441]),\n",
       " 'split4_test_f1_micro': array([0.59951903, 0.73574763, 0.73885981, 0.74140614, 0.74098175,\n",
       "        0.73970859, 0.74069883, 0.73999151, 0.73999151, 0.73985005,\n",
       "        0.73985005, 0.73985005, 0.73985005, 0.73985005, 0.73985005,\n",
       "        0.73985005, 0.73985005, 0.73985005, 0.73574763, 0.74140614,\n",
       "        0.73970859, 0.73999151, 0.73985005, 0.73985005, 0.73985005,\n",
       "        0.73985005, 0.73985005, 0.73985005, 0.73985005, 0.73985005]),\n",
       " 'split5_test_f1_micro': array([0.59966049, 0.74112321, 0.74932805, 0.7487622 , 0.7510256 ,\n",
       "        0.75045975, 0.75144999, 0.75159146, 0.75144999, 0.75144999,\n",
       "        0.75144999, 0.75144999, 0.75144999, 0.75144999, 0.75144999,\n",
       "        0.75144999, 0.75144999, 0.75144999, 0.74112321, 0.7487622 ,\n",
       "        0.75045975, 0.75159146, 0.75144999, 0.75144999, 0.75144999,\n",
       "        0.75144999, 0.75144999, 0.75144999, 0.75144999, 0.75144999]),\n",
       " 'split6_test_f1_micro': array([0.59966049, 0.73475739, 0.74027444, 0.74352808, 0.74352808,\n",
       "        0.74437686, 0.74550856, 0.74508417, 0.74550856, 0.7453671 ,\n",
       "        0.7453671 , 0.7453671 , 0.7453671 , 0.7453671 , 0.7453671 ,\n",
       "        0.7453671 , 0.7453671 , 0.7453671 , 0.73475739, 0.74352808,\n",
       "        0.74423539, 0.74508417, 0.7453671 , 0.7453671 , 0.7453671 ,\n",
       "        0.7453671 , 0.7453671 , 0.7453671 , 0.7453671 , 0.7453671 ]),\n",
       " 'split7_test_f1_micro': array([0.59966049, 0.73942566, 0.74437686, 0.74862074, 0.74777196,\n",
       "        0.74847928, 0.74777196, 0.7476305 , 0.74734757, 0.74734757,\n",
       "        0.74734757, 0.74734757, 0.74734757, 0.74734757, 0.74734757,\n",
       "        0.74734757, 0.74734757, 0.74734757, 0.73942566, 0.74862074,\n",
       "        0.74847928, 0.7476305 , 0.74734757, 0.74734757, 0.74734757,\n",
       "        0.74734757, 0.74734757, 0.74734757, 0.74734757, 0.74734757]),\n",
       " 'split8_test_f1_micro': array([0.59966049, 0.74253784, 0.74748904, 0.75074268, 0.74932805,\n",
       "        0.75159146, 0.7510256 , 0.75116707, 0.75116707, 0.75130853,\n",
       "        0.75130853, 0.75130853, 0.75130853, 0.75130853, 0.75130853,\n",
       "        0.75130853, 0.75130853, 0.75130853, 0.74253784, 0.75074268,\n",
       "        0.75159146, 0.75116707, 0.75130853, 0.75130853, 0.75130853,\n",
       "        0.75130853, 0.75130853, 0.75130853, 0.75130853, 0.75130853]),\n",
       " 'split9_test_f1_micro': array([0.59966049, 0.73645494, 0.74211345, 0.74480124, 0.74381101,\n",
       "        0.74494271, 0.74437686, 0.74423539, 0.74451832, 0.74451832,\n",
       "        0.74451832, 0.74451832, 0.74451832, 0.74451832, 0.74451832,\n",
       "        0.74437686, 0.74451832, 0.74451832, 0.73645494, 0.74480124,\n",
       "        0.74494271, 0.74423539, 0.74451832, 0.74451832, 0.74451832,\n",
       "        0.74451832, 0.74451832, 0.74451832, 0.74451832, 0.74451832]),\n",
       " 'mean_test_f1_micro': array([0.59960109, 0.73859843, 0.74414358, 0.74584113, 0.74608158,\n",
       "        0.74654839, 0.74684545, 0.74673228, 0.74674643, 0.74671814,\n",
       "        0.74671814, 0.74671814, 0.74671814, 0.74671814, 0.74671814,\n",
       "        0.74670399, 0.74671814, 0.74671814, 0.73859843, 0.74585527,\n",
       "        0.74653425, 0.74673228, 0.74673228, 0.74673228, 0.74673228,\n",
       "        0.74673228, 0.74673228, 0.74673228, 0.74671814, 0.74673228]),\n",
       " 'std_test_f1_micro': array([6.25593973e-05, 3.80933182e-03, 3.03569661e-03, 3.03339310e-03,\n",
       "        2.84936828e-03, 3.28952441e-03, 3.04295037e-03, 3.26787170e-03,\n",
       "        3.17503813e-03, 3.23438925e-03, 3.23438925e-03, 3.23438925e-03,\n",
       "        3.23438925e-03, 3.23438925e-03, 3.23438925e-03, 3.24427393e-03,\n",
       "        3.23438925e-03, 3.23438925e-03, 3.80933182e-03, 3.02354677e-03,\n",
       "        3.29912264e-03, 3.26787170e-03, 3.23077048e-03, 3.23077048e-03,\n",
       "        3.23077048e-03, 3.23077048e-03, 3.23077048e-03, 3.23077048e-03,\n",
       "        3.23438925e-03, 3.23077048e-03]),\n",
       " 'rank_test_f1_micro': array([30, 28, 27, 26, 24, 22,  1, 10,  2, 12, 12, 12, 12, 12, 12, 21, 12,\n",
       "        12, 28, 25, 23, 10,  3,  3,  3,  3,  3,  3, 12,  3])}"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_model.cv_results_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "19851888",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([30, 28, 27, 26, 24, 22,  1, 10,  2, 12, 12, 12, 12, 12, 12, 21, 12,\n",
       "       12, 28, 25, 23, 10,  3,  3,  3,  3,  3,  3, 12,  3])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_model.cv_results_['rank_test_accuracy']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "09bdbbb9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'classifier': LogisticRegression(max_iter=10000),\n",
       " 'classifier__C': 0.0001,\n",
       " 'classifier__penalty': 'l1',\n",
       " 'classifier__solver': 'saga'}"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_model.cv_results_['params'][ np.argmin(best_model.cv_results_['mean_test_accuracy'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "ff293d70",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>classifier</th>\n",
       "      <th>C</th>\n",
       "      <th>penalty</th>\n",
       "      <th>solver</th>\n",
       "      <th>score_acc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.599601</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.738598</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.744144</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.745841</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0100</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.746082</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0100</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.746548</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.1000</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.746845</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.1000</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.746732</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.746746</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.746718</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>10.0000</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.746718</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>10.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.746718</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>100.0000</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.746718</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>100.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.746718</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>1000.0000</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.746718</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>1000.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.746704</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>10000.0000</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.746718</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>10000.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.746718</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.738598</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.745855</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0100</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.746534</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.1000</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.746732</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.746732</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>10.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.746732</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>100.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.746732</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>1000.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.746732</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>10000.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.746732</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>NaN</td>\n",
       "      <td>none</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.746732</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>NaN</td>\n",
       "      <td>none</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.746718</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.746732</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                            classifier           C penalty solver  score_acc\n",
       "0   LogisticRegression(max_iter=10000)      0.0001      l1   saga   0.599601\n",
       "1   LogisticRegression(max_iter=10000)      0.0001      l2   saga   0.738598\n",
       "2   LogisticRegression(max_iter=10000)      0.0010      l1   saga   0.744144\n",
       "3   LogisticRegression(max_iter=10000)      0.0010      l2   saga   0.745841\n",
       "4   LogisticRegression(max_iter=10000)      0.0100      l1   saga   0.746082\n",
       "5   LogisticRegression(max_iter=10000)      0.0100      l2   saga   0.746548\n",
       "6   LogisticRegression(max_iter=10000)      0.1000      l1   saga   0.746845\n",
       "7   LogisticRegression(max_iter=10000)      0.1000      l2   saga   0.746732\n",
       "8   LogisticRegression(max_iter=10000)      1.0000      l1   saga   0.746746\n",
       "9   LogisticRegression(max_iter=10000)      1.0000      l2   saga   0.746718\n",
       "10  LogisticRegression(max_iter=10000)     10.0000      l1   saga   0.746718\n",
       "11  LogisticRegression(max_iter=10000)     10.0000      l2   saga   0.746718\n",
       "12  LogisticRegression(max_iter=10000)    100.0000      l1   saga   0.746718\n",
       "13  LogisticRegression(max_iter=10000)    100.0000      l2   saga   0.746718\n",
       "14  LogisticRegression(max_iter=10000)   1000.0000      l1   saga   0.746718\n",
       "15  LogisticRegression(max_iter=10000)   1000.0000      l2   saga   0.746704\n",
       "16  LogisticRegression(max_iter=10000)  10000.0000      l1   saga   0.746718\n",
       "17  LogisticRegression(max_iter=10000)  10000.0000      l2   saga   0.746718\n",
       "18  LogisticRegression(max_iter=10000)      0.0001      l2  lbfgs   0.738598\n",
       "19  LogisticRegression(max_iter=10000)      0.0010      l2  lbfgs   0.745855\n",
       "20  LogisticRegression(max_iter=10000)      0.0100      l2  lbfgs   0.746534\n",
       "21  LogisticRegression(max_iter=10000)      0.1000      l2  lbfgs   0.746732\n",
       "22  LogisticRegression(max_iter=10000)      1.0000      l2  lbfgs   0.746732\n",
       "23  LogisticRegression(max_iter=10000)     10.0000      l2  lbfgs   0.746732\n",
       "24  LogisticRegression(max_iter=10000)    100.0000      l2  lbfgs   0.746732\n",
       "25  LogisticRegression(max_iter=10000)   1000.0000      l2  lbfgs   0.746732\n",
       "26  LogisticRegression(max_iter=10000)  10000.0000      l2  lbfgs   0.746732\n",
       "27  LogisticRegression(max_iter=10000)         NaN    none  lbfgs   0.746732\n",
       "28  LogisticRegression(max_iter=10000)         NaN    none   saga   0.746718\n",
       "29  LogisticRegression(max_iter=10000)         NaN     NaN    NaN   0.746732"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results = pd.DataFrame( best_model.cv_results_['params'] )\n",
    "\n",
    "results['score_acc'] = best_model.cv_results_['mean_test_accuracy']\n",
    "cols = results.columns.to_series().str.split('__').apply(lambda x: x[-1])\n",
    "results.columns = cols\n",
    "\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "69c1c465",
   "metadata": {},
   "outputs": [],
   "source": [
    "acc_list.append(results[0:1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "05385bec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'classifier': LogisticRegression(max_iter=10000),\n",
       " 'classifier__C': 0.0001,\n",
       " 'classifier__penalty': 'l2',\n",
       " 'classifier__solver': 'saga'}"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_model.cv_results_['params'][ np.argmin(best_model.cv_results_['rank_test_precision'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "2ef6f409",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>classifier</th>\n",
       "      <th>C</th>\n",
       "      <th>penalty</th>\n",
       "      <th>solver</th>\n",
       "      <th>score_precision</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.711806</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.700039</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.697754</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0100</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.695360</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0100</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.695721</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.1000</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.695813</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.1000</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.695562</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.695544</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.695462</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>10.0000</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.695462</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>10.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.695462</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>100.0000</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.695462</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>100.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.695462</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>1000.0000</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.695462</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>1000.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.695451</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>10000.0000</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.695462</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>10000.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.695462</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.711806</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.697766</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0100</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.695695</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.1000</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.695562</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.695489</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>10.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.695489</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>100.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.695489</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>1000.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.695489</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>10000.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.695489</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>NaN</td>\n",
       "      <td>none</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.695489</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>NaN</td>\n",
       "      <td>none</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.695462</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.695489</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                            classifier           C penalty solver  \\\n",
       "0   LogisticRegression(max_iter=10000)      0.0001      l1   saga   \n",
       "1   LogisticRegression(max_iter=10000)      0.0001      l2   saga   \n",
       "2   LogisticRegression(max_iter=10000)      0.0010      l1   saga   \n",
       "3   LogisticRegression(max_iter=10000)      0.0010      l2   saga   \n",
       "4   LogisticRegression(max_iter=10000)      0.0100      l1   saga   \n",
       "5   LogisticRegression(max_iter=10000)      0.0100      l2   saga   \n",
       "6   LogisticRegression(max_iter=10000)      0.1000      l1   saga   \n",
       "7   LogisticRegression(max_iter=10000)      0.1000      l2   saga   \n",
       "8   LogisticRegression(max_iter=10000)      1.0000      l1   saga   \n",
       "9   LogisticRegression(max_iter=10000)      1.0000      l2   saga   \n",
       "10  LogisticRegression(max_iter=10000)     10.0000      l1   saga   \n",
       "11  LogisticRegression(max_iter=10000)     10.0000      l2   saga   \n",
       "12  LogisticRegression(max_iter=10000)    100.0000      l1   saga   \n",
       "13  LogisticRegression(max_iter=10000)    100.0000      l2   saga   \n",
       "14  LogisticRegression(max_iter=10000)   1000.0000      l1   saga   \n",
       "15  LogisticRegression(max_iter=10000)   1000.0000      l2   saga   \n",
       "16  LogisticRegression(max_iter=10000)  10000.0000      l1   saga   \n",
       "17  LogisticRegression(max_iter=10000)  10000.0000      l2   saga   \n",
       "18  LogisticRegression(max_iter=10000)      0.0001      l2  lbfgs   \n",
       "19  LogisticRegression(max_iter=10000)      0.0010      l2  lbfgs   \n",
       "20  LogisticRegression(max_iter=10000)      0.0100      l2  lbfgs   \n",
       "21  LogisticRegression(max_iter=10000)      0.1000      l2  lbfgs   \n",
       "22  LogisticRegression(max_iter=10000)      1.0000      l2  lbfgs   \n",
       "23  LogisticRegression(max_iter=10000)     10.0000      l2  lbfgs   \n",
       "24  LogisticRegression(max_iter=10000)    100.0000      l2  lbfgs   \n",
       "25  LogisticRegression(max_iter=10000)   1000.0000      l2  lbfgs   \n",
       "26  LogisticRegression(max_iter=10000)  10000.0000      l2  lbfgs   \n",
       "27  LogisticRegression(max_iter=10000)         NaN    none  lbfgs   \n",
       "28  LogisticRegression(max_iter=10000)         NaN    none   saga   \n",
       "29  LogisticRegression(max_iter=10000)         NaN     NaN    NaN   \n",
       "\n",
       "    score_precision  \n",
       "0          0.000000  \n",
       "1          0.711806  \n",
       "2          0.700039  \n",
       "3          0.697754  \n",
       "4          0.695360  \n",
       "5          0.695721  \n",
       "6          0.695813  \n",
       "7          0.695562  \n",
       "8          0.695544  \n",
       "9          0.695462  \n",
       "10         0.695462  \n",
       "11         0.695462  \n",
       "12         0.695462  \n",
       "13         0.695462  \n",
       "14         0.695462  \n",
       "15         0.695451  \n",
       "16         0.695462  \n",
       "17         0.695462  \n",
       "18         0.711806  \n",
       "19         0.697766  \n",
       "20         0.695695  \n",
       "21         0.695562  \n",
       "22         0.695489  \n",
       "23         0.695489  \n",
       "24         0.695489  \n",
       "25         0.695489  \n",
       "26         0.695489  \n",
       "27         0.695489  \n",
       "28         0.695462  \n",
       "29         0.695489  "
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results = pd.DataFrame( best_model.cv_results_['params'] )\n",
    "\n",
    "results['score_precision'] = best_model.cv_results_['mean_test_precision']\n",
    "cols = results.columns.to_series().str.split('__').apply(lambda x: x[-1])\n",
    "results.columns = cols\n",
    "\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "be514fd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "roc_auc_ovr_List.append(results[1:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "8cb4c8d2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'classifier': LogisticRegression(max_iter=10000),\n",
       " 'classifier__C': 0.1,\n",
       " 'classifier__penalty': 'l1',\n",
       " 'classifier__solver': 'saga'}"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_model.cv_results_['params'][ np.argmin(best_model.cv_results_['rank_test_f1_micro'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "e1fc8e88",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>classifier</th>\n",
       "      <th>C</th>\n",
       "      <th>penalty</th>\n",
       "      <th>solver</th>\n",
       "      <th>score_f1_micro</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.599601</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.738598</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.744144</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.745841</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0100</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.746082</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0100</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.746548</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.1000</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.746845</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.1000</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.746732</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.746746</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.746718</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>10.0000</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.746718</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>10.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.746718</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>100.0000</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.746718</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>100.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.746718</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>1000.0000</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.746718</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>1000.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.746704</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>10000.0000</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.746718</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>10000.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.746718</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.738598</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.745855</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0100</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.746534</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.1000</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.746732</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.746732</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>10.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.746732</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>100.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.746732</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>1000.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.746732</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>10000.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.746732</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>NaN</td>\n",
       "      <td>none</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.746732</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>NaN</td>\n",
       "      <td>none</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.746718</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.746732</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                            classifier           C penalty solver  \\\n",
       "0   LogisticRegression(max_iter=10000)      0.0001      l1   saga   \n",
       "1   LogisticRegression(max_iter=10000)      0.0001      l2   saga   \n",
       "2   LogisticRegression(max_iter=10000)      0.0010      l1   saga   \n",
       "3   LogisticRegression(max_iter=10000)      0.0010      l2   saga   \n",
       "4   LogisticRegression(max_iter=10000)      0.0100      l1   saga   \n",
       "5   LogisticRegression(max_iter=10000)      0.0100      l2   saga   \n",
       "6   LogisticRegression(max_iter=10000)      0.1000      l1   saga   \n",
       "7   LogisticRegression(max_iter=10000)      0.1000      l2   saga   \n",
       "8   LogisticRegression(max_iter=10000)      1.0000      l1   saga   \n",
       "9   LogisticRegression(max_iter=10000)      1.0000      l2   saga   \n",
       "10  LogisticRegression(max_iter=10000)     10.0000      l1   saga   \n",
       "11  LogisticRegression(max_iter=10000)     10.0000      l2   saga   \n",
       "12  LogisticRegression(max_iter=10000)    100.0000      l1   saga   \n",
       "13  LogisticRegression(max_iter=10000)    100.0000      l2   saga   \n",
       "14  LogisticRegression(max_iter=10000)   1000.0000      l1   saga   \n",
       "15  LogisticRegression(max_iter=10000)   1000.0000      l2   saga   \n",
       "16  LogisticRegression(max_iter=10000)  10000.0000      l1   saga   \n",
       "17  LogisticRegression(max_iter=10000)  10000.0000      l2   saga   \n",
       "18  LogisticRegression(max_iter=10000)      0.0001      l2  lbfgs   \n",
       "19  LogisticRegression(max_iter=10000)      0.0010      l2  lbfgs   \n",
       "20  LogisticRegression(max_iter=10000)      0.0100      l2  lbfgs   \n",
       "21  LogisticRegression(max_iter=10000)      0.1000      l2  lbfgs   \n",
       "22  LogisticRegression(max_iter=10000)      1.0000      l2  lbfgs   \n",
       "23  LogisticRegression(max_iter=10000)     10.0000      l2  lbfgs   \n",
       "24  LogisticRegression(max_iter=10000)    100.0000      l2  lbfgs   \n",
       "25  LogisticRegression(max_iter=10000)   1000.0000      l2  lbfgs   \n",
       "26  LogisticRegression(max_iter=10000)  10000.0000      l2  lbfgs   \n",
       "27  LogisticRegression(max_iter=10000)         NaN    none  lbfgs   \n",
       "28  LogisticRegression(max_iter=10000)         NaN    none   saga   \n",
       "29  LogisticRegression(max_iter=10000)         NaN     NaN    NaN   \n",
       "\n",
       "    score_f1_micro  \n",
       "0         0.599601  \n",
       "1         0.738598  \n",
       "2         0.744144  \n",
       "3         0.745841  \n",
       "4         0.746082  \n",
       "5         0.746548  \n",
       "6         0.746845  \n",
       "7         0.746732  \n",
       "8         0.746746  \n",
       "9         0.746718  \n",
       "10        0.746718  \n",
       "11        0.746718  \n",
       "12        0.746718  \n",
       "13        0.746718  \n",
       "14        0.746718  \n",
       "15        0.746704  \n",
       "16        0.746718  \n",
       "17        0.746718  \n",
       "18        0.738598  \n",
       "19        0.745855  \n",
       "20        0.746534  \n",
       "21        0.746732  \n",
       "22        0.746732  \n",
       "23        0.746732  \n",
       "24        0.746732  \n",
       "25        0.746732  \n",
       "26        0.746732  \n",
       "27        0.746732  \n",
       "28        0.746718  \n",
       "29        0.746732  "
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results = pd.DataFrame( best_model.cv_results_['params'] )\n",
    "\n",
    "results['score_f1_micro'] = best_model.cv_results_['mean_test_f1_micro']\n",
    "cols = results.columns.to_series().str.split('__').apply(lambda x: x[-1])\n",
    "results.columns = cols\n",
    "\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "a8c83f8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "f1_micro_List.append(results[6:7])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cd3a031",
   "metadata": {},
   "source": [
    "## Trial 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "f0c67633",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, Y_train, Y_test = train_test_split(X, y, test_size=0.2, shuffle=True, random_state = 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "382815ad",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Guest-All\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\Guest-All\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\Guest-All\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\Guest-All\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\Guest-All\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\Guest-All\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\Guest-All\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\Guest-All\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\Guest-All\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\Guest-All\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\Guest-All\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1183: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Guest-All\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1183: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Guest-All\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1183: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Guest-All\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1183: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Guest-All\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1183: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Guest-All\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1183: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Guest-All\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1183: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Guest-All\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1183: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Guest-All\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1183: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Guest-All\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1183: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Guest-All\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1183: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Guest-All\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1183: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Guest-All\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1183: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Guest-All\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1183: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Guest-All\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1183: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Guest-All\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1183: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Guest-All\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1183: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Guest-All\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1183: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Guest-All\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1183: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Guest-All\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1183: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Create a pipeline - RF is a stand in, we will populate the classifier part below\n",
    "pipe = Pipeline([('std', StandardScaler()),\n",
    "                 ('classifier', LogisticRegression())])\n",
    "\n",
    "# Create search space of candidate learning algorithms and their hyperparameters\n",
    "# note lbfgs can't do l1, and if you pass penalty='none' it expects no C value\n",
    "search_space = [{'classifier': [LogisticRegression(max_iter=10000)],\n",
    "                 'classifier__solver': ['saga'],\n",
    "                 'classifier__penalty': ['l1', 'l2'],\n",
    "                 'classifier__C': np.logspace(-4, 4, 9)},\n",
    "                {'classifier': [LogisticRegression(max_iter=10000)],\n",
    "                 'classifier__solver': ['lbfgs'],\n",
    "                 'classifier__penalty': ['l2'],\n",
    "                 'classifier__C': np.logspace(-4, 4, 9)},\n",
    "                {'classifier': [LogisticRegression(max_iter=10000)],\n",
    "                 'classifier__solver': ['lbfgs','saga'],\n",
    "                 'classifier__penalty': ['none']},\n",
    "                {'classifier': [LogisticRegression(max_iter=10000)]}\n",
    "                ]\n",
    "\n",
    "# Create grid search\n",
    "clf = GridSearchCV(pipe, search_space, cv=StratifiedKFold(n_splits=10),\n",
    "                   scoring=['accuracy', 'precision', 'f1_micro'], refit=False,\n",
    "                   verbose=0)\n",
    "\n",
    "# Fit grid search\n",
    "best_model = clf.fit(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "befaa8ab",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'mean_fit_time': array([0.74817042, 0.83071849, 0.92255409, 0.82142038, 0.94145184,\n",
       "        0.73918786, 0.8548209 , 0.78852119, 0.93946428, 0.89510469,\n",
       "        0.85916581, 0.72617605, 0.80873239, 0.7384202 , 0.85223684,\n",
       "        0.7268471 , 0.83755894, 0.73307202, 0.092466  , 0.10507827,\n",
       "        0.11189511, 0.10659344, 0.10310664, 0.10507123, 0.10861759,\n",
       "        0.10577168, 0.10335834, 0.10896537, 0.66941097, 0.10435119]),\n",
       " 'std_fit_time': array([0.09641451, 0.19576579, 0.2168164 , 0.20265125, 0.1898917 ,\n",
       "        0.11603264, 0.21304173, 0.18510777, 0.21696737, 0.13338066,\n",
       "        0.14125117, 0.05174776, 0.06571516, 0.04711891, 0.04958419,\n",
       "        0.06313497, 0.08856912, 0.07403344, 0.00538718, 0.00497292,\n",
       "        0.01822604, 0.00518751, 0.00474961, 0.00865724, 0.00812979,\n",
       "        0.00742767, 0.00487715, 0.0103398 , 0.05389684, 0.00552636]),\n",
       " 'mean_score_time': array([0.01286085, 0.01367836, 0.01758161, 0.01593397, 0.01870573,\n",
       "        0.01962535, 0.01059575, 0.01865206, 0.01605554, 0.01572831,\n",
       "        0.00889137, 0.01343741, 0.01096196, 0.01313806, 0.01334236,\n",
       "        0.01368287, 0.01124554, 0.01382372, 0.0101984 , 0.01010003,\n",
       "        0.0139775 , 0.01409228, 0.01282747, 0.01182768, 0.01170945,\n",
       "        0.01324115, 0.01318958, 0.01089203, 0.01139426, 0.01349416]),\n",
       " 'std_score_time': array([0.00389911, 0.00728806, 0.01086602, 0.00634311, 0.00610739,\n",
       "        0.01806364, 0.00325853, 0.01325758, 0.00951637, 0.00925127,\n",
       "        0.00237626, 0.00355002, 0.00367036, 0.00346654, 0.00362544,\n",
       "        0.00367724, 0.00387655, 0.00323809, 0.0037525 , 0.00325656,\n",
       "        0.00405881, 0.00320216, 0.00389063, 0.00420627, 0.00370701,\n",
       "        0.00424987, 0.00431161, 0.00427618, 0.00375891, 0.00371615]),\n",
       " 'param_classifier': masked_array(data=[LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000)],\n",
       "              mask=[False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'param_classifier__C': masked_array(data=[0.0001, 0.0001, 0.001, 0.001, 0.01, 0.01, 0.1, 0.1,\n",
       "                    1.0, 1.0, 10.0, 10.0, 100.0, 100.0, 1000.0, 1000.0,\n",
       "                    10000.0, 10000.0, 0.0001, 0.001, 0.01, 0.1, 1.0, 10.0,\n",
       "                    100.0, 1000.0, 10000.0, --, --, --],\n",
       "              mask=[False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False,  True,  True,  True],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'param_classifier__penalty': masked_array(data=['l1', 'l2', 'l1', 'l2', 'l1', 'l2', 'l1', 'l2', 'l1',\n",
       "                    'l2', 'l1', 'l2', 'l1', 'l2', 'l1', 'l2', 'l1', 'l2',\n",
       "                    'l2', 'l2', 'l2', 'l2', 'l2', 'l2', 'l2', 'l2', 'l2',\n",
       "                    'none', 'none', --],\n",
       "              mask=[False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False,  True],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'param_classifier__solver': masked_array(data=['saga', 'saga', 'saga', 'saga', 'saga', 'saga', 'saga',\n",
       "                    'saga', 'saga', 'saga', 'saga', 'saga', 'saga', 'saga',\n",
       "                    'saga', 'saga', 'saga', 'saga', 'lbfgs', 'lbfgs',\n",
       "                    'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs',\n",
       "                    'lbfgs', 'lbfgs', 'saga', --],\n",
       "              mask=[False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False,  True],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'params': [{'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 0.0001,\n",
       "   'classifier__penalty': 'l1',\n",
       "   'classifier__solver': 'saga'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 0.0001,\n",
       "   'classifier__penalty': 'l2',\n",
       "   'classifier__solver': 'saga'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 0.001,\n",
       "   'classifier__penalty': 'l1',\n",
       "   'classifier__solver': 'saga'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 0.001,\n",
       "   'classifier__penalty': 'l2',\n",
       "   'classifier__solver': 'saga'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 0.01,\n",
       "   'classifier__penalty': 'l1',\n",
       "   'classifier__solver': 'saga'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 0.01,\n",
       "   'classifier__penalty': 'l2',\n",
       "   'classifier__solver': 'saga'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 0.1,\n",
       "   'classifier__penalty': 'l1',\n",
       "   'classifier__solver': 'saga'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 0.1,\n",
       "   'classifier__penalty': 'l2',\n",
       "   'classifier__solver': 'saga'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 1.0,\n",
       "   'classifier__penalty': 'l1',\n",
       "   'classifier__solver': 'saga'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 1.0,\n",
       "   'classifier__penalty': 'l2',\n",
       "   'classifier__solver': 'saga'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 10.0,\n",
       "   'classifier__penalty': 'l1',\n",
       "   'classifier__solver': 'saga'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 10.0,\n",
       "   'classifier__penalty': 'l2',\n",
       "   'classifier__solver': 'saga'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 100.0,\n",
       "   'classifier__penalty': 'l1',\n",
       "   'classifier__solver': 'saga'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 100.0,\n",
       "   'classifier__penalty': 'l2',\n",
       "   'classifier__solver': 'saga'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 1000.0,\n",
       "   'classifier__penalty': 'l1',\n",
       "   'classifier__solver': 'saga'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 1000.0,\n",
       "   'classifier__penalty': 'l2',\n",
       "   'classifier__solver': 'saga'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 10000.0,\n",
       "   'classifier__penalty': 'l1',\n",
       "   'classifier__solver': 'saga'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 10000.0,\n",
       "   'classifier__penalty': 'l2',\n",
       "   'classifier__solver': 'saga'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 0.0001,\n",
       "   'classifier__penalty': 'l2',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 0.001,\n",
       "   'classifier__penalty': 'l2',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 0.01,\n",
       "   'classifier__penalty': 'l2',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 0.1,\n",
       "   'classifier__penalty': 'l2',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 1.0,\n",
       "   'classifier__penalty': 'l2',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 10.0,\n",
       "   'classifier__penalty': 'l2',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 100.0,\n",
       "   'classifier__penalty': 'l2',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 1000.0,\n",
       "   'classifier__penalty': 'l2',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 10000.0,\n",
       "   'classifier__penalty': 'l2',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__penalty': 'none',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__penalty': 'none',\n",
       "   'classifier__solver': 'saga'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000)}],\n",
       " 'split0_test_accuracy': array([0.59929279, 0.74441301, 0.74865629, 0.75289958, 0.75417256,\n",
       "        0.75516266, 0.75544554, 0.7553041 , 0.75558699, 0.75558699,\n",
       "        0.75558699, 0.75558699, 0.75558699, 0.75558699, 0.75558699,\n",
       "        0.75558699, 0.75558699, 0.75558699, 0.74441301, 0.75289958,\n",
       "        0.75516266, 0.7553041 , 0.75558699, 0.75558699, 0.75558699,\n",
       "        0.75558699, 0.75558699, 0.75558699, 0.75558699, 0.75558699]),\n",
       " 'split1_test_accuracy': array([0.59929279, 0.73055163, 0.7407355 , 0.74257426, 0.74441301,\n",
       "        0.74512023, 0.74554455, 0.74540311, 0.745686  , 0.74540311,\n",
       "        0.74526167, 0.74526167, 0.74526167, 0.74526167, 0.74526167,\n",
       "        0.74526167, 0.74526167, 0.74526167, 0.73055163, 0.74257426,\n",
       "        0.74526167, 0.74540311, 0.74540311, 0.74526167, 0.74526167,\n",
       "        0.74526167, 0.74526167, 0.74526167, 0.74540311, 0.74540311]),\n",
       " 'split2_test_accuracy': array([0.5992361 , 0.73645494, 0.74366954, 0.7453671 , 0.74437686,\n",
       "        0.74565002, 0.74550856, 0.74522563, 0.74522563, 0.74522563,\n",
       "        0.74522563, 0.74522563, 0.74522563, 0.74522563, 0.74522563,\n",
       "        0.74522563, 0.74522563, 0.74522563, 0.73645494, 0.7453671 ,\n",
       "        0.74565002, 0.74522563, 0.74522563, 0.74522563, 0.74522563,\n",
       "        0.74522563, 0.74522563, 0.74522563, 0.74522563, 0.74522563]),\n",
       " 'split3_test_accuracy': array([0.5992361 , 0.74069883, 0.74197199, 0.74890366, 0.74918659,\n",
       "        0.74862074, 0.74833781, 0.74805489, 0.74791342, 0.74791342,\n",
       "        0.74791342, 0.74791342, 0.74791342, 0.74791342, 0.74791342,\n",
       "        0.74791342, 0.74791342, 0.74791342, 0.74069883, 0.74890366,\n",
       "        0.74862074, 0.74805489, 0.74791342, 0.74791342, 0.74791342,\n",
       "        0.74791342, 0.74791342, 0.74791342, 0.74791342, 0.74791342]),\n",
       " 'split4_test_accuracy': array([0.59937756, 0.74296223, 0.74494271, 0.74904513, 0.74961098,\n",
       "        0.74932805, 0.75003537, 0.75031829, 0.75031829, 0.75031829,\n",
       "        0.75031829, 0.75031829, 0.75031829, 0.75031829, 0.75031829,\n",
       "        0.75031829, 0.75031829, 0.75031829, 0.74296223, 0.74904513,\n",
       "        0.74932805, 0.75031829, 0.75031829, 0.75031829, 0.75031829,\n",
       "        0.75031829, 0.75031829, 0.75031829, 0.75031829, 0.75031829]),\n",
       " 'split5_test_accuracy': array([0.59937756, 0.74239638, 0.74409393, 0.74522563, 0.7464988 ,\n",
       "        0.74734757, 0.74734757, 0.74805489, 0.74805489, 0.74805489,\n",
       "        0.74819635, 0.74819635, 0.74819635, 0.74819635, 0.74819635,\n",
       "        0.74819635, 0.74819635, 0.74819635, 0.74239638, 0.74522563,\n",
       "        0.74720611, 0.74791342, 0.74819635, 0.74819635, 0.74819635,\n",
       "        0.74819635, 0.74819635, 0.74819635, 0.74819635, 0.74819635]),\n",
       " 'split6_test_accuracy': array([0.59937756, 0.73376715, 0.74069883, 0.74423539, 0.74253784,\n",
       "        0.74310369, 0.7426793 , 0.74324515, 0.74310369, 0.74324515,\n",
       "        0.74324515, 0.74324515, 0.74324515, 0.74324515, 0.74324515,\n",
       "        0.74324515, 0.74324515, 0.74324515, 0.73376715, 0.74423539,\n",
       "        0.74310369, 0.74324515, 0.74324515, 0.74324515, 0.74324515,\n",
       "        0.74324515, 0.74324515, 0.74324515, 0.74324515, 0.74324515]),\n",
       " 'split7_test_accuracy': array([0.59937756, 0.73942566, 0.74338662, 0.74678172, 0.74593295,\n",
       "        0.7464988 , 0.74607441, 0.74607441, 0.74593295, 0.74593295,\n",
       "        0.74593295, 0.74593295, 0.74593295, 0.74593295, 0.74593295,\n",
       "        0.74593295, 0.74593295, 0.74593295, 0.73942566, 0.74678172,\n",
       "        0.74635733, 0.74607441, 0.74593295, 0.74593295, 0.74593295,\n",
       "        0.74593295, 0.74593295, 0.74593295, 0.74593295, 0.74593295]),\n",
       " 'split8_test_accuracy': array([0.59937756, 0.7464988 , 0.74932805, 0.7510256 , 0.75442071,\n",
       "        0.75498656, 0.75456217, 0.75498656, 0.75498656, 0.75498656,\n",
       "        0.75498656, 0.75498656, 0.75498656, 0.75498656, 0.75498656,\n",
       "        0.75498656, 0.75498656, 0.75498656, 0.7464988 , 0.7510256 ,\n",
       "        0.75498656, 0.75498656, 0.75498656, 0.75498656, 0.75498656,\n",
       "        0.75498656, 0.75498656, 0.75498656, 0.75498656, 0.75498656]),\n",
       " 'split9_test_accuracy': array([0.59937756, 0.73518178, 0.73914274, 0.73786957, 0.73885981,\n",
       "        0.73956712, 0.73970859, 0.73956712, 0.73970859, 0.73970859,\n",
       "        0.73970859, 0.73970859, 0.73970859, 0.73970859, 0.73970859,\n",
       "        0.73970859, 0.73970859, 0.73970859, 0.73518178, 0.73786957,\n",
       "        0.73956712, 0.73956712, 0.73970859, 0.73970859, 0.73970859,\n",
       "        0.73970859, 0.73970859, 0.73970859, 0.73970859, 0.73970859]),\n",
       " 'mean_test_accuracy': array([0.59933232, 0.73923504, 0.74366262, 0.74639276, 0.74700101,\n",
       "        0.74753854, 0.74752439, 0.74762342, 0.7476517 , 0.74763756,\n",
       "        0.74763756, 0.74763756, 0.74763756, 0.74763756, 0.74763756,\n",
       "        0.74763756, 0.74763756, 0.74763756, 0.73923504, 0.74639276,\n",
       "        0.7475244 , 0.74760927, 0.7476517 , 0.74763756, 0.74763756,\n",
       "        0.74763756, 0.74763756, 0.74763756, 0.7476517 , 0.7476517 ]),\n",
       " 'std_test_accuracy': array([5.82443238e-05, 4.84866889e-03, 3.15554206e-03, 4.14230596e-03,\n",
       "        4.68238893e-03, 4.60127526e-03, 4.62515302e-03, 4.66899425e-03,\n",
       "        4.69672293e-03, 4.69590768e-03, 4.70431369e-03, 4.70431369e-03,\n",
       "        4.70431369e-03, 4.70431369e-03, 4.70431369e-03, 4.70431369e-03,\n",
       "        4.70431369e-03, 4.70431369e-03, 4.84866889e-03, 4.14230596e-03,\n",
       "        4.59825470e-03, 4.66787970e-03, 4.69735641e-03, 4.70431369e-03,\n",
       "        4.70431369e-03, 4.70431369e-03, 4.70431369e-03, 4.70431369e-03,\n",
       "        4.69735641e-03, 4.69735641e-03]),\n",
       " 'rank_test_accuracy': array([30, 28, 27, 25, 24, 21, 23, 19,  4, 18,  5,  5,  5,  5,  5,  5,  5,\n",
       "         5, 28, 25, 22, 20,  1,  5,  5,  5,  5,  5,  1,  1]),\n",
       " 'split0_test_precision': array([0.        , 0.72401747, 0.70657277, 0.70900693, 0.7076223 ,\n",
       "        0.70871212, 0.70877458, 0.7086644 , 0.70888469, 0.70888469,\n",
       "        0.70888469, 0.70888469, 0.70888469, 0.70888469, 0.70888469,\n",
       "        0.70888469, 0.70888469, 0.70888469, 0.72401747, 0.70900693,\n",
       "        0.70871212, 0.7086644 , 0.70888469, 0.70888469, 0.70888469,\n",
       "        0.70888469, 0.70888469, 0.70888469, 0.70888469, 0.70888469]),\n",
       " 'split1_test_precision': array([0.        , 0.70386643, 0.6973165 , 0.69669903, 0.69565217,\n",
       "        0.69698128, 0.69702744, 0.69691193, 0.69699277, 0.6967619 ,\n",
       "        0.69649657, 0.69649657, 0.69649657, 0.69649657, 0.69649657,\n",
       "        0.69649657, 0.69649657, 0.69649657, 0.70386643, 0.69669903,\n",
       "        0.69709702, 0.69691193, 0.6967619 , 0.69649657, 0.69649657,\n",
       "        0.69649657, 0.69649657, 0.69649657, 0.6967619 , 0.6967619 ]),\n",
       " 'split2_test_precision': array([0.        , 0.70620748, 0.69315172, 0.69380863, 0.6894387 ,\n",
       "        0.6910299 , 0.69077491, 0.6904059 , 0.6904059 , 0.6904059 ,\n",
       "        0.6904059 , 0.6904059 , 0.6904059 , 0.6904059 , 0.6904059 ,\n",
       "        0.6904059 , 0.6904059 , 0.6904059 , 0.70620748, 0.69380863,\n",
       "        0.6910299 , 0.6904059 , 0.6904059 , 0.6904059 , 0.6904059 ,\n",
       "        0.6904059 , 0.6904059 , 0.6904059 , 0.6904059 , 0.6904059 ]),\n",
       " 'split3_test_precision': array([0.        , 0.7179599 , 0.70155813, 0.70472136, 0.70244461,\n",
       "        0.70198929, 0.7012987 , 0.70091673, 0.70080245, 0.70080245,\n",
       "        0.70080245, 0.70080245, 0.70080245, 0.70080245, 0.70080245,\n",
       "        0.70080245, 0.70080245, 0.70080245, 0.7179599 , 0.70472136,\n",
       "        0.70198929, 0.70091673, 0.70080245, 0.70080245, 0.70080245,\n",
       "        0.70080245, 0.70080245, 0.70080245, 0.70080245, 0.70080245]),\n",
       " 'split4_test_precision': array([0.        , 0.71568211, 0.69949593, 0.69992441, 0.69857891,\n",
       "        0.69820494, 0.69861992, 0.69914147, 0.69899291, 0.69899291,\n",
       "        0.69899291, 0.69899291, 0.69899291, 0.69899291, 0.69899291,\n",
       "        0.69899291, 0.69899291, 0.69899291, 0.71568211, 0.69992441,\n",
       "        0.69820494, 0.69914147, 0.69899291, 0.69899291, 0.69899291,\n",
       "        0.69899291, 0.69899291, 0.69899291, 0.69899291, 0.69899291]),\n",
       " 'split5_test_precision': array([0.        , 0.716118  , 0.69925984, 0.69713193, 0.69667171,\n",
       "        0.69720965, 0.69735849, 0.69807765, 0.69807765, 0.69807765,\n",
       "        0.69819141, 0.69819141, 0.69819141, 0.69819141, 0.69819141,\n",
       "        0.69819141, 0.69819141, 0.69819141, 0.716118  , 0.69713193,\n",
       "        0.69709544, 0.69781462, 0.69819141, 0.69819141, 0.69819141,\n",
       "        0.69819141, 0.69819141, 0.69819141, 0.69819141, 0.69819141]),\n",
       " 'split6_test_precision': array([0.        , 0.70598439, 0.69860835, 0.69922179, 0.69386973,\n",
       "        0.69463602, 0.6938385 , 0.69445507, 0.6941896 , 0.69445507,\n",
       "        0.69445507, 0.69445507, 0.69445507, 0.69445507, 0.69445507,\n",
       "        0.69445507, 0.69445507, 0.69445507, 0.70598439, 0.69922179,\n",
       "        0.69463602, 0.69445507, 0.69445507, 0.69445507, 0.69445507,\n",
       "        0.69445507, 0.69445507, 0.69445507, 0.69445507, 0.69445507]),\n",
       " 'split7_test_precision': array([0.        , 0.71208226, 0.697134  , 0.69734848, 0.69299553,\n",
       "        0.69345238, 0.69296613, 0.6926793 , 0.69256506, 0.69256506,\n",
       "        0.69256506, 0.69256506, 0.69256506, 0.69256506, 0.69256506,\n",
       "        0.69256506, 0.69256506, 0.69256506, 0.71208226, 0.69734848,\n",
       "        0.6931945 , 0.6926793 , 0.69256506, 0.69256506, 0.69256506,\n",
       "        0.69256506, 0.69256506, 0.69256506, 0.69256506, 0.69256506]),\n",
       " 'split8_test_precision': array([0.        , 0.71978022, 0.70416025, 0.70150376, 0.70236337,\n",
       "        0.70265291, 0.70113678, 0.70176082, 0.70176082, 0.70176082,\n",
       "        0.70176082, 0.70176082, 0.70176082, 0.70176082, 0.70176082,\n",
       "        0.70176082, 0.70176082, 0.70176082, 0.71978022, 0.70150376,\n",
       "        0.70265291, 0.70176082, 0.70176082, 0.70176082, 0.70176082,\n",
       "        0.70176082, 0.70176082, 0.70176082, 0.70176082, 0.70176082]),\n",
       " 'split9_test_precision': array([0.        , 0.7027027 , 0.69311962, 0.68605093, 0.68561747,\n",
       "        0.68606834, 0.68618619, 0.68592871, 0.68618619, 0.68618619,\n",
       "        0.68618619, 0.68618619, 0.68618619, 0.68618619, 0.68618619,\n",
       "        0.68618619, 0.68618619, 0.68618619, 0.7027027 , 0.68605093,\n",
       "        0.68606834, 0.68592871, 0.68618619, 0.68618619, 0.68618619,\n",
       "        0.68618619, 0.68618619, 0.68618619, 0.68618619, 0.68618619]),\n",
       " 'mean_test_precision': array([0.        , 0.7124401 , 0.69903771, 0.69854173, 0.69652545,\n",
       "        0.69709368, 0.69679816, 0.6968942 , 0.6968858 , 0.69688926,\n",
       "        0.69687411, 0.69687411, 0.69687411, 0.69687411, 0.69687411,\n",
       "        0.69687411, 0.69687411, 0.69687411, 0.7124401 , 0.69854173,\n",
       "        0.69706805, 0.69686789, 0.69690064, 0.69687411, 0.69687411,\n",
       "        0.69687411, 0.69687411, 0.69687411, 0.69690064, 0.69690064]),\n",
       " 'std_test_precision': array([0.        , 0.00700994, 0.00407516, 0.00586588, 0.00621271,\n",
       "        0.00607246, 0.00598779, 0.00609022, 0.00609393, 0.00608279,\n",
       "        0.00608623, 0.00608623, 0.00608623, 0.00608623, 0.00608623,\n",
       "        0.00608623, 0.00608623, 0.00608623, 0.00700994, 0.00586588,\n",
       "        0.00608818, 0.00608562, 0.00608511, 0.00608623, 0.00608623,\n",
       "        0.00608623, 0.00608623, 0.00608623, 0.00608511, 0.00608511]),\n",
       " 'rank_test_precision': array([30,  1,  3,  4, 29,  6, 28, 11, 13, 12, 14, 14, 14, 14, 14, 14, 14,\n",
       "        14,  1,  4,  7, 27,  8, 14, 14, 14, 14, 14,  8,  8]),\n",
       " 'split0_test_f1_micro': array([0.59929279, 0.74441301, 0.74865629, 0.75289958, 0.75417256,\n",
       "        0.75516266, 0.75544554, 0.7553041 , 0.75558699, 0.75558699,\n",
       "        0.75558699, 0.75558699, 0.75558699, 0.75558699, 0.75558699,\n",
       "        0.75558699, 0.75558699, 0.75558699, 0.74441301, 0.75289958,\n",
       "        0.75516266, 0.7553041 , 0.75558699, 0.75558699, 0.75558699,\n",
       "        0.75558699, 0.75558699, 0.75558699, 0.75558699, 0.75558699]),\n",
       " 'split1_test_f1_micro': array([0.59929279, 0.73055163, 0.7407355 , 0.74257426, 0.74441301,\n",
       "        0.74512023, 0.74554455, 0.74540311, 0.745686  , 0.74540311,\n",
       "        0.74526167, 0.74526167, 0.74526167, 0.74526167, 0.74526167,\n",
       "        0.74526167, 0.74526167, 0.74526167, 0.73055163, 0.74257426,\n",
       "        0.74526167, 0.74540311, 0.74540311, 0.74526167, 0.74526167,\n",
       "        0.74526167, 0.74526167, 0.74526167, 0.74540311, 0.74540311]),\n",
       " 'split2_test_f1_micro': array([0.5992361 , 0.73645494, 0.74366954, 0.7453671 , 0.74437686,\n",
       "        0.74565002, 0.74550856, 0.74522563, 0.74522563, 0.74522563,\n",
       "        0.74522563, 0.74522563, 0.74522563, 0.74522563, 0.74522563,\n",
       "        0.74522563, 0.74522563, 0.74522563, 0.73645494, 0.7453671 ,\n",
       "        0.74565002, 0.74522563, 0.74522563, 0.74522563, 0.74522563,\n",
       "        0.74522563, 0.74522563, 0.74522563, 0.74522563, 0.74522563]),\n",
       " 'split3_test_f1_micro': array([0.5992361 , 0.74069883, 0.74197199, 0.74890366, 0.74918659,\n",
       "        0.74862074, 0.74833781, 0.74805489, 0.74791342, 0.74791342,\n",
       "        0.74791342, 0.74791342, 0.74791342, 0.74791342, 0.74791342,\n",
       "        0.74791342, 0.74791342, 0.74791342, 0.74069883, 0.74890366,\n",
       "        0.74862074, 0.74805489, 0.74791342, 0.74791342, 0.74791342,\n",
       "        0.74791342, 0.74791342, 0.74791342, 0.74791342, 0.74791342]),\n",
       " 'split4_test_f1_micro': array([0.59937756, 0.74296223, 0.74494271, 0.74904513, 0.74961098,\n",
       "        0.74932805, 0.75003537, 0.75031829, 0.75031829, 0.75031829,\n",
       "        0.75031829, 0.75031829, 0.75031829, 0.75031829, 0.75031829,\n",
       "        0.75031829, 0.75031829, 0.75031829, 0.74296223, 0.74904513,\n",
       "        0.74932805, 0.75031829, 0.75031829, 0.75031829, 0.75031829,\n",
       "        0.75031829, 0.75031829, 0.75031829, 0.75031829, 0.75031829]),\n",
       " 'split5_test_f1_micro': array([0.59937756, 0.74239638, 0.74409393, 0.74522563, 0.7464988 ,\n",
       "        0.74734757, 0.74734757, 0.74805489, 0.74805489, 0.74805489,\n",
       "        0.74819635, 0.74819635, 0.74819635, 0.74819635, 0.74819635,\n",
       "        0.74819635, 0.74819635, 0.74819635, 0.74239638, 0.74522563,\n",
       "        0.74720611, 0.74791342, 0.74819635, 0.74819635, 0.74819635,\n",
       "        0.74819635, 0.74819635, 0.74819635, 0.74819635, 0.74819635]),\n",
       " 'split6_test_f1_micro': array([0.59937756, 0.73376715, 0.74069883, 0.74423539, 0.74253784,\n",
       "        0.74310369, 0.7426793 , 0.74324515, 0.74310369, 0.74324515,\n",
       "        0.74324515, 0.74324515, 0.74324515, 0.74324515, 0.74324515,\n",
       "        0.74324515, 0.74324515, 0.74324515, 0.73376715, 0.74423539,\n",
       "        0.74310369, 0.74324515, 0.74324515, 0.74324515, 0.74324515,\n",
       "        0.74324515, 0.74324515, 0.74324515, 0.74324515, 0.74324515]),\n",
       " 'split7_test_f1_micro': array([0.59937756, 0.73942566, 0.74338662, 0.74678172, 0.74593295,\n",
       "        0.7464988 , 0.74607441, 0.74607441, 0.74593295, 0.74593295,\n",
       "        0.74593295, 0.74593295, 0.74593295, 0.74593295, 0.74593295,\n",
       "        0.74593295, 0.74593295, 0.74593295, 0.73942566, 0.74678172,\n",
       "        0.74635733, 0.74607441, 0.74593295, 0.74593295, 0.74593295,\n",
       "        0.74593295, 0.74593295, 0.74593295, 0.74593295, 0.74593295]),\n",
       " 'split8_test_f1_micro': array([0.59937756, 0.7464988 , 0.74932805, 0.7510256 , 0.75442071,\n",
       "        0.75498656, 0.75456217, 0.75498656, 0.75498656, 0.75498656,\n",
       "        0.75498656, 0.75498656, 0.75498656, 0.75498656, 0.75498656,\n",
       "        0.75498656, 0.75498656, 0.75498656, 0.7464988 , 0.7510256 ,\n",
       "        0.75498656, 0.75498656, 0.75498656, 0.75498656, 0.75498656,\n",
       "        0.75498656, 0.75498656, 0.75498656, 0.75498656, 0.75498656]),\n",
       " 'split9_test_f1_micro': array([0.59937756, 0.73518178, 0.73914274, 0.73786957, 0.73885981,\n",
       "        0.73956712, 0.73970859, 0.73956712, 0.73970859, 0.73970859,\n",
       "        0.73970859, 0.73970859, 0.73970859, 0.73970859, 0.73970859,\n",
       "        0.73970859, 0.73970859, 0.73970859, 0.73518178, 0.73786957,\n",
       "        0.73956712, 0.73956712, 0.73970859, 0.73970859, 0.73970859,\n",
       "        0.73970859, 0.73970859, 0.73970859, 0.73970859, 0.73970859]),\n",
       " 'mean_test_f1_micro': array([0.59933232, 0.73923504, 0.74366262, 0.74639276, 0.74700101,\n",
       "        0.74753854, 0.74752439, 0.74762342, 0.7476517 , 0.74763756,\n",
       "        0.74763756, 0.74763756, 0.74763756, 0.74763756, 0.74763756,\n",
       "        0.74763756, 0.74763756, 0.74763756, 0.73923504, 0.74639276,\n",
       "        0.7475244 , 0.74760927, 0.7476517 , 0.74763756, 0.74763756,\n",
       "        0.74763756, 0.74763756, 0.74763756, 0.7476517 , 0.7476517 ]),\n",
       " 'std_test_f1_micro': array([5.82443238e-05, 4.84866889e-03, 3.15554206e-03, 4.14230596e-03,\n",
       "        4.68238893e-03, 4.60127526e-03, 4.62515302e-03, 4.66899425e-03,\n",
       "        4.69672293e-03, 4.69590768e-03, 4.70431369e-03, 4.70431369e-03,\n",
       "        4.70431369e-03, 4.70431369e-03, 4.70431369e-03, 4.70431369e-03,\n",
       "        4.70431369e-03, 4.70431369e-03, 4.84866889e-03, 4.14230596e-03,\n",
       "        4.59825470e-03, 4.66787970e-03, 4.69735641e-03, 4.70431369e-03,\n",
       "        4.70431369e-03, 4.70431369e-03, 4.70431369e-03, 4.70431369e-03,\n",
       "        4.69735641e-03, 4.69735641e-03]),\n",
       " 'rank_test_f1_micro': array([30, 28, 27, 25, 24, 21, 23, 19,  4, 18,  5,  5,  5,  5,  5,  5,  5,\n",
       "         5, 28, 25, 22, 20,  1,  5,  5,  5,  5,  5,  1,  1])}"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_model.cv_results_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "9aa08645",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([30, 28, 27, 25, 24, 21, 23, 19,  4, 18,  5,  5,  5,  5,  5,  5,  5,\n",
       "        5, 28, 25, 22, 20,  1,  5,  5,  5,  5,  5,  1,  1])"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_model.cv_results_['rank_test_accuracy']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "abdb8b79",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'classifier': LogisticRegression(max_iter=10000),\n",
       " 'classifier__C': 0.0001,\n",
       " 'classifier__penalty': 'l1',\n",
       " 'classifier__solver': 'saga'}"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_model.cv_results_['params'][ np.argmin(best_model.cv_results_['mean_test_accuracy'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "f4602352",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>classifier</th>\n",
       "      <th>C</th>\n",
       "      <th>penalty</th>\n",
       "      <th>solver</th>\n",
       "      <th>score_acc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.599332</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.739235</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.743663</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.746393</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0100</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.747001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0100</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.747539</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.1000</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.747524</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.1000</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.747623</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.747652</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.747638</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>10.0000</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.747638</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>10.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.747638</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>100.0000</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.747638</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>100.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.747638</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>1000.0000</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.747638</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>1000.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.747638</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>10000.0000</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.747638</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>10000.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.747638</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.739235</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.746393</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0100</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.747524</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.1000</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.747609</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.747652</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>10.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.747638</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>100.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.747638</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>1000.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.747638</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>10000.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.747638</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>NaN</td>\n",
       "      <td>none</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.747638</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>NaN</td>\n",
       "      <td>none</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.747652</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.747652</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                            classifier           C penalty solver  score_acc\n",
       "0   LogisticRegression(max_iter=10000)      0.0001      l1   saga   0.599332\n",
       "1   LogisticRegression(max_iter=10000)      0.0001      l2   saga   0.739235\n",
       "2   LogisticRegression(max_iter=10000)      0.0010      l1   saga   0.743663\n",
       "3   LogisticRegression(max_iter=10000)      0.0010      l2   saga   0.746393\n",
       "4   LogisticRegression(max_iter=10000)      0.0100      l1   saga   0.747001\n",
       "5   LogisticRegression(max_iter=10000)      0.0100      l2   saga   0.747539\n",
       "6   LogisticRegression(max_iter=10000)      0.1000      l1   saga   0.747524\n",
       "7   LogisticRegression(max_iter=10000)      0.1000      l2   saga   0.747623\n",
       "8   LogisticRegression(max_iter=10000)      1.0000      l1   saga   0.747652\n",
       "9   LogisticRegression(max_iter=10000)      1.0000      l2   saga   0.747638\n",
       "10  LogisticRegression(max_iter=10000)     10.0000      l1   saga   0.747638\n",
       "11  LogisticRegression(max_iter=10000)     10.0000      l2   saga   0.747638\n",
       "12  LogisticRegression(max_iter=10000)    100.0000      l1   saga   0.747638\n",
       "13  LogisticRegression(max_iter=10000)    100.0000      l2   saga   0.747638\n",
       "14  LogisticRegression(max_iter=10000)   1000.0000      l1   saga   0.747638\n",
       "15  LogisticRegression(max_iter=10000)   1000.0000      l2   saga   0.747638\n",
       "16  LogisticRegression(max_iter=10000)  10000.0000      l1   saga   0.747638\n",
       "17  LogisticRegression(max_iter=10000)  10000.0000      l2   saga   0.747638\n",
       "18  LogisticRegression(max_iter=10000)      0.0001      l2  lbfgs   0.739235\n",
       "19  LogisticRegression(max_iter=10000)      0.0010      l2  lbfgs   0.746393\n",
       "20  LogisticRegression(max_iter=10000)      0.0100      l2  lbfgs   0.747524\n",
       "21  LogisticRegression(max_iter=10000)      0.1000      l2  lbfgs   0.747609\n",
       "22  LogisticRegression(max_iter=10000)      1.0000      l2  lbfgs   0.747652\n",
       "23  LogisticRegression(max_iter=10000)     10.0000      l2  lbfgs   0.747638\n",
       "24  LogisticRegression(max_iter=10000)    100.0000      l2  lbfgs   0.747638\n",
       "25  LogisticRegression(max_iter=10000)   1000.0000      l2  lbfgs   0.747638\n",
       "26  LogisticRegression(max_iter=10000)  10000.0000      l2  lbfgs   0.747638\n",
       "27  LogisticRegression(max_iter=10000)         NaN    none  lbfgs   0.747638\n",
       "28  LogisticRegression(max_iter=10000)         NaN    none   saga   0.747652\n",
       "29  LogisticRegression(max_iter=10000)         NaN     NaN    NaN   0.747652"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results = pd.DataFrame( best_model.cv_results_['params'] )\n",
    "\n",
    "results['score_acc'] = best_model.cv_results_['mean_test_accuracy']\n",
    "cols = results.columns.to_series().str.split('__').apply(lambda x: x[-1])\n",
    "results.columns = cols\n",
    "\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "dce721ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "acc_list.append(results[0:1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "ab7ddcc5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'classifier': LogisticRegression(max_iter=10000),\n",
       " 'classifier__C': 0.0001,\n",
       " 'classifier__penalty': 'l2',\n",
       " 'classifier__solver': 'saga'}"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_model.cv_results_['params'][ np.argmin(best_model.cv_results_['rank_test_precision'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "c6c37019",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>classifier</th>\n",
       "      <th>C</th>\n",
       "      <th>penalty</th>\n",
       "      <th>solver</th>\n",
       "      <th>score_precision</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.712440</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.699038</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.698542</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0100</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.696525</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0100</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.697094</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.1000</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.696798</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.1000</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.696894</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.696886</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.696889</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>10.0000</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.696874</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>10.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.696874</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>100.0000</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.696874</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>100.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.696874</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>1000.0000</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.696874</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>1000.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.696874</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>10000.0000</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.696874</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>10000.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.696874</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.712440</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.698542</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0100</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.697068</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.1000</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.696868</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.696901</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>10.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.696874</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>100.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.696874</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>1000.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.696874</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>10000.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.696874</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>NaN</td>\n",
       "      <td>none</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.696874</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>NaN</td>\n",
       "      <td>none</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.696901</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.696901</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                            classifier           C penalty solver  \\\n",
       "0   LogisticRegression(max_iter=10000)      0.0001      l1   saga   \n",
       "1   LogisticRegression(max_iter=10000)      0.0001      l2   saga   \n",
       "2   LogisticRegression(max_iter=10000)      0.0010      l1   saga   \n",
       "3   LogisticRegression(max_iter=10000)      0.0010      l2   saga   \n",
       "4   LogisticRegression(max_iter=10000)      0.0100      l1   saga   \n",
       "5   LogisticRegression(max_iter=10000)      0.0100      l2   saga   \n",
       "6   LogisticRegression(max_iter=10000)      0.1000      l1   saga   \n",
       "7   LogisticRegression(max_iter=10000)      0.1000      l2   saga   \n",
       "8   LogisticRegression(max_iter=10000)      1.0000      l1   saga   \n",
       "9   LogisticRegression(max_iter=10000)      1.0000      l2   saga   \n",
       "10  LogisticRegression(max_iter=10000)     10.0000      l1   saga   \n",
       "11  LogisticRegression(max_iter=10000)     10.0000      l2   saga   \n",
       "12  LogisticRegression(max_iter=10000)    100.0000      l1   saga   \n",
       "13  LogisticRegression(max_iter=10000)    100.0000      l2   saga   \n",
       "14  LogisticRegression(max_iter=10000)   1000.0000      l1   saga   \n",
       "15  LogisticRegression(max_iter=10000)   1000.0000      l2   saga   \n",
       "16  LogisticRegression(max_iter=10000)  10000.0000      l1   saga   \n",
       "17  LogisticRegression(max_iter=10000)  10000.0000      l2   saga   \n",
       "18  LogisticRegression(max_iter=10000)      0.0001      l2  lbfgs   \n",
       "19  LogisticRegression(max_iter=10000)      0.0010      l2  lbfgs   \n",
       "20  LogisticRegression(max_iter=10000)      0.0100      l2  lbfgs   \n",
       "21  LogisticRegression(max_iter=10000)      0.1000      l2  lbfgs   \n",
       "22  LogisticRegression(max_iter=10000)      1.0000      l2  lbfgs   \n",
       "23  LogisticRegression(max_iter=10000)     10.0000      l2  lbfgs   \n",
       "24  LogisticRegression(max_iter=10000)    100.0000      l2  lbfgs   \n",
       "25  LogisticRegression(max_iter=10000)   1000.0000      l2  lbfgs   \n",
       "26  LogisticRegression(max_iter=10000)  10000.0000      l2  lbfgs   \n",
       "27  LogisticRegression(max_iter=10000)         NaN    none  lbfgs   \n",
       "28  LogisticRegression(max_iter=10000)         NaN    none   saga   \n",
       "29  LogisticRegression(max_iter=10000)         NaN     NaN    NaN   \n",
       "\n",
       "    score_precision  \n",
       "0          0.000000  \n",
       "1          0.712440  \n",
       "2          0.699038  \n",
       "3          0.698542  \n",
       "4          0.696525  \n",
       "5          0.697094  \n",
       "6          0.696798  \n",
       "7          0.696894  \n",
       "8          0.696886  \n",
       "9          0.696889  \n",
       "10         0.696874  \n",
       "11         0.696874  \n",
       "12         0.696874  \n",
       "13         0.696874  \n",
       "14         0.696874  \n",
       "15         0.696874  \n",
       "16         0.696874  \n",
       "17         0.696874  \n",
       "18         0.712440  \n",
       "19         0.698542  \n",
       "20         0.697068  \n",
       "21         0.696868  \n",
       "22         0.696901  \n",
       "23         0.696874  \n",
       "24         0.696874  \n",
       "25         0.696874  \n",
       "26         0.696874  \n",
       "27         0.696874  \n",
       "28         0.696901  \n",
       "29         0.696901  "
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results = pd.DataFrame( best_model.cv_results_['params'] )\n",
    "\n",
    "results['score_precision'] = best_model.cv_results_['mean_test_precision']\n",
    "cols = results.columns.to_series().str.split('__').apply(lambda x: x[-1])\n",
    "results.columns = cols\n",
    "\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "30976105",
   "metadata": {},
   "outputs": [],
   "source": [
    "roc_auc_ovr_List.append(results[1:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "ca7bc287",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'classifier': LogisticRegression(max_iter=10000),\n",
       " 'classifier__C': 1.0,\n",
       " 'classifier__penalty': 'l2',\n",
       " 'classifier__solver': 'lbfgs'}"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_model.cv_results_['params'][ np.argmin(best_model.cv_results_['rank_test_f1_micro'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "affe1adf",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>classifier</th>\n",
       "      <th>C</th>\n",
       "      <th>penalty</th>\n",
       "      <th>solver</th>\n",
       "      <th>score_f1_micro</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.599332</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.739235</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.743663</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.746393</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0100</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.747001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0100</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.747539</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.1000</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.747524</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.1000</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.747623</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.747652</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.747638</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>10.0000</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.747638</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>10.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.747638</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>100.0000</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.747638</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>100.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.747638</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>1000.0000</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.747638</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>1000.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.747638</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>10000.0000</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.747638</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>10000.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.747638</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.739235</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.746393</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0100</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.747524</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.1000</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.747609</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.747652</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>10.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.747638</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>100.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.747638</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>1000.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.747638</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>10000.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.747638</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>NaN</td>\n",
       "      <td>none</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.747638</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>NaN</td>\n",
       "      <td>none</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.747652</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.747652</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                            classifier           C penalty solver  \\\n",
       "0   LogisticRegression(max_iter=10000)      0.0001      l1   saga   \n",
       "1   LogisticRegression(max_iter=10000)      0.0001      l2   saga   \n",
       "2   LogisticRegression(max_iter=10000)      0.0010      l1   saga   \n",
       "3   LogisticRegression(max_iter=10000)      0.0010      l2   saga   \n",
       "4   LogisticRegression(max_iter=10000)      0.0100      l1   saga   \n",
       "5   LogisticRegression(max_iter=10000)      0.0100      l2   saga   \n",
       "6   LogisticRegression(max_iter=10000)      0.1000      l1   saga   \n",
       "7   LogisticRegression(max_iter=10000)      0.1000      l2   saga   \n",
       "8   LogisticRegression(max_iter=10000)      1.0000      l1   saga   \n",
       "9   LogisticRegression(max_iter=10000)      1.0000      l2   saga   \n",
       "10  LogisticRegression(max_iter=10000)     10.0000      l1   saga   \n",
       "11  LogisticRegression(max_iter=10000)     10.0000      l2   saga   \n",
       "12  LogisticRegression(max_iter=10000)    100.0000      l1   saga   \n",
       "13  LogisticRegression(max_iter=10000)    100.0000      l2   saga   \n",
       "14  LogisticRegression(max_iter=10000)   1000.0000      l1   saga   \n",
       "15  LogisticRegression(max_iter=10000)   1000.0000      l2   saga   \n",
       "16  LogisticRegression(max_iter=10000)  10000.0000      l1   saga   \n",
       "17  LogisticRegression(max_iter=10000)  10000.0000      l2   saga   \n",
       "18  LogisticRegression(max_iter=10000)      0.0001      l2  lbfgs   \n",
       "19  LogisticRegression(max_iter=10000)      0.0010      l2  lbfgs   \n",
       "20  LogisticRegression(max_iter=10000)      0.0100      l2  lbfgs   \n",
       "21  LogisticRegression(max_iter=10000)      0.1000      l2  lbfgs   \n",
       "22  LogisticRegression(max_iter=10000)      1.0000      l2  lbfgs   \n",
       "23  LogisticRegression(max_iter=10000)     10.0000      l2  lbfgs   \n",
       "24  LogisticRegression(max_iter=10000)    100.0000      l2  lbfgs   \n",
       "25  LogisticRegression(max_iter=10000)   1000.0000      l2  lbfgs   \n",
       "26  LogisticRegression(max_iter=10000)  10000.0000      l2  lbfgs   \n",
       "27  LogisticRegression(max_iter=10000)         NaN    none  lbfgs   \n",
       "28  LogisticRegression(max_iter=10000)         NaN    none   saga   \n",
       "29  LogisticRegression(max_iter=10000)         NaN     NaN    NaN   \n",
       "\n",
       "    score_f1_micro  \n",
       "0         0.599332  \n",
       "1         0.739235  \n",
       "2         0.743663  \n",
       "3         0.746393  \n",
       "4         0.747001  \n",
       "5         0.747539  \n",
       "6         0.747524  \n",
       "7         0.747623  \n",
       "8         0.747652  \n",
       "9         0.747638  \n",
       "10        0.747638  \n",
       "11        0.747638  \n",
       "12        0.747638  \n",
       "13        0.747638  \n",
       "14        0.747638  \n",
       "15        0.747638  \n",
       "16        0.747638  \n",
       "17        0.747638  \n",
       "18        0.739235  \n",
       "19        0.746393  \n",
       "20        0.747524  \n",
       "21        0.747609  \n",
       "22        0.747652  \n",
       "23        0.747638  \n",
       "24        0.747638  \n",
       "25        0.747638  \n",
       "26        0.747638  \n",
       "27        0.747638  \n",
       "28        0.747652  \n",
       "29        0.747652  "
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results = pd.DataFrame( best_model.cv_results_['params'] )\n",
    "\n",
    "results['score_f1_micro'] = best_model.cv_results_['mean_test_f1_micro']\n",
    "cols = results.columns.to_series().str.split('__').apply(lambda x: x[-1])\n",
    "results.columns = cols\n",
    "\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "7439a073",
   "metadata": {},
   "outputs": [],
   "source": [
    "f1_micro_List.append(results[22:23])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efd1babe",
   "metadata": {},
   "source": [
    "## Trial 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "1a0f0acc",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, Y_train, Y_test = train_test_split(X, y, test_size=0.2, shuffle=True, random_state = 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "7eb408a1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Guest-All\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\Guest-All\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\Guest-All\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\Guest-All\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\Guest-All\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\Guest-All\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\Guest-All\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\Guest-All\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\Guest-All\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\Guest-All\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\Guest-All\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1183: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Guest-All\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1183: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Guest-All\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1183: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Guest-All\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1183: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Guest-All\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1183: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Guest-All\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1183: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Guest-All\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1183: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Guest-All\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1183: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Guest-All\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1183: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Guest-All\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1183: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Guest-All\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1183: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Guest-All\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1183: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Guest-All\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1183: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Guest-All\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1183: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Guest-All\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1183: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Guest-All\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1183: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Guest-All\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1183: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Guest-All\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1183: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Guest-All\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1183: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Guest-All\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1183: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Create a pipeline - RF is a stand in, we will populate the classifier part below\n",
    "pipe = Pipeline([('std', StandardScaler()),\n",
    "                 ('classifier', LogisticRegression())])\n",
    "\n",
    "# Create search space of candidate learning algorithms and their hyperparameters\n",
    "# note lbfgs can't do l1, and if you pass penalty='none' it expects no C value\n",
    "search_space = [{'classifier': [LogisticRegression(max_iter=10000)],\n",
    "                 'classifier__solver': ['saga'],\n",
    "                 'classifier__penalty': ['l1', 'l2'],\n",
    "                 'classifier__C': np.logspace(-4, 4, 9)},\n",
    "                {'classifier': [LogisticRegression(max_iter=10000)],\n",
    "                 'classifier__solver': ['lbfgs'],\n",
    "                 'classifier__penalty': ['l2'],\n",
    "                 'classifier__C': np.logspace(-4, 4, 9)},\n",
    "                {'classifier': [LogisticRegression(max_iter=10000)],\n",
    "                 'classifier__solver': ['lbfgs','saga'],\n",
    "                 'classifier__penalty': ['none']},\n",
    "                {'classifier': [LogisticRegression(max_iter=10000)]}\n",
    "                ]\n",
    "\n",
    "# Create grid search\n",
    "clf = GridSearchCV(pipe, search_space, cv=StratifiedKFold(n_splits=10),\n",
    "                   scoring=['accuracy', 'precision', 'f1_micro'], refit=False,\n",
    "                   verbose=0)\n",
    "\n",
    "# Fit grid search\n",
    "best_model = clf.fit(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "bfa6938c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'mean_fit_time': array([0.78632846, 0.69170606, 0.76409299, 0.70536613, 0.80667536,\n",
       "        0.67185853, 0.82659299, 0.707657  , 0.8070874 , 0.75679822,\n",
       "        0.84421754, 0.76328862, 0.90275533, 0.75771015, 1.03116047,\n",
       "        0.90111012, 1.03263774, 0.85767303, 0.16159616, 0.10001793,\n",
       "        0.16521969, 0.11602077, 0.15887327, 0.1203758 , 0.14901106,\n",
       "        0.14540467, 0.11140745, 0.18314481, 0.92291272, 0.1095053 ]),\n",
       " 'std_fit_time': array([0.08007347, 0.06232404, 0.06881341, 0.0450978 , 0.06945838,\n",
       "        0.05971017, 0.07673319, 0.05602309, 0.09284077, 0.04248772,\n",
       "        0.05653993, 0.05518273, 0.06264076, 0.15162162, 0.2047497 ,\n",
       "        0.25847903, 0.28298604, 0.20248017, 0.04727136, 0.01287666,\n",
       "        0.07455344, 0.02874773, 0.07616799, 0.02222603, 0.06828156,\n",
       "        0.0529832 , 0.02575629, 0.06630067, 0.23659766, 0.01350251]),\n",
       " 'mean_score_time': array([0.01278284, 0.01367896, 0.01289692, 0.01204021, 0.01356025,\n",
       "        0.013661  , 0.01358058, 0.0112427 , 0.0136925 , 0.01433871,\n",
       "        0.0128299 , 0.01279817, 0.01202769, 0.01437755, 0.01366148,\n",
       "        0.01286209, 0.01411366, 0.01393919, 0.01808312, 0.01103325,\n",
       "        0.01526468, 0.0120784 , 0.01279984, 0.01367362, 0.01777642,\n",
       "        0.01329234, 0.01735117, 0.01817527, 0.01863701, 0.01161554]),\n",
       " 'std_score_time': array([0.00540273, 0.00361438, 0.00398496, 0.00400564, 0.00364444,\n",
       "        0.00368806, 0.00365924, 0.00390161, 0.00373044, 0.00316802,\n",
       "        0.00394355, 0.00392625, 0.00369747, 0.00461317, 0.00371494,\n",
       "        0.00764931, 0.00775891, 0.00480007, 0.00666915, 0.00365842,\n",
       "        0.00838944, 0.0054815 , 0.00640656, 0.00498726, 0.00872983,\n",
       "        0.00321214, 0.00494997, 0.00753558, 0.00652107, 0.00382788]),\n",
       " 'param_classifier': masked_array(data=[LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000)],\n",
       "              mask=[False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'param_classifier__C': masked_array(data=[0.0001, 0.0001, 0.001, 0.001, 0.01, 0.01, 0.1, 0.1,\n",
       "                    1.0, 1.0, 10.0, 10.0, 100.0, 100.0, 1000.0, 1000.0,\n",
       "                    10000.0, 10000.0, 0.0001, 0.001, 0.01, 0.1, 1.0, 10.0,\n",
       "                    100.0, 1000.0, 10000.0, --, --, --],\n",
       "              mask=[False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False,  True,  True,  True],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'param_classifier__penalty': masked_array(data=['l1', 'l2', 'l1', 'l2', 'l1', 'l2', 'l1', 'l2', 'l1',\n",
       "                    'l2', 'l1', 'l2', 'l1', 'l2', 'l1', 'l2', 'l1', 'l2',\n",
       "                    'l2', 'l2', 'l2', 'l2', 'l2', 'l2', 'l2', 'l2', 'l2',\n",
       "                    'none', 'none', --],\n",
       "              mask=[False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False,  True],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'param_classifier__solver': masked_array(data=['saga', 'saga', 'saga', 'saga', 'saga', 'saga', 'saga',\n",
       "                    'saga', 'saga', 'saga', 'saga', 'saga', 'saga', 'saga',\n",
       "                    'saga', 'saga', 'saga', 'saga', 'lbfgs', 'lbfgs',\n",
       "                    'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs',\n",
       "                    'lbfgs', 'lbfgs', 'saga', --],\n",
       "              mask=[False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False,  True],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'params': [{'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 0.0001,\n",
       "   'classifier__penalty': 'l1',\n",
       "   'classifier__solver': 'saga'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 0.0001,\n",
       "   'classifier__penalty': 'l2',\n",
       "   'classifier__solver': 'saga'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 0.001,\n",
       "   'classifier__penalty': 'l1',\n",
       "   'classifier__solver': 'saga'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 0.001,\n",
       "   'classifier__penalty': 'l2',\n",
       "   'classifier__solver': 'saga'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 0.01,\n",
       "   'classifier__penalty': 'l1',\n",
       "   'classifier__solver': 'saga'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 0.01,\n",
       "   'classifier__penalty': 'l2',\n",
       "   'classifier__solver': 'saga'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 0.1,\n",
       "   'classifier__penalty': 'l1',\n",
       "   'classifier__solver': 'saga'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 0.1,\n",
       "   'classifier__penalty': 'l2',\n",
       "   'classifier__solver': 'saga'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 1.0,\n",
       "   'classifier__penalty': 'l1',\n",
       "   'classifier__solver': 'saga'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 1.0,\n",
       "   'classifier__penalty': 'l2',\n",
       "   'classifier__solver': 'saga'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 10.0,\n",
       "   'classifier__penalty': 'l1',\n",
       "   'classifier__solver': 'saga'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 10.0,\n",
       "   'classifier__penalty': 'l2',\n",
       "   'classifier__solver': 'saga'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 100.0,\n",
       "   'classifier__penalty': 'l1',\n",
       "   'classifier__solver': 'saga'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 100.0,\n",
       "   'classifier__penalty': 'l2',\n",
       "   'classifier__solver': 'saga'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 1000.0,\n",
       "   'classifier__penalty': 'l1',\n",
       "   'classifier__solver': 'saga'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 1000.0,\n",
       "   'classifier__penalty': 'l2',\n",
       "   'classifier__solver': 'saga'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 10000.0,\n",
       "   'classifier__penalty': 'l1',\n",
       "   'classifier__solver': 'saga'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 10000.0,\n",
       "   'classifier__penalty': 'l2',\n",
       "   'classifier__solver': 'saga'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 0.0001,\n",
       "   'classifier__penalty': 'l2',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 0.001,\n",
       "   'classifier__penalty': 'l2',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 0.01,\n",
       "   'classifier__penalty': 'l2',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 0.1,\n",
       "   'classifier__penalty': 'l2',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 1.0,\n",
       "   'classifier__penalty': 'l2',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 10.0,\n",
       "   'classifier__penalty': 'l2',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 100.0,\n",
       "   'classifier__penalty': 'l2',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 1000.0,\n",
       "   'classifier__penalty': 'l2',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 10000.0,\n",
       "   'classifier__penalty': 'l2',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__penalty': 'none',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__penalty': 'none',\n",
       "   'classifier__solver': 'saga'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000)}],\n",
       " 'split0_test_accuracy': array([0.60014144, 0.73536068, 0.74257426, 0.74314003, 0.74441301,\n",
       "        0.74526167, 0.74512023, 0.74526167, 0.74512023, 0.74512023,\n",
       "        0.74512023, 0.74512023, 0.74512023, 0.74512023, 0.74512023,\n",
       "        0.74512023, 0.74512023, 0.74512023, 0.73536068, 0.74314003,\n",
       "        0.74526167, 0.74526167, 0.74512023, 0.74512023, 0.74512023,\n",
       "        0.74512023, 0.74512023, 0.74512023, 0.74512023, 0.74512023]),\n",
       " 'split1_test_accuracy': array([0.60014144, 0.74384724, 0.74483734, 0.74851485, 0.74851485,\n",
       "        0.74893918, 0.74936351, 0.74950495, 0.74950495, 0.74964639,\n",
       "        0.74950495, 0.74964639, 0.74964639, 0.74964639, 0.74964639,\n",
       "        0.74964639, 0.74950495, 0.74964639, 0.74398868, 0.74851485,\n",
       "        0.74893918, 0.74950495, 0.74964639, 0.74964639, 0.74964639,\n",
       "        0.74964639, 0.74964639, 0.74964639, 0.74964639, 0.74964639]),\n",
       " 'split2_test_accuracy': array([0.60022634, 0.74324515, 0.74480124, 0.74678172, 0.74734757,\n",
       "        0.74805489, 0.74833781, 0.74819635, 0.74819635, 0.74791342,\n",
       "        0.74791342, 0.74791342, 0.74791342, 0.74791342, 0.74791342,\n",
       "        0.74791342, 0.74791342, 0.74791342, 0.74324515, 0.74678172,\n",
       "        0.74805489, 0.74819635, 0.74791342, 0.74791342, 0.74791342,\n",
       "        0.74791342, 0.74791342, 0.74791342, 0.74791342, 0.74791342]),\n",
       " 'split3_test_accuracy': array([0.60022634, 0.734333  , 0.74098175, 0.74593295, 0.74692319,\n",
       "        0.74805489, 0.74734757, 0.74720611, 0.74734757, 0.74734757,\n",
       "        0.74734757, 0.74734757, 0.74734757, 0.74734757, 0.74734757,\n",
       "        0.74734757, 0.74734757, 0.74734757, 0.734333  , 0.74593295,\n",
       "        0.74805489, 0.74720611, 0.74734757, 0.74734757, 0.74734757,\n",
       "        0.74734757, 0.74734757, 0.74734757, 0.74734757, 0.74734757]),\n",
       " 'split4_test_accuracy': array([0.60022634, 0.73999151, 0.7415476 , 0.74437686, 0.74239638,\n",
       "        0.74451832, 0.74451832, 0.74423539, 0.74423539, 0.74423539,\n",
       "        0.74423539, 0.74423539, 0.74423539, 0.74423539, 0.74423539,\n",
       "        0.74423539, 0.74423539, 0.74423539, 0.73999151, 0.74437686,\n",
       "        0.74451832, 0.74423539, 0.74423539, 0.74423539, 0.74423539,\n",
       "        0.74423539, 0.74423539, 0.74423539, 0.74423539, 0.74423539]),\n",
       " 'split5_test_accuracy': array([0.60022634, 0.73673787, 0.74664026, 0.75060122, 0.75017683,\n",
       "        0.7498939 , 0.74932805, 0.7487622 , 0.74890366, 0.7487622 ,\n",
       "        0.7487622 , 0.7487622 , 0.7487622 , 0.7487622 , 0.7487622 ,\n",
       "        0.7487622 , 0.7487622 , 0.7487622 , 0.73673787, 0.75060122,\n",
       "        0.7498939 , 0.7487622 , 0.7487622 , 0.7487622 , 0.7487622 ,\n",
       "        0.7487622 , 0.7487622 , 0.7487622 , 0.7487622 , 0.7487622 ]),\n",
       " 'split6_test_accuracy': array([0.60022634, 0.74366954, 0.74635733, 0.7476305 , 0.7476305 ,\n",
       "        0.74819635, 0.74833781, 0.74847928, 0.74847928, 0.74847928,\n",
       "        0.74847928, 0.74847928, 0.74847928, 0.74847928, 0.74847928,\n",
       "        0.74847928, 0.74847928, 0.74847928, 0.74366954, 0.7476305 ,\n",
       "        0.74819635, 0.74847928, 0.74847928, 0.74847928, 0.74847928,\n",
       "        0.74847928, 0.74847928, 0.74847928, 0.74847928, 0.74847928]),\n",
       " 'split7_test_accuracy': array([0.60008488, 0.72697694, 0.73390862, 0.7392842 , 0.7392842 ,\n",
       "        0.73843542, 0.73900127, 0.73871835, 0.73885981, 0.73871835,\n",
       "        0.73871835, 0.73871835, 0.73871835, 0.73871835, 0.73871835,\n",
       "        0.73871835, 0.73871835, 0.73871835, 0.7271184 , 0.7392842 ,\n",
       "        0.73843542, 0.73871835, 0.73871835, 0.73871835, 0.73871835,\n",
       "        0.73871835, 0.73871835, 0.73871835, 0.73871835, 0.73871835]),\n",
       " 'split8_test_accuracy': array([0.60008488, 0.73942566, 0.74565002, 0.74862074, 0.74720611,\n",
       "        0.74862074, 0.74847928, 0.7487622 , 0.74862074, 0.7487622 ,\n",
       "        0.74862074, 0.7487622 , 0.7487622 , 0.7487622 , 0.7487622 ,\n",
       "        0.7487622 , 0.7487622 , 0.7487622 , 0.73942566, 0.74862074,\n",
       "        0.74862074, 0.7487622 , 0.7487622 , 0.7487622 , 0.7487622 ,\n",
       "        0.7487622 , 0.7487622 , 0.7487622 , 0.7487622 , 0.7487622 ]),\n",
       " 'split9_test_accuracy': array([0.60008488, 0.73829396, 0.74720611, 0.74777196, 0.74961098,\n",
       "        0.74946951, 0.75003537, 0.75017683, 0.75017683, 0.75017683,\n",
       "        0.75017683, 0.75017683, 0.75017683, 0.75017683, 0.75017683,\n",
       "        0.75017683, 0.75017683, 0.75017683, 0.73829396, 0.74777196,\n",
       "        0.74946951, 0.75017683, 0.75017683, 0.75017683, 0.75017683,\n",
       "        0.75017683, 0.75017683, 0.75017683, 0.75017683, 0.75017683]),\n",
       " 'mean_test_accuracy': array([0.60016692, 0.73818816, 0.74345045, 0.7462655 , 0.74635036,\n",
       "        0.74694449, 0.74698692, 0.74693033, 0.74694448, 0.74691619,\n",
       "        0.7468879 , 0.74691619, 0.74691619, 0.74691619, 0.74691619,\n",
       "        0.74691619, 0.74690204, 0.74691619, 0.73821645, 0.7462655 ,\n",
       "        0.74694449, 0.74693033, 0.74691619, 0.74691619, 0.74691619,\n",
       "        0.74691619, 0.74691619, 0.74691619, 0.74691619, 0.74691619]),\n",
       " 'std_test_accuracy': array([6.25659081e-05, 4.93041876e-03, 3.77949357e-03, 3.09820938e-03,\n",
       "        3.20889487e-03, 3.27137472e-03, 3.15876875e-03, 3.24718008e-03,\n",
       "        3.22128113e-03, 3.25794205e-03, 3.23850673e-03, 3.25794205e-03,\n",
       "        3.25794205e-03, 3.25794205e-03, 3.25794205e-03, 3.25794205e-03,\n",
       "        3.24634462e-03, 3.25794205e-03, 4.91478627e-03, 3.09820938e-03,\n",
       "        3.27137472e-03, 3.24718008e-03, 3.25794205e-03, 3.25794205e-03,\n",
       "        3.25794205e-03, 3.25794205e-03, 3.25794205e-03, 3.25794205e-03,\n",
       "        3.25794205e-03, 3.25794205e-03]),\n",
       " 'rank_test_accuracy': array([30, 29, 27, 25, 24,  2,  1,  5,  4,  7, 23,  7,  7,  7,  7,  7, 22,\n",
       "         7, 28, 25,  2,  5,  7,  7,  7,  7,  7,  7,  7,  7]),\n",
       " 'split0_test_precision': array([0.        , 0.70873362, 0.6978389 , 0.69494794, 0.69303558,\n",
       "        0.69387755, 0.69317753, 0.69358491, 0.69332327, 0.69332327,\n",
       "        0.69332327, 0.69332327, 0.69332327, 0.69332327, 0.69332327,\n",
       "        0.69332327, 0.69332327, 0.69332327, 0.70873362, 0.69494794,\n",
       "        0.69387755, 0.69358491, 0.69332327, 0.69332327, 0.69332327,\n",
       "        0.69332327, 0.69332327, 0.69332327, 0.69332327, 0.69332327]),\n",
       " 'split1_test_precision': array([0.        , 0.72048611, 0.70066693, 0.70088089, 0.69829868,\n",
       "        0.69849057, 0.69853218, 0.69894499, 0.69879518, 0.69905838,\n",
       "        0.69879518, 0.69905838, 0.69905838, 0.69905838, 0.69905838,\n",
       "        0.69905838, 0.69879518, 0.69905838, 0.72079896, 0.70088089,\n",
       "        0.69849057, 0.69894499, 0.69905838, 0.69905838, 0.69905838,\n",
       "        0.69905838, 0.69905838, 0.69905838, 0.69905838, 0.69905838]),\n",
       " 'split2_test_precision': array([0.        , 0.72006966, 0.69960937, 0.69923077, 0.69682059,\n",
       "        0.69709544, 0.69702672, 0.69676448, 0.69676448, 0.6962406 ,\n",
       "        0.6962406 , 0.6962406 , 0.6962406 , 0.6962406 , 0.6962406 ,\n",
       "        0.6962406 , 0.6962406 , 0.6962406 , 0.72006966, 0.69923077,\n",
       "        0.69709544, 0.69676448, 0.6962406 , 0.6962406 , 0.6962406 ,\n",
       "        0.6962406 , 0.6962406 , 0.6962406 , 0.6962406 , 0.6962406 ]),\n",
       " 'split3_test_precision': array([0.        , 0.70273738, 0.69395712, 0.69626524, 0.69632715,\n",
       "        0.69739328, 0.69652305, 0.69625992, 0.69637462, 0.69637462,\n",
       "        0.69637462, 0.69637462, 0.69637462, 0.69637462, 0.69637462,\n",
       "        0.69637462, 0.69637462, 0.69637462, 0.70273738, 0.69626524,\n",
       "        0.69739328, 0.69625992, 0.69637462, 0.69637462, 0.69637462,\n",
       "        0.69637462, 0.69637462, 0.69637462, 0.69637462, 0.69637462]),\n",
       " 'split4_test_precision': array([0.        , 0.71590909, 0.69972011, 0.69848072, 0.69289827,\n",
       "        0.69555215, 0.69525268, 0.69472073, 0.69472073, 0.69472073,\n",
       "        0.69472073, 0.69472073, 0.69472073, 0.69472073, 0.69472073,\n",
       "        0.69472073, 0.69472073, 0.69472073, 0.71590909, 0.69848072,\n",
       "        0.69555215, 0.69472073, 0.69472073, 0.69472073, 0.69472073,\n",
       "        0.69472073, 0.69472073, 0.69472073, 0.69472073, 0.69472073]),\n",
       " 'split5_test_precision': array([0.        , 0.71511369, 0.70875353, 0.70802348, 0.70400308,\n",
       "        0.70346154, 0.70207055, 0.70145817, 0.70172745, 0.70145817,\n",
       "        0.70145817, 0.70145817, 0.70145817, 0.70145817, 0.70145817,\n",
       "        0.70145817, 0.70145817, 0.70145817, 0.71511369, 0.70802348,\n",
       "        0.70346154, 0.70145817, 0.70145817, 0.70145817, 0.70145817,\n",
       "        0.70145817, 0.70145817, 0.70145817, 0.70145817, 0.70145817]),\n",
       " 'split6_test_precision': array([0.        , 0.71072319, 0.69556986, 0.69296296, 0.69042398,\n",
       "        0.69157509, 0.69126781, 0.69152047, 0.69152047, 0.69152047,\n",
       "        0.69152047, 0.69152047, 0.69152047, 0.69152047, 0.69152047,\n",
       "        0.69152047, 0.69152047, 0.69152047, 0.71072319, 0.69296296,\n",
       "        0.69157509, 0.69152047, 0.69152047, 0.69152047, 0.69152047,\n",
       "        0.69152047, 0.69152047, 0.69152047, 0.69152047, 0.69152047]),\n",
       " 'split7_test_precision': array([0.        , 0.69525468, 0.6854902 , 0.68850575, 0.68650493,\n",
       "        0.68522727, 0.68612585, 0.68560606, 0.68586586, 0.68560606,\n",
       "        0.68560606, 0.68560606, 0.68560606, 0.68560606, 0.68560606,\n",
       "        0.68560606, 0.68560606, 0.68560606, 0.69538729, 0.68850575,\n",
       "        0.68522727, 0.68560606, 0.68560606, 0.68560606, 0.68560606,\n",
       "        0.68560606, 0.68560606, 0.68560606, 0.68560606, 0.68560606]),\n",
       " 'split8_test_precision': array([0.        , 0.71219302, 0.70263883, 0.70053476, 0.69637462,\n",
       "        0.69736842, 0.69681051, 0.69748215, 0.69722014, 0.69748215,\n",
       "        0.69722014, 0.69748215, 0.69748215, 0.69748215, 0.69748215,\n",
       "        0.69748215, 0.69748215, 0.69748215, 0.71219302, 0.70053476,\n",
       "        0.69736842, 0.69748215, 0.69748215, 0.69748215, 0.69748215,\n",
       "        0.69748215, 0.69748215, 0.69748215, 0.69748215, 0.69748215]),\n",
       " 'split9_test_precision': array([0.        , 0.71101512, 0.7056962 , 0.7027972 , 0.7016406 ,\n",
       "        0.70152672, 0.70182788, 0.70194138, 0.70194138, 0.70194138,\n",
       "        0.70194138, 0.70194138, 0.70194138, 0.70194138, 0.70194138,\n",
       "        0.70194138, 0.70194138, 0.70194138, 0.71101512, 0.7027972 ,\n",
       "        0.70152672, 0.70194138, 0.70194138, 0.70194138, 0.70194138,\n",
       "        0.70194138, 0.70194138, 0.70194138, 0.70194138, 0.70194138]),\n",
       " 'mean_test_precision': array([0.        , 0.71122356, 0.69899411, 0.69826297, 0.69563275,\n",
       "        0.6961568 , 0.69586148, 0.69582833, 0.69582536, 0.69577258,\n",
       "        0.69572006, 0.69577258, 0.69577258, 0.69577258, 0.69577258,\n",
       "        0.69577258, 0.69574626, 0.69577258, 0.7112681 , 0.69826297,\n",
       "        0.6961568 , 0.69582833, 0.69577258, 0.69577258, 0.69577258,\n",
       "        0.69577258, 0.69577258, 0.69577258, 0.69577258, 0.69577258]),\n",
       " 'std_test_precision': array([0.        , 0.0073184 , 0.00614768, 0.00516816, 0.00490284,\n",
       "        0.00487367, 0.00455453, 0.00460313, 0.00457595, 0.00461763,\n",
       "        0.00459031, 0.00461763, 0.00461763, 0.00461763, 0.00461763,\n",
       "        0.00461763, 0.00459954, 0.00461763, 0.0073297 , 0.00516816,\n",
       "        0.00487367, 0.00460313, 0.00461763, 0.00461763, 0.00461763,\n",
       "        0.00461763, 0.00461763, 0.00461763, 0.00461763, 0.00461763]),\n",
       " 'rank_test_precision': array([30,  2,  3,  4, 29,  6,  8,  9, 11, 12, 28, 12, 12, 12, 12, 12, 27,\n",
       "        12,  1,  4,  6,  9, 12, 12, 12, 12, 12, 12, 12, 12]),\n",
       " 'split0_test_f1_micro': array([0.60014144, 0.73536068, 0.74257426, 0.74314003, 0.74441301,\n",
       "        0.74526167, 0.74512023, 0.74526167, 0.74512023, 0.74512023,\n",
       "        0.74512023, 0.74512023, 0.74512023, 0.74512023, 0.74512023,\n",
       "        0.74512023, 0.74512023, 0.74512023, 0.73536068, 0.74314003,\n",
       "        0.74526167, 0.74526167, 0.74512023, 0.74512023, 0.74512023,\n",
       "        0.74512023, 0.74512023, 0.74512023, 0.74512023, 0.74512023]),\n",
       " 'split1_test_f1_micro': array([0.60014144, 0.74384724, 0.74483734, 0.74851485, 0.74851485,\n",
       "        0.74893918, 0.74936351, 0.74950495, 0.74950495, 0.74964639,\n",
       "        0.74950495, 0.74964639, 0.74964639, 0.74964639, 0.74964639,\n",
       "        0.74964639, 0.74950495, 0.74964639, 0.74398868, 0.74851485,\n",
       "        0.74893918, 0.74950495, 0.74964639, 0.74964639, 0.74964639,\n",
       "        0.74964639, 0.74964639, 0.74964639, 0.74964639, 0.74964639]),\n",
       " 'split2_test_f1_micro': array([0.60022634, 0.74324515, 0.74480124, 0.74678172, 0.74734757,\n",
       "        0.74805489, 0.74833781, 0.74819635, 0.74819635, 0.74791342,\n",
       "        0.74791342, 0.74791342, 0.74791342, 0.74791342, 0.74791342,\n",
       "        0.74791342, 0.74791342, 0.74791342, 0.74324515, 0.74678172,\n",
       "        0.74805489, 0.74819635, 0.74791342, 0.74791342, 0.74791342,\n",
       "        0.74791342, 0.74791342, 0.74791342, 0.74791342, 0.74791342]),\n",
       " 'split3_test_f1_micro': array([0.60022634, 0.734333  , 0.74098175, 0.74593295, 0.74692319,\n",
       "        0.74805489, 0.74734757, 0.74720611, 0.74734757, 0.74734757,\n",
       "        0.74734757, 0.74734757, 0.74734757, 0.74734757, 0.74734757,\n",
       "        0.74734757, 0.74734757, 0.74734757, 0.734333  , 0.74593295,\n",
       "        0.74805489, 0.74720611, 0.74734757, 0.74734757, 0.74734757,\n",
       "        0.74734757, 0.74734757, 0.74734757, 0.74734757, 0.74734757]),\n",
       " 'split4_test_f1_micro': array([0.60022634, 0.73999151, 0.7415476 , 0.74437686, 0.74239638,\n",
       "        0.74451832, 0.74451832, 0.74423539, 0.74423539, 0.74423539,\n",
       "        0.74423539, 0.74423539, 0.74423539, 0.74423539, 0.74423539,\n",
       "        0.74423539, 0.74423539, 0.74423539, 0.73999151, 0.74437686,\n",
       "        0.74451832, 0.74423539, 0.74423539, 0.74423539, 0.74423539,\n",
       "        0.74423539, 0.74423539, 0.74423539, 0.74423539, 0.74423539]),\n",
       " 'split5_test_f1_micro': array([0.60022634, 0.73673787, 0.74664026, 0.75060122, 0.75017683,\n",
       "        0.7498939 , 0.74932805, 0.7487622 , 0.74890366, 0.7487622 ,\n",
       "        0.7487622 , 0.7487622 , 0.7487622 , 0.7487622 , 0.7487622 ,\n",
       "        0.7487622 , 0.7487622 , 0.7487622 , 0.73673787, 0.75060122,\n",
       "        0.7498939 , 0.7487622 , 0.7487622 , 0.7487622 , 0.7487622 ,\n",
       "        0.7487622 , 0.7487622 , 0.7487622 , 0.7487622 , 0.7487622 ]),\n",
       " 'split6_test_f1_micro': array([0.60022634, 0.74366954, 0.74635733, 0.7476305 , 0.7476305 ,\n",
       "        0.74819635, 0.74833781, 0.74847928, 0.74847928, 0.74847928,\n",
       "        0.74847928, 0.74847928, 0.74847928, 0.74847928, 0.74847928,\n",
       "        0.74847928, 0.74847928, 0.74847928, 0.74366954, 0.7476305 ,\n",
       "        0.74819635, 0.74847928, 0.74847928, 0.74847928, 0.74847928,\n",
       "        0.74847928, 0.74847928, 0.74847928, 0.74847928, 0.74847928]),\n",
       " 'split7_test_f1_micro': array([0.60008488, 0.72697694, 0.73390862, 0.7392842 , 0.7392842 ,\n",
       "        0.73843542, 0.73900127, 0.73871835, 0.73885981, 0.73871835,\n",
       "        0.73871835, 0.73871835, 0.73871835, 0.73871835, 0.73871835,\n",
       "        0.73871835, 0.73871835, 0.73871835, 0.7271184 , 0.7392842 ,\n",
       "        0.73843542, 0.73871835, 0.73871835, 0.73871835, 0.73871835,\n",
       "        0.73871835, 0.73871835, 0.73871835, 0.73871835, 0.73871835]),\n",
       " 'split8_test_f1_micro': array([0.60008488, 0.73942566, 0.74565002, 0.74862074, 0.74720611,\n",
       "        0.74862074, 0.74847928, 0.7487622 , 0.74862074, 0.7487622 ,\n",
       "        0.74862074, 0.7487622 , 0.7487622 , 0.7487622 , 0.7487622 ,\n",
       "        0.7487622 , 0.7487622 , 0.7487622 , 0.73942566, 0.74862074,\n",
       "        0.74862074, 0.7487622 , 0.7487622 , 0.7487622 , 0.7487622 ,\n",
       "        0.7487622 , 0.7487622 , 0.7487622 , 0.7487622 , 0.7487622 ]),\n",
       " 'split9_test_f1_micro': array([0.60008488, 0.73829396, 0.74720611, 0.74777196, 0.74961098,\n",
       "        0.74946951, 0.75003537, 0.75017683, 0.75017683, 0.75017683,\n",
       "        0.75017683, 0.75017683, 0.75017683, 0.75017683, 0.75017683,\n",
       "        0.75017683, 0.75017683, 0.75017683, 0.73829396, 0.74777196,\n",
       "        0.74946951, 0.75017683, 0.75017683, 0.75017683, 0.75017683,\n",
       "        0.75017683, 0.75017683, 0.75017683, 0.75017683, 0.75017683]),\n",
       " 'mean_test_f1_micro': array([0.60016692, 0.73818816, 0.74345045, 0.7462655 , 0.74635036,\n",
       "        0.74694449, 0.74698692, 0.74693033, 0.74694448, 0.74691619,\n",
       "        0.7468879 , 0.74691619, 0.74691619, 0.74691619, 0.74691619,\n",
       "        0.74691619, 0.74690204, 0.74691619, 0.73821645, 0.7462655 ,\n",
       "        0.74694449, 0.74693033, 0.74691619, 0.74691619, 0.74691619,\n",
       "        0.74691619, 0.74691619, 0.74691619, 0.74691619, 0.74691619]),\n",
       " 'std_test_f1_micro': array([6.25659081e-05, 4.93041876e-03, 3.77949357e-03, 3.09820938e-03,\n",
       "        3.20889487e-03, 3.27137472e-03, 3.15876875e-03, 3.24718008e-03,\n",
       "        3.22128113e-03, 3.25794205e-03, 3.23850673e-03, 3.25794205e-03,\n",
       "        3.25794205e-03, 3.25794205e-03, 3.25794205e-03, 3.25794205e-03,\n",
       "        3.24634462e-03, 3.25794205e-03, 4.91478627e-03, 3.09820938e-03,\n",
       "        3.27137472e-03, 3.24718008e-03, 3.25794205e-03, 3.25794205e-03,\n",
       "        3.25794205e-03, 3.25794205e-03, 3.25794205e-03, 3.25794205e-03,\n",
       "        3.25794205e-03, 3.25794205e-03]),\n",
       " 'rank_test_f1_micro': array([30, 29, 27, 25, 24,  2,  1,  5,  4,  7, 23,  7,  7,  7,  7,  7, 22,\n",
       "         7, 28, 25,  2,  5,  7,  7,  7,  7,  7,  7,  7,  7])}"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_model.cv_results_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "d443f4d3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([30, 29, 27, 25, 24,  2,  1,  5,  4,  7, 23,  7,  7,  7,  7,  7, 22,\n",
       "        7, 28, 25,  2,  5,  7,  7,  7,  7,  7,  7,  7,  7])"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_model.cv_results_['rank_test_accuracy']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "4f10e207",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'classifier': LogisticRegression(max_iter=10000),\n",
       " 'classifier__C': 0.0001,\n",
       " 'classifier__penalty': 'l1',\n",
       " 'classifier__solver': 'saga'}"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_model.cv_results_['params'][ np.argmin(best_model.cv_results_['mean_test_accuracy'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "51791136",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>classifier</th>\n",
       "      <th>C</th>\n",
       "      <th>penalty</th>\n",
       "      <th>solver</th>\n",
       "      <th>score_acc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.600167</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.738188</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.743450</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.746266</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0100</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.746350</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0100</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.746944</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.1000</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.746987</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.1000</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.746930</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.746944</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.746916</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>10.0000</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.746888</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>10.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.746916</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>100.0000</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.746916</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>100.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.746916</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>1000.0000</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.746916</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>1000.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.746916</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>10000.0000</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.746902</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>10000.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.746916</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.738216</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.746266</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0100</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.746944</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.1000</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.746930</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.746916</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>10.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.746916</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>100.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.746916</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>1000.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.746916</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>10000.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.746916</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>NaN</td>\n",
       "      <td>none</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.746916</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>NaN</td>\n",
       "      <td>none</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.746916</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.746916</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                            classifier           C penalty solver  score_acc\n",
       "0   LogisticRegression(max_iter=10000)      0.0001      l1   saga   0.600167\n",
       "1   LogisticRegression(max_iter=10000)      0.0001      l2   saga   0.738188\n",
       "2   LogisticRegression(max_iter=10000)      0.0010      l1   saga   0.743450\n",
       "3   LogisticRegression(max_iter=10000)      0.0010      l2   saga   0.746266\n",
       "4   LogisticRegression(max_iter=10000)      0.0100      l1   saga   0.746350\n",
       "5   LogisticRegression(max_iter=10000)      0.0100      l2   saga   0.746944\n",
       "6   LogisticRegression(max_iter=10000)      0.1000      l1   saga   0.746987\n",
       "7   LogisticRegression(max_iter=10000)      0.1000      l2   saga   0.746930\n",
       "8   LogisticRegression(max_iter=10000)      1.0000      l1   saga   0.746944\n",
       "9   LogisticRegression(max_iter=10000)      1.0000      l2   saga   0.746916\n",
       "10  LogisticRegression(max_iter=10000)     10.0000      l1   saga   0.746888\n",
       "11  LogisticRegression(max_iter=10000)     10.0000      l2   saga   0.746916\n",
       "12  LogisticRegression(max_iter=10000)    100.0000      l1   saga   0.746916\n",
       "13  LogisticRegression(max_iter=10000)    100.0000      l2   saga   0.746916\n",
       "14  LogisticRegression(max_iter=10000)   1000.0000      l1   saga   0.746916\n",
       "15  LogisticRegression(max_iter=10000)   1000.0000      l2   saga   0.746916\n",
       "16  LogisticRegression(max_iter=10000)  10000.0000      l1   saga   0.746902\n",
       "17  LogisticRegression(max_iter=10000)  10000.0000      l2   saga   0.746916\n",
       "18  LogisticRegression(max_iter=10000)      0.0001      l2  lbfgs   0.738216\n",
       "19  LogisticRegression(max_iter=10000)      0.0010      l2  lbfgs   0.746266\n",
       "20  LogisticRegression(max_iter=10000)      0.0100      l2  lbfgs   0.746944\n",
       "21  LogisticRegression(max_iter=10000)      0.1000      l2  lbfgs   0.746930\n",
       "22  LogisticRegression(max_iter=10000)      1.0000      l2  lbfgs   0.746916\n",
       "23  LogisticRegression(max_iter=10000)     10.0000      l2  lbfgs   0.746916\n",
       "24  LogisticRegression(max_iter=10000)    100.0000      l2  lbfgs   0.746916\n",
       "25  LogisticRegression(max_iter=10000)   1000.0000      l2  lbfgs   0.746916\n",
       "26  LogisticRegression(max_iter=10000)  10000.0000      l2  lbfgs   0.746916\n",
       "27  LogisticRegression(max_iter=10000)         NaN    none  lbfgs   0.746916\n",
       "28  LogisticRegression(max_iter=10000)         NaN    none   saga   0.746916\n",
       "29  LogisticRegression(max_iter=10000)         NaN     NaN    NaN   0.746916"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results = pd.DataFrame( best_model.cv_results_['params'] )\n",
    "\n",
    "results['score_acc'] = best_model.cv_results_['mean_test_accuracy']\n",
    "cols = results.columns.to_series().str.split('__').apply(lambda x: x[-1])\n",
    "results.columns = cols\n",
    "\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "153b94ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "acc_list.append(results[0:1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "a7cbb487",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'classifier': LogisticRegression(max_iter=10000),\n",
       " 'classifier__C': 0.0001,\n",
       " 'classifier__penalty': 'l2',\n",
       " 'classifier__solver': 'lbfgs'}"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_model.cv_results_['params'][ np.argmin(best_model.cv_results_['rank_test_precision'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "155a06f9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>classifier</th>\n",
       "      <th>C</th>\n",
       "      <th>penalty</th>\n",
       "      <th>solver</th>\n",
       "      <th>score_precision</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.711224</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.698994</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.698263</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0100</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.695633</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0100</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.696157</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.1000</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.695861</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.1000</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.695828</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.695825</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.695773</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>10.0000</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.695720</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>10.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.695773</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>100.0000</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.695773</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>100.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.695773</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>1000.0000</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.695773</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>1000.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.695773</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>10000.0000</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.695746</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>10000.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.695773</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.711268</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.698263</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0100</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.696157</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.1000</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.695828</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.695773</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>10.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.695773</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>100.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.695773</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>1000.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.695773</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>10000.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.695773</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>NaN</td>\n",
       "      <td>none</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.695773</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>NaN</td>\n",
       "      <td>none</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.695773</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.695773</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                            classifier           C penalty solver  \\\n",
       "0   LogisticRegression(max_iter=10000)      0.0001      l1   saga   \n",
       "1   LogisticRegression(max_iter=10000)      0.0001      l2   saga   \n",
       "2   LogisticRegression(max_iter=10000)      0.0010      l1   saga   \n",
       "3   LogisticRegression(max_iter=10000)      0.0010      l2   saga   \n",
       "4   LogisticRegression(max_iter=10000)      0.0100      l1   saga   \n",
       "5   LogisticRegression(max_iter=10000)      0.0100      l2   saga   \n",
       "6   LogisticRegression(max_iter=10000)      0.1000      l1   saga   \n",
       "7   LogisticRegression(max_iter=10000)      0.1000      l2   saga   \n",
       "8   LogisticRegression(max_iter=10000)      1.0000      l1   saga   \n",
       "9   LogisticRegression(max_iter=10000)      1.0000      l2   saga   \n",
       "10  LogisticRegression(max_iter=10000)     10.0000      l1   saga   \n",
       "11  LogisticRegression(max_iter=10000)     10.0000      l2   saga   \n",
       "12  LogisticRegression(max_iter=10000)    100.0000      l1   saga   \n",
       "13  LogisticRegression(max_iter=10000)    100.0000      l2   saga   \n",
       "14  LogisticRegression(max_iter=10000)   1000.0000      l1   saga   \n",
       "15  LogisticRegression(max_iter=10000)   1000.0000      l2   saga   \n",
       "16  LogisticRegression(max_iter=10000)  10000.0000      l1   saga   \n",
       "17  LogisticRegression(max_iter=10000)  10000.0000      l2   saga   \n",
       "18  LogisticRegression(max_iter=10000)      0.0001      l2  lbfgs   \n",
       "19  LogisticRegression(max_iter=10000)      0.0010      l2  lbfgs   \n",
       "20  LogisticRegression(max_iter=10000)      0.0100      l2  lbfgs   \n",
       "21  LogisticRegression(max_iter=10000)      0.1000      l2  lbfgs   \n",
       "22  LogisticRegression(max_iter=10000)      1.0000      l2  lbfgs   \n",
       "23  LogisticRegression(max_iter=10000)     10.0000      l2  lbfgs   \n",
       "24  LogisticRegression(max_iter=10000)    100.0000      l2  lbfgs   \n",
       "25  LogisticRegression(max_iter=10000)   1000.0000      l2  lbfgs   \n",
       "26  LogisticRegression(max_iter=10000)  10000.0000      l2  lbfgs   \n",
       "27  LogisticRegression(max_iter=10000)         NaN    none  lbfgs   \n",
       "28  LogisticRegression(max_iter=10000)         NaN    none   saga   \n",
       "29  LogisticRegression(max_iter=10000)         NaN     NaN    NaN   \n",
       "\n",
       "    score_precision  \n",
       "0          0.000000  \n",
       "1          0.711224  \n",
       "2          0.698994  \n",
       "3          0.698263  \n",
       "4          0.695633  \n",
       "5          0.696157  \n",
       "6          0.695861  \n",
       "7          0.695828  \n",
       "8          0.695825  \n",
       "9          0.695773  \n",
       "10         0.695720  \n",
       "11         0.695773  \n",
       "12         0.695773  \n",
       "13         0.695773  \n",
       "14         0.695773  \n",
       "15         0.695773  \n",
       "16         0.695746  \n",
       "17         0.695773  \n",
       "18         0.711268  \n",
       "19         0.698263  \n",
       "20         0.696157  \n",
       "21         0.695828  \n",
       "22         0.695773  \n",
       "23         0.695773  \n",
       "24         0.695773  \n",
       "25         0.695773  \n",
       "26         0.695773  \n",
       "27         0.695773  \n",
       "28         0.695773  \n",
       "29         0.695773  "
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results = pd.DataFrame( best_model.cv_results_['params'] )\n",
    "\n",
    "results['score_precision'] = best_model.cv_results_['mean_test_precision']\n",
    "cols = results.columns.to_series().str.split('__').apply(lambda x: x[-1])\n",
    "results.columns = cols\n",
    "\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "a4de3306",
   "metadata": {},
   "outputs": [],
   "source": [
    "roc_auc_ovr_List.append(results[18:19])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "58513be7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'classifier': LogisticRegression(max_iter=10000),\n",
       " 'classifier__C': 0.1,\n",
       " 'classifier__penalty': 'l1',\n",
       " 'classifier__solver': 'saga'}"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_model.cv_results_['params'][ np.argmin(best_model.cv_results_['rank_test_f1_micro'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "6f4588d3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>classifier</th>\n",
       "      <th>C</th>\n",
       "      <th>penalty</th>\n",
       "      <th>solver</th>\n",
       "      <th>score_f1_micro</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.600167</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.738188</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.743450</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.746266</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0100</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.746350</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0100</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.746944</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.1000</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.746987</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.1000</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.746930</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.746944</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.746916</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>10.0000</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.746888</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>10.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.746916</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>100.0000</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.746916</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>100.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.746916</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>1000.0000</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.746916</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>1000.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.746916</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>10000.0000</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.746902</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>10000.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.746916</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.738216</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.746266</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0100</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.746944</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.1000</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.746930</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.746916</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>10.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.746916</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>100.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.746916</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>1000.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.746916</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>10000.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.746916</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>NaN</td>\n",
       "      <td>none</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.746916</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>NaN</td>\n",
       "      <td>none</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.746916</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.746916</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                            classifier           C penalty solver  \\\n",
       "0   LogisticRegression(max_iter=10000)      0.0001      l1   saga   \n",
       "1   LogisticRegression(max_iter=10000)      0.0001      l2   saga   \n",
       "2   LogisticRegression(max_iter=10000)      0.0010      l1   saga   \n",
       "3   LogisticRegression(max_iter=10000)      0.0010      l2   saga   \n",
       "4   LogisticRegression(max_iter=10000)      0.0100      l1   saga   \n",
       "5   LogisticRegression(max_iter=10000)      0.0100      l2   saga   \n",
       "6   LogisticRegression(max_iter=10000)      0.1000      l1   saga   \n",
       "7   LogisticRegression(max_iter=10000)      0.1000      l2   saga   \n",
       "8   LogisticRegression(max_iter=10000)      1.0000      l1   saga   \n",
       "9   LogisticRegression(max_iter=10000)      1.0000      l2   saga   \n",
       "10  LogisticRegression(max_iter=10000)     10.0000      l1   saga   \n",
       "11  LogisticRegression(max_iter=10000)     10.0000      l2   saga   \n",
       "12  LogisticRegression(max_iter=10000)    100.0000      l1   saga   \n",
       "13  LogisticRegression(max_iter=10000)    100.0000      l2   saga   \n",
       "14  LogisticRegression(max_iter=10000)   1000.0000      l1   saga   \n",
       "15  LogisticRegression(max_iter=10000)   1000.0000      l2   saga   \n",
       "16  LogisticRegression(max_iter=10000)  10000.0000      l1   saga   \n",
       "17  LogisticRegression(max_iter=10000)  10000.0000      l2   saga   \n",
       "18  LogisticRegression(max_iter=10000)      0.0001      l2  lbfgs   \n",
       "19  LogisticRegression(max_iter=10000)      0.0010      l2  lbfgs   \n",
       "20  LogisticRegression(max_iter=10000)      0.0100      l2  lbfgs   \n",
       "21  LogisticRegression(max_iter=10000)      0.1000      l2  lbfgs   \n",
       "22  LogisticRegression(max_iter=10000)      1.0000      l2  lbfgs   \n",
       "23  LogisticRegression(max_iter=10000)     10.0000      l2  lbfgs   \n",
       "24  LogisticRegression(max_iter=10000)    100.0000      l2  lbfgs   \n",
       "25  LogisticRegression(max_iter=10000)   1000.0000      l2  lbfgs   \n",
       "26  LogisticRegression(max_iter=10000)  10000.0000      l2  lbfgs   \n",
       "27  LogisticRegression(max_iter=10000)         NaN    none  lbfgs   \n",
       "28  LogisticRegression(max_iter=10000)         NaN    none   saga   \n",
       "29  LogisticRegression(max_iter=10000)         NaN     NaN    NaN   \n",
       "\n",
       "    score_f1_micro  \n",
       "0         0.600167  \n",
       "1         0.738188  \n",
       "2         0.743450  \n",
       "3         0.746266  \n",
       "4         0.746350  \n",
       "5         0.746944  \n",
       "6         0.746987  \n",
       "7         0.746930  \n",
       "8         0.746944  \n",
       "9         0.746916  \n",
       "10        0.746888  \n",
       "11        0.746916  \n",
       "12        0.746916  \n",
       "13        0.746916  \n",
       "14        0.746916  \n",
       "15        0.746916  \n",
       "16        0.746902  \n",
       "17        0.746916  \n",
       "18        0.738216  \n",
       "19        0.746266  \n",
       "20        0.746944  \n",
       "21        0.746930  \n",
       "22        0.746916  \n",
       "23        0.746916  \n",
       "24        0.746916  \n",
       "25        0.746916  \n",
       "26        0.746916  \n",
       "27        0.746916  \n",
       "28        0.746916  \n",
       "29        0.746916  "
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results = pd.DataFrame( best_model.cv_results_['params'] )\n",
    "\n",
    "results['score_f1_micro'] = best_model.cv_results_['mean_test_f1_micro']\n",
    "cols = results.columns.to_series().str.split('__').apply(lambda x: x[-1])\n",
    "results.columns = cols\n",
    "\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "4f243c59",
   "metadata": {},
   "outputs": [],
   "source": [
    "f1_micro_List.append(results[6:7])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94d1e17e",
   "metadata": {},
   "source": [
    "## Trial 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "82374d4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, Y_train, Y_test = train_test_split(X, y, test_size=0.2, shuffle=True, random_state = 6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "64a330b7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Guest-All\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\Guest-All\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\Guest-All\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\Guest-All\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\Guest-All\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\Guest-All\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\Guest-All\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\Guest-All\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\Guest-All\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\Guest-All\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\Guest-All\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1183: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Guest-All\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1183: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Guest-All\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1183: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Guest-All\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1183: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Guest-All\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1183: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Guest-All\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1183: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Guest-All\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1183: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Guest-All\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1183: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Guest-All\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1183: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Guest-All\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1183: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Guest-All\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1183: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Guest-All\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1183: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Guest-All\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1183: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Guest-All\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1183: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Guest-All\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1183: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Guest-All\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1183: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Guest-All\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1183: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Guest-All\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1183: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Guest-All\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1183: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Guest-All\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1183: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Create a pipeline - RF is a stand in, we will populate the classifier part below\n",
    "pipe = Pipeline([('std', StandardScaler()),\n",
    "                 ('classifier', LogisticRegression())])\n",
    "\n",
    "# Create search space of candidate learning algorithms and their hyperparameters\n",
    "# note lbfgs can't do l1, and if you pass penalty='none' it expects no C value\n",
    "search_space = [{'classifier': [LogisticRegression(max_iter=10000)],\n",
    "                 'classifier__solver': ['saga'],\n",
    "                 'classifier__penalty': ['l1', 'l2'],\n",
    "                 'classifier__C': np.logspace(-4, 4, 9)},\n",
    "                {'classifier': [LogisticRegression(max_iter=10000)],\n",
    "                 'classifier__solver': ['lbfgs'],\n",
    "                 'classifier__penalty': ['l2'],\n",
    "                 'classifier__C': np.logspace(-4, 4, 9)},\n",
    "                {'classifier': [LogisticRegression(max_iter=10000)],\n",
    "                 'classifier__solver': ['lbfgs','saga'],\n",
    "                 'classifier__penalty': ['none']},\n",
    "                {'classifier': [LogisticRegression(max_iter=10000)]}\n",
    "                ]\n",
    "\n",
    "# Create grid search\n",
    "clf = GridSearchCV(pipe, search_space, cv=StratifiedKFold(n_splits=10),\n",
    "                   scoring=['accuracy', 'precision', 'f1_micro'], refit=False,\n",
    "                   verbose=0)\n",
    "\n",
    "# Fit grid search\n",
    "best_model = clf.fit(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "b0094308",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'mean_fit_time': array([0.7238282 , 0.68301589, 0.78766301, 0.68140607, 0.93995371,\n",
       "        0.8689219 , 0.94830196, 0.86871164, 0.95786221, 0.82102621,\n",
       "        0.98706701, 0.94717734, 1.00512938, 0.86453068, 0.84875662,\n",
       "        0.74552827, 0.84238129, 0.73053272, 0.0881928 , 0.10968409,\n",
       "        0.10658653, 0.1032645 , 0.11247077, 0.11144149, 0.10793869,\n",
       "        0.11058743, 0.11037636, 0.10850587, 0.70750315, 0.10802708]),\n",
       " 'std_fit_time': array([0.05273963, 0.0525078 , 0.05275092, 0.0526618 , 0.19288239,\n",
       "        0.1593121 , 0.23436877, 0.21568159, 0.23249995, 0.21485933,\n",
       "        0.15452449, 0.23078503, 0.18278039, 0.19824046, 0.04588361,\n",
       "        0.0550034 , 0.04550636, 0.02985085, 0.00176807, 0.01031615,\n",
       "        0.0030262 , 0.00489431, 0.0152273 , 0.00840067, 0.0069901 ,\n",
       "        0.00601162, 0.01008524, 0.00521619, 0.06967234, 0.00573974]),\n",
       " 'mean_score_time': array([0.01214473, 0.01318908, 0.01283658, 0.01276052, 0.01828327,\n",
       "        0.01596189, 0.01122289, 0.01884975, 0.01533678, 0.01294482,\n",
       "        0.01627233, 0.01768112, 0.01434982, 0.01850073, 0.01290586,\n",
       "        0.00969644, 0.01442373, 0.0120326 , 0.01213348, 0.01203814,\n",
       "        0.01126077, 0.0125268 , 0.0144141 , 0.01441407, 0.0136467 ,\n",
       "        0.01296635, 0.01493497, 0.01397581, 0.01533608, 0.01407545]),\n",
       " 'std_score_time': array([0.00364779, 0.00387922, 0.003969  , 0.00388699, 0.01295613,\n",
       "        0.00830218, 0.00394823, 0.00802308, 0.00843702, 0.00497599,\n",
       "        0.0075789 , 0.0069955 , 0.00779141, 0.00878343, 0.00401624,\n",
       "        0.00318671, 0.00300708, 0.0040654 , 0.00383754, 0.00397503,\n",
       "        0.00389844, 0.00371143, 0.00320252, 0.0032156 , 0.00375246,\n",
       "        0.00385892, 0.00377682, 0.00399821, 0.00153779, 0.00471793]),\n",
       " 'param_classifier': masked_array(data=[LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000)],\n",
       "              mask=[False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'param_classifier__C': masked_array(data=[0.0001, 0.0001, 0.001, 0.001, 0.01, 0.01, 0.1, 0.1,\n",
       "                    1.0, 1.0, 10.0, 10.0, 100.0, 100.0, 1000.0, 1000.0,\n",
       "                    10000.0, 10000.0, 0.0001, 0.001, 0.01, 0.1, 1.0, 10.0,\n",
       "                    100.0, 1000.0, 10000.0, --, --, --],\n",
       "              mask=[False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False,  True,  True,  True],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'param_classifier__penalty': masked_array(data=['l1', 'l2', 'l1', 'l2', 'l1', 'l2', 'l1', 'l2', 'l1',\n",
       "                    'l2', 'l1', 'l2', 'l1', 'l2', 'l1', 'l2', 'l1', 'l2',\n",
       "                    'l2', 'l2', 'l2', 'l2', 'l2', 'l2', 'l2', 'l2', 'l2',\n",
       "                    'none', 'none', --],\n",
       "              mask=[False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False,  True],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'param_classifier__solver': masked_array(data=['saga', 'saga', 'saga', 'saga', 'saga', 'saga', 'saga',\n",
       "                    'saga', 'saga', 'saga', 'saga', 'saga', 'saga', 'saga',\n",
       "                    'saga', 'saga', 'saga', 'saga', 'lbfgs', 'lbfgs',\n",
       "                    'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs',\n",
       "                    'lbfgs', 'lbfgs', 'saga', --],\n",
       "              mask=[False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False,  True],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'params': [{'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 0.0001,\n",
       "   'classifier__penalty': 'l1',\n",
       "   'classifier__solver': 'saga'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 0.0001,\n",
       "   'classifier__penalty': 'l2',\n",
       "   'classifier__solver': 'saga'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 0.001,\n",
       "   'classifier__penalty': 'l1',\n",
       "   'classifier__solver': 'saga'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 0.001,\n",
       "   'classifier__penalty': 'l2',\n",
       "   'classifier__solver': 'saga'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 0.01,\n",
       "   'classifier__penalty': 'l1',\n",
       "   'classifier__solver': 'saga'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 0.01,\n",
       "   'classifier__penalty': 'l2',\n",
       "   'classifier__solver': 'saga'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 0.1,\n",
       "   'classifier__penalty': 'l1',\n",
       "   'classifier__solver': 'saga'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 0.1,\n",
       "   'classifier__penalty': 'l2',\n",
       "   'classifier__solver': 'saga'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 1.0,\n",
       "   'classifier__penalty': 'l1',\n",
       "   'classifier__solver': 'saga'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 1.0,\n",
       "   'classifier__penalty': 'l2',\n",
       "   'classifier__solver': 'saga'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 10.0,\n",
       "   'classifier__penalty': 'l1',\n",
       "   'classifier__solver': 'saga'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 10.0,\n",
       "   'classifier__penalty': 'l2',\n",
       "   'classifier__solver': 'saga'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 100.0,\n",
       "   'classifier__penalty': 'l1',\n",
       "   'classifier__solver': 'saga'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 100.0,\n",
       "   'classifier__penalty': 'l2',\n",
       "   'classifier__solver': 'saga'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 1000.0,\n",
       "   'classifier__penalty': 'l1',\n",
       "   'classifier__solver': 'saga'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 1000.0,\n",
       "   'classifier__penalty': 'l2',\n",
       "   'classifier__solver': 'saga'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 10000.0,\n",
       "   'classifier__penalty': 'l1',\n",
       "   'classifier__solver': 'saga'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 10000.0,\n",
       "   'classifier__penalty': 'l2',\n",
       "   'classifier__solver': 'saga'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 0.0001,\n",
       "   'classifier__penalty': 'l2',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 0.001,\n",
       "   'classifier__penalty': 'l2',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 0.01,\n",
       "   'classifier__penalty': 'l2',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 0.1,\n",
       "   'classifier__penalty': 'l2',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 1.0,\n",
       "   'classifier__penalty': 'l2',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 10.0,\n",
       "   'classifier__penalty': 'l2',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 100.0,\n",
       "   'classifier__penalty': 'l2',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 1000.0,\n",
       "   'classifier__penalty': 'l2',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 10000.0,\n",
       "   'classifier__penalty': 'l2',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__penalty': 'none',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__penalty': 'none',\n",
       "   'classifier__solver': 'saga'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000)}],\n",
       " 'split0_test_accuracy': array([0.59943423, 0.73536068, 0.74186704, 0.73932107, 0.74002829,\n",
       "        0.74002829, 0.74002829, 0.74031117, 0.74016973, 0.74031117,\n",
       "        0.74031117, 0.74031117, 0.74031117, 0.74031117, 0.74031117,\n",
       "        0.74031117, 0.74031117, 0.74031117, 0.73536068, 0.73932107,\n",
       "        0.74002829, 0.74031117, 0.74031117, 0.74031117, 0.74031117,\n",
       "        0.74031117, 0.74031117, 0.74031117, 0.74031117, 0.74031117]),\n",
       " 'split1_test_accuracy': array([0.59943423, 0.73422914, 0.73536068, 0.74016973, 0.74031117,\n",
       "        0.74059406, 0.74059406, 0.74002829, 0.74016973, 0.74016973,\n",
       "        0.74016973, 0.74016973, 0.74016973, 0.74016973, 0.74016973,\n",
       "        0.74016973, 0.74016973, 0.74016973, 0.73422914, 0.74016973,\n",
       "        0.74059406, 0.74002829, 0.74016973, 0.74016973, 0.74016973,\n",
       "        0.74016973, 0.74016973, 0.74016973, 0.74016973, 0.74016973]),\n",
       " 'split2_test_accuracy': array([0.59937756, 0.74183053, 0.74225492, 0.74338662, 0.7426793 ,\n",
       "        0.74423539, 0.74409393, 0.74423539, 0.74409393, 0.74409393,\n",
       "        0.74409393, 0.74409393, 0.74409393, 0.74409393, 0.74409393,\n",
       "        0.74409393, 0.74409393, 0.74409393, 0.74183053, 0.74338662,\n",
       "        0.74423539, 0.74423539, 0.74409393, 0.74409393, 0.74409393,\n",
       "        0.74409393, 0.74409393, 0.74409393, 0.74409393, 0.74409393]),\n",
       " 'split3_test_accuracy': array([0.59937756, 0.74140614, 0.74550856, 0.74890366, 0.75031829,\n",
       "        0.74904513, 0.74975244, 0.74961098, 0.74961098, 0.74975244,\n",
       "        0.74961098, 0.74961098, 0.74961098, 0.74961098, 0.74961098,\n",
       "        0.74961098, 0.74961098, 0.74961098, 0.74140614, 0.74890366,\n",
       "        0.74890366, 0.74961098, 0.74961098, 0.74961098, 0.74961098,\n",
       "        0.74961098, 0.74961098, 0.74961098, 0.74961098, 0.74961098]),\n",
       " 'split4_test_accuracy': array([0.59937756, 0.7453671 , 0.75031829, 0.75159146, 0.75144999,\n",
       "        0.75201584, 0.75201584, 0.75215731, 0.75229877, 0.75215731,\n",
       "        0.75229877, 0.75229877, 0.75229877, 0.75229877, 0.75229877,\n",
       "        0.75229877, 0.75229877, 0.75229877, 0.7453671 , 0.75159146,\n",
       "        0.75201584, 0.75215731, 0.75215731, 0.75229877, 0.75229877,\n",
       "        0.75229877, 0.75229877, 0.75229877, 0.75229877, 0.75215731]),\n",
       " 'split5_test_accuracy': array([0.59937756, 0.73107936, 0.73786957, 0.74055736, 0.74239638,\n",
       "        0.74183053, 0.74239638, 0.74296223, 0.74282077, 0.74296223,\n",
       "        0.74296223, 0.74296223, 0.74296223, 0.74296223, 0.74296223,\n",
       "        0.74296223, 0.74296223, 0.74296223, 0.73107936, 0.74055736,\n",
       "        0.74183053, 0.74296223, 0.74296223, 0.74296223, 0.74296223,\n",
       "        0.74296223, 0.74296223, 0.74296223, 0.74296223, 0.74296223]),\n",
       " 'split6_test_accuracy': array([0.59937756, 0.73008912, 0.73744518, 0.74055736, 0.74253784,\n",
       "        0.74126468, 0.74183053, 0.74168906, 0.7415476 , 0.7415476 ,\n",
       "        0.7415476 , 0.7415476 , 0.7415476 , 0.7415476 , 0.7415476 ,\n",
       "        0.7415476 , 0.74168906, 0.7415476 , 0.73008912, 0.74055736,\n",
       "        0.74126468, 0.7415476 , 0.7415476 , 0.7415476 , 0.7415476 ,\n",
       "        0.7415476 , 0.7415476 , 0.7415476 , 0.7415476 , 0.7415476 ]),\n",
       " 'split7_test_accuracy': array([0.59937756, 0.73603056, 0.74324515, 0.74494271, 0.74565002,\n",
       "        0.74565002, 0.74607441, 0.74607441, 0.74621587, 0.74621587,\n",
       "        0.74621587, 0.74607441, 0.74621587, 0.74621587, 0.74621587,\n",
       "        0.74621587, 0.74621587, 0.74621587, 0.73617202, 0.74494271,\n",
       "        0.74565002, 0.74607441, 0.74593295, 0.74607441, 0.74607441,\n",
       "        0.74607441, 0.74607441, 0.74607441, 0.74621587, 0.74593295]),\n",
       " 'split8_test_accuracy': array([0.59937756, 0.73985005, 0.74847928, 0.75159146, 0.75215731,\n",
       "        0.75244023, 0.75258169, 0.75215731, 0.75215731, 0.75215731,\n",
       "        0.75215731, 0.75215731, 0.75215731, 0.75215731, 0.75215731,\n",
       "        0.75215731, 0.75215731, 0.75215731, 0.73985005, 0.75173292,\n",
       "        0.75244023, 0.75215731, 0.75215731, 0.75215731, 0.75215731,\n",
       "        0.75215731, 0.75215731, 0.75215731, 0.75215731, 0.75215731]),\n",
       " 'split9_test_accuracy': array([0.59937756, 0.74522563, 0.7510256 , 0.7548451 , 0.75456217,\n",
       "        0.75442071, 0.75413778, 0.75456217, 0.75456217, 0.75456217,\n",
       "        0.75456217, 0.75456217, 0.75456217, 0.75456217, 0.75456217,\n",
       "        0.75456217, 0.75456217, 0.75456217, 0.74522563, 0.7548451 ,\n",
       "        0.75442071, 0.75456217, 0.75456217, 0.75456217, 0.75456217,\n",
       "        0.75456217, 0.75456217, 0.75456217, 0.75456217, 0.75456217]),\n",
       " 'mean_test_accuracy': array([0.5993889 , 0.73804683, 0.74333743, 0.74558665, 0.74620908,\n",
       "        0.74615249, 0.74635054, 0.74637883, 0.74636469, 0.74639298,\n",
       "        0.74639298, 0.74637883, 0.74639298, 0.74639298, 0.74639298,\n",
       "        0.74639298, 0.74640712, 0.74639298, 0.73806098, 0.7456008 ,\n",
       "        0.74613834, 0.74636469, 0.74635054, 0.74637883, 0.74637883,\n",
       "        0.74637883, 0.74637883, 0.74637883, 0.74639298, 0.74635054]),\n",
       " 'std_test_accuracy': array([2.26660501e-05, 5.21249205e-03, 5.20172880e-03, 5.41552882e-03,\n",
       "        5.13110489e-03, 5.15310810e-03, 5.07424309e-03, 5.09603113e-03,\n",
       "        5.14005893e-03, 5.10645203e-03, 5.11350119e-03, 5.11416721e-03,\n",
       "        5.11350119e-03, 5.11350119e-03, 5.11350119e-03, 5.11350119e-03,\n",
       "        5.10025564e-03, 5.11350119e-03, 5.20719012e-03, 5.43135753e-03,\n",
       "        5.14533615e-03, 5.10920931e-03, 5.09892448e-03, 5.11416721e-03,\n",
       "        5.11416721e-03, 5.11416721e-03, 5.11416721e-03, 5.11416721e-03,\n",
       "        5.11350119e-03, 5.09892448e-03]),\n",
       " 'rank_test_accuracy': array([30, 29, 27, 26, 22, 23, 21, 10, 17,  2,  2, 11,  2,  2,  2,  2,  1,\n",
       "         2, 28, 25, 24, 17, 19, 11, 11, 11, 11, 11,  2, 19]),\n",
       " 'split0_test_precision': array([0.        , 0.70845987, 0.69861933, 0.69085295, 0.6891172 ,\n",
       "        0.68882979, 0.68854325, 0.68892261, 0.68880455, 0.68892261,\n",
       "        0.68892261, 0.68892261, 0.68892261, 0.68892261, 0.68892261,\n",
       "        0.68892261, 0.68892261, 0.68892261, 0.70845987, 0.69085295,\n",
       "        0.68882979, 0.68892261, 0.68892261, 0.68892261, 0.68892261,\n",
       "        0.68892261, 0.68892261, 0.68892261, 0.68892261, 0.68892261]),\n",
       " 'split1_test_precision': array([0.        , 0.70459425, 0.68865332, 0.6890916 , 0.68707739,\n",
       "        0.68717179, 0.68661182, 0.68572496, 0.68584236, 0.68584236,\n",
       "        0.68584236, 0.68584236, 0.68584236, 0.68584236, 0.68584236,\n",
       "        0.68584236, 0.68584236, 0.68584236, 0.70459425, 0.6890916 ,\n",
       "        0.68717179, 0.68572496, 0.68584236, 0.68584236, 0.68584236,\n",
       "        0.68584236, 0.68584236, 0.68584236, 0.68584236, 0.68584236]),\n",
       " 'split2_test_precision': array([0.        , 0.7161872 , 0.69711163, 0.69472073, 0.6913487 ,\n",
       "        0.69262603, 0.69222097, 0.6924812 , 0.69222097, 0.69222097,\n",
       "        0.69222097, 0.69222097, 0.69222097, 0.69222097, 0.69222097,\n",
       "        0.69222097, 0.69222097, 0.69222097, 0.7161872 , 0.69472073,\n",
       "        0.69262603, 0.6924812 , 0.69222097, 0.69222097, 0.69222097,\n",
       "        0.69222097, 0.69222097, 0.69222097, 0.69222097, 0.69222097]),\n",
       " 'split3_test_precision': array([0.        , 0.71600688, 0.69949788, 0.70287908, 0.7010931 ,\n",
       "        0.69962264, 0.69988718, 0.6999247 , 0.69977427, 0.70003764,\n",
       "        0.69977427, 0.69977427, 0.69977427, 0.69977427, 0.69977427,\n",
       "        0.69977427, 0.69977427, 0.69977427, 0.71600688, 0.70287908,\n",
       "        0.69935873, 0.6999247 , 0.69977427, 0.69977427, 0.69977427,\n",
       "        0.69977427, 0.69977427, 0.69977427, 0.69977427, 0.69977427]),\n",
       " 'split4_test_precision': array([0.        , 0.72395833, 0.70929776, 0.70756173, 0.70413976,\n",
       "        0.70505511, 0.70489935, 0.70516717, 0.70527915, 0.70516717,\n",
       "        0.70527915, 0.70527915, 0.70527915, 0.70527915, 0.70527915,\n",
       "        0.70527915, 0.70527915, 0.70527915, 0.72395833, 0.70756173,\n",
       "        0.70505511, 0.70516717, 0.70516717, 0.70527915, 0.70527915,\n",
       "        0.70527915, 0.70527915, 0.70527915, 0.70527915, 0.70516717]),\n",
       " 'split5_test_precision': array([0.        , 0.70247934, 0.69113627, 0.69016768, 0.69053901,\n",
       "        0.68964218, 0.68996618, 0.69071778, 0.69031532, 0.69057454,\n",
       "        0.69057454, 0.69057454, 0.69057454, 0.69057454, 0.69057454,\n",
       "        0.69057454, 0.69057454, 0.69057454, 0.70247934, 0.69016768,\n",
       "        0.68964218, 0.69071778, 0.69057454, 0.69057454, 0.69057454,\n",
       "        0.69057454, 0.69057454, 0.69057454, 0.69057454, 0.69057454]),\n",
       " 'split6_test_precision': array([0.        , 0.69948187, 0.69152276, 0.69016768, 0.69051205,\n",
       "        0.68832144, 0.68907247, 0.68867217, 0.68841395, 0.68827276,\n",
       "        0.68827276, 0.68827276, 0.68827276, 0.68827276, 0.68827276,\n",
       "        0.68827276, 0.68853073, 0.68827276, 0.69948187, 0.69016768,\n",
       "        0.68832144, 0.68841395, 0.68827276, 0.68827276, 0.68827276,\n",
       "        0.68827276, 0.68827276, 0.68827276, 0.68827276, 0.68827276]),\n",
       " 'split7_test_precision': array([0.        , 0.70963542, 0.70011806, 0.69735328, 0.69583333,\n",
       "        0.69538927, 0.69543913, 0.69558657, 0.69570136, 0.69570136,\n",
       "        0.69570136, 0.69543913, 0.69570136, 0.69570136, 0.69570136,\n",
       "        0.69570136, 0.69570136, 0.69570136, 0.70976139, 0.69735328,\n",
       "        0.69538927, 0.69558657, 0.69532428, 0.69543913, 0.69543913,\n",
       "        0.69543913, 0.69543913, 0.69543913, 0.69570136, 0.69532428]),\n",
       " 'split8_test_precision': array([0.        , 0.70852583, 0.7012987 , 0.70104634, 0.69970414,\n",
       "        0.69977843, 0.69974179, 0.69926199, 0.69926199, 0.69926199,\n",
       "        0.69926199, 0.69926199, 0.69926199, 0.69926199, 0.69926199,\n",
       "        0.69926199, 0.69926199, 0.69926199, 0.70852583, 0.70130841,\n",
       "        0.69977843, 0.69926199, 0.69926199, 0.69926199, 0.69926199,\n",
       "        0.69926199, 0.69926199, 0.69926199, 0.69926199, 0.69926199]),\n",
       " 'split9_test_precision': array([0.        , 0.72191132, 0.71457166, 0.71224411, 0.7091117 ,\n",
       "        0.70820669, 0.70766894, 0.70815939, 0.70815939, 0.70815939,\n",
       "        0.70815939, 0.70815939, 0.70815939, 0.70815939, 0.70815939,\n",
       "        0.70815939, 0.70815939, 0.70815939, 0.72191132, 0.71224411,\n",
       "        0.70820669, 0.70815939, 0.70815939, 0.70815939, 0.70815939,\n",
       "        0.70815939, 0.70815939, 0.70815939, 0.70815939, 0.70815939]),\n",
       " 'mean_test_precision': array([0.        , 0.71112403, 0.69918274, 0.69760852, 0.69584764,\n",
       "        0.69546434, 0.69540511, 0.69546185, 0.69537733, 0.69541608,\n",
       "        0.69540094, 0.69537472, 0.69540094, 0.69540094, 0.69540094,\n",
       "        0.69540094, 0.69542674, 0.69540094, 0.71113663, 0.69763473,\n",
       "        0.69543795, 0.69543603, 0.69535204, 0.69537472, 0.69537472,\n",
       "        0.69537472, 0.69537472, 0.69537472, 0.69540094, 0.69535204]),\n",
       " 'std_test_precision': array([0.        , 0.00774297, 0.00763859, 0.00768229, 0.00697273,\n",
       "        0.00704285, 0.00695463, 0.0071053 , 0.00717041, 0.00715685,\n",
       "        0.00715565, 0.00715498, 0.00715565, 0.00715565, 0.00715565,\n",
       "        0.00715565, 0.00713033, 0.00715565, 0.00774064, 0.00769441,\n",
       "        0.00702769, 0.00713036, 0.00713951, 0.00715498, 0.00715498,\n",
       "        0.00715498, 0.00715498, 0.00715498, 0.00715565, 0.00713951]),\n",
       " 'rank_test_precision': array([30,  2,  3,  5,  6,  7, 13,  8, 21, 12, 14, 22, 14, 14, 14, 14, 11,\n",
       "        14,  1,  4,  9, 10, 28, 22, 22, 22, 22, 22, 14, 28]),\n",
       " 'split0_test_f1_micro': array([0.59943423, 0.73536068, 0.74186704, 0.73932107, 0.74002829,\n",
       "        0.74002829, 0.74002829, 0.74031117, 0.74016973, 0.74031117,\n",
       "        0.74031117, 0.74031117, 0.74031117, 0.74031117, 0.74031117,\n",
       "        0.74031117, 0.74031117, 0.74031117, 0.73536068, 0.73932107,\n",
       "        0.74002829, 0.74031117, 0.74031117, 0.74031117, 0.74031117,\n",
       "        0.74031117, 0.74031117, 0.74031117, 0.74031117, 0.74031117]),\n",
       " 'split1_test_f1_micro': array([0.59943423, 0.73422914, 0.73536068, 0.74016973, 0.74031117,\n",
       "        0.74059406, 0.74059406, 0.74002829, 0.74016973, 0.74016973,\n",
       "        0.74016973, 0.74016973, 0.74016973, 0.74016973, 0.74016973,\n",
       "        0.74016973, 0.74016973, 0.74016973, 0.73422914, 0.74016973,\n",
       "        0.74059406, 0.74002829, 0.74016973, 0.74016973, 0.74016973,\n",
       "        0.74016973, 0.74016973, 0.74016973, 0.74016973, 0.74016973]),\n",
       " 'split2_test_f1_micro': array([0.59937756, 0.74183053, 0.74225492, 0.74338662, 0.7426793 ,\n",
       "        0.74423539, 0.74409393, 0.74423539, 0.74409393, 0.74409393,\n",
       "        0.74409393, 0.74409393, 0.74409393, 0.74409393, 0.74409393,\n",
       "        0.74409393, 0.74409393, 0.74409393, 0.74183053, 0.74338662,\n",
       "        0.74423539, 0.74423539, 0.74409393, 0.74409393, 0.74409393,\n",
       "        0.74409393, 0.74409393, 0.74409393, 0.74409393, 0.74409393]),\n",
       " 'split3_test_f1_micro': array([0.59937756, 0.74140614, 0.74550856, 0.74890366, 0.75031829,\n",
       "        0.74904513, 0.74975244, 0.74961098, 0.74961098, 0.74975244,\n",
       "        0.74961098, 0.74961098, 0.74961098, 0.74961098, 0.74961098,\n",
       "        0.74961098, 0.74961098, 0.74961098, 0.74140614, 0.74890366,\n",
       "        0.74890366, 0.74961098, 0.74961098, 0.74961098, 0.74961098,\n",
       "        0.74961098, 0.74961098, 0.74961098, 0.74961098, 0.74961098]),\n",
       " 'split4_test_f1_micro': array([0.59937756, 0.7453671 , 0.75031829, 0.75159146, 0.75144999,\n",
       "        0.75201584, 0.75201584, 0.75215731, 0.75229877, 0.75215731,\n",
       "        0.75229877, 0.75229877, 0.75229877, 0.75229877, 0.75229877,\n",
       "        0.75229877, 0.75229877, 0.75229877, 0.7453671 , 0.75159146,\n",
       "        0.75201584, 0.75215731, 0.75215731, 0.75229877, 0.75229877,\n",
       "        0.75229877, 0.75229877, 0.75229877, 0.75229877, 0.75215731]),\n",
       " 'split5_test_f1_micro': array([0.59937756, 0.73107936, 0.73786957, 0.74055736, 0.74239638,\n",
       "        0.74183053, 0.74239638, 0.74296223, 0.74282077, 0.74296223,\n",
       "        0.74296223, 0.74296223, 0.74296223, 0.74296223, 0.74296223,\n",
       "        0.74296223, 0.74296223, 0.74296223, 0.73107936, 0.74055736,\n",
       "        0.74183053, 0.74296223, 0.74296223, 0.74296223, 0.74296223,\n",
       "        0.74296223, 0.74296223, 0.74296223, 0.74296223, 0.74296223]),\n",
       " 'split6_test_f1_micro': array([0.59937756, 0.73008912, 0.73744518, 0.74055736, 0.74253784,\n",
       "        0.74126468, 0.74183053, 0.74168906, 0.7415476 , 0.7415476 ,\n",
       "        0.7415476 , 0.7415476 , 0.7415476 , 0.7415476 , 0.7415476 ,\n",
       "        0.7415476 , 0.74168906, 0.7415476 , 0.73008912, 0.74055736,\n",
       "        0.74126468, 0.7415476 , 0.7415476 , 0.7415476 , 0.7415476 ,\n",
       "        0.7415476 , 0.7415476 , 0.7415476 , 0.7415476 , 0.7415476 ]),\n",
       " 'split7_test_f1_micro': array([0.59937756, 0.73603056, 0.74324515, 0.74494271, 0.74565002,\n",
       "        0.74565002, 0.74607441, 0.74607441, 0.74621587, 0.74621587,\n",
       "        0.74621587, 0.74607441, 0.74621587, 0.74621587, 0.74621587,\n",
       "        0.74621587, 0.74621587, 0.74621587, 0.73617202, 0.74494271,\n",
       "        0.74565002, 0.74607441, 0.74593295, 0.74607441, 0.74607441,\n",
       "        0.74607441, 0.74607441, 0.74607441, 0.74621587, 0.74593295]),\n",
       " 'split8_test_f1_micro': array([0.59937756, 0.73985005, 0.74847928, 0.75159146, 0.75215731,\n",
       "        0.75244023, 0.75258169, 0.75215731, 0.75215731, 0.75215731,\n",
       "        0.75215731, 0.75215731, 0.75215731, 0.75215731, 0.75215731,\n",
       "        0.75215731, 0.75215731, 0.75215731, 0.73985005, 0.75173292,\n",
       "        0.75244023, 0.75215731, 0.75215731, 0.75215731, 0.75215731,\n",
       "        0.75215731, 0.75215731, 0.75215731, 0.75215731, 0.75215731]),\n",
       " 'split9_test_f1_micro': array([0.59937756, 0.74522563, 0.7510256 , 0.7548451 , 0.75456217,\n",
       "        0.75442071, 0.75413778, 0.75456217, 0.75456217, 0.75456217,\n",
       "        0.75456217, 0.75456217, 0.75456217, 0.75456217, 0.75456217,\n",
       "        0.75456217, 0.75456217, 0.75456217, 0.74522563, 0.7548451 ,\n",
       "        0.75442071, 0.75456217, 0.75456217, 0.75456217, 0.75456217,\n",
       "        0.75456217, 0.75456217, 0.75456217, 0.75456217, 0.75456217]),\n",
       " 'mean_test_f1_micro': array([0.5993889 , 0.73804683, 0.74333743, 0.74558665, 0.74620908,\n",
       "        0.74615249, 0.74635054, 0.74637883, 0.74636469, 0.74639298,\n",
       "        0.74639298, 0.74637883, 0.74639298, 0.74639298, 0.74639298,\n",
       "        0.74639298, 0.74640712, 0.74639298, 0.73806098, 0.7456008 ,\n",
       "        0.74613834, 0.74636469, 0.74635054, 0.74637883, 0.74637883,\n",
       "        0.74637883, 0.74637883, 0.74637883, 0.74639298, 0.74635054]),\n",
       " 'std_test_f1_micro': array([2.26660501e-05, 5.21249205e-03, 5.20172880e-03, 5.41552882e-03,\n",
       "        5.13110489e-03, 5.15310810e-03, 5.07424309e-03, 5.09603113e-03,\n",
       "        5.14005893e-03, 5.10645203e-03, 5.11350119e-03, 5.11416721e-03,\n",
       "        5.11350119e-03, 5.11350119e-03, 5.11350119e-03, 5.11350119e-03,\n",
       "        5.10025564e-03, 5.11350119e-03, 5.20719012e-03, 5.43135753e-03,\n",
       "        5.14533615e-03, 5.10920931e-03, 5.09892448e-03, 5.11416721e-03,\n",
       "        5.11416721e-03, 5.11416721e-03, 5.11416721e-03, 5.11416721e-03,\n",
       "        5.11350119e-03, 5.09892448e-03]),\n",
       " 'rank_test_f1_micro': array([30, 29, 27, 26, 22, 23, 21, 10, 17,  2,  2, 11,  2,  2,  2,  2,  1,\n",
       "         2, 28, 25, 24, 17, 19, 11, 11, 11, 11, 11,  2, 19])}"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_model.cv_results_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "50a53953",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([30, 29, 27, 26, 22, 23, 21, 10, 17,  2,  2, 11,  2,  2,  2,  2,  1,\n",
       "        2, 28, 25, 24, 17, 19, 11, 11, 11, 11, 11,  2, 19])"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_model.cv_results_['rank_test_accuracy']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "ed852058",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'classifier': LogisticRegression(max_iter=10000),\n",
       " 'classifier__C': 0.0001,\n",
       " 'classifier__penalty': 'l1',\n",
       " 'classifier__solver': 'saga'}"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_model.cv_results_['params'][ np.argmin(best_model.cv_results_['mean_test_accuracy'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "cb83803b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>classifier</th>\n",
       "      <th>C</th>\n",
       "      <th>penalty</th>\n",
       "      <th>solver</th>\n",
       "      <th>score_acc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.599389</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.738047</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.743337</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.745587</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0100</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.746209</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0100</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.746152</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.1000</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.746351</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.1000</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.746379</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.746365</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.746393</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>10.0000</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.746393</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>10.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.746379</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>100.0000</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.746393</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>100.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.746393</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>1000.0000</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.746393</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>1000.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.746393</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>10000.0000</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.746407</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>10000.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.746393</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.738061</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.745601</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0100</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.746138</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.1000</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.746365</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.746351</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>10.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.746379</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>100.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.746379</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>1000.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.746379</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>10000.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.746379</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>NaN</td>\n",
       "      <td>none</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.746379</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>NaN</td>\n",
       "      <td>none</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.746393</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.746351</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                            classifier           C penalty solver  score_acc\n",
       "0   LogisticRegression(max_iter=10000)      0.0001      l1   saga   0.599389\n",
       "1   LogisticRegression(max_iter=10000)      0.0001      l2   saga   0.738047\n",
       "2   LogisticRegression(max_iter=10000)      0.0010      l1   saga   0.743337\n",
       "3   LogisticRegression(max_iter=10000)      0.0010      l2   saga   0.745587\n",
       "4   LogisticRegression(max_iter=10000)      0.0100      l1   saga   0.746209\n",
       "5   LogisticRegression(max_iter=10000)      0.0100      l2   saga   0.746152\n",
       "6   LogisticRegression(max_iter=10000)      0.1000      l1   saga   0.746351\n",
       "7   LogisticRegression(max_iter=10000)      0.1000      l2   saga   0.746379\n",
       "8   LogisticRegression(max_iter=10000)      1.0000      l1   saga   0.746365\n",
       "9   LogisticRegression(max_iter=10000)      1.0000      l2   saga   0.746393\n",
       "10  LogisticRegression(max_iter=10000)     10.0000      l1   saga   0.746393\n",
       "11  LogisticRegression(max_iter=10000)     10.0000      l2   saga   0.746379\n",
       "12  LogisticRegression(max_iter=10000)    100.0000      l1   saga   0.746393\n",
       "13  LogisticRegression(max_iter=10000)    100.0000      l2   saga   0.746393\n",
       "14  LogisticRegression(max_iter=10000)   1000.0000      l1   saga   0.746393\n",
       "15  LogisticRegression(max_iter=10000)   1000.0000      l2   saga   0.746393\n",
       "16  LogisticRegression(max_iter=10000)  10000.0000      l1   saga   0.746407\n",
       "17  LogisticRegression(max_iter=10000)  10000.0000      l2   saga   0.746393\n",
       "18  LogisticRegression(max_iter=10000)      0.0001      l2  lbfgs   0.738061\n",
       "19  LogisticRegression(max_iter=10000)      0.0010      l2  lbfgs   0.745601\n",
       "20  LogisticRegression(max_iter=10000)      0.0100      l2  lbfgs   0.746138\n",
       "21  LogisticRegression(max_iter=10000)      0.1000      l2  lbfgs   0.746365\n",
       "22  LogisticRegression(max_iter=10000)      1.0000      l2  lbfgs   0.746351\n",
       "23  LogisticRegression(max_iter=10000)     10.0000      l2  lbfgs   0.746379\n",
       "24  LogisticRegression(max_iter=10000)    100.0000      l2  lbfgs   0.746379\n",
       "25  LogisticRegression(max_iter=10000)   1000.0000      l2  lbfgs   0.746379\n",
       "26  LogisticRegression(max_iter=10000)  10000.0000      l2  lbfgs   0.746379\n",
       "27  LogisticRegression(max_iter=10000)         NaN    none  lbfgs   0.746379\n",
       "28  LogisticRegression(max_iter=10000)         NaN    none   saga   0.746393\n",
       "29  LogisticRegression(max_iter=10000)         NaN     NaN    NaN   0.746351"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results = pd.DataFrame( best_model.cv_results_['params'] )\n",
    "\n",
    "results['score_acc'] = best_model.cv_results_['mean_test_accuracy']\n",
    "cols = results.columns.to_series().str.split('__').apply(lambda x: x[-1])\n",
    "results.columns = cols\n",
    "\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "e5605c9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "acc_list.append(results[0:1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "687c473b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'classifier': LogisticRegression(max_iter=10000),\n",
       " 'classifier__C': 0.0001,\n",
       " 'classifier__penalty': 'l2',\n",
       " 'classifier__solver': 'lbfgs'}"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_model.cv_results_['params'][ np.argmin(best_model.cv_results_['rank_test_precision'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "f9394193",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>classifier</th>\n",
       "      <th>C</th>\n",
       "      <th>penalty</th>\n",
       "      <th>solver</th>\n",
       "      <th>score_precision</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.711124</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.699183</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.697609</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0100</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.695848</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0100</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.695464</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.1000</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.695405</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.1000</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.695462</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.695377</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.695416</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>10.0000</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.695401</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>10.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.695375</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>100.0000</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.695401</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>100.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.695401</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>1000.0000</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.695401</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>1000.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.695401</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>10000.0000</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.695427</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>10000.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.695401</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.711137</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.697635</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0100</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.695438</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.1000</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.695436</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.695352</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>10.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.695375</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>100.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.695375</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>1000.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.695375</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>10000.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.695375</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>NaN</td>\n",
       "      <td>none</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.695375</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>NaN</td>\n",
       "      <td>none</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.695401</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.695352</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                            classifier           C penalty solver  \\\n",
       "0   LogisticRegression(max_iter=10000)      0.0001      l1   saga   \n",
       "1   LogisticRegression(max_iter=10000)      0.0001      l2   saga   \n",
       "2   LogisticRegression(max_iter=10000)      0.0010      l1   saga   \n",
       "3   LogisticRegression(max_iter=10000)      0.0010      l2   saga   \n",
       "4   LogisticRegression(max_iter=10000)      0.0100      l1   saga   \n",
       "5   LogisticRegression(max_iter=10000)      0.0100      l2   saga   \n",
       "6   LogisticRegression(max_iter=10000)      0.1000      l1   saga   \n",
       "7   LogisticRegression(max_iter=10000)      0.1000      l2   saga   \n",
       "8   LogisticRegression(max_iter=10000)      1.0000      l1   saga   \n",
       "9   LogisticRegression(max_iter=10000)      1.0000      l2   saga   \n",
       "10  LogisticRegression(max_iter=10000)     10.0000      l1   saga   \n",
       "11  LogisticRegression(max_iter=10000)     10.0000      l2   saga   \n",
       "12  LogisticRegression(max_iter=10000)    100.0000      l1   saga   \n",
       "13  LogisticRegression(max_iter=10000)    100.0000      l2   saga   \n",
       "14  LogisticRegression(max_iter=10000)   1000.0000      l1   saga   \n",
       "15  LogisticRegression(max_iter=10000)   1000.0000      l2   saga   \n",
       "16  LogisticRegression(max_iter=10000)  10000.0000      l1   saga   \n",
       "17  LogisticRegression(max_iter=10000)  10000.0000      l2   saga   \n",
       "18  LogisticRegression(max_iter=10000)      0.0001      l2  lbfgs   \n",
       "19  LogisticRegression(max_iter=10000)      0.0010      l2  lbfgs   \n",
       "20  LogisticRegression(max_iter=10000)      0.0100      l2  lbfgs   \n",
       "21  LogisticRegression(max_iter=10000)      0.1000      l2  lbfgs   \n",
       "22  LogisticRegression(max_iter=10000)      1.0000      l2  lbfgs   \n",
       "23  LogisticRegression(max_iter=10000)     10.0000      l2  lbfgs   \n",
       "24  LogisticRegression(max_iter=10000)    100.0000      l2  lbfgs   \n",
       "25  LogisticRegression(max_iter=10000)   1000.0000      l2  lbfgs   \n",
       "26  LogisticRegression(max_iter=10000)  10000.0000      l2  lbfgs   \n",
       "27  LogisticRegression(max_iter=10000)         NaN    none  lbfgs   \n",
       "28  LogisticRegression(max_iter=10000)         NaN    none   saga   \n",
       "29  LogisticRegression(max_iter=10000)         NaN     NaN    NaN   \n",
       "\n",
       "    score_precision  \n",
       "0          0.000000  \n",
       "1          0.711124  \n",
       "2          0.699183  \n",
       "3          0.697609  \n",
       "4          0.695848  \n",
       "5          0.695464  \n",
       "6          0.695405  \n",
       "7          0.695462  \n",
       "8          0.695377  \n",
       "9          0.695416  \n",
       "10         0.695401  \n",
       "11         0.695375  \n",
       "12         0.695401  \n",
       "13         0.695401  \n",
       "14         0.695401  \n",
       "15         0.695401  \n",
       "16         0.695427  \n",
       "17         0.695401  \n",
       "18         0.711137  \n",
       "19         0.697635  \n",
       "20         0.695438  \n",
       "21         0.695436  \n",
       "22         0.695352  \n",
       "23         0.695375  \n",
       "24         0.695375  \n",
       "25         0.695375  \n",
       "26         0.695375  \n",
       "27         0.695375  \n",
       "28         0.695401  \n",
       "29         0.695352  "
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results = pd.DataFrame( best_model.cv_results_['params'] )\n",
    "\n",
    "results['score_precision'] = best_model.cv_results_['mean_test_precision']\n",
    "cols = results.columns.to_series().str.split('__').apply(lambda x: x[-1])\n",
    "results.columns = cols\n",
    "\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "f6cdf507",
   "metadata": {},
   "outputs": [],
   "source": [
    "roc_auc_ovr_List.append(results[18:19])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "342313d7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'classifier': LogisticRegression(max_iter=10000),\n",
       " 'classifier__C': 10000.0,\n",
       " 'classifier__penalty': 'l1',\n",
       " 'classifier__solver': 'saga'}"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_model.cv_results_['params'][ np.argmin(best_model.cv_results_['rank_test_f1_micro'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "2d1e6c10",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>classifier</th>\n",
       "      <th>C</th>\n",
       "      <th>penalty</th>\n",
       "      <th>solver</th>\n",
       "      <th>score_f1_micro</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.599389</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.738047</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.743337</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.745587</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0100</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.746209</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0100</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.746152</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.1000</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.746351</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.1000</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.746379</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.746365</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.746393</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>10.0000</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.746393</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>10.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.746379</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>100.0000</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.746393</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>100.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.746393</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>1000.0000</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.746393</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>1000.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.746393</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>10000.0000</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.746407</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>10000.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.746393</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.738061</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.745601</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0100</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.746138</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.1000</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.746365</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.746351</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>10.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.746379</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>100.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.746379</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>1000.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.746379</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>10000.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.746379</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>NaN</td>\n",
       "      <td>none</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.746379</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>NaN</td>\n",
       "      <td>none</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.746393</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.746351</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                            classifier           C penalty solver  \\\n",
       "0   LogisticRegression(max_iter=10000)      0.0001      l1   saga   \n",
       "1   LogisticRegression(max_iter=10000)      0.0001      l2   saga   \n",
       "2   LogisticRegression(max_iter=10000)      0.0010      l1   saga   \n",
       "3   LogisticRegression(max_iter=10000)      0.0010      l2   saga   \n",
       "4   LogisticRegression(max_iter=10000)      0.0100      l1   saga   \n",
       "5   LogisticRegression(max_iter=10000)      0.0100      l2   saga   \n",
       "6   LogisticRegression(max_iter=10000)      0.1000      l1   saga   \n",
       "7   LogisticRegression(max_iter=10000)      0.1000      l2   saga   \n",
       "8   LogisticRegression(max_iter=10000)      1.0000      l1   saga   \n",
       "9   LogisticRegression(max_iter=10000)      1.0000      l2   saga   \n",
       "10  LogisticRegression(max_iter=10000)     10.0000      l1   saga   \n",
       "11  LogisticRegression(max_iter=10000)     10.0000      l2   saga   \n",
       "12  LogisticRegression(max_iter=10000)    100.0000      l1   saga   \n",
       "13  LogisticRegression(max_iter=10000)    100.0000      l2   saga   \n",
       "14  LogisticRegression(max_iter=10000)   1000.0000      l1   saga   \n",
       "15  LogisticRegression(max_iter=10000)   1000.0000      l2   saga   \n",
       "16  LogisticRegression(max_iter=10000)  10000.0000      l1   saga   \n",
       "17  LogisticRegression(max_iter=10000)  10000.0000      l2   saga   \n",
       "18  LogisticRegression(max_iter=10000)      0.0001      l2  lbfgs   \n",
       "19  LogisticRegression(max_iter=10000)      0.0010      l2  lbfgs   \n",
       "20  LogisticRegression(max_iter=10000)      0.0100      l2  lbfgs   \n",
       "21  LogisticRegression(max_iter=10000)      0.1000      l2  lbfgs   \n",
       "22  LogisticRegression(max_iter=10000)      1.0000      l2  lbfgs   \n",
       "23  LogisticRegression(max_iter=10000)     10.0000      l2  lbfgs   \n",
       "24  LogisticRegression(max_iter=10000)    100.0000      l2  lbfgs   \n",
       "25  LogisticRegression(max_iter=10000)   1000.0000      l2  lbfgs   \n",
       "26  LogisticRegression(max_iter=10000)  10000.0000      l2  lbfgs   \n",
       "27  LogisticRegression(max_iter=10000)         NaN    none  lbfgs   \n",
       "28  LogisticRegression(max_iter=10000)         NaN    none   saga   \n",
       "29  LogisticRegression(max_iter=10000)         NaN     NaN    NaN   \n",
       "\n",
       "    score_f1_micro  \n",
       "0         0.599389  \n",
       "1         0.738047  \n",
       "2         0.743337  \n",
       "3         0.745587  \n",
       "4         0.746209  \n",
       "5         0.746152  \n",
       "6         0.746351  \n",
       "7         0.746379  \n",
       "8         0.746365  \n",
       "9         0.746393  \n",
       "10        0.746393  \n",
       "11        0.746379  \n",
       "12        0.746393  \n",
       "13        0.746393  \n",
       "14        0.746393  \n",
       "15        0.746393  \n",
       "16        0.746407  \n",
       "17        0.746393  \n",
       "18        0.738061  \n",
       "19        0.745601  \n",
       "20        0.746138  \n",
       "21        0.746365  \n",
       "22        0.746351  \n",
       "23        0.746379  \n",
       "24        0.746379  \n",
       "25        0.746379  \n",
       "26        0.746379  \n",
       "27        0.746379  \n",
       "28        0.746393  \n",
       "29        0.746351  "
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results = pd.DataFrame( best_model.cv_results_['params'] )\n",
    "\n",
    "results['score_f1_micro'] = best_model.cv_results_['mean_test_f1_micro']\n",
    "cols = results.columns.to_series().str.split('__').apply(lambda x: x[-1])\n",
    "results.columns = cols\n",
    "\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "cd4d8e65",
   "metadata": {},
   "outputs": [],
   "source": [
    "f1_micro_List.append(results[16:17])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c967a56e",
   "metadata": {},
   "source": [
    "## Trial 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "1542f63b",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, Y_train, Y_test = train_test_split(X, y, test_size=0.2, shuffle=True, random_state = 7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "5380e5ce",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Guest-All\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\Guest-All\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\Guest-All\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\Guest-All\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\Guest-All\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\Guest-All\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\Guest-All\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\Guest-All\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\Guest-All\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\Guest-All\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\Guest-All\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1183: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Guest-All\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1183: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Guest-All\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1183: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Guest-All\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1183: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Guest-All\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1183: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Guest-All\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1183: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Guest-All\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1183: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Guest-All\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1183: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Guest-All\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1183: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Guest-All\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1183: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Guest-All\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1183: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Guest-All\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1183: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Guest-All\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1183: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Guest-All\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1183: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Guest-All\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1183: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Guest-All\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1183: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Guest-All\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1183: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Guest-All\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1183: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Guest-All\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1183: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Guest-All\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1183: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Create a pipeline - RF is a stand in, we will populate the classifier part below\n",
    "pipe = Pipeline([('std', StandardScaler()),\n",
    "                 ('classifier', LogisticRegression())])\n",
    "\n",
    "# Create search space of candidate learning algorithms and their hyperparameters\n",
    "# note lbfgs can't do l1, and if you pass penalty='none' it expects no C value\n",
    "search_space = [{'classifier': [LogisticRegression(max_iter=10000)],\n",
    "                 'classifier__solver': ['saga'],\n",
    "                 'classifier__penalty': ['l1', 'l2'],\n",
    "                 'classifier__C': np.logspace(-4, 4, 9)},\n",
    "                {'classifier': [LogisticRegression(max_iter=10000)],\n",
    "                 'classifier__solver': ['lbfgs'],\n",
    "                 'classifier__penalty': ['l2'],\n",
    "                 'classifier__C': np.logspace(-4, 4, 9)},\n",
    "                {'classifier': [LogisticRegression(max_iter=10000)],\n",
    "                 'classifier__solver': ['lbfgs','saga'],\n",
    "                 'classifier__penalty': ['none']},\n",
    "                {'classifier': [LogisticRegression(max_iter=10000)]}\n",
    "                ]\n",
    "\n",
    "# Create grid search\n",
    "clf = GridSearchCV(pipe, search_space, cv=StratifiedKFold(n_splits=10),\n",
    "                   scoring=['accuracy', 'precision', 'f1_micro'], refit=False,\n",
    "                   verbose=0)\n",
    "\n",
    "# Fit grid search\n",
    "best_model = clf.fit(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "2795f9d4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'mean_fit_time': array([0.79539828, 0.70283976, 0.75831919, 0.71503546, 0.81088836,\n",
       "        0.66977479, 0.79641614, 0.70065887, 0.84632592, 0.72151012,\n",
       "        0.81058123, 0.71849756, 0.86259899, 0.81215818, 1.02216477,\n",
       "        0.88454225, 1.08990109, 0.82144222, 0.13285568, 0.10635614,\n",
       "        0.19018342, 0.10386753, 0.1802973 , 0.11456366, 0.16782601,\n",
       "        0.12415531, 0.14035251, 0.15104771, 0.88259692, 0.12138464]),\n",
       " 'std_fit_time': array([0.06473928, 0.05706672, 0.06677237, 0.04451318, 0.04502492,\n",
       "        0.07361829, 0.06383502, 0.06927684, 0.07096526, 0.05831116,\n",
       "        0.06705389, 0.03117345, 0.07805396, 0.15051027, 0.20421449,\n",
       "        0.23976828, 0.18383775, 0.15933493, 0.05272069, 0.00847073,\n",
       "        0.04398974, 0.00406263, 0.06897117, 0.02799582, 0.07164384,\n",
       "        0.02543507, 0.06170585, 0.05423636, 0.19641659, 0.03158882]),\n",
       " 'mean_score_time': array([0.01314368, 0.01248751, 0.01442001, 0.01354373, 0.01323316,\n",
       "        0.01533308, 0.01185451, 0.01202853, 0.01283832, 0.0121161 ,\n",
       "        0.01156256, 0.01187704, 0.01199572, 0.01264174, 0.02101855,\n",
       "        0.01363771, 0.01663425, 0.01834247, 0.0133368 , 0.01200516,\n",
       "        0.02588615, 0.01133687, 0.01997874, 0.01243708, 0.01846087,\n",
       "        0.01406846, 0.01588728, 0.01407306, 0.0159936 , 0.01332386]),\n",
       " 'std_score_time': array([0.00429633, 0.00371825, 0.00321439, 0.00369226, 0.0041161 ,\n",
       "        0.00236224, 0.00411968, 0.0039753 , 0.00344858, 0.00402428,\n",
       "        0.00372238, 0.0038296 , 0.00408966, 0.00535907, 0.01423848,\n",
       "        0.00524424, 0.0059857 , 0.00908501, 0.00335645, 0.00384601,\n",
       "        0.01599876, 0.00399945, 0.00496909, 0.00382225, 0.00794181,\n",
       "        0.00481017, 0.01033344, 0.00794258, 0.00714798, 0.00552544]),\n",
       " 'param_classifier': masked_array(data=[LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000)],\n",
       "              mask=[False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'param_classifier__C': masked_array(data=[0.0001, 0.0001, 0.001, 0.001, 0.01, 0.01, 0.1, 0.1,\n",
       "                    1.0, 1.0, 10.0, 10.0, 100.0, 100.0, 1000.0, 1000.0,\n",
       "                    10000.0, 10000.0, 0.0001, 0.001, 0.01, 0.1, 1.0, 10.0,\n",
       "                    100.0, 1000.0, 10000.0, --, --, --],\n",
       "              mask=[False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False,  True,  True,  True],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'param_classifier__penalty': masked_array(data=['l1', 'l2', 'l1', 'l2', 'l1', 'l2', 'l1', 'l2', 'l1',\n",
       "                    'l2', 'l1', 'l2', 'l1', 'l2', 'l1', 'l2', 'l1', 'l2',\n",
       "                    'l2', 'l2', 'l2', 'l2', 'l2', 'l2', 'l2', 'l2', 'l2',\n",
       "                    'none', 'none', --],\n",
       "              mask=[False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False,  True],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'param_classifier__solver': masked_array(data=['saga', 'saga', 'saga', 'saga', 'saga', 'saga', 'saga',\n",
       "                    'saga', 'saga', 'saga', 'saga', 'saga', 'saga', 'saga',\n",
       "                    'saga', 'saga', 'saga', 'saga', 'lbfgs', 'lbfgs',\n",
       "                    'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs',\n",
       "                    'lbfgs', 'lbfgs', 'saga', --],\n",
       "              mask=[False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False,  True],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'params': [{'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 0.0001,\n",
       "   'classifier__penalty': 'l1',\n",
       "   'classifier__solver': 'saga'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 0.0001,\n",
       "   'classifier__penalty': 'l2',\n",
       "   'classifier__solver': 'saga'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 0.001,\n",
       "   'classifier__penalty': 'l1',\n",
       "   'classifier__solver': 'saga'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 0.001,\n",
       "   'classifier__penalty': 'l2',\n",
       "   'classifier__solver': 'saga'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 0.01,\n",
       "   'classifier__penalty': 'l1',\n",
       "   'classifier__solver': 'saga'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 0.01,\n",
       "   'classifier__penalty': 'l2',\n",
       "   'classifier__solver': 'saga'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 0.1,\n",
       "   'classifier__penalty': 'l1',\n",
       "   'classifier__solver': 'saga'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 0.1,\n",
       "   'classifier__penalty': 'l2',\n",
       "   'classifier__solver': 'saga'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 1.0,\n",
       "   'classifier__penalty': 'l1',\n",
       "   'classifier__solver': 'saga'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 1.0,\n",
       "   'classifier__penalty': 'l2',\n",
       "   'classifier__solver': 'saga'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 10.0,\n",
       "   'classifier__penalty': 'l1',\n",
       "   'classifier__solver': 'saga'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 10.0,\n",
       "   'classifier__penalty': 'l2',\n",
       "   'classifier__solver': 'saga'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 100.0,\n",
       "   'classifier__penalty': 'l1',\n",
       "   'classifier__solver': 'saga'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 100.0,\n",
       "   'classifier__penalty': 'l2',\n",
       "   'classifier__solver': 'saga'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 1000.0,\n",
       "   'classifier__penalty': 'l1',\n",
       "   'classifier__solver': 'saga'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 1000.0,\n",
       "   'classifier__penalty': 'l2',\n",
       "   'classifier__solver': 'saga'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 10000.0,\n",
       "   'classifier__penalty': 'l1',\n",
       "   'classifier__solver': 'saga'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 10000.0,\n",
       "   'classifier__penalty': 'l2',\n",
       "   'classifier__solver': 'saga'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 0.0001,\n",
       "   'classifier__penalty': 'l2',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 0.001,\n",
       "   'classifier__penalty': 'l2',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 0.01,\n",
       "   'classifier__penalty': 'l2',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 0.1,\n",
       "   'classifier__penalty': 'l2',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 1.0,\n",
       "   'classifier__penalty': 'l2',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 10.0,\n",
       "   'classifier__penalty': 'l2',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 100.0,\n",
       "   'classifier__penalty': 'l2',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 1000.0,\n",
       "   'classifier__penalty': 'l2',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 10000.0,\n",
       "   'classifier__penalty': 'l2',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__penalty': 'none',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__penalty': 'none',\n",
       "   'classifier__solver': 'saga'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000)}],\n",
       " 'split0_test_accuracy': array([0.59971711, 0.73536068, 0.73663366, 0.73946252, 0.74002829,\n",
       "        0.74031117, 0.74059406, 0.74059406, 0.74045262, 0.74045262,\n",
       "        0.74045262, 0.74045262, 0.74045262, 0.74045262, 0.74045262,\n",
       "        0.74031117, 0.74045262, 0.74045262, 0.73536068, 0.73946252,\n",
       "        0.74031117, 0.74059406, 0.74031117, 0.74016973, 0.74016973,\n",
       "        0.74016973, 0.74016973, 0.74016973, 0.74045262, 0.74031117]),\n",
       " 'split1_test_accuracy': array([0.59971711, 0.74045262, 0.74582744, 0.74752475, 0.74893918,\n",
       "        0.74964639, 0.75021216, 0.75007072, 0.75021216, 0.75021216,\n",
       "        0.75021216, 0.75021216, 0.75021216, 0.75021216, 0.75021216,\n",
       "        0.75021216, 0.75021216, 0.75021216, 0.74045262, 0.74752475,\n",
       "        0.74964639, 0.75007072, 0.75021216, 0.75021216, 0.75021216,\n",
       "        0.75021216, 0.75021216, 0.75021216, 0.75021216, 0.75021216]),\n",
       " 'split2_test_accuracy': array([0.59980195, 0.7381525 , 0.74084029, 0.7415476 , 0.74211345,\n",
       "        0.74239638, 0.74296223, 0.74296223, 0.74282077, 0.74282077,\n",
       "        0.74282077, 0.74282077, 0.74282077, 0.74282077, 0.74282077,\n",
       "        0.74282077, 0.74282077, 0.74282077, 0.7381525 , 0.7415476 ,\n",
       "        0.74239638, 0.74296223, 0.74282077, 0.74282077, 0.74282077,\n",
       "        0.74282077, 0.74282077, 0.74282077, 0.74282077, 0.74282077]),\n",
       " 'split3_test_accuracy': array([0.59980195, 0.73263545, 0.73504032, 0.73942566, 0.74098175,\n",
       "        0.74126468, 0.74098175, 0.74126468, 0.74126468, 0.74126468,\n",
       "        0.74126468, 0.74126468, 0.74126468, 0.74126468, 0.74126468,\n",
       "        0.74126468, 0.74126468, 0.74126468, 0.73263545, 0.73942566,\n",
       "        0.74126468, 0.74126468, 0.74126468, 0.74126468, 0.74126468,\n",
       "        0.74126468, 0.74126468, 0.74126468, 0.74126468, 0.74126468]),\n",
       " 'split4_test_accuracy': array([0.59980195, 0.73631348, 0.74593295, 0.74904513, 0.75088414,\n",
       "        0.75159146, 0.75173292, 0.75144999, 0.75130853, 0.75116707,\n",
       "        0.7510256 , 0.75116707, 0.75116707, 0.75116707, 0.75116707,\n",
       "        0.75116707, 0.75116707, 0.75116707, 0.73631348, 0.74904513,\n",
       "        0.75159146, 0.75144999, 0.75116707, 0.75116707, 0.75116707,\n",
       "        0.75116707, 0.75116707, 0.75116707, 0.75116707, 0.75116707]),\n",
       " 'split5_test_accuracy': array([0.59980195, 0.73376715, 0.7404159 , 0.74451832, 0.74239638,\n",
       "        0.74310369, 0.74296223, 0.74296223, 0.74296223, 0.74296223,\n",
       "        0.74296223, 0.74296223, 0.74296223, 0.74296223, 0.74296223,\n",
       "        0.74324515, 0.74296223, 0.74296223, 0.73376715, 0.74451832,\n",
       "        0.74310369, 0.74296223, 0.74296223, 0.74296223, 0.74296223,\n",
       "        0.74296223, 0.74296223, 0.74296223, 0.74296223, 0.74296223]),\n",
       " 'split6_test_accuracy': array([0.59980195, 0.73376715, 0.73857688, 0.74296223, 0.74168906,\n",
       "        0.74239638, 0.74282077, 0.74282077, 0.74282077, 0.74282077,\n",
       "        0.74282077, 0.74282077, 0.74282077, 0.74282077, 0.74282077,\n",
       "        0.74282077, 0.74282077, 0.74282077, 0.73376715, 0.74296223,\n",
       "        0.74239638, 0.74282077, 0.74282077, 0.74282077, 0.74282077,\n",
       "        0.74282077, 0.74282077, 0.74282077, 0.74282077, 0.74282077]),\n",
       " 'split7_test_accuracy': array([0.59980195, 0.74381101, 0.74522563, 0.74904513, 0.7487622 ,\n",
       "        0.74932805, 0.74862074, 0.74904513, 0.74847928, 0.74847928,\n",
       "        0.74847928, 0.74847928, 0.74847928, 0.74847928, 0.74847928,\n",
       "        0.74847928, 0.74847928, 0.74847928, 0.74381101, 0.74904513,\n",
       "        0.74932805, 0.74904513, 0.74847928, 0.74847928, 0.74847928,\n",
       "        0.74847928, 0.74847928, 0.74847928, 0.74847928, 0.74847928]),\n",
       " 'split8_test_accuracy': array([0.59980195, 0.74027444, 0.74678172, 0.74862074, 0.74918659,\n",
       "        0.75031829, 0.75130853, 0.75074268, 0.7510256 , 0.75088414,\n",
       "        0.75088414, 0.75088414, 0.75088414, 0.7510256 , 0.7510256 ,\n",
       "        0.75088414, 0.75088414, 0.75088414, 0.74027444, 0.74862074,\n",
       "        0.75031829, 0.75074268, 0.75088414, 0.75088414, 0.75088414,\n",
       "        0.75088414, 0.75088414, 0.75088414, 0.75088414, 0.75088414]),\n",
       " 'split9_test_accuracy': array([0.59980195, 0.74352808, 0.74522563, 0.75328901, 0.75244023,\n",
       "        0.75215731, 0.75244023, 0.75258169, 0.75258169, 0.75258169,\n",
       "        0.75258169, 0.75258169, 0.75258169, 0.75258169, 0.75258169,\n",
       "        0.75272316, 0.75258169, 0.75258169, 0.74352808, 0.75328901,\n",
       "        0.75215731, 0.75258169, 0.75272316, 0.75258169, 0.75258169,\n",
       "        0.75258169, 0.75258169, 0.75258169, 0.75258169, 0.75272316]),\n",
       " 'mean_test_accuracy': array([0.59978498, 0.73780626, 0.74205004, 0.74554411, 0.74574213,\n",
       "        0.74625138, 0.74646356, 0.74644942, 0.74639283, 0.74636454,\n",
       "        0.74635039, 0.74636454, 0.74636454, 0.74637869, 0.74637869,\n",
       "        0.74639283, 0.74636454, 0.74636454, 0.73780626, 0.74554411,\n",
       "        0.74625138, 0.74644942, 0.74636454, 0.74633625, 0.74633625,\n",
       "        0.74633625, 0.74633625, 0.74633625, 0.74636454, 0.74636454]),\n",
       " 'std_test_accuracy': array([3.39350468e-05, 3.86764129e-03, 4.08134036e-03, 4.42716477e-03,\n",
       "        4.45655684e-03, 4.48008299e-03, 4.56021678e-03, 4.46709128e-03,\n",
       "        4.49156662e-03, 4.46175101e-03, 4.44670074e-03, 4.46175101e-03,\n",
       "        4.46175101e-03, 4.47625895e-03, 4.47625895e-03, 4.47984897e-03,\n",
       "        4.46175101e-03, 4.46175101e-03, 3.86764129e-03, 4.42716477e-03,\n",
       "        4.48008299e-03, 4.46709128e-03, 4.50048472e-03, 4.49987818e-03,\n",
       "        4.49987818e-03, 4.49987818e-03, 4.49987818e-03, 4.49987818e-03,\n",
       "        4.46175101e-03, 4.50048472e-03]),\n",
       " 'rank_test_accuracy': array([30, 28, 27, 25, 24, 22,  1,  2,  5, 10, 16, 10, 10,  6,  6,  4, 10,\n",
       "        10, 28, 25, 22,  2,  8, 17, 17, 17, 17, 17, 10,  8]),\n",
       " 'split0_test_precision': array([0.        , 0.71320587, 0.69406576, 0.69342208, 0.69239721,\n",
       "        0.69174383, 0.69227799, 0.69227799, 0.69215913, 0.69215913,\n",
       "        0.69215913, 0.69215913, 0.69215913, 0.69215913, 0.69215913,\n",
       "        0.69189189, 0.69215913, 0.69215913, 0.71320587, 0.69342208,\n",
       "        0.69174383, 0.69227799, 0.69189189, 0.69162486, 0.69162486,\n",
       "        0.69162486, 0.69162486, 0.69162486, 0.69215913, 0.69189189]),\n",
       " 'split1_test_precision': array([0.        , 0.71984092, 0.70553124, 0.70450098, 0.70374662,\n",
       "        0.70400308, 0.70430108, 0.70403071, 0.70414428, 0.70414428,\n",
       "        0.70414428, 0.70414428, 0.70414428, 0.70414428, 0.70414428,\n",
       "        0.70414428, 0.70414428, 0.70414428, 0.71984092, 0.70450098,\n",
       "        0.70400308, 0.70403071, 0.70414428, 0.70414428, 0.70414428,\n",
       "        0.70414428, 0.70414428, 0.70414428, 0.70414428, 0.70414428]),\n",
       " 'split2_test_precision': array([0.        , 0.70987124, 0.69633714, 0.6899166 , 0.68838951,\n",
       "        0.68848168, 0.68866518, 0.68880597, 0.68854905, 0.68854905,\n",
       "        0.68854905, 0.68854905, 0.68854905, 0.68854905, 0.68854905,\n",
       "        0.68854905, 0.68854905, 0.68854905, 0.70987124, 0.6899166 ,\n",
       "        0.68848168, 0.68880597, 0.68854905, 0.68854905, 0.68854905,\n",
       "        0.68854905, 0.68854905, 0.68854905, 0.68854905, 0.68854905]),\n",
       " 'split3_test_precision': array([0.        , 0.70107066, 0.68527132, 0.68643748, 0.68717179,\n",
       "        0.68684604, 0.68633308, 0.68656716, 0.68656716, 0.68656716,\n",
       "        0.68656716, 0.68656716, 0.68656716, 0.68656716, 0.68656716,\n",
       "        0.68656716, 0.68656716, 0.68656716, 0.70107066, 0.68643748,\n",
       "        0.68684604, 0.68656716, 0.68656716, 0.68656716, 0.68656716,\n",
       "        0.68656716, 0.68656716, 0.68656716, 0.68656716, 0.68656716]),\n",
       " 'split4_test_precision': array([0.        , 0.71153003, 0.70471661, 0.70343232, 0.70242608,\n",
       "        0.7032967 , 0.70279456, 0.70272315, 0.70215176, 0.70203927,\n",
       "        0.70177425, 0.70203927, 0.70203927, 0.70203927, 0.70203927,\n",
       "        0.70203927, 0.70203927, 0.70203927, 0.71153003, 0.70343232,\n",
       "        0.7032967 , 0.70272315, 0.70203927, 0.70203927, 0.70203927,\n",
       "        0.70203927, 0.70203927, 0.70203927, 0.70203927, 0.70203927]),\n",
       " 'split5_test_precision': array([0.        , 0.70524491, 0.69597792, 0.69590195, 0.69004525,\n",
       "        0.69062853, 0.69008264, 0.69008264, 0.69008264, 0.69008264,\n",
       "        0.69008264, 0.69008264, 0.69008264, 0.69008264, 0.69008264,\n",
       "        0.6906015 , 0.69008264, 0.69008264, 0.70524491, 0.69590195,\n",
       "        0.69062853, 0.69008264, 0.69008264, 0.69008264, 0.69008264,\n",
       "        0.69008264, 0.69008264, 0.69008264, 0.69008264, 0.69008264]),\n",
       " 'split6_test_precision': array([0.        , 0.70226399, 0.68800307, 0.68951311, 0.68539741,\n",
       "        0.68529412, 0.68564084, 0.68550459, 0.68550459, 0.68550459,\n",
       "        0.68550459, 0.68550459, 0.68550459, 0.68550459, 0.68550459,\n",
       "        0.68550459, 0.68550459, 0.68550459, 0.70226399, 0.68951311,\n",
       "        0.68529412, 0.68550459, 0.68550459, 0.68550459, 0.68550459,\n",
       "        0.68550459, 0.68550459, 0.68550459, 0.68550459, 0.68550459]),\n",
       " 'split7_test_precision': array([0.        , 0.71845494, 0.70188531, 0.70390414, 0.70118456,\n",
       "        0.70148685, 0.69984802, 0.70064663, 0.69958223, 0.69958223,\n",
       "        0.69958223, 0.69958223, 0.69958223, 0.69958223, 0.69958223,\n",
       "        0.69958223, 0.69958223, 0.69958223, 0.71845494, 0.70390414,\n",
       "        0.70148685, 0.70064663, 0.69958223, 0.69958223, 0.69958223,\n",
       "        0.69958223, 0.69958223, 0.69958223, 0.69958223, 0.69958223]),\n",
       " 'split8_test_precision': array([0.        , 0.71190781, 0.70412574, 0.7018419 , 0.70015163,\n",
       "        0.70045215, 0.701089  , 0.70048854, 0.70086434, 0.70060105,\n",
       "        0.70060105, 0.70060105, 0.70060105, 0.70071348, 0.70071348,\n",
       "        0.70060105, 0.70060105, 0.70060105, 0.71190781, 0.7018419 ,\n",
       "        0.70045215, 0.70048854, 0.70060105, 0.70060105, 0.70060105,\n",
       "        0.70060105, 0.70060105, 0.70060105, 0.70060105, 0.70060105]),\n",
       " 'split9_test_precision': array([0.        , 0.71953328, 0.70316206, 0.70954036, 0.70552381,\n",
       "        0.70483073, 0.70427868, 0.70470053, 0.70454545, 0.70454545,\n",
       "        0.70454545, 0.70454545, 0.70454545, 0.70454545, 0.70454545,\n",
       "        0.70481243, 0.70454545, 0.70454545, 0.71953328, 0.70954036,\n",
       "        0.70483073, 0.70470053, 0.70481243, 0.70454545, 0.70454545,\n",
       "        0.70454545, 0.70454545, 0.70454545, 0.70454545, 0.70481243]),\n",
       " 'mean_test_precision': array([0.        , 0.71129236, 0.69790761, 0.69784109, 0.69564339,\n",
       "        0.69570637, 0.69553111, 0.69558279, 0.69541506, 0.69537749,\n",
       "        0.69535098, 0.69537749, 0.69537749, 0.69538873, 0.69538873,\n",
       "        0.69542935, 0.69537749, 0.69537749, 0.71129236, 0.69784109,\n",
       "        0.69570637, 0.69558279, 0.69537746, 0.69532406, 0.69532406,\n",
       "        0.69532406, 0.69532406, 0.69532406, 0.69537749, 0.69537746]),\n",
       " 'std_test_precision': array([0.        , 0.00648826, 0.00682132, 0.00742118, 0.00729083,\n",
       "        0.00739082, 0.00724946, 0.00724668, 0.00717417, 0.00714402,\n",
       "        0.00711971, 0.00714402, 0.00714402, 0.00715231, 0.00715231,\n",
       "        0.00715455, 0.00714402, 0.00714402, 0.00648826, 0.00742118,\n",
       "        0.00739082, 0.00724668, 0.00719116, 0.00716984, 0.00716984,\n",
       "        0.00716984, 0.00716984, 0.00716984, 0.00714402, 0.00719116]),\n",
       " 'rank_test_precision': array([30,  1,  3,  4,  8,  6, 11,  9, 13, 16, 24, 16, 16, 14, 14, 12, 16,\n",
       "        16,  1,  4,  6,  9, 22, 25, 25, 25, 25, 25, 16, 22]),\n",
       " 'split0_test_f1_micro': array([0.59971711, 0.73536068, 0.73663366, 0.73946252, 0.74002829,\n",
       "        0.74031117, 0.74059406, 0.74059406, 0.74045262, 0.74045262,\n",
       "        0.74045262, 0.74045262, 0.74045262, 0.74045262, 0.74045262,\n",
       "        0.74031117, 0.74045262, 0.74045262, 0.73536068, 0.73946252,\n",
       "        0.74031117, 0.74059406, 0.74031117, 0.74016973, 0.74016973,\n",
       "        0.74016973, 0.74016973, 0.74016973, 0.74045262, 0.74031117]),\n",
       " 'split1_test_f1_micro': array([0.59971711, 0.74045262, 0.74582744, 0.74752475, 0.74893918,\n",
       "        0.74964639, 0.75021216, 0.75007072, 0.75021216, 0.75021216,\n",
       "        0.75021216, 0.75021216, 0.75021216, 0.75021216, 0.75021216,\n",
       "        0.75021216, 0.75021216, 0.75021216, 0.74045262, 0.74752475,\n",
       "        0.74964639, 0.75007072, 0.75021216, 0.75021216, 0.75021216,\n",
       "        0.75021216, 0.75021216, 0.75021216, 0.75021216, 0.75021216]),\n",
       " 'split2_test_f1_micro': array([0.59980195, 0.7381525 , 0.74084029, 0.7415476 , 0.74211345,\n",
       "        0.74239638, 0.74296223, 0.74296223, 0.74282077, 0.74282077,\n",
       "        0.74282077, 0.74282077, 0.74282077, 0.74282077, 0.74282077,\n",
       "        0.74282077, 0.74282077, 0.74282077, 0.7381525 , 0.7415476 ,\n",
       "        0.74239638, 0.74296223, 0.74282077, 0.74282077, 0.74282077,\n",
       "        0.74282077, 0.74282077, 0.74282077, 0.74282077, 0.74282077]),\n",
       " 'split3_test_f1_micro': array([0.59980195, 0.73263545, 0.73504032, 0.73942566, 0.74098175,\n",
       "        0.74126468, 0.74098175, 0.74126468, 0.74126468, 0.74126468,\n",
       "        0.74126468, 0.74126468, 0.74126468, 0.74126468, 0.74126468,\n",
       "        0.74126468, 0.74126468, 0.74126468, 0.73263545, 0.73942566,\n",
       "        0.74126468, 0.74126468, 0.74126468, 0.74126468, 0.74126468,\n",
       "        0.74126468, 0.74126468, 0.74126468, 0.74126468, 0.74126468]),\n",
       " 'split4_test_f1_micro': array([0.59980195, 0.73631348, 0.74593295, 0.74904513, 0.75088414,\n",
       "        0.75159146, 0.75173292, 0.75144999, 0.75130853, 0.75116707,\n",
       "        0.7510256 , 0.75116707, 0.75116707, 0.75116707, 0.75116707,\n",
       "        0.75116707, 0.75116707, 0.75116707, 0.73631348, 0.74904513,\n",
       "        0.75159146, 0.75144999, 0.75116707, 0.75116707, 0.75116707,\n",
       "        0.75116707, 0.75116707, 0.75116707, 0.75116707, 0.75116707]),\n",
       " 'split5_test_f1_micro': array([0.59980195, 0.73376715, 0.7404159 , 0.74451832, 0.74239638,\n",
       "        0.74310369, 0.74296223, 0.74296223, 0.74296223, 0.74296223,\n",
       "        0.74296223, 0.74296223, 0.74296223, 0.74296223, 0.74296223,\n",
       "        0.74324515, 0.74296223, 0.74296223, 0.73376715, 0.74451832,\n",
       "        0.74310369, 0.74296223, 0.74296223, 0.74296223, 0.74296223,\n",
       "        0.74296223, 0.74296223, 0.74296223, 0.74296223, 0.74296223]),\n",
       " 'split6_test_f1_micro': array([0.59980195, 0.73376715, 0.73857688, 0.74296223, 0.74168906,\n",
       "        0.74239638, 0.74282077, 0.74282077, 0.74282077, 0.74282077,\n",
       "        0.74282077, 0.74282077, 0.74282077, 0.74282077, 0.74282077,\n",
       "        0.74282077, 0.74282077, 0.74282077, 0.73376715, 0.74296223,\n",
       "        0.74239638, 0.74282077, 0.74282077, 0.74282077, 0.74282077,\n",
       "        0.74282077, 0.74282077, 0.74282077, 0.74282077, 0.74282077]),\n",
       " 'split7_test_f1_micro': array([0.59980195, 0.74381101, 0.74522563, 0.74904513, 0.7487622 ,\n",
       "        0.74932805, 0.74862074, 0.74904513, 0.74847928, 0.74847928,\n",
       "        0.74847928, 0.74847928, 0.74847928, 0.74847928, 0.74847928,\n",
       "        0.74847928, 0.74847928, 0.74847928, 0.74381101, 0.74904513,\n",
       "        0.74932805, 0.74904513, 0.74847928, 0.74847928, 0.74847928,\n",
       "        0.74847928, 0.74847928, 0.74847928, 0.74847928, 0.74847928]),\n",
       " 'split8_test_f1_micro': array([0.59980195, 0.74027444, 0.74678172, 0.74862074, 0.74918659,\n",
       "        0.75031829, 0.75130853, 0.75074268, 0.7510256 , 0.75088414,\n",
       "        0.75088414, 0.75088414, 0.75088414, 0.7510256 , 0.7510256 ,\n",
       "        0.75088414, 0.75088414, 0.75088414, 0.74027444, 0.74862074,\n",
       "        0.75031829, 0.75074268, 0.75088414, 0.75088414, 0.75088414,\n",
       "        0.75088414, 0.75088414, 0.75088414, 0.75088414, 0.75088414]),\n",
       " 'split9_test_f1_micro': array([0.59980195, 0.74352808, 0.74522563, 0.75328901, 0.75244023,\n",
       "        0.75215731, 0.75244023, 0.75258169, 0.75258169, 0.75258169,\n",
       "        0.75258169, 0.75258169, 0.75258169, 0.75258169, 0.75258169,\n",
       "        0.75272316, 0.75258169, 0.75258169, 0.74352808, 0.75328901,\n",
       "        0.75215731, 0.75258169, 0.75272316, 0.75258169, 0.75258169,\n",
       "        0.75258169, 0.75258169, 0.75258169, 0.75258169, 0.75272316]),\n",
       " 'mean_test_f1_micro': array([0.59978498, 0.73780626, 0.74205004, 0.74554411, 0.74574213,\n",
       "        0.74625138, 0.74646356, 0.74644942, 0.74639283, 0.74636454,\n",
       "        0.74635039, 0.74636454, 0.74636454, 0.74637869, 0.74637869,\n",
       "        0.74639283, 0.74636454, 0.74636454, 0.73780626, 0.74554411,\n",
       "        0.74625138, 0.74644942, 0.74636454, 0.74633625, 0.74633625,\n",
       "        0.74633625, 0.74633625, 0.74633625, 0.74636454, 0.74636454]),\n",
       " 'std_test_f1_micro': array([3.39350468e-05, 3.86764129e-03, 4.08134036e-03, 4.42716477e-03,\n",
       "        4.45655684e-03, 4.48008299e-03, 4.56021678e-03, 4.46709128e-03,\n",
       "        4.49156662e-03, 4.46175101e-03, 4.44670074e-03, 4.46175101e-03,\n",
       "        4.46175101e-03, 4.47625895e-03, 4.47625895e-03, 4.47984897e-03,\n",
       "        4.46175101e-03, 4.46175101e-03, 3.86764129e-03, 4.42716477e-03,\n",
       "        4.48008299e-03, 4.46709128e-03, 4.50048472e-03, 4.49987818e-03,\n",
       "        4.49987818e-03, 4.49987818e-03, 4.49987818e-03, 4.49987818e-03,\n",
       "        4.46175101e-03, 4.50048472e-03]),\n",
       " 'rank_test_f1_micro': array([30, 28, 27, 25, 24, 22,  1,  2,  5, 10, 16, 10, 10,  6,  6,  4, 10,\n",
       "        10, 28, 25, 22,  2,  8, 17, 17, 17, 17, 17, 10,  8])}"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_model.cv_results_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "5a869ea2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([30, 28, 27, 25, 24, 22,  1,  2,  5, 10, 16, 10, 10,  6,  6,  4, 10,\n",
       "       10, 28, 25, 22,  2,  8, 17, 17, 17, 17, 17, 10,  8])"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_model.cv_results_['rank_test_accuracy']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "3bcc2637",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'classifier': LogisticRegression(max_iter=10000),\n",
       " 'classifier__C': 0.0001,\n",
       " 'classifier__penalty': 'l1',\n",
       " 'classifier__solver': 'saga'}"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_model.cv_results_['params'][ np.argmin(best_model.cv_results_['mean_test_accuracy'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "0216cb96",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>classifier</th>\n",
       "      <th>C</th>\n",
       "      <th>penalty</th>\n",
       "      <th>solver</th>\n",
       "      <th>score_acc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.599785</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.737806</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.742050</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.745544</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0100</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.745742</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0100</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.746251</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.1000</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.746464</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.1000</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.746449</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.746393</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.746365</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>10.0000</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.746350</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>10.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.746365</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>100.0000</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.746365</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>100.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.746379</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>1000.0000</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.746379</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>1000.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.746393</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>10000.0000</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.746365</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>10000.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.746365</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.737806</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.745544</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0100</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.746251</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.1000</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.746449</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.746365</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>10.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.746336</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>100.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.746336</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>1000.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.746336</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>10000.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.746336</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>NaN</td>\n",
       "      <td>none</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.746336</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>NaN</td>\n",
       "      <td>none</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.746365</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.746365</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                            classifier           C penalty solver  score_acc\n",
       "0   LogisticRegression(max_iter=10000)      0.0001      l1   saga   0.599785\n",
       "1   LogisticRegression(max_iter=10000)      0.0001      l2   saga   0.737806\n",
       "2   LogisticRegression(max_iter=10000)      0.0010      l1   saga   0.742050\n",
       "3   LogisticRegression(max_iter=10000)      0.0010      l2   saga   0.745544\n",
       "4   LogisticRegression(max_iter=10000)      0.0100      l1   saga   0.745742\n",
       "5   LogisticRegression(max_iter=10000)      0.0100      l2   saga   0.746251\n",
       "6   LogisticRegression(max_iter=10000)      0.1000      l1   saga   0.746464\n",
       "7   LogisticRegression(max_iter=10000)      0.1000      l2   saga   0.746449\n",
       "8   LogisticRegression(max_iter=10000)      1.0000      l1   saga   0.746393\n",
       "9   LogisticRegression(max_iter=10000)      1.0000      l2   saga   0.746365\n",
       "10  LogisticRegression(max_iter=10000)     10.0000      l1   saga   0.746350\n",
       "11  LogisticRegression(max_iter=10000)     10.0000      l2   saga   0.746365\n",
       "12  LogisticRegression(max_iter=10000)    100.0000      l1   saga   0.746365\n",
       "13  LogisticRegression(max_iter=10000)    100.0000      l2   saga   0.746379\n",
       "14  LogisticRegression(max_iter=10000)   1000.0000      l1   saga   0.746379\n",
       "15  LogisticRegression(max_iter=10000)   1000.0000      l2   saga   0.746393\n",
       "16  LogisticRegression(max_iter=10000)  10000.0000      l1   saga   0.746365\n",
       "17  LogisticRegression(max_iter=10000)  10000.0000      l2   saga   0.746365\n",
       "18  LogisticRegression(max_iter=10000)      0.0001      l2  lbfgs   0.737806\n",
       "19  LogisticRegression(max_iter=10000)      0.0010      l2  lbfgs   0.745544\n",
       "20  LogisticRegression(max_iter=10000)      0.0100      l2  lbfgs   0.746251\n",
       "21  LogisticRegression(max_iter=10000)      0.1000      l2  lbfgs   0.746449\n",
       "22  LogisticRegression(max_iter=10000)      1.0000      l2  lbfgs   0.746365\n",
       "23  LogisticRegression(max_iter=10000)     10.0000      l2  lbfgs   0.746336\n",
       "24  LogisticRegression(max_iter=10000)    100.0000      l2  lbfgs   0.746336\n",
       "25  LogisticRegression(max_iter=10000)   1000.0000      l2  lbfgs   0.746336\n",
       "26  LogisticRegression(max_iter=10000)  10000.0000      l2  lbfgs   0.746336\n",
       "27  LogisticRegression(max_iter=10000)         NaN    none  lbfgs   0.746336\n",
       "28  LogisticRegression(max_iter=10000)         NaN    none   saga   0.746365\n",
       "29  LogisticRegression(max_iter=10000)         NaN     NaN    NaN   0.746365"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results = pd.DataFrame( best_model.cv_results_['params'] )\n",
    "\n",
    "results['score_acc'] = best_model.cv_results_['mean_test_accuracy']\n",
    "cols = results.columns.to_series().str.split('__').apply(lambda x: x[-1])\n",
    "results.columns = cols\n",
    "\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "83604c0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "acc_list.append(results[0:1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "9c89d3be",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'classifier': LogisticRegression(max_iter=10000),\n",
       " 'classifier__C': 0.0001,\n",
       " 'classifier__penalty': 'l2',\n",
       " 'classifier__solver': 'saga'}"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_model.cv_results_['params'][ np.argmin(best_model.cv_results_['rank_test_precision'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "69ab11b2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>classifier</th>\n",
       "      <th>C</th>\n",
       "      <th>penalty</th>\n",
       "      <th>solver</th>\n",
       "      <th>score_precision</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.711292</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.697908</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.697841</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0100</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.695643</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0100</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.695706</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.1000</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.695531</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.1000</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.695583</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.695415</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.695377</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>10.0000</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.695351</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>10.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.695377</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>100.0000</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.695377</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>100.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.695389</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>1000.0000</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.695389</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>1000.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.695429</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>10000.0000</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.695377</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>10000.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.695377</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.711292</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.697841</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0100</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.695706</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.1000</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.695583</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.695377</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>10.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.695324</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>100.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.695324</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>1000.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.695324</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>10000.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.695324</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>NaN</td>\n",
       "      <td>none</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.695324</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>NaN</td>\n",
       "      <td>none</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.695377</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.695377</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                            classifier           C penalty solver  \\\n",
       "0   LogisticRegression(max_iter=10000)      0.0001      l1   saga   \n",
       "1   LogisticRegression(max_iter=10000)      0.0001      l2   saga   \n",
       "2   LogisticRegression(max_iter=10000)      0.0010      l1   saga   \n",
       "3   LogisticRegression(max_iter=10000)      0.0010      l2   saga   \n",
       "4   LogisticRegression(max_iter=10000)      0.0100      l1   saga   \n",
       "5   LogisticRegression(max_iter=10000)      0.0100      l2   saga   \n",
       "6   LogisticRegression(max_iter=10000)      0.1000      l1   saga   \n",
       "7   LogisticRegression(max_iter=10000)      0.1000      l2   saga   \n",
       "8   LogisticRegression(max_iter=10000)      1.0000      l1   saga   \n",
       "9   LogisticRegression(max_iter=10000)      1.0000      l2   saga   \n",
       "10  LogisticRegression(max_iter=10000)     10.0000      l1   saga   \n",
       "11  LogisticRegression(max_iter=10000)     10.0000      l2   saga   \n",
       "12  LogisticRegression(max_iter=10000)    100.0000      l1   saga   \n",
       "13  LogisticRegression(max_iter=10000)    100.0000      l2   saga   \n",
       "14  LogisticRegression(max_iter=10000)   1000.0000      l1   saga   \n",
       "15  LogisticRegression(max_iter=10000)   1000.0000      l2   saga   \n",
       "16  LogisticRegression(max_iter=10000)  10000.0000      l1   saga   \n",
       "17  LogisticRegression(max_iter=10000)  10000.0000      l2   saga   \n",
       "18  LogisticRegression(max_iter=10000)      0.0001      l2  lbfgs   \n",
       "19  LogisticRegression(max_iter=10000)      0.0010      l2  lbfgs   \n",
       "20  LogisticRegression(max_iter=10000)      0.0100      l2  lbfgs   \n",
       "21  LogisticRegression(max_iter=10000)      0.1000      l2  lbfgs   \n",
       "22  LogisticRegression(max_iter=10000)      1.0000      l2  lbfgs   \n",
       "23  LogisticRegression(max_iter=10000)     10.0000      l2  lbfgs   \n",
       "24  LogisticRegression(max_iter=10000)    100.0000      l2  lbfgs   \n",
       "25  LogisticRegression(max_iter=10000)   1000.0000      l2  lbfgs   \n",
       "26  LogisticRegression(max_iter=10000)  10000.0000      l2  lbfgs   \n",
       "27  LogisticRegression(max_iter=10000)         NaN    none  lbfgs   \n",
       "28  LogisticRegression(max_iter=10000)         NaN    none   saga   \n",
       "29  LogisticRegression(max_iter=10000)         NaN     NaN    NaN   \n",
       "\n",
       "    score_precision  \n",
       "0          0.000000  \n",
       "1          0.711292  \n",
       "2          0.697908  \n",
       "3          0.697841  \n",
       "4          0.695643  \n",
       "5          0.695706  \n",
       "6          0.695531  \n",
       "7          0.695583  \n",
       "8          0.695415  \n",
       "9          0.695377  \n",
       "10         0.695351  \n",
       "11         0.695377  \n",
       "12         0.695377  \n",
       "13         0.695389  \n",
       "14         0.695389  \n",
       "15         0.695429  \n",
       "16         0.695377  \n",
       "17         0.695377  \n",
       "18         0.711292  \n",
       "19         0.697841  \n",
       "20         0.695706  \n",
       "21         0.695583  \n",
       "22         0.695377  \n",
       "23         0.695324  \n",
       "24         0.695324  \n",
       "25         0.695324  \n",
       "26         0.695324  \n",
       "27         0.695324  \n",
       "28         0.695377  \n",
       "29         0.695377  "
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results = pd.DataFrame( best_model.cv_results_['params'] )\n",
    "\n",
    "results['score_precision'] = best_model.cv_results_['mean_test_precision']\n",
    "cols = results.columns.to_series().str.split('__').apply(lambda x: x[-1])\n",
    "results.columns = cols\n",
    "\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "c584772f",
   "metadata": {},
   "outputs": [],
   "source": [
    "roc_auc_ovr_List.append(results[1:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "70f619b8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'classifier': LogisticRegression(max_iter=10000),\n",
       " 'classifier__C': 0.1,\n",
       " 'classifier__penalty': 'l1',\n",
       " 'classifier__solver': 'saga'}"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_model.cv_results_['params'][ np.argmin(best_model.cv_results_['rank_test_f1_micro'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "11465a39",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>classifier</th>\n",
       "      <th>C</th>\n",
       "      <th>penalty</th>\n",
       "      <th>solver</th>\n",
       "      <th>score_f1_micro</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.599785</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.737806</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.742050</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.745544</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0100</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.745742</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0100</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.746251</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.1000</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.746464</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.1000</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.746449</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.746393</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.746365</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>10.0000</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.746350</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>10.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.746365</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>100.0000</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.746365</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>100.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.746379</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>1000.0000</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.746379</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>1000.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.746393</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>10000.0000</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.746365</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>10000.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.746365</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.737806</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.745544</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0100</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.746251</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.1000</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.746449</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.746365</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>10.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.746336</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>100.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.746336</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>1000.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.746336</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>10000.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.746336</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>NaN</td>\n",
       "      <td>none</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.746336</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>NaN</td>\n",
       "      <td>none</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.746365</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.746365</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                            classifier           C penalty solver  \\\n",
       "0   LogisticRegression(max_iter=10000)      0.0001      l1   saga   \n",
       "1   LogisticRegression(max_iter=10000)      0.0001      l2   saga   \n",
       "2   LogisticRegression(max_iter=10000)      0.0010      l1   saga   \n",
       "3   LogisticRegression(max_iter=10000)      0.0010      l2   saga   \n",
       "4   LogisticRegression(max_iter=10000)      0.0100      l1   saga   \n",
       "5   LogisticRegression(max_iter=10000)      0.0100      l2   saga   \n",
       "6   LogisticRegression(max_iter=10000)      0.1000      l1   saga   \n",
       "7   LogisticRegression(max_iter=10000)      0.1000      l2   saga   \n",
       "8   LogisticRegression(max_iter=10000)      1.0000      l1   saga   \n",
       "9   LogisticRegression(max_iter=10000)      1.0000      l2   saga   \n",
       "10  LogisticRegression(max_iter=10000)     10.0000      l1   saga   \n",
       "11  LogisticRegression(max_iter=10000)     10.0000      l2   saga   \n",
       "12  LogisticRegression(max_iter=10000)    100.0000      l1   saga   \n",
       "13  LogisticRegression(max_iter=10000)    100.0000      l2   saga   \n",
       "14  LogisticRegression(max_iter=10000)   1000.0000      l1   saga   \n",
       "15  LogisticRegression(max_iter=10000)   1000.0000      l2   saga   \n",
       "16  LogisticRegression(max_iter=10000)  10000.0000      l1   saga   \n",
       "17  LogisticRegression(max_iter=10000)  10000.0000      l2   saga   \n",
       "18  LogisticRegression(max_iter=10000)      0.0001      l2  lbfgs   \n",
       "19  LogisticRegression(max_iter=10000)      0.0010      l2  lbfgs   \n",
       "20  LogisticRegression(max_iter=10000)      0.0100      l2  lbfgs   \n",
       "21  LogisticRegression(max_iter=10000)      0.1000      l2  lbfgs   \n",
       "22  LogisticRegression(max_iter=10000)      1.0000      l2  lbfgs   \n",
       "23  LogisticRegression(max_iter=10000)     10.0000      l2  lbfgs   \n",
       "24  LogisticRegression(max_iter=10000)    100.0000      l2  lbfgs   \n",
       "25  LogisticRegression(max_iter=10000)   1000.0000      l2  lbfgs   \n",
       "26  LogisticRegression(max_iter=10000)  10000.0000      l2  lbfgs   \n",
       "27  LogisticRegression(max_iter=10000)         NaN    none  lbfgs   \n",
       "28  LogisticRegression(max_iter=10000)         NaN    none   saga   \n",
       "29  LogisticRegression(max_iter=10000)         NaN     NaN    NaN   \n",
       "\n",
       "    score_f1_micro  \n",
       "0         0.599785  \n",
       "1         0.737806  \n",
       "2         0.742050  \n",
       "3         0.745544  \n",
       "4         0.745742  \n",
       "5         0.746251  \n",
       "6         0.746464  \n",
       "7         0.746449  \n",
       "8         0.746393  \n",
       "9         0.746365  \n",
       "10        0.746350  \n",
       "11        0.746365  \n",
       "12        0.746365  \n",
       "13        0.746379  \n",
       "14        0.746379  \n",
       "15        0.746393  \n",
       "16        0.746365  \n",
       "17        0.746365  \n",
       "18        0.737806  \n",
       "19        0.745544  \n",
       "20        0.746251  \n",
       "21        0.746449  \n",
       "22        0.746365  \n",
       "23        0.746336  \n",
       "24        0.746336  \n",
       "25        0.746336  \n",
       "26        0.746336  \n",
       "27        0.746336  \n",
       "28        0.746365  \n",
       "29        0.746365  "
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results = pd.DataFrame( best_model.cv_results_['params'] )\n",
    "\n",
    "results['score_f1_micro'] = best_model.cv_results_['mean_test_f1_micro']\n",
    "cols = results.columns.to_series().str.split('__').apply(lambda x: x[-1])\n",
    "results.columns = cols\n",
    "\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "4a693627",
   "metadata": {},
   "outputs": [],
   "source": [
    "f1_micro_List.append(results[6:7])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a0b1170",
   "metadata": {},
   "source": [
    "## Trial 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "d430cae5",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, Y_train, Y_test = train_test_split(X, y, test_size=0.2, shuffle=True, random_state = 8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "ec6bf17a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Guest-All\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\Guest-All\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\Guest-All\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\Guest-All\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\Guest-All\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\Guest-All\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\Guest-All\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\Guest-All\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\Guest-All\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\Guest-All\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\Guest-All\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1183: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Guest-All\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1183: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Guest-All\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1183: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Guest-All\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1183: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Guest-All\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1183: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Guest-All\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1183: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Guest-All\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1183: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Guest-All\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1183: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Guest-All\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1183: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Guest-All\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1183: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Guest-All\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1183: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Guest-All\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1183: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Guest-All\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1183: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Guest-All\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1183: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Guest-All\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1183: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Guest-All\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1183: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Guest-All\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1183: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Guest-All\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1183: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Guest-All\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1183: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Guest-All\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1183: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Create a pipeline - RF is a stand in, we will populate the classifier part below\n",
    "pipe = Pipeline([('std', StandardScaler()),\n",
    "                 ('classifier', LogisticRegression())])\n",
    "\n",
    "# Create search space of candidate learning algorithms and their hyperparameters\n",
    "# note lbfgs can't do l1, and if you pass penalty='none' it expects no C value\n",
    "search_space = [{'classifier': [LogisticRegression(max_iter=10000)],\n",
    "                 'classifier__solver': ['saga'],\n",
    "                 'classifier__penalty': ['l1', 'l2'],\n",
    "                 'classifier__C': np.logspace(-4, 4, 9)},\n",
    "                {'classifier': [LogisticRegression(max_iter=10000)],\n",
    "                 'classifier__solver': ['lbfgs'],\n",
    "                 'classifier__penalty': ['l2'],\n",
    "                 'classifier__C': np.logspace(-4, 4, 9)},\n",
    "                {'classifier': [LogisticRegression(max_iter=10000)],\n",
    "                 'classifier__solver': ['lbfgs','saga'],\n",
    "                 'classifier__penalty': ['none']},\n",
    "                {'classifier': [LogisticRegression(max_iter=10000)]}\n",
    "                ]\n",
    "\n",
    "# Create grid search\n",
    "clf = GridSearchCV(pipe, search_space, cv=StratifiedKFold(n_splits=10),\n",
    "                   scoring=['accuracy', 'precision', 'f1_micro'], refit=False,\n",
    "                   verbose=0)\n",
    "\n",
    "# Fit grid search\n",
    "best_model = clf.fit(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "b6aa2042",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'mean_fit_time': array([0.75963612, 0.70419698, 0.73650377, 0.6975759 , 0.78547673,\n",
       "        0.72312765, 1.01477489, 0.86022205, 1.10970495, 0.85233052,\n",
       "        0.99277034, 0.83934462, 1.03166394, 0.85490468, 1.04923179,\n",
       "        0.75445869, 0.78232036, 0.72454593, 0.0917403 , 0.10399435,\n",
       "        0.10750997, 0.1098058 , 0.10713108, 0.10839021, 0.10747228,\n",
       "        0.10636795, 0.10876765, 0.10564013, 0.72149885, 0.10869601]),\n",
       " 'std_fit_time': array([0.07955152, 0.07005131, 0.05341697, 0.05611961, 0.07649789,\n",
       "        0.16172454, 0.28443228, 0.20647911, 0.24805301, 0.20393432,\n",
       "        0.18884795, 0.19014833, 0.25463941, 0.21010194, 0.22602989,\n",
       "        0.15440629, 0.06949143, 0.0451283 , 0.00391602, 0.00558546,\n",
       "        0.01178968, 0.00832468, 0.00559549, 0.00691205, 0.01296543,\n",
       "        0.00776117, 0.00541714, 0.00554944, 0.04602745, 0.00543743]),\n",
       " 'mean_score_time': array([0.01193368, 0.01313634, 0.01193237, 0.0121654 , 0.0132374 ,\n",
       "        0.01607313, 0.01489971, 0.01282377, 0.01726813, 0.02325668,\n",
       "        0.01327603, 0.01427064, 0.01622682, 0.01587229, 0.01566257,\n",
       "        0.01376872, 0.01315939, 0.01359992, 0.01203458, 0.01126819,\n",
       "        0.01281157, 0.01369412, 0.01112795, 0.01150289, 0.0143003 ,\n",
       "        0.01247411, 0.01353991, 0.00997808, 0.01531365, 0.01286283]),\n",
       " 'std_score_time': array([0.00395698, 0.0036436 , 0.00347467, 0.00427958, 0.00350517,\n",
       "        0.00714955, 0.00664297, 0.00526106, 0.00670472, 0.01807306,\n",
       "        0.00825671, 0.00709963, 0.01386424, 0.00729751, 0.00800764,\n",
       "        0.00768057, 0.00314498, 0.00366893, 0.0039712 , 0.00379663,\n",
       "        0.00384521, 0.00372348, 0.00413936, 0.00381645, 0.00345695,\n",
       "        0.00366459, 0.00362331, 0.00314066, 0.00429772, 0.00384114]),\n",
       " 'param_classifier': masked_array(data=[LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000)],\n",
       "              mask=[False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'param_classifier__C': masked_array(data=[0.0001, 0.0001, 0.001, 0.001, 0.01, 0.01, 0.1, 0.1,\n",
       "                    1.0, 1.0, 10.0, 10.0, 100.0, 100.0, 1000.0, 1000.0,\n",
       "                    10000.0, 10000.0, 0.0001, 0.001, 0.01, 0.1, 1.0, 10.0,\n",
       "                    100.0, 1000.0, 10000.0, --, --, --],\n",
       "              mask=[False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False,  True,  True,  True],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'param_classifier__penalty': masked_array(data=['l1', 'l2', 'l1', 'l2', 'l1', 'l2', 'l1', 'l2', 'l1',\n",
       "                    'l2', 'l1', 'l2', 'l1', 'l2', 'l1', 'l2', 'l1', 'l2',\n",
       "                    'l2', 'l2', 'l2', 'l2', 'l2', 'l2', 'l2', 'l2', 'l2',\n",
       "                    'none', 'none', --],\n",
       "              mask=[False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False,  True],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'param_classifier__solver': masked_array(data=['saga', 'saga', 'saga', 'saga', 'saga', 'saga', 'saga',\n",
       "                    'saga', 'saga', 'saga', 'saga', 'saga', 'saga', 'saga',\n",
       "                    'saga', 'saga', 'saga', 'saga', 'lbfgs', 'lbfgs',\n",
       "                    'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs',\n",
       "                    'lbfgs', 'lbfgs', 'saga', --],\n",
       "              mask=[False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False,  True],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'params': [{'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 0.0001,\n",
       "   'classifier__penalty': 'l1',\n",
       "   'classifier__solver': 'saga'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 0.0001,\n",
       "   'classifier__penalty': 'l2',\n",
       "   'classifier__solver': 'saga'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 0.001,\n",
       "   'classifier__penalty': 'l1',\n",
       "   'classifier__solver': 'saga'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 0.001,\n",
       "   'classifier__penalty': 'l2',\n",
       "   'classifier__solver': 'saga'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 0.01,\n",
       "   'classifier__penalty': 'l1',\n",
       "   'classifier__solver': 'saga'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 0.01,\n",
       "   'classifier__penalty': 'l2',\n",
       "   'classifier__solver': 'saga'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 0.1,\n",
       "   'classifier__penalty': 'l1',\n",
       "   'classifier__solver': 'saga'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 0.1,\n",
       "   'classifier__penalty': 'l2',\n",
       "   'classifier__solver': 'saga'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 1.0,\n",
       "   'classifier__penalty': 'l1',\n",
       "   'classifier__solver': 'saga'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 1.0,\n",
       "   'classifier__penalty': 'l2',\n",
       "   'classifier__solver': 'saga'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 10.0,\n",
       "   'classifier__penalty': 'l1',\n",
       "   'classifier__solver': 'saga'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 10.0,\n",
       "   'classifier__penalty': 'l2',\n",
       "   'classifier__solver': 'saga'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 100.0,\n",
       "   'classifier__penalty': 'l1',\n",
       "   'classifier__solver': 'saga'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 100.0,\n",
       "   'classifier__penalty': 'l2',\n",
       "   'classifier__solver': 'saga'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 1000.0,\n",
       "   'classifier__penalty': 'l1',\n",
       "   'classifier__solver': 'saga'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 1000.0,\n",
       "   'classifier__penalty': 'l2',\n",
       "   'classifier__solver': 'saga'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 10000.0,\n",
       "   'classifier__penalty': 'l1',\n",
       "   'classifier__solver': 'saga'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 10000.0,\n",
       "   'classifier__penalty': 'l2',\n",
       "   'classifier__solver': 'saga'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 0.0001,\n",
       "   'classifier__penalty': 'l2',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 0.001,\n",
       "   'classifier__penalty': 'l2',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 0.01,\n",
       "   'classifier__penalty': 'l2',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 0.1,\n",
       "   'classifier__penalty': 'l2',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 1.0,\n",
       "   'classifier__penalty': 'l2',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 10.0,\n",
       "   'classifier__penalty': 'l2',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 100.0,\n",
       "   'classifier__penalty': 'l2',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 1000.0,\n",
       "   'classifier__penalty': 'l2',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 10000.0,\n",
       "   'classifier__penalty': 'l2',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__penalty': 'none',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__penalty': 'none',\n",
       "   'classifier__solver': 'saga'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000)}],\n",
       " 'split0_test_accuracy': array([0.6       , 0.73847242, 0.745686  , 0.74922207, 0.74964639,\n",
       "        0.75049505, 0.75021216, 0.75035361, 0.75049505, 0.75049505,\n",
       "        0.75049505, 0.75049505, 0.75049505, 0.75049505, 0.75049505,\n",
       "        0.75049505, 0.75049505, 0.75049505, 0.73847242, 0.74922207,\n",
       "        0.75049505, 0.75035361, 0.75049505, 0.75049505, 0.75049505,\n",
       "        0.75049505, 0.75049505, 0.75049505, 0.75049505, 0.75049505]),\n",
       " 'split1_test_accuracy': array([0.6       , 0.73705799, 0.74031117, 0.74413013, 0.74483734,\n",
       "        0.7446959 , 0.74497878, 0.74526167, 0.74512023, 0.74526167,\n",
       "        0.74540311, 0.74540311, 0.74540311, 0.74540311, 0.74540311,\n",
       "        0.74540311, 0.74540311, 0.74540311, 0.73705799, 0.74413013,\n",
       "        0.7446959 , 0.74526167, 0.74526167, 0.74540311, 0.74540311,\n",
       "        0.74540311, 0.74540311, 0.74540311, 0.74526167, 0.74526167]),\n",
       " 'split2_test_accuracy': array([0.60008488, 0.73405008, 0.74055736, 0.74197199, 0.74211345,\n",
       "        0.74366954, 0.74381101, 0.74423539, 0.74423539, 0.74423539,\n",
       "        0.74423539, 0.74423539, 0.74423539, 0.74423539, 0.74423539,\n",
       "        0.74423539, 0.74423539, 0.74423539, 0.73405008, 0.74197199,\n",
       "        0.74366954, 0.74423539, 0.74423539, 0.74423539, 0.74423539,\n",
       "        0.74423539, 0.74423539, 0.74423539, 0.74423539, 0.74423539]),\n",
       " 'split3_test_accuracy': array([0.59994341, 0.7370208 , 0.74522563, 0.74296223, 0.74253784,\n",
       "        0.74324515, 0.7426793 , 0.74324515, 0.74324515, 0.74324515,\n",
       "        0.74324515, 0.74324515, 0.74324515, 0.74324515, 0.74324515,\n",
       "        0.74324515, 0.74324515, 0.74324515, 0.7370208 , 0.74296223,\n",
       "        0.74324515, 0.74310369, 0.74324515, 0.74324515, 0.74324515,\n",
       "        0.74324515, 0.74324515, 0.74324515, 0.74324515, 0.74324515]),\n",
       " 'split4_test_accuracy': array([0.59994341, 0.74451832, 0.74352808, 0.74664026, 0.74522563,\n",
       "        0.74565002, 0.74607441, 0.74621587, 0.74621587, 0.74621587,\n",
       "        0.74621587, 0.74621587, 0.74621587, 0.74621587, 0.74621587,\n",
       "        0.74621587, 0.74621587, 0.74621587, 0.74451832, 0.74664026,\n",
       "        0.74565002, 0.74621587, 0.74621587, 0.74621587, 0.74621587,\n",
       "        0.74621587, 0.74621587, 0.74621587, 0.74621587, 0.74621587]),\n",
       " 'split5_test_accuracy': array([0.59994341, 0.72994766, 0.73518178, 0.7404159 , 0.73970859,\n",
       "        0.73985005, 0.7404159 , 0.7404159 , 0.74055736, 0.7404159 ,\n",
       "        0.7404159 , 0.7404159 , 0.7404159 , 0.7404159 , 0.7404159 ,\n",
       "        0.7404159 , 0.7404159 , 0.7404159 , 0.72994766, 0.7404159 ,\n",
       "        0.73985005, 0.7404159 , 0.7404159 , 0.7404159 , 0.7404159 ,\n",
       "        0.7404159 , 0.7404159 , 0.7404159 , 0.7404159 , 0.7404159 ]),\n",
       " 'split6_test_accuracy': array([0.59994341, 0.73574763, 0.73730372, 0.74211345, 0.74168906,\n",
       "        0.74253784, 0.74239638, 0.74239638, 0.74239638, 0.74239638,\n",
       "        0.74239638, 0.74239638, 0.74239638, 0.74239638, 0.74239638,\n",
       "        0.74239638, 0.74239638, 0.74239638, 0.73574763, 0.74211345,\n",
       "        0.74253784, 0.74239638, 0.74239638, 0.74239638, 0.74239638,\n",
       "        0.74239638, 0.74239638, 0.74239638, 0.74239638, 0.74239638]),\n",
       " 'split7_test_accuracy': array([0.59994341, 0.73277691, 0.74112321, 0.74352808, 0.74395247,\n",
       "        0.74494271, 0.74494271, 0.74494271, 0.74508417, 0.74508417,\n",
       "        0.74508417, 0.74508417, 0.74508417, 0.74508417, 0.74508417,\n",
       "        0.74508417, 0.74508417, 0.74508417, 0.73277691, 0.74352808,\n",
       "        0.74494271, 0.74494271, 0.74508417, 0.74508417, 0.74508417,\n",
       "        0.74508417, 0.74508417, 0.74508417, 0.74508417, 0.74508417]),\n",
       " 'split8_test_accuracy': array([0.59994341, 0.74027444, 0.7476305 , 0.74635733, 0.74890366,\n",
       "        0.74819635, 0.74805489, 0.74819635, 0.74819635, 0.74819635,\n",
       "        0.74819635, 0.74819635, 0.74819635, 0.74819635, 0.74819635,\n",
       "        0.74819635, 0.74819635, 0.74819635, 0.74027444, 0.74635733,\n",
       "        0.74819635, 0.74819635, 0.74819635, 0.74819635, 0.74819635,\n",
       "        0.74819635, 0.74819635, 0.74819635, 0.74819635, 0.74819635]),\n",
       " 'split9_test_accuracy': array([0.59994341, 0.74706465, 0.75173292, 0.75569387, 0.75541095,\n",
       "        0.75583534, 0.75555241, 0.75555241, 0.75583534, 0.75583534,\n",
       "        0.75569387, 0.75569387, 0.75569387, 0.75569387, 0.75569387,\n",
       "        0.75569387, 0.75569387, 0.75569387, 0.74706465, 0.75569387,\n",
       "        0.75583534, 0.75555241, 0.75583534, 0.75569387, 0.75569387,\n",
       "        0.75569387, 0.75569387, 0.75569387, 0.75569387, 0.75583534]),\n",
       " 'mean_test_accuracy': array([0.59996888, 0.73769309, 0.74282804, 0.74530353, 0.74540254,\n",
       "        0.7459118 , 0.7459118 , 0.74608154, 0.74613813, 0.74613813,\n",
       "        0.74613813, 0.74613813, 0.74613813, 0.74613813, 0.74613813,\n",
       "        0.74613813, 0.74613813, 0.74613813, 0.73769309, 0.74530353,\n",
       "        0.7459118 , 0.7460674 , 0.74613813, 0.74613813, 0.74613813,\n",
       "        0.74613813, 0.74613813, 0.74613813, 0.74612398, 0.74613813]),\n",
       " 'std_test_accuracy': array([4.46448828e-05, 4.93713230e-03, 4.69198601e-03, 4.25895510e-03,\n",
       "        4.45112861e-03, 4.32960916e-03, 4.18675199e-03, 4.14174827e-03,\n",
       "        4.20170470e-03, 4.21751387e-03, 4.18237643e-03, 4.18237643e-03,\n",
       "        4.18237643e-03, 4.18237643e-03, 4.18237643e-03, 4.18237643e-03,\n",
       "        4.18237643e-03, 4.18237643e-03, 4.93713230e-03, 4.25895510e-03,\n",
       "        4.32960916e-03, 4.15164166e-03, 4.21751387e-03, 4.18237643e-03,\n",
       "        4.18237643e-03, 4.18237643e-03, 4.18237643e-03, 4.18237643e-03,\n",
       "        4.18507653e-03, 4.21751387e-03]),\n",
       " 'rank_test_accuracy': array([30, 28, 27, 25, 24, 21, 21, 19,  1,  2,  5,  5,  5,  5,  5,  5,  5,\n",
       "         5, 28, 25, 21, 20,  2,  5,  5,  5,  5,  5, 18,  2]),\n",
       " 'split0_test_precision': array([0.        , 0.71707317, 0.70749396, 0.70759543, 0.70551671,\n",
       "        0.70604183, 0.70549536, 0.70576849, 0.70588235, 0.70588235,\n",
       "        0.70588235, 0.70588235, 0.70588235, 0.70588235, 0.70588235,\n",
       "        0.70588235, 0.70588235, 0.70588235, 0.71707317, 0.70759543,\n",
       "        0.70604183, 0.70576849, 0.70588235, 0.70588235, 0.70588235,\n",
       "        0.70588235, 0.70588235, 0.70588235, 0.70588235, 0.70588235]),\n",
       " 'split1_test_precision': array([0.        , 0.70767252, 0.69390149, 0.69321198, 0.69133034,\n",
       "        0.6910721 , 0.69087523, 0.69139023, 0.69113264, 0.69139023,\n",
       "        0.69150522, 0.69150522, 0.69150522, 0.69150522, 0.69150522,\n",
       "        0.69150522, 0.69150522, 0.69150522, 0.70767252, 0.69321198,\n",
       "        0.6910721 , 0.69139023, 0.69139023, 0.69150522, 0.69150522,\n",
       "        0.69150522, 0.69150522, 0.69150522, 0.69124767, 0.69139023]),\n",
       " 'split2_test_precision': array([0.        , 0.70330614, 0.6940211 , 0.69104762, 0.68829707,\n",
       "        0.69100489, 0.69097744, 0.69161339, 0.69161339, 0.69161339,\n",
       "        0.69161339, 0.69161339, 0.69161339, 0.69161339, 0.69161339,\n",
       "        0.69161339, 0.69161339, 0.69161339, 0.70330614, 0.69104762,\n",
       "        0.69100489, 0.69161339, 0.69161339, 0.69161339, 0.69161339,\n",
       "        0.69161339, 0.69161339, 0.69161339, 0.69161339, 0.69161339]),\n",
       " 'split3_test_precision': array([0.        , 0.71371857, 0.70417495, 0.69676917, 0.69354839,\n",
       "        0.69428462, 0.69351745, 0.69398698, 0.69398698, 0.69398698,\n",
       "        0.69398698, 0.69398698, 0.69398698, 0.69398698, 0.69398698,\n",
       "        0.69398698, 0.69398698, 0.69398698, 0.71371857, 0.69676917,\n",
       "        0.69428462, 0.69386973, 0.69398698, 0.69398698, 0.69398698,\n",
       "        0.69398698, 0.69398698, 0.69398698, 0.69398698, 0.69398698]),\n",
       " 'split4_test_precision': array([0.        , 0.71689304, 0.69303918, 0.6951449 , 0.6899741 ,\n",
       "        0.6900369 , 0.69023941, 0.69035346, 0.69035346, 0.69035346,\n",
       "        0.69035346, 0.69035346, 0.69035346, 0.69035346, 0.69035346,\n",
       "        0.69035346, 0.69035346, 0.69035346, 0.71689304, 0.6951449 ,\n",
       "        0.6900369 , 0.69035346, 0.69035346, 0.69035346, 0.69035346,\n",
       "        0.69035346, 0.69035346, 0.69035346, 0.69035346, 0.69035346]),\n",
       " 'split5_test_precision': array([0.        , 0.7003925 , 0.68833727, 0.68972105, 0.68543544,\n",
       "        0.68527538, 0.68533035, 0.68560748, 0.68572496, 0.68546881,\n",
       "        0.68546881, 0.68546881, 0.68546881, 0.68546881, 0.68546881,\n",
       "        0.68546881, 0.68546881, 0.68546881, 0.7003925 , 0.68972105,\n",
       "        0.68527538, 0.68560748, 0.68546881, 0.68546881, 0.68546881,\n",
       "        0.68546881, 0.68546881, 0.68546881, 0.68546881, 0.68546881]),\n",
       " 'split6_test_precision': array([0.        , 0.71108179, 0.69197311, 0.69424043, 0.69166029,\n",
       "        0.69192688, 0.69151769, 0.69151769, 0.69151769, 0.69151769,\n",
       "        0.69151769, 0.69151769, 0.69151769, 0.69151769, 0.69151769,\n",
       "        0.69151769, 0.69151769, 0.69151769, 0.71108179, 0.69424043,\n",
       "        0.69192688, 0.69151769, 0.69151769, 0.69151769, 0.69151769,\n",
       "        0.69151769, 0.69151769, 0.69151769, 0.69151769, 0.69151769]),\n",
       " 'split7_test_precision': array([0.        , 0.70107066, 0.6935609 , 0.69230769, 0.69049401,\n",
       "        0.69130272, 0.6905913 , 0.6905913 , 0.69070632, 0.69070632,\n",
       "        0.69070632, 0.69070632, 0.69070632, 0.69070632, 0.69070632,\n",
       "        0.69070632, 0.69070632, 0.69070632, 0.70107066, 0.69230769,\n",
       "        0.69130272, 0.6905913 , 0.69070632, 0.69070632, 0.69070632,\n",
       "        0.69070632, 0.69070632, 0.69070632, 0.69070632, 0.69070632]),\n",
       " 'split8_test_precision': array([0.        , 0.71659389, 0.7088    , 0.70112709, 0.70242215,\n",
       "        0.70030581, 0.70003821, 0.70015279, 0.70015279, 0.70015279,\n",
       "        0.70015279, 0.70015279, 0.70015279, 0.70015279, 0.70015279,\n",
       "        0.70015279, 0.70015279, 0.70015279, 0.71659389, 0.70112709,\n",
       "        0.70030581, 0.70015279, 0.70015279, 0.70015279, 0.70015279,\n",
       "        0.70015279, 0.70015279, 0.70015279, 0.70015279, 0.70015279]),\n",
       " 'split9_test_precision': array([0.        , 0.71648626, 0.70626682, 0.70765749, 0.70542056,\n",
       "        0.70437685, 0.70430906, 0.70415739, 0.70437685, 0.70437685,\n",
       "        0.70411568, 0.70411568, 0.70411568, 0.70411568, 0.70411568,\n",
       "        0.70411568, 0.70411568, 0.70411568, 0.71648626, 0.70765749,\n",
       "        0.70437685, 0.70415739, 0.70437685, 0.70411568, 0.70411568,\n",
       "        0.70411568, 0.70411568, 0.70411568, 0.70411568, 0.70437685]),\n",
       " 'mean_test_precision': array([0.        , 0.71042886, 0.69815688, 0.69688229, 0.69440991,\n",
       "        0.6945628 , 0.69428915, 0.69451392, 0.69454474, 0.69454489,\n",
       "        0.69453027, 0.69453027, 0.69453027, 0.69453027, 0.69453027,\n",
       "        0.69453027, 0.69453027, 0.69453027, 0.71042886, 0.69688229,\n",
       "        0.6945628 , 0.69450219, 0.69454489, 0.69453027, 0.69453027,\n",
       "        0.69453027, 0.69453027, 0.69453027, 0.69450451, 0.69454489]),\n",
       " 'std_test_precision': array([0.        , 0.00646928, 0.00720858, 0.00615329, 0.00692457,\n",
       "        0.00640549, 0.00632663, 0.00625224, 0.00629651, 0.00631944,\n",
       "        0.00627352, 0.00627352, 0.00627352, 0.00627352, 0.00627352,\n",
       "        0.00627352, 0.00627352, 0.00627352, 0.00646928, 0.00615329,\n",
       "        0.00640549, 0.00625333, 0.00631944, 0.00627352, 0.00627352,\n",
       "        0.00627352, 0.00627352, 0.00627352, 0.00628641, 0.00631944]),\n",
       " 'rank_test_precision': array([30,  1,  3,  4, 28,  6, 29, 25, 11,  8, 12, 12, 12, 12, 12, 12, 12,\n",
       "        12,  1,  4,  6, 27,  8, 12, 12, 12, 12, 12, 26,  8]),\n",
       " 'split0_test_f1_micro': array([0.6       , 0.73847242, 0.745686  , 0.74922207, 0.74964639,\n",
       "        0.75049505, 0.75021216, 0.75035361, 0.75049505, 0.75049505,\n",
       "        0.75049505, 0.75049505, 0.75049505, 0.75049505, 0.75049505,\n",
       "        0.75049505, 0.75049505, 0.75049505, 0.73847242, 0.74922207,\n",
       "        0.75049505, 0.75035361, 0.75049505, 0.75049505, 0.75049505,\n",
       "        0.75049505, 0.75049505, 0.75049505, 0.75049505, 0.75049505]),\n",
       " 'split1_test_f1_micro': array([0.6       , 0.73705799, 0.74031117, 0.74413013, 0.74483734,\n",
       "        0.7446959 , 0.74497878, 0.74526167, 0.74512023, 0.74526167,\n",
       "        0.74540311, 0.74540311, 0.74540311, 0.74540311, 0.74540311,\n",
       "        0.74540311, 0.74540311, 0.74540311, 0.73705799, 0.74413013,\n",
       "        0.7446959 , 0.74526167, 0.74526167, 0.74540311, 0.74540311,\n",
       "        0.74540311, 0.74540311, 0.74540311, 0.74526167, 0.74526167]),\n",
       " 'split2_test_f1_micro': array([0.60008488, 0.73405008, 0.74055736, 0.74197199, 0.74211345,\n",
       "        0.74366954, 0.74381101, 0.74423539, 0.74423539, 0.74423539,\n",
       "        0.74423539, 0.74423539, 0.74423539, 0.74423539, 0.74423539,\n",
       "        0.74423539, 0.74423539, 0.74423539, 0.73405008, 0.74197199,\n",
       "        0.74366954, 0.74423539, 0.74423539, 0.74423539, 0.74423539,\n",
       "        0.74423539, 0.74423539, 0.74423539, 0.74423539, 0.74423539]),\n",
       " 'split3_test_f1_micro': array([0.59994341, 0.7370208 , 0.74522563, 0.74296223, 0.74253784,\n",
       "        0.74324515, 0.7426793 , 0.74324515, 0.74324515, 0.74324515,\n",
       "        0.74324515, 0.74324515, 0.74324515, 0.74324515, 0.74324515,\n",
       "        0.74324515, 0.74324515, 0.74324515, 0.7370208 , 0.74296223,\n",
       "        0.74324515, 0.74310369, 0.74324515, 0.74324515, 0.74324515,\n",
       "        0.74324515, 0.74324515, 0.74324515, 0.74324515, 0.74324515]),\n",
       " 'split4_test_f1_micro': array([0.59994341, 0.74451832, 0.74352808, 0.74664026, 0.74522563,\n",
       "        0.74565002, 0.74607441, 0.74621587, 0.74621587, 0.74621587,\n",
       "        0.74621587, 0.74621587, 0.74621587, 0.74621587, 0.74621587,\n",
       "        0.74621587, 0.74621587, 0.74621587, 0.74451832, 0.74664026,\n",
       "        0.74565002, 0.74621587, 0.74621587, 0.74621587, 0.74621587,\n",
       "        0.74621587, 0.74621587, 0.74621587, 0.74621587, 0.74621587]),\n",
       " 'split5_test_f1_micro': array([0.59994341, 0.72994766, 0.73518178, 0.7404159 , 0.73970859,\n",
       "        0.73985005, 0.7404159 , 0.7404159 , 0.74055736, 0.7404159 ,\n",
       "        0.7404159 , 0.7404159 , 0.7404159 , 0.7404159 , 0.7404159 ,\n",
       "        0.7404159 , 0.7404159 , 0.7404159 , 0.72994766, 0.7404159 ,\n",
       "        0.73985005, 0.7404159 , 0.7404159 , 0.7404159 , 0.7404159 ,\n",
       "        0.7404159 , 0.7404159 , 0.7404159 , 0.7404159 , 0.7404159 ]),\n",
       " 'split6_test_f1_micro': array([0.59994341, 0.73574763, 0.73730372, 0.74211345, 0.74168906,\n",
       "        0.74253784, 0.74239638, 0.74239638, 0.74239638, 0.74239638,\n",
       "        0.74239638, 0.74239638, 0.74239638, 0.74239638, 0.74239638,\n",
       "        0.74239638, 0.74239638, 0.74239638, 0.73574763, 0.74211345,\n",
       "        0.74253784, 0.74239638, 0.74239638, 0.74239638, 0.74239638,\n",
       "        0.74239638, 0.74239638, 0.74239638, 0.74239638, 0.74239638]),\n",
       " 'split7_test_f1_micro': array([0.59994341, 0.73277691, 0.74112321, 0.74352808, 0.74395247,\n",
       "        0.74494271, 0.74494271, 0.74494271, 0.74508417, 0.74508417,\n",
       "        0.74508417, 0.74508417, 0.74508417, 0.74508417, 0.74508417,\n",
       "        0.74508417, 0.74508417, 0.74508417, 0.73277691, 0.74352808,\n",
       "        0.74494271, 0.74494271, 0.74508417, 0.74508417, 0.74508417,\n",
       "        0.74508417, 0.74508417, 0.74508417, 0.74508417, 0.74508417]),\n",
       " 'split8_test_f1_micro': array([0.59994341, 0.74027444, 0.7476305 , 0.74635733, 0.74890366,\n",
       "        0.74819635, 0.74805489, 0.74819635, 0.74819635, 0.74819635,\n",
       "        0.74819635, 0.74819635, 0.74819635, 0.74819635, 0.74819635,\n",
       "        0.74819635, 0.74819635, 0.74819635, 0.74027444, 0.74635733,\n",
       "        0.74819635, 0.74819635, 0.74819635, 0.74819635, 0.74819635,\n",
       "        0.74819635, 0.74819635, 0.74819635, 0.74819635, 0.74819635]),\n",
       " 'split9_test_f1_micro': array([0.59994341, 0.74706465, 0.75173292, 0.75569387, 0.75541095,\n",
       "        0.75583534, 0.75555241, 0.75555241, 0.75583534, 0.75583534,\n",
       "        0.75569387, 0.75569387, 0.75569387, 0.75569387, 0.75569387,\n",
       "        0.75569387, 0.75569387, 0.75569387, 0.74706465, 0.75569387,\n",
       "        0.75583534, 0.75555241, 0.75583534, 0.75569387, 0.75569387,\n",
       "        0.75569387, 0.75569387, 0.75569387, 0.75569387, 0.75583534]),\n",
       " 'mean_test_f1_micro': array([0.59996888, 0.73769309, 0.74282804, 0.74530353, 0.74540254,\n",
       "        0.7459118 , 0.7459118 , 0.74608154, 0.74613813, 0.74613813,\n",
       "        0.74613813, 0.74613813, 0.74613813, 0.74613813, 0.74613813,\n",
       "        0.74613813, 0.74613813, 0.74613813, 0.73769309, 0.74530353,\n",
       "        0.7459118 , 0.7460674 , 0.74613813, 0.74613813, 0.74613813,\n",
       "        0.74613813, 0.74613813, 0.74613813, 0.74612398, 0.74613813]),\n",
       " 'std_test_f1_micro': array([4.46448828e-05, 4.93713230e-03, 4.69198601e-03, 4.25895510e-03,\n",
       "        4.45112861e-03, 4.32960916e-03, 4.18675199e-03, 4.14174827e-03,\n",
       "        4.20170470e-03, 4.21751387e-03, 4.18237643e-03, 4.18237643e-03,\n",
       "        4.18237643e-03, 4.18237643e-03, 4.18237643e-03, 4.18237643e-03,\n",
       "        4.18237643e-03, 4.18237643e-03, 4.93713230e-03, 4.25895510e-03,\n",
       "        4.32960916e-03, 4.15164166e-03, 4.21751387e-03, 4.18237643e-03,\n",
       "        4.18237643e-03, 4.18237643e-03, 4.18237643e-03, 4.18237643e-03,\n",
       "        4.18507653e-03, 4.21751387e-03]),\n",
       " 'rank_test_f1_micro': array([30, 28, 27, 25, 24, 21, 21, 19,  1,  2,  5,  5,  5,  5,  5,  5,  5,\n",
       "         5, 28, 25, 21, 20,  2,  5,  5,  5,  5,  5, 18,  2])}"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_model.cv_results_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "a76a5ef1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([30, 28, 27, 25, 24, 21, 21, 19,  1,  2,  5,  5,  5,  5,  5,  5,  5,\n",
       "        5, 28, 25, 21, 20,  2,  5,  5,  5,  5,  5, 18,  2])"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_model.cv_results_['rank_test_accuracy']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "80177251",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'classifier': LogisticRegression(max_iter=10000),\n",
       " 'classifier__C': 0.0001,\n",
       " 'classifier__penalty': 'l1',\n",
       " 'classifier__solver': 'saga'}"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_model.cv_results_['params'][ np.argmin(best_model.cv_results_['mean_test_accuracy'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "543c8047",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>classifier</th>\n",
       "      <th>C</th>\n",
       "      <th>penalty</th>\n",
       "      <th>solver</th>\n",
       "      <th>score_acc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.599969</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.737693</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.742828</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.745304</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0100</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.745403</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0100</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.745912</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.1000</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.745912</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.1000</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.746082</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.746138</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.746138</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>10.0000</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.746138</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>10.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.746138</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>100.0000</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.746138</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>100.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.746138</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>1000.0000</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.746138</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>1000.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.746138</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>10000.0000</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.746138</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>10000.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.746138</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.737693</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.745304</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0100</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.745912</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.1000</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.746067</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.746138</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>10.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.746138</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>100.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.746138</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>1000.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.746138</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>10000.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.746138</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>NaN</td>\n",
       "      <td>none</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.746138</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>NaN</td>\n",
       "      <td>none</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.746124</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.746138</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                            classifier           C penalty solver  score_acc\n",
       "0   LogisticRegression(max_iter=10000)      0.0001      l1   saga   0.599969\n",
       "1   LogisticRegression(max_iter=10000)      0.0001      l2   saga   0.737693\n",
       "2   LogisticRegression(max_iter=10000)      0.0010      l1   saga   0.742828\n",
       "3   LogisticRegression(max_iter=10000)      0.0010      l2   saga   0.745304\n",
       "4   LogisticRegression(max_iter=10000)      0.0100      l1   saga   0.745403\n",
       "5   LogisticRegression(max_iter=10000)      0.0100      l2   saga   0.745912\n",
       "6   LogisticRegression(max_iter=10000)      0.1000      l1   saga   0.745912\n",
       "7   LogisticRegression(max_iter=10000)      0.1000      l2   saga   0.746082\n",
       "8   LogisticRegression(max_iter=10000)      1.0000      l1   saga   0.746138\n",
       "9   LogisticRegression(max_iter=10000)      1.0000      l2   saga   0.746138\n",
       "10  LogisticRegression(max_iter=10000)     10.0000      l1   saga   0.746138\n",
       "11  LogisticRegression(max_iter=10000)     10.0000      l2   saga   0.746138\n",
       "12  LogisticRegression(max_iter=10000)    100.0000      l1   saga   0.746138\n",
       "13  LogisticRegression(max_iter=10000)    100.0000      l2   saga   0.746138\n",
       "14  LogisticRegression(max_iter=10000)   1000.0000      l1   saga   0.746138\n",
       "15  LogisticRegression(max_iter=10000)   1000.0000      l2   saga   0.746138\n",
       "16  LogisticRegression(max_iter=10000)  10000.0000      l1   saga   0.746138\n",
       "17  LogisticRegression(max_iter=10000)  10000.0000      l2   saga   0.746138\n",
       "18  LogisticRegression(max_iter=10000)      0.0001      l2  lbfgs   0.737693\n",
       "19  LogisticRegression(max_iter=10000)      0.0010      l2  lbfgs   0.745304\n",
       "20  LogisticRegression(max_iter=10000)      0.0100      l2  lbfgs   0.745912\n",
       "21  LogisticRegression(max_iter=10000)      0.1000      l2  lbfgs   0.746067\n",
       "22  LogisticRegression(max_iter=10000)      1.0000      l2  lbfgs   0.746138\n",
       "23  LogisticRegression(max_iter=10000)     10.0000      l2  lbfgs   0.746138\n",
       "24  LogisticRegression(max_iter=10000)    100.0000      l2  lbfgs   0.746138\n",
       "25  LogisticRegression(max_iter=10000)   1000.0000      l2  lbfgs   0.746138\n",
       "26  LogisticRegression(max_iter=10000)  10000.0000      l2  lbfgs   0.746138\n",
       "27  LogisticRegression(max_iter=10000)         NaN    none  lbfgs   0.746138\n",
       "28  LogisticRegression(max_iter=10000)         NaN    none   saga   0.746124\n",
       "29  LogisticRegression(max_iter=10000)         NaN     NaN    NaN   0.746138"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results = pd.DataFrame( best_model.cv_results_['params'] )\n",
    "\n",
    "results['score_acc'] = best_model.cv_results_['mean_test_accuracy']\n",
    "cols = results.columns.to_series().str.split('__').apply(lambda x: x[-1])\n",
    "results.columns = cols\n",
    "\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "557091b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "acc_list.append(results[0:1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "b765827e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'classifier': LogisticRegression(max_iter=10000),\n",
       " 'classifier__C': 0.0001,\n",
       " 'classifier__penalty': 'l2',\n",
       " 'classifier__solver': 'saga'}"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_model.cv_results_['params'][ np.argmin(best_model.cv_results_['rank_test_precision'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "3d3fe844",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>classifier</th>\n",
       "      <th>C</th>\n",
       "      <th>penalty</th>\n",
       "      <th>solver</th>\n",
       "      <th>score_precision</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.710429</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.698157</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.696882</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0100</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.694410</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0100</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.694563</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.1000</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.694289</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.1000</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.694514</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.694545</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.694545</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>10.0000</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.694530</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>10.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.694530</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>100.0000</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.694530</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>100.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.694530</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>1000.0000</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.694530</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>1000.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.694530</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>10000.0000</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.694530</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>10000.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.694530</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.710429</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.696882</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0100</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.694563</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.1000</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.694502</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.694545</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>10.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.694530</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>100.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.694530</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>1000.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.694530</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>10000.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.694530</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>NaN</td>\n",
       "      <td>none</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.694530</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>NaN</td>\n",
       "      <td>none</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.694505</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.694545</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                            classifier           C penalty solver  \\\n",
       "0   LogisticRegression(max_iter=10000)      0.0001      l1   saga   \n",
       "1   LogisticRegression(max_iter=10000)      0.0001      l2   saga   \n",
       "2   LogisticRegression(max_iter=10000)      0.0010      l1   saga   \n",
       "3   LogisticRegression(max_iter=10000)      0.0010      l2   saga   \n",
       "4   LogisticRegression(max_iter=10000)      0.0100      l1   saga   \n",
       "5   LogisticRegression(max_iter=10000)      0.0100      l2   saga   \n",
       "6   LogisticRegression(max_iter=10000)      0.1000      l1   saga   \n",
       "7   LogisticRegression(max_iter=10000)      0.1000      l2   saga   \n",
       "8   LogisticRegression(max_iter=10000)      1.0000      l1   saga   \n",
       "9   LogisticRegression(max_iter=10000)      1.0000      l2   saga   \n",
       "10  LogisticRegression(max_iter=10000)     10.0000      l1   saga   \n",
       "11  LogisticRegression(max_iter=10000)     10.0000      l2   saga   \n",
       "12  LogisticRegression(max_iter=10000)    100.0000      l1   saga   \n",
       "13  LogisticRegression(max_iter=10000)    100.0000      l2   saga   \n",
       "14  LogisticRegression(max_iter=10000)   1000.0000      l1   saga   \n",
       "15  LogisticRegression(max_iter=10000)   1000.0000      l2   saga   \n",
       "16  LogisticRegression(max_iter=10000)  10000.0000      l1   saga   \n",
       "17  LogisticRegression(max_iter=10000)  10000.0000      l2   saga   \n",
       "18  LogisticRegression(max_iter=10000)      0.0001      l2  lbfgs   \n",
       "19  LogisticRegression(max_iter=10000)      0.0010      l2  lbfgs   \n",
       "20  LogisticRegression(max_iter=10000)      0.0100      l2  lbfgs   \n",
       "21  LogisticRegression(max_iter=10000)      0.1000      l2  lbfgs   \n",
       "22  LogisticRegression(max_iter=10000)      1.0000      l2  lbfgs   \n",
       "23  LogisticRegression(max_iter=10000)     10.0000      l2  lbfgs   \n",
       "24  LogisticRegression(max_iter=10000)    100.0000      l2  lbfgs   \n",
       "25  LogisticRegression(max_iter=10000)   1000.0000      l2  lbfgs   \n",
       "26  LogisticRegression(max_iter=10000)  10000.0000      l2  lbfgs   \n",
       "27  LogisticRegression(max_iter=10000)         NaN    none  lbfgs   \n",
       "28  LogisticRegression(max_iter=10000)         NaN    none   saga   \n",
       "29  LogisticRegression(max_iter=10000)         NaN     NaN    NaN   \n",
       "\n",
       "    score_precision  \n",
       "0          0.000000  \n",
       "1          0.710429  \n",
       "2          0.698157  \n",
       "3          0.696882  \n",
       "4          0.694410  \n",
       "5          0.694563  \n",
       "6          0.694289  \n",
       "7          0.694514  \n",
       "8          0.694545  \n",
       "9          0.694545  \n",
       "10         0.694530  \n",
       "11         0.694530  \n",
       "12         0.694530  \n",
       "13         0.694530  \n",
       "14         0.694530  \n",
       "15         0.694530  \n",
       "16         0.694530  \n",
       "17         0.694530  \n",
       "18         0.710429  \n",
       "19         0.696882  \n",
       "20         0.694563  \n",
       "21         0.694502  \n",
       "22         0.694545  \n",
       "23         0.694530  \n",
       "24         0.694530  \n",
       "25         0.694530  \n",
       "26         0.694530  \n",
       "27         0.694530  \n",
       "28         0.694505  \n",
       "29         0.694545  "
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results = pd.DataFrame( best_model.cv_results_['params'] )\n",
    "\n",
    "results['score_precision'] = best_model.cv_results_['mean_test_precision']\n",
    "cols = results.columns.to_series().str.split('__').apply(lambda x: x[-1])\n",
    "results.columns = cols\n",
    "\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "b27ecb6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "roc_auc_ovr_List.append(results[1:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "531a42ca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'classifier': LogisticRegression(max_iter=10000),\n",
       " 'classifier__C': 1.0,\n",
       " 'classifier__penalty': 'l1',\n",
       " 'classifier__solver': 'saga'}"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_model.cv_results_['params'][ np.argmin(best_model.cv_results_['rank_test_f1_micro'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "37066f63",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>classifier</th>\n",
       "      <th>C</th>\n",
       "      <th>penalty</th>\n",
       "      <th>solver</th>\n",
       "      <th>score_f1_micro</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.599969</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.737693</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.742828</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.745304</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0100</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.745403</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0100</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.745912</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.1000</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.745912</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.1000</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.746082</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.746138</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.746138</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>10.0000</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.746138</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>10.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.746138</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>100.0000</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.746138</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>100.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.746138</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>1000.0000</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.746138</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>1000.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.746138</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>10000.0000</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.746138</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>10000.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.746138</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.737693</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.745304</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0100</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.745912</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.1000</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.746067</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.746138</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>10.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.746138</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>100.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.746138</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>1000.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.746138</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>10000.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.746138</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>NaN</td>\n",
       "      <td>none</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.746138</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>NaN</td>\n",
       "      <td>none</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.746124</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.746138</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                            classifier           C penalty solver  \\\n",
       "0   LogisticRegression(max_iter=10000)      0.0001      l1   saga   \n",
       "1   LogisticRegression(max_iter=10000)      0.0001      l2   saga   \n",
       "2   LogisticRegression(max_iter=10000)      0.0010      l1   saga   \n",
       "3   LogisticRegression(max_iter=10000)      0.0010      l2   saga   \n",
       "4   LogisticRegression(max_iter=10000)      0.0100      l1   saga   \n",
       "5   LogisticRegression(max_iter=10000)      0.0100      l2   saga   \n",
       "6   LogisticRegression(max_iter=10000)      0.1000      l1   saga   \n",
       "7   LogisticRegression(max_iter=10000)      0.1000      l2   saga   \n",
       "8   LogisticRegression(max_iter=10000)      1.0000      l1   saga   \n",
       "9   LogisticRegression(max_iter=10000)      1.0000      l2   saga   \n",
       "10  LogisticRegression(max_iter=10000)     10.0000      l1   saga   \n",
       "11  LogisticRegression(max_iter=10000)     10.0000      l2   saga   \n",
       "12  LogisticRegression(max_iter=10000)    100.0000      l1   saga   \n",
       "13  LogisticRegression(max_iter=10000)    100.0000      l2   saga   \n",
       "14  LogisticRegression(max_iter=10000)   1000.0000      l1   saga   \n",
       "15  LogisticRegression(max_iter=10000)   1000.0000      l2   saga   \n",
       "16  LogisticRegression(max_iter=10000)  10000.0000      l1   saga   \n",
       "17  LogisticRegression(max_iter=10000)  10000.0000      l2   saga   \n",
       "18  LogisticRegression(max_iter=10000)      0.0001      l2  lbfgs   \n",
       "19  LogisticRegression(max_iter=10000)      0.0010      l2  lbfgs   \n",
       "20  LogisticRegression(max_iter=10000)      0.0100      l2  lbfgs   \n",
       "21  LogisticRegression(max_iter=10000)      0.1000      l2  lbfgs   \n",
       "22  LogisticRegression(max_iter=10000)      1.0000      l2  lbfgs   \n",
       "23  LogisticRegression(max_iter=10000)     10.0000      l2  lbfgs   \n",
       "24  LogisticRegression(max_iter=10000)    100.0000      l2  lbfgs   \n",
       "25  LogisticRegression(max_iter=10000)   1000.0000      l2  lbfgs   \n",
       "26  LogisticRegression(max_iter=10000)  10000.0000      l2  lbfgs   \n",
       "27  LogisticRegression(max_iter=10000)         NaN    none  lbfgs   \n",
       "28  LogisticRegression(max_iter=10000)         NaN    none   saga   \n",
       "29  LogisticRegression(max_iter=10000)         NaN     NaN    NaN   \n",
       "\n",
       "    score_f1_micro  \n",
       "0         0.599969  \n",
       "1         0.737693  \n",
       "2         0.742828  \n",
       "3         0.745304  \n",
       "4         0.745403  \n",
       "5         0.745912  \n",
       "6         0.745912  \n",
       "7         0.746082  \n",
       "8         0.746138  \n",
       "9         0.746138  \n",
       "10        0.746138  \n",
       "11        0.746138  \n",
       "12        0.746138  \n",
       "13        0.746138  \n",
       "14        0.746138  \n",
       "15        0.746138  \n",
       "16        0.746138  \n",
       "17        0.746138  \n",
       "18        0.737693  \n",
       "19        0.745304  \n",
       "20        0.745912  \n",
       "21        0.746067  \n",
       "22        0.746138  \n",
       "23        0.746138  \n",
       "24        0.746138  \n",
       "25        0.746138  \n",
       "26        0.746138  \n",
       "27        0.746138  \n",
       "28        0.746124  \n",
       "29        0.746138  "
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results = pd.DataFrame( best_model.cv_results_['params'] )\n",
    "\n",
    "results['score_f1_micro'] = best_model.cv_results_['mean_test_f1_micro']\n",
    "cols = results.columns.to_series().str.split('__').apply(lambda x: x[-1])\n",
    "results.columns = cols\n",
    "\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "56f4f911",
   "metadata": {},
   "outputs": [],
   "source": [
    "f1_micro_List.append(results[8:9])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b7bf1d0",
   "metadata": {},
   "source": [
    "## Trial 9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "b5b093d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, Y_train, Y_test = train_test_split(X, y, test_size=0.2, shuffle=True, random_state = 9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "90fdec31",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Guest-All\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\Guest-All\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\Guest-All\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\Guest-All\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\Guest-All\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\Guest-All\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\Guest-All\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\Guest-All\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\Guest-All\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\Guest-All\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\Guest-All\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1183: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Guest-All\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1183: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Guest-All\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1183: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Guest-All\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1183: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Guest-All\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1183: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Guest-All\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1183: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Guest-All\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1183: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Guest-All\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1183: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Guest-All\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1183: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Guest-All\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1183: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Guest-All\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1183: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Guest-All\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1183: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Guest-All\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1183: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Guest-All\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1183: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Guest-All\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1183: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Guest-All\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1183: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Guest-All\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1183: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Guest-All\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1183: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Guest-All\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1183: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Guest-All\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1183: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Create a pipeline - RF is a stand in, we will populate the classifier part below\n",
    "pipe = Pipeline([('std', StandardScaler()),\n",
    "                 ('classifier', LogisticRegression())])\n",
    "\n",
    "# Create search space of candidate learning algorithms and their hyperparameters\n",
    "# note lbfgs can't do l1, and if you pass penalty='none' it expects no C value\n",
    "search_space = [{'classifier': [LogisticRegression(max_iter=10000)],\n",
    "                 'classifier__solver': ['saga'],\n",
    "                 'classifier__penalty': ['l1', 'l2'],\n",
    "                 'classifier__C': np.logspace(-4, 4, 9)},\n",
    "                {'classifier': [LogisticRegression(max_iter=10000)],\n",
    "                 'classifier__solver': ['lbfgs'],\n",
    "                 'classifier__penalty': ['l2'],\n",
    "                 'classifier__C': np.logspace(-4, 4, 9)},\n",
    "                {'classifier': [LogisticRegression(max_iter=10000)],\n",
    "                 'classifier__solver': ['lbfgs','saga'],\n",
    "                 'classifier__penalty': ['none']},\n",
    "                {'classifier': [LogisticRegression(max_iter=10000)]}\n",
    "                ]\n",
    "\n",
    "# Create grid search\n",
    "clf = GridSearchCV(pipe, search_space, cv=StratifiedKFold(n_splits=10),\n",
    "                   scoring=['accuracy', 'precision', 'f1_micro'], refit=False,\n",
    "                   verbose=0)\n",
    "\n",
    "# Fit grid search\n",
    "best_model = clf.fit(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "079fae53",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'mean_fit_time': array([0.77233086, 0.67152948, 0.73942969, 0.74273899, 0.78678741,\n",
       "        0.72425344, 0.80673678, 0.72626941, 0.82866087, 0.74388728,\n",
       "        0.8334384 , 0.74620621, 0.81735277, 0.714224  , 0.81575456,\n",
       "        0.71197946, 0.84078777, 0.7170037 , 0.09123001, 0.11032681,\n",
       "        0.11012316, 0.10789406, 0.10457311, 0.11086674, 0.10560794,\n",
       "        0.10639765, 0.10909858, 0.10499191, 0.77965147, 0.10989406]),\n",
       " 'std_fit_time': array([0.05297136, 0.04824414, 0.08927078, 0.06940731, 0.05755375,\n",
       "        0.09943079, 0.08814884, 0.04516295, 0.07418463, 0.1000181 ,\n",
       "        0.0672726 , 0.06316461, 0.08141024, 0.06466874, 0.05241215,\n",
       "        0.05030272, 0.05917558, 0.0445536 , 0.00582009, 0.01993988,\n",
       "        0.00615108, 0.00525358, 0.00578845, 0.01488547, 0.00524369,\n",
       "        0.00510661, 0.01382222, 0.00481068, 0.17654032, 0.02690411]),\n",
       " 'mean_score_time': array([0.01405358, 0.01212013, 0.01106737, 0.01448443, 0.01525598,\n",
       "        0.011589  , 0.01308002, 0.01449947, 0.01204782, 0.01122949,\n",
       "        0.01180952, 0.01134427, 0.01198058, 0.01169784, 0.01309597,\n",
       "        0.01144214, 0.01289186, 0.011922  , 0.01258056, 0.01221926,\n",
       "        0.01373727, 0.01318552, 0.01240616, 0.01228521, 0.01291761,\n",
       "        0.01251516, 0.01296279, 0.01452994, 0.01525338, 0.01394837]),\n",
       " 'std_score_time': array([0.00405096, 0.00406363, 0.00374986, 0.00322296, 0.00242531,\n",
       "        0.00375449, 0.00414607, 0.00476055, 0.00391465, 0.00380758,\n",
       "        0.00371648, 0.00399369, 0.00374378, 0.00369973, 0.0037579 ,\n",
       "        0.00360366, 0.00400446, 0.00465371, 0.00379674, 0.00422265,\n",
       "        0.00340238, 0.00383156, 0.00384947, 0.00431079, 0.00397483,\n",
       "        0.0037357 , 0.0039686 , 0.00325662, 0.00435618, 0.00329923]),\n",
       " 'param_classifier': masked_array(data=[LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000)],\n",
       "              mask=[False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'param_classifier__C': masked_array(data=[0.0001, 0.0001, 0.001, 0.001, 0.01, 0.01, 0.1, 0.1,\n",
       "                    1.0, 1.0, 10.0, 10.0, 100.0, 100.0, 1000.0, 1000.0,\n",
       "                    10000.0, 10000.0, 0.0001, 0.001, 0.01, 0.1, 1.0, 10.0,\n",
       "                    100.0, 1000.0, 10000.0, --, --, --],\n",
       "              mask=[False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False,  True,  True,  True],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'param_classifier__penalty': masked_array(data=['l1', 'l2', 'l1', 'l2', 'l1', 'l2', 'l1', 'l2', 'l1',\n",
       "                    'l2', 'l1', 'l2', 'l1', 'l2', 'l1', 'l2', 'l1', 'l2',\n",
       "                    'l2', 'l2', 'l2', 'l2', 'l2', 'l2', 'l2', 'l2', 'l2',\n",
       "                    'none', 'none', --],\n",
       "              mask=[False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False,  True],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'param_classifier__solver': masked_array(data=['saga', 'saga', 'saga', 'saga', 'saga', 'saga', 'saga',\n",
       "                    'saga', 'saga', 'saga', 'saga', 'saga', 'saga', 'saga',\n",
       "                    'saga', 'saga', 'saga', 'saga', 'lbfgs', 'lbfgs',\n",
       "                    'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs',\n",
       "                    'lbfgs', 'lbfgs', 'saga', --],\n",
       "              mask=[False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False,  True],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'params': [{'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 0.0001,\n",
       "   'classifier__penalty': 'l1',\n",
       "   'classifier__solver': 'saga'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 0.0001,\n",
       "   'classifier__penalty': 'l2',\n",
       "   'classifier__solver': 'saga'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 0.001,\n",
       "   'classifier__penalty': 'l1',\n",
       "   'classifier__solver': 'saga'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 0.001,\n",
       "   'classifier__penalty': 'l2',\n",
       "   'classifier__solver': 'saga'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 0.01,\n",
       "   'classifier__penalty': 'l1',\n",
       "   'classifier__solver': 'saga'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 0.01,\n",
       "   'classifier__penalty': 'l2',\n",
       "   'classifier__solver': 'saga'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 0.1,\n",
       "   'classifier__penalty': 'l1',\n",
       "   'classifier__solver': 'saga'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 0.1,\n",
       "   'classifier__penalty': 'l2',\n",
       "   'classifier__solver': 'saga'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 1.0,\n",
       "   'classifier__penalty': 'l1',\n",
       "   'classifier__solver': 'saga'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 1.0,\n",
       "   'classifier__penalty': 'l2',\n",
       "   'classifier__solver': 'saga'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 10.0,\n",
       "   'classifier__penalty': 'l1',\n",
       "   'classifier__solver': 'saga'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 10.0,\n",
       "   'classifier__penalty': 'l2',\n",
       "   'classifier__solver': 'saga'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 100.0,\n",
       "   'classifier__penalty': 'l1',\n",
       "   'classifier__solver': 'saga'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 100.0,\n",
       "   'classifier__penalty': 'l2',\n",
       "   'classifier__solver': 'saga'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 1000.0,\n",
       "   'classifier__penalty': 'l1',\n",
       "   'classifier__solver': 'saga'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 1000.0,\n",
       "   'classifier__penalty': 'l2',\n",
       "   'classifier__solver': 'saga'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 10000.0,\n",
       "   'classifier__penalty': 'l1',\n",
       "   'classifier__solver': 'saga'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 10000.0,\n",
       "   'classifier__penalty': 'l2',\n",
       "   'classifier__solver': 'saga'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 0.0001,\n",
       "   'classifier__penalty': 'l2',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 0.001,\n",
       "   'classifier__penalty': 'l2',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 0.01,\n",
       "   'classifier__penalty': 'l2',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 0.1,\n",
       "   'classifier__penalty': 'l2',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 1.0,\n",
       "   'classifier__penalty': 'l2',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 10.0,\n",
       "   'classifier__penalty': 'l2',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 100.0,\n",
       "   'classifier__penalty': 'l2',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 1000.0,\n",
       "   'classifier__penalty': 'l2',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 10000.0,\n",
       "   'classifier__penalty': 'l2',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__penalty': 'none',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__penalty': 'none',\n",
       "   'classifier__solver': 'saga'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000)}],\n",
       " 'split0_test_accuracy': array([0.60014144, 0.73691655, 0.74045262, 0.74413013, 0.74483734,\n",
       "        0.74526167, 0.74554455, 0.74554455, 0.74554455, 0.74554455,\n",
       "        0.74554455, 0.74554455, 0.74554455, 0.74554455, 0.74554455,\n",
       "        0.74554455, 0.74554455, 0.74554455, 0.73691655, 0.74413013,\n",
       "        0.74540311, 0.74554455, 0.74554455, 0.74554455, 0.74554455,\n",
       "        0.74554455, 0.74554455, 0.74554455, 0.74554455, 0.74554455]),\n",
       " 'split1_test_accuracy': array([0.60014144, 0.73550212, 0.74031117, 0.74299859, 0.7437058 ,\n",
       "        0.74441301, 0.74455446, 0.74455446, 0.74427157, 0.74441301,\n",
       "        0.74427157, 0.74427157, 0.74427157, 0.74427157, 0.74427157,\n",
       "        0.74427157, 0.74427157, 0.74427157, 0.73550212, 0.74299859,\n",
       "        0.74441301, 0.74455446, 0.74427157, 0.74427157, 0.74427157,\n",
       "        0.74427157, 0.74427157, 0.74427157, 0.74427157, 0.74427157]),\n",
       " 'split2_test_accuracy': array([0.60008488, 0.73122082, 0.73504032, 0.73532324, 0.73475739,\n",
       "        0.73659641, 0.73603056, 0.73603056, 0.73603056, 0.73603056,\n",
       "        0.73603056, 0.73603056, 0.73603056, 0.73603056, 0.73603056,\n",
       "        0.73603056, 0.73603056, 0.73603056, 0.73122082, 0.73532324,\n",
       "        0.73659641, 0.73603056, 0.73603056, 0.73603056, 0.73603056,\n",
       "        0.73603056, 0.73603056, 0.73603056, 0.73603056, 0.73603056]),\n",
       " 'split3_test_accuracy': array([0.60008488, 0.73334276, 0.74098175, 0.7404159 , 0.74197199,\n",
       "        0.74168906, 0.74211345, 0.74211345, 0.74211345, 0.74211345,\n",
       "        0.74211345, 0.74211345, 0.74211345, 0.74211345, 0.74211345,\n",
       "        0.74211345, 0.74211345, 0.74211345, 0.73334276, 0.7404159 ,\n",
       "        0.74183053, 0.74211345, 0.74211345, 0.74211345, 0.74211345,\n",
       "        0.74211345, 0.74211345, 0.74211345, 0.74211345, 0.74211345]),\n",
       " 'split4_test_accuracy': array([0.60008488, 0.74352808, 0.74862074, 0.75017683, 0.75144999,\n",
       "        0.75088414, 0.75060122, 0.75017683, 0.75003537, 0.75003537,\n",
       "        0.75003537, 0.75003537, 0.75003537, 0.75003537, 0.75003537,\n",
       "        0.75003537, 0.75003537, 0.75003537, 0.74352808, 0.75017683,\n",
       "        0.75088414, 0.75017683, 0.75003537, 0.75003537, 0.75003537,\n",
       "        0.75003537, 0.75003537, 0.75003537, 0.75003537, 0.75003537]),\n",
       " 'split5_test_accuracy': array([0.60008488, 0.74324515, 0.74565002, 0.75526949, 0.75286462,\n",
       "        0.75385486, 0.75399632, 0.75427925, 0.75456217, 0.75456217,\n",
       "        0.75456217, 0.75456217, 0.75456217, 0.75456217, 0.75456217,\n",
       "        0.75456217, 0.75456217, 0.75456217, 0.74324515, 0.75526949,\n",
       "        0.75399632, 0.75427925, 0.75456217, 0.75456217, 0.75456217,\n",
       "        0.75456217, 0.75456217, 0.75456217, 0.75456217, 0.75456217]),\n",
       " 'split6_test_accuracy': array([0.60008488, 0.73150375, 0.73914274, 0.73758665, 0.73914274,\n",
       "        0.73786957, 0.73829396, 0.73758665, 0.73786957, 0.73786957,\n",
       "        0.73786957, 0.73786957, 0.73786957, 0.73786957, 0.73786957,\n",
       "        0.73786957, 0.73786957, 0.73786957, 0.73150375, 0.73758665,\n",
       "        0.73786957, 0.73758665, 0.73786957, 0.73786957, 0.73786957,\n",
       "        0.73786957, 0.73786957, 0.73786957, 0.73786957, 0.73786957]),\n",
       " 'split7_test_accuracy': array([0.60008488, 0.74253784, 0.74805489, 0.75130853, 0.75159146,\n",
       "        0.75130853, 0.75144999, 0.75130853, 0.75130853, 0.75130853,\n",
       "        0.75130853, 0.75130853, 0.75130853, 0.75130853, 0.75130853,\n",
       "        0.75130853, 0.75130853, 0.75130853, 0.74253784, 0.75130853,\n",
       "        0.75130853, 0.75130853, 0.75130853, 0.75130853, 0.75130853,\n",
       "        0.75130853, 0.75130853, 0.75130853, 0.75130853, 0.75130853]),\n",
       " 'split8_test_accuracy': array([0.60008488, 0.73758665, 0.74550856, 0.74961098, 0.75074268,\n",
       "        0.7498939 , 0.75045975, 0.75003537, 0.75045975, 0.75045975,\n",
       "        0.75045975, 0.75045975, 0.75045975, 0.75045975, 0.75045975,\n",
       "        0.75045975, 0.75045975, 0.75045975, 0.73758665, 0.74961098,\n",
       "        0.7498939 , 0.75003537, 0.75045975, 0.75045975, 0.75045975,\n",
       "        0.75045975, 0.75045975, 0.75045975, 0.75045975, 0.75045975]),\n",
       " 'split9_test_accuracy': array([0.60022634, 0.73461593, 0.74480124, 0.74635733, 0.74720611,\n",
       "        0.74706465, 0.74706465, 0.74720611, 0.74706465, 0.74706465,\n",
       "        0.74706465, 0.74706465, 0.74706465, 0.74706465, 0.74706465,\n",
       "        0.74706465, 0.74706465, 0.74706465, 0.73461593, 0.74635733,\n",
       "        0.74706465, 0.74720611, 0.74706465, 0.74706465, 0.74706465,\n",
       "        0.74706465, 0.74706465, 0.74706465, 0.74706465, 0.74706465]),\n",
       " 'mean_test_accuracy': array([0.60011034, 0.73699997, 0.7428564 , 0.74531777, 0.74582701,\n",
       "        0.74588358, 0.74601089, 0.74588357, 0.74592602, 0.74594016,\n",
       "        0.74592602, 0.74592602, 0.74592602, 0.74592602, 0.74592602,\n",
       "        0.74592602, 0.74592602, 0.74592602, 0.73699997, 0.74531777,\n",
       "        0.74592602, 0.74588357, 0.74592602, 0.74592602, 0.74592602,\n",
       "        0.74592602, 0.74592602, 0.74592602, 0.74592602, 0.74592602]),\n",
       " 'std_test_accuracy': array([4.46420938e-05, 4.44462672e-03, 4.11734951e-03, 6.07064200e-03,\n",
       "        5.72709861e-03, 5.53239395e-03, 5.58580010e-03, 5.65112623e-03,\n",
       "        5.67923808e-03, 5.67527478e-03, 5.67923808e-03, 5.67923808e-03,\n",
       "        5.67923808e-03, 5.67923808e-03, 5.67923808e-03, 5.67923808e-03,\n",
       "        5.67923808e-03, 5.67923808e-03, 4.44462672e-03, 6.07064200e-03,\n",
       "        5.54083443e-03, 5.65112623e-03, 5.67923808e-03, 5.67923808e-03,\n",
       "        5.67923808e-03, 5.67923808e-03, 5.67923808e-03, 5.67923808e-03,\n",
       "        5.67923808e-03, 5.67923808e-03]),\n",
       " 'rank_test_accuracy': array([30, 28, 27, 25, 24, 21,  1, 22,  3,  2,  3,  3,  3,  3,  3,  3,  3,\n",
       "         3, 28, 25,  3, 22,  3,  3,  3,  3,  3,  3,  3,  3]),\n",
       " 'split0_test_precision': array([0.        , 0.70409455, 0.69091609, 0.69049401, 0.68867577,\n",
       "        0.68846436, 0.68855466, 0.68841642, 0.68841642, 0.68841642,\n",
       "        0.68841642, 0.68841642, 0.68841642, 0.68841642, 0.68841642,\n",
       "        0.68841642, 0.68841642, 0.68841642, 0.70409455, 0.69049401,\n",
       "        0.68871738, 0.68841642, 0.68841642, 0.68841642, 0.68841642,\n",
       "        0.68841642, 0.68841642, 0.68841642, 0.68841642, 0.68841642]),\n",
       " 'split1_test_precision': array([0.        , 0.70130416, 0.6912389 , 0.68899701, 0.68817204,\n",
       "        0.68791452, 0.68802947, 0.68802947, 0.68779948, 0.68791452,\n",
       "        0.68779948, 0.68779948, 0.68779948, 0.68779948, 0.68779948,\n",
       "        0.68779948, 0.68779948, 0.68779948, 0.70130416, 0.68899701,\n",
       "        0.68791452, 0.68802947, 0.68779948, 0.68779948, 0.68779948,\n",
       "        0.68779948, 0.68779948, 0.68779948, 0.68779948, 0.68779948]),\n",
       " 'split2_test_precision': array([0.        , 0.7023134 , 0.68691223, 0.68370484, 0.67989418,\n",
       "        0.68214421, 0.68084306, 0.68084306, 0.68084306, 0.68084306,\n",
       "        0.68084306, 0.68084306, 0.68084306, 0.68084306, 0.68084306,\n",
       "        0.68084306, 0.68084306, 0.68084306, 0.7023134 , 0.68370484,\n",
       "        0.68214421, 0.68084306, 0.68084306, 0.68084306, 0.68084306,\n",
       "        0.68084306, 0.68084306, 0.68084306, 0.68084306, 0.68084306]),\n",
       " 'split3_test_precision': array([0.        , 0.71026786, 0.69856459, 0.69466248, 0.69310743,\n",
       "        0.69287091, 0.69307692, 0.69292852, 0.69292852, 0.69292852,\n",
       "        0.69292852, 0.69292852, 0.69292852, 0.69292852, 0.69292852,\n",
       "        0.69292852, 0.69292852, 0.69292852, 0.71045576, 0.69466248,\n",
       "        0.69313801, 0.69292852, 0.69292852, 0.69292852, 0.69292852,\n",
       "        0.69292852, 0.69292852, 0.69292852, 0.69292852, 0.69292852]),\n",
       " 'split4_test_precision': array([0.        , 0.72120419, 0.70701893, 0.70163436, 0.70203927,\n",
       "        0.70037594, 0.7       , 0.69936114, 0.69924812, 0.69924812,\n",
       "        0.69924812, 0.69924812, 0.69924812, 0.69924812, 0.69924812,\n",
       "        0.69924812, 0.69924812, 0.69924812, 0.72120419, 0.70163436,\n",
       "        0.70037594, 0.69936114, 0.69924812, 0.69924812, 0.69924812,\n",
       "        0.69924812, 0.69924812, 0.69924812, 0.69924812, 0.69924812]),\n",
       " 'split5_test_precision': array([0.        , 0.72251539, 0.70408568, 0.71334111, 0.70705521,\n",
       "        0.70768055, 0.70731707, 0.70769817, 0.70792079, 0.70792079,\n",
       "        0.70792079, 0.70792079, 0.70792079, 0.70792079, 0.70792079,\n",
       "        0.70792079, 0.70792079, 0.70792079, 0.72251539, 0.71334111,\n",
       "        0.70779221, 0.70769817, 0.70792079, 0.70792079, 0.70792079,\n",
       "        0.70792079, 0.70792079, 0.70792079, 0.70792079, 0.70792079]),\n",
       " 'split6_test_precision': array([0.        , 0.70239651, 0.69573875, 0.68925234, 0.68853088,\n",
       "        0.68687644, 0.68709307, 0.68592196, 0.68630451, 0.68630451,\n",
       "        0.68630451, 0.68630451, 0.68630451, 0.68630451, 0.68630451,\n",
       "        0.68630451, 0.68630451, 0.68630451, 0.70239651, 0.68925234,\n",
       "        0.68687644, 0.68592196, 0.68630451, 0.68630451, 0.68630451,\n",
       "        0.68630451, 0.68630451, 0.68630451, 0.68630451, 0.68630451]),\n",
       " 'split7_test_precision': array([0.        , 0.71711945, 0.70366044, 0.70471084, 0.7027641 ,\n",
       "        0.7028463 , 0.70249811, 0.70223231, 0.70223231, 0.70223231,\n",
       "        0.70223231, 0.70223231, 0.70223231, 0.70223231, 0.70223231,\n",
       "        0.70223231, 0.70223231, 0.70223231, 0.71711945, 0.70471084,\n",
       "        0.7028463 , 0.70223231, 0.70223231, 0.70223231, 0.70223231,\n",
       "        0.70223231, 0.70223231, 0.70223231, 0.70223231, 0.70223231]),\n",
       " 'split8_test_precision': array([0.        , 0.71447485, 0.70429253, 0.70492439, 0.70332188,\n",
       "        0.70156072, 0.70201444, 0.70152091, 0.701861  , 0.701861  ,\n",
       "        0.701861  , 0.701861  , 0.701861  , 0.701861  , 0.701861  ,\n",
       "        0.701861  , 0.701861  , 0.701861  , 0.71447485, 0.70492439,\n",
       "        0.70156072, 0.70152091, 0.701861  , 0.701861  , 0.701861  ,\n",
       "        0.701861  , 0.701861  , 0.701861  , 0.701861  , 0.701861  ]),\n",
       " 'split9_test_precision': array([0.        , 0.7017842 , 0.69836957, 0.69556986, 0.69581606,\n",
       "        0.69467367, 0.69438202, 0.69464219, 0.69438202, 0.69438202,\n",
       "        0.69438202, 0.69438202, 0.69438202, 0.69438202, 0.69438202,\n",
       "        0.69438202, 0.69438202, 0.69438202, 0.7017842 , 0.69556986,\n",
       "        0.69467367, 0.69464219, 0.69438202, 0.69438202, 0.69438202,\n",
       "        0.69438202, 0.69438202, 0.69438202, 0.69438202, 0.69438202]),\n",
       " 'mean_test_precision': array([0.        , 0.70974746, 0.69807977, 0.69672912, 0.69493768,\n",
       "        0.69454076, 0.69438088, 0.69415941, 0.69419362, 0.69420513,\n",
       "        0.69419362, 0.69419362, 0.69419362, 0.69419362, 0.69419362,\n",
       "        0.69419362, 0.69419362, 0.69419362, 0.70976625, 0.69672912,\n",
       "        0.69460394, 0.69415941, 0.69419362, 0.69419362, 0.69419362,\n",
       "        0.69419362, 0.69419362, 0.69419362, 0.69419362, 0.69419362]),\n",
       " 'std_test_precision': array([0.        , 0.00804488, 0.00643701, 0.00872182, 0.00827814,\n",
       "        0.00788047, 0.00797152, 0.00804492, 0.00808557, 0.00807654,\n",
       "        0.00808557, 0.00808557, 0.00808557, 0.00808557, 0.00808557,\n",
       "        0.00808557, 0.00808557, 0.00808557, 0.0080463 , 0.00872182,\n",
       "        0.0078746 , 0.00804492, 0.00808557, 0.00808557, 0.00808557,\n",
       "        0.00808557, 0.00808557, 0.00808557, 0.00808557, 0.00808557]),\n",
       " 'rank_test_precision': array([30,  2,  3,  4,  6,  8,  9, 28, 11, 10, 11, 11, 11, 11, 11, 11, 11,\n",
       "        11,  1,  4,  7, 28, 11, 11, 11, 11, 11, 11, 11, 11]),\n",
       " 'split0_test_f1_micro': array([0.60014144, 0.73691655, 0.74045262, 0.74413013, 0.74483734,\n",
       "        0.74526167, 0.74554455, 0.74554455, 0.74554455, 0.74554455,\n",
       "        0.74554455, 0.74554455, 0.74554455, 0.74554455, 0.74554455,\n",
       "        0.74554455, 0.74554455, 0.74554455, 0.73691655, 0.74413013,\n",
       "        0.74540311, 0.74554455, 0.74554455, 0.74554455, 0.74554455,\n",
       "        0.74554455, 0.74554455, 0.74554455, 0.74554455, 0.74554455]),\n",
       " 'split1_test_f1_micro': array([0.60014144, 0.73550212, 0.74031117, 0.74299859, 0.7437058 ,\n",
       "        0.74441301, 0.74455446, 0.74455446, 0.74427157, 0.74441301,\n",
       "        0.74427157, 0.74427157, 0.74427157, 0.74427157, 0.74427157,\n",
       "        0.74427157, 0.74427157, 0.74427157, 0.73550212, 0.74299859,\n",
       "        0.74441301, 0.74455446, 0.74427157, 0.74427157, 0.74427157,\n",
       "        0.74427157, 0.74427157, 0.74427157, 0.74427157, 0.74427157]),\n",
       " 'split2_test_f1_micro': array([0.60008488, 0.73122082, 0.73504032, 0.73532324, 0.73475739,\n",
       "        0.73659641, 0.73603056, 0.73603056, 0.73603056, 0.73603056,\n",
       "        0.73603056, 0.73603056, 0.73603056, 0.73603056, 0.73603056,\n",
       "        0.73603056, 0.73603056, 0.73603056, 0.73122082, 0.73532324,\n",
       "        0.73659641, 0.73603056, 0.73603056, 0.73603056, 0.73603056,\n",
       "        0.73603056, 0.73603056, 0.73603056, 0.73603056, 0.73603056]),\n",
       " 'split3_test_f1_micro': array([0.60008488, 0.73334276, 0.74098175, 0.7404159 , 0.74197199,\n",
       "        0.74168906, 0.74211345, 0.74211345, 0.74211345, 0.74211345,\n",
       "        0.74211345, 0.74211345, 0.74211345, 0.74211345, 0.74211345,\n",
       "        0.74211345, 0.74211345, 0.74211345, 0.73334276, 0.7404159 ,\n",
       "        0.74183053, 0.74211345, 0.74211345, 0.74211345, 0.74211345,\n",
       "        0.74211345, 0.74211345, 0.74211345, 0.74211345, 0.74211345]),\n",
       " 'split4_test_f1_micro': array([0.60008488, 0.74352808, 0.74862074, 0.75017683, 0.75144999,\n",
       "        0.75088414, 0.75060122, 0.75017683, 0.75003537, 0.75003537,\n",
       "        0.75003537, 0.75003537, 0.75003537, 0.75003537, 0.75003537,\n",
       "        0.75003537, 0.75003537, 0.75003537, 0.74352808, 0.75017683,\n",
       "        0.75088414, 0.75017683, 0.75003537, 0.75003537, 0.75003537,\n",
       "        0.75003537, 0.75003537, 0.75003537, 0.75003537, 0.75003537]),\n",
       " 'split5_test_f1_micro': array([0.60008488, 0.74324515, 0.74565002, 0.75526949, 0.75286462,\n",
       "        0.75385486, 0.75399632, 0.75427925, 0.75456217, 0.75456217,\n",
       "        0.75456217, 0.75456217, 0.75456217, 0.75456217, 0.75456217,\n",
       "        0.75456217, 0.75456217, 0.75456217, 0.74324515, 0.75526949,\n",
       "        0.75399632, 0.75427925, 0.75456217, 0.75456217, 0.75456217,\n",
       "        0.75456217, 0.75456217, 0.75456217, 0.75456217, 0.75456217]),\n",
       " 'split6_test_f1_micro': array([0.60008488, 0.73150375, 0.73914274, 0.73758665, 0.73914274,\n",
       "        0.73786957, 0.73829396, 0.73758665, 0.73786957, 0.73786957,\n",
       "        0.73786957, 0.73786957, 0.73786957, 0.73786957, 0.73786957,\n",
       "        0.73786957, 0.73786957, 0.73786957, 0.73150375, 0.73758665,\n",
       "        0.73786957, 0.73758665, 0.73786957, 0.73786957, 0.73786957,\n",
       "        0.73786957, 0.73786957, 0.73786957, 0.73786957, 0.73786957]),\n",
       " 'split7_test_f1_micro': array([0.60008488, 0.74253784, 0.74805489, 0.75130853, 0.75159146,\n",
       "        0.75130853, 0.75144999, 0.75130853, 0.75130853, 0.75130853,\n",
       "        0.75130853, 0.75130853, 0.75130853, 0.75130853, 0.75130853,\n",
       "        0.75130853, 0.75130853, 0.75130853, 0.74253784, 0.75130853,\n",
       "        0.75130853, 0.75130853, 0.75130853, 0.75130853, 0.75130853,\n",
       "        0.75130853, 0.75130853, 0.75130853, 0.75130853, 0.75130853]),\n",
       " 'split8_test_f1_micro': array([0.60008488, 0.73758665, 0.74550856, 0.74961098, 0.75074268,\n",
       "        0.7498939 , 0.75045975, 0.75003537, 0.75045975, 0.75045975,\n",
       "        0.75045975, 0.75045975, 0.75045975, 0.75045975, 0.75045975,\n",
       "        0.75045975, 0.75045975, 0.75045975, 0.73758665, 0.74961098,\n",
       "        0.7498939 , 0.75003537, 0.75045975, 0.75045975, 0.75045975,\n",
       "        0.75045975, 0.75045975, 0.75045975, 0.75045975, 0.75045975]),\n",
       " 'split9_test_f1_micro': array([0.60022634, 0.73461593, 0.74480124, 0.74635733, 0.74720611,\n",
       "        0.74706465, 0.74706465, 0.74720611, 0.74706465, 0.74706465,\n",
       "        0.74706465, 0.74706465, 0.74706465, 0.74706465, 0.74706465,\n",
       "        0.74706465, 0.74706465, 0.74706465, 0.73461593, 0.74635733,\n",
       "        0.74706465, 0.74720611, 0.74706465, 0.74706465, 0.74706465,\n",
       "        0.74706465, 0.74706465, 0.74706465, 0.74706465, 0.74706465]),\n",
       " 'mean_test_f1_micro': array([0.60011034, 0.73699997, 0.7428564 , 0.74531777, 0.74582701,\n",
       "        0.74588358, 0.74601089, 0.74588357, 0.74592602, 0.74594016,\n",
       "        0.74592602, 0.74592602, 0.74592602, 0.74592602, 0.74592602,\n",
       "        0.74592602, 0.74592602, 0.74592602, 0.73699997, 0.74531777,\n",
       "        0.74592602, 0.74588357, 0.74592602, 0.74592602, 0.74592602,\n",
       "        0.74592602, 0.74592602, 0.74592602, 0.74592602, 0.74592602]),\n",
       " 'std_test_f1_micro': array([4.46420938e-05, 4.44462672e-03, 4.11734951e-03, 6.07064200e-03,\n",
       "        5.72709861e-03, 5.53239395e-03, 5.58580010e-03, 5.65112623e-03,\n",
       "        5.67923808e-03, 5.67527478e-03, 5.67923808e-03, 5.67923808e-03,\n",
       "        5.67923808e-03, 5.67923808e-03, 5.67923808e-03, 5.67923808e-03,\n",
       "        5.67923808e-03, 5.67923808e-03, 4.44462672e-03, 6.07064200e-03,\n",
       "        5.54083443e-03, 5.65112623e-03, 5.67923808e-03, 5.67923808e-03,\n",
       "        5.67923808e-03, 5.67923808e-03, 5.67923808e-03, 5.67923808e-03,\n",
       "        5.67923808e-03, 5.67923808e-03]),\n",
       " 'rank_test_f1_micro': array([30, 28, 27, 25, 24, 21,  1, 22,  4,  2,  4,  4,  4,  4,  4,  4,  4,\n",
       "         4, 28, 25,  3, 22,  4,  4,  4,  4,  4,  4,  4,  4])}"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_model.cv_results_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "32aa74f3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([30, 28, 27, 25, 24, 21,  1, 22,  3,  2,  3,  3,  3,  3,  3,  3,  3,\n",
       "        3, 28, 25,  3, 22,  3,  3,  3,  3,  3,  3,  3,  3])"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_model.cv_results_['rank_test_accuracy']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "b234d790",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'classifier': LogisticRegression(max_iter=10000),\n",
       " 'classifier__C': 0.0001,\n",
       " 'classifier__penalty': 'l1',\n",
       " 'classifier__solver': 'saga'}"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_model.cv_results_['params'][ np.argmin(best_model.cv_results_['mean_test_accuracy'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "ae51b2e8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>classifier</th>\n",
       "      <th>C</th>\n",
       "      <th>penalty</th>\n",
       "      <th>solver</th>\n",
       "      <th>score_acc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.600110</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.737000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.742856</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.745318</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0100</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.745827</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0100</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.745884</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.1000</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.746011</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.1000</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.745884</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.745926</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.745940</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>10.0000</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.745926</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>10.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.745926</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>100.0000</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.745926</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>100.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.745926</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>1000.0000</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.745926</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>1000.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.745926</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>10000.0000</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.745926</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>10000.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.745926</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.737000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.745318</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0100</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.745926</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.1000</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.745884</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.745926</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>10.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.745926</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>100.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.745926</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>1000.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.745926</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>10000.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.745926</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>NaN</td>\n",
       "      <td>none</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.745926</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>NaN</td>\n",
       "      <td>none</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.745926</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.745926</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                            classifier           C penalty solver  score_acc\n",
       "0   LogisticRegression(max_iter=10000)      0.0001      l1   saga   0.600110\n",
       "1   LogisticRegression(max_iter=10000)      0.0001      l2   saga   0.737000\n",
       "2   LogisticRegression(max_iter=10000)      0.0010      l1   saga   0.742856\n",
       "3   LogisticRegression(max_iter=10000)      0.0010      l2   saga   0.745318\n",
       "4   LogisticRegression(max_iter=10000)      0.0100      l1   saga   0.745827\n",
       "5   LogisticRegression(max_iter=10000)      0.0100      l2   saga   0.745884\n",
       "6   LogisticRegression(max_iter=10000)      0.1000      l1   saga   0.746011\n",
       "7   LogisticRegression(max_iter=10000)      0.1000      l2   saga   0.745884\n",
       "8   LogisticRegression(max_iter=10000)      1.0000      l1   saga   0.745926\n",
       "9   LogisticRegression(max_iter=10000)      1.0000      l2   saga   0.745940\n",
       "10  LogisticRegression(max_iter=10000)     10.0000      l1   saga   0.745926\n",
       "11  LogisticRegression(max_iter=10000)     10.0000      l2   saga   0.745926\n",
       "12  LogisticRegression(max_iter=10000)    100.0000      l1   saga   0.745926\n",
       "13  LogisticRegression(max_iter=10000)    100.0000      l2   saga   0.745926\n",
       "14  LogisticRegression(max_iter=10000)   1000.0000      l1   saga   0.745926\n",
       "15  LogisticRegression(max_iter=10000)   1000.0000      l2   saga   0.745926\n",
       "16  LogisticRegression(max_iter=10000)  10000.0000      l1   saga   0.745926\n",
       "17  LogisticRegression(max_iter=10000)  10000.0000      l2   saga   0.745926\n",
       "18  LogisticRegression(max_iter=10000)      0.0001      l2  lbfgs   0.737000\n",
       "19  LogisticRegression(max_iter=10000)      0.0010      l2  lbfgs   0.745318\n",
       "20  LogisticRegression(max_iter=10000)      0.0100      l2  lbfgs   0.745926\n",
       "21  LogisticRegression(max_iter=10000)      0.1000      l2  lbfgs   0.745884\n",
       "22  LogisticRegression(max_iter=10000)      1.0000      l2  lbfgs   0.745926\n",
       "23  LogisticRegression(max_iter=10000)     10.0000      l2  lbfgs   0.745926\n",
       "24  LogisticRegression(max_iter=10000)    100.0000      l2  lbfgs   0.745926\n",
       "25  LogisticRegression(max_iter=10000)   1000.0000      l2  lbfgs   0.745926\n",
       "26  LogisticRegression(max_iter=10000)  10000.0000      l2  lbfgs   0.745926\n",
       "27  LogisticRegression(max_iter=10000)         NaN    none  lbfgs   0.745926\n",
       "28  LogisticRegression(max_iter=10000)         NaN    none   saga   0.745926\n",
       "29  LogisticRegression(max_iter=10000)         NaN     NaN    NaN   0.745926"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results = pd.DataFrame( best_model.cv_results_['params'] )\n",
    "\n",
    "results['score_acc'] = best_model.cv_results_['mean_test_accuracy']\n",
    "cols = results.columns.to_series().str.split('__').apply(lambda x: x[-1])\n",
    "results.columns = cols\n",
    "\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "2ed188a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "acc_list.append(results[0:1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "fb96018d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'classifier': LogisticRegression(max_iter=10000),\n",
       " 'classifier__C': 0.0001,\n",
       " 'classifier__penalty': 'l2',\n",
       " 'classifier__solver': 'lbfgs'}"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_model.cv_results_['params'][ np.argmin(best_model.cv_results_['rank_test_precision'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "c299c812",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>classifier</th>\n",
       "      <th>C</th>\n",
       "      <th>penalty</th>\n",
       "      <th>solver</th>\n",
       "      <th>score_precision</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.709747</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.698080</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.696729</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0100</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.694938</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0100</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.694541</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.1000</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.694381</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.1000</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.694159</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.694194</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.694205</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>10.0000</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.694194</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>10.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.694194</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>100.0000</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.694194</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>100.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.694194</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>1000.0000</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.694194</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>1000.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.694194</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>10000.0000</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.694194</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>10000.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.694194</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.709766</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.696729</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0100</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.694604</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.1000</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.694159</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.694194</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>10.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.694194</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>100.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.694194</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>1000.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.694194</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>10000.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.694194</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>NaN</td>\n",
       "      <td>none</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.694194</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>NaN</td>\n",
       "      <td>none</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.694194</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.694194</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                            classifier           C penalty solver  \\\n",
       "0   LogisticRegression(max_iter=10000)      0.0001      l1   saga   \n",
       "1   LogisticRegression(max_iter=10000)      0.0001      l2   saga   \n",
       "2   LogisticRegression(max_iter=10000)      0.0010      l1   saga   \n",
       "3   LogisticRegression(max_iter=10000)      0.0010      l2   saga   \n",
       "4   LogisticRegression(max_iter=10000)      0.0100      l1   saga   \n",
       "5   LogisticRegression(max_iter=10000)      0.0100      l2   saga   \n",
       "6   LogisticRegression(max_iter=10000)      0.1000      l1   saga   \n",
       "7   LogisticRegression(max_iter=10000)      0.1000      l2   saga   \n",
       "8   LogisticRegression(max_iter=10000)      1.0000      l1   saga   \n",
       "9   LogisticRegression(max_iter=10000)      1.0000      l2   saga   \n",
       "10  LogisticRegression(max_iter=10000)     10.0000      l1   saga   \n",
       "11  LogisticRegression(max_iter=10000)     10.0000      l2   saga   \n",
       "12  LogisticRegression(max_iter=10000)    100.0000      l1   saga   \n",
       "13  LogisticRegression(max_iter=10000)    100.0000      l2   saga   \n",
       "14  LogisticRegression(max_iter=10000)   1000.0000      l1   saga   \n",
       "15  LogisticRegression(max_iter=10000)   1000.0000      l2   saga   \n",
       "16  LogisticRegression(max_iter=10000)  10000.0000      l1   saga   \n",
       "17  LogisticRegression(max_iter=10000)  10000.0000      l2   saga   \n",
       "18  LogisticRegression(max_iter=10000)      0.0001      l2  lbfgs   \n",
       "19  LogisticRegression(max_iter=10000)      0.0010      l2  lbfgs   \n",
       "20  LogisticRegression(max_iter=10000)      0.0100      l2  lbfgs   \n",
       "21  LogisticRegression(max_iter=10000)      0.1000      l2  lbfgs   \n",
       "22  LogisticRegression(max_iter=10000)      1.0000      l2  lbfgs   \n",
       "23  LogisticRegression(max_iter=10000)     10.0000      l2  lbfgs   \n",
       "24  LogisticRegression(max_iter=10000)    100.0000      l2  lbfgs   \n",
       "25  LogisticRegression(max_iter=10000)   1000.0000      l2  lbfgs   \n",
       "26  LogisticRegression(max_iter=10000)  10000.0000      l2  lbfgs   \n",
       "27  LogisticRegression(max_iter=10000)         NaN    none  lbfgs   \n",
       "28  LogisticRegression(max_iter=10000)         NaN    none   saga   \n",
       "29  LogisticRegression(max_iter=10000)         NaN     NaN    NaN   \n",
       "\n",
       "    score_precision  \n",
       "0          0.000000  \n",
       "1          0.709747  \n",
       "2          0.698080  \n",
       "3          0.696729  \n",
       "4          0.694938  \n",
       "5          0.694541  \n",
       "6          0.694381  \n",
       "7          0.694159  \n",
       "8          0.694194  \n",
       "9          0.694205  \n",
       "10         0.694194  \n",
       "11         0.694194  \n",
       "12         0.694194  \n",
       "13         0.694194  \n",
       "14         0.694194  \n",
       "15         0.694194  \n",
       "16         0.694194  \n",
       "17         0.694194  \n",
       "18         0.709766  \n",
       "19         0.696729  \n",
       "20         0.694604  \n",
       "21         0.694159  \n",
       "22         0.694194  \n",
       "23         0.694194  \n",
       "24         0.694194  \n",
       "25         0.694194  \n",
       "26         0.694194  \n",
       "27         0.694194  \n",
       "28         0.694194  \n",
       "29         0.694194  "
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results = pd.DataFrame( best_model.cv_results_['params'] )\n",
    "\n",
    "results['score_precision'] = best_model.cv_results_['mean_test_precision']\n",
    "cols = results.columns.to_series().str.split('__').apply(lambda x: x[-1])\n",
    "results.columns = cols\n",
    "\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "950c010d",
   "metadata": {},
   "outputs": [],
   "source": [
    "roc_auc_ovr_List.append(results[18:19])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "a902208d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'classifier': LogisticRegression(max_iter=10000),\n",
       " 'classifier__C': 0.1,\n",
       " 'classifier__penalty': 'l1',\n",
       " 'classifier__solver': 'saga'}"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_model.cv_results_['params'][ np.argmin(best_model.cv_results_['rank_test_f1_micro'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "a3a9405e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>classifier</th>\n",
       "      <th>C</th>\n",
       "      <th>penalty</th>\n",
       "      <th>solver</th>\n",
       "      <th>score_f1_micro</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.600110</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.737000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.742856</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.745318</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0100</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.745827</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0100</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.745884</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.1000</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.746011</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.1000</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.745884</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.745926</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.745940</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>10.0000</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.745926</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>10.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.745926</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>100.0000</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.745926</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>100.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.745926</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>1000.0000</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.745926</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>1000.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.745926</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>10000.0000</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.745926</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>10000.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.745926</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.737000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.745318</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0100</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.745926</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.1000</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.745884</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.745926</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>10.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.745926</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>100.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.745926</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>1000.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.745926</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>10000.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.745926</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>NaN</td>\n",
       "      <td>none</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.745926</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>NaN</td>\n",
       "      <td>none</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.745926</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.745926</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                            classifier           C penalty solver  \\\n",
       "0   LogisticRegression(max_iter=10000)      0.0001      l1   saga   \n",
       "1   LogisticRegression(max_iter=10000)      0.0001      l2   saga   \n",
       "2   LogisticRegression(max_iter=10000)      0.0010      l1   saga   \n",
       "3   LogisticRegression(max_iter=10000)      0.0010      l2   saga   \n",
       "4   LogisticRegression(max_iter=10000)      0.0100      l1   saga   \n",
       "5   LogisticRegression(max_iter=10000)      0.0100      l2   saga   \n",
       "6   LogisticRegression(max_iter=10000)      0.1000      l1   saga   \n",
       "7   LogisticRegression(max_iter=10000)      0.1000      l2   saga   \n",
       "8   LogisticRegression(max_iter=10000)      1.0000      l1   saga   \n",
       "9   LogisticRegression(max_iter=10000)      1.0000      l2   saga   \n",
       "10  LogisticRegression(max_iter=10000)     10.0000      l1   saga   \n",
       "11  LogisticRegression(max_iter=10000)     10.0000      l2   saga   \n",
       "12  LogisticRegression(max_iter=10000)    100.0000      l1   saga   \n",
       "13  LogisticRegression(max_iter=10000)    100.0000      l2   saga   \n",
       "14  LogisticRegression(max_iter=10000)   1000.0000      l1   saga   \n",
       "15  LogisticRegression(max_iter=10000)   1000.0000      l2   saga   \n",
       "16  LogisticRegression(max_iter=10000)  10000.0000      l1   saga   \n",
       "17  LogisticRegression(max_iter=10000)  10000.0000      l2   saga   \n",
       "18  LogisticRegression(max_iter=10000)      0.0001      l2  lbfgs   \n",
       "19  LogisticRegression(max_iter=10000)      0.0010      l2  lbfgs   \n",
       "20  LogisticRegression(max_iter=10000)      0.0100      l2  lbfgs   \n",
       "21  LogisticRegression(max_iter=10000)      0.1000      l2  lbfgs   \n",
       "22  LogisticRegression(max_iter=10000)      1.0000      l2  lbfgs   \n",
       "23  LogisticRegression(max_iter=10000)     10.0000      l2  lbfgs   \n",
       "24  LogisticRegression(max_iter=10000)    100.0000      l2  lbfgs   \n",
       "25  LogisticRegression(max_iter=10000)   1000.0000      l2  lbfgs   \n",
       "26  LogisticRegression(max_iter=10000)  10000.0000      l2  lbfgs   \n",
       "27  LogisticRegression(max_iter=10000)         NaN    none  lbfgs   \n",
       "28  LogisticRegression(max_iter=10000)         NaN    none   saga   \n",
       "29  LogisticRegression(max_iter=10000)         NaN     NaN    NaN   \n",
       "\n",
       "    score_f1_micro  \n",
       "0         0.600110  \n",
       "1         0.737000  \n",
       "2         0.742856  \n",
       "3         0.745318  \n",
       "4         0.745827  \n",
       "5         0.745884  \n",
       "6         0.746011  \n",
       "7         0.745884  \n",
       "8         0.745926  \n",
       "9         0.745940  \n",
       "10        0.745926  \n",
       "11        0.745926  \n",
       "12        0.745926  \n",
       "13        0.745926  \n",
       "14        0.745926  \n",
       "15        0.745926  \n",
       "16        0.745926  \n",
       "17        0.745926  \n",
       "18        0.737000  \n",
       "19        0.745318  \n",
       "20        0.745926  \n",
       "21        0.745884  \n",
       "22        0.745926  \n",
       "23        0.745926  \n",
       "24        0.745926  \n",
       "25        0.745926  \n",
       "26        0.745926  \n",
       "27        0.745926  \n",
       "28        0.745926  \n",
       "29        0.745926  "
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results = pd.DataFrame( best_model.cv_results_['params'] )\n",
    "\n",
    "results['score_f1_micro'] = best_model.cv_results_['mean_test_f1_micro']\n",
    "cols = results.columns.to_series().str.split('__').apply(lambda x: x[-1])\n",
    "results.columns = cols\n",
    "\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "40b8e54c",
   "metadata": {},
   "outputs": [],
   "source": [
    "f1_micro_List.append(results[6:7])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e446b35",
   "metadata": {},
   "source": [
    "## Trial 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "6b8df2fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, Y_train, Y_test = train_test_split(X, y, test_size=0.2, shuffle=True, random_state = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "8cf50a94",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Guest-All\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\Guest-All\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\Guest-All\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\Guest-All\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\Guest-All\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\Guest-All\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\Guest-All\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\Guest-All\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\Guest-All\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\Guest-All\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\Guest-All\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1183: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Guest-All\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1183: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Guest-All\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1183: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Guest-All\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1183: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Guest-All\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1183: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Guest-All\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1183: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Guest-All\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1183: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Guest-All\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1183: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Guest-All\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1183: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Guest-All\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1183: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Guest-All\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1183: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Guest-All\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1183: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Guest-All\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1183: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Guest-All\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1183: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Guest-All\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1183: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Guest-All\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1183: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Guest-All\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1183: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Guest-All\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1183: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Guest-All\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1183: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Guest-All\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1183: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Create a pipeline - RF is a stand in, we will populate the classifier part below\n",
    "pipe = Pipeline([('std', StandardScaler()),\n",
    "                 ('classifier', LogisticRegression())])\n",
    "\n",
    "# Create search space of candidate learning algorithms and their hyperparameters\n",
    "# note lbfgs can't do l1, and if you pass penalty='none' it expects no C value\n",
    "search_space = [{'classifier': [LogisticRegression(max_iter=10000)],\n",
    "                 'classifier__solver': ['saga'],\n",
    "                 'classifier__penalty': ['l1', 'l2'],\n",
    "                 'classifier__C': np.logspace(-4, 4, 9)},\n",
    "                {'classifier': [LogisticRegression(max_iter=10000)],\n",
    "                 'classifier__solver': ['lbfgs'],\n",
    "                 'classifier__penalty': ['l2'],\n",
    "                 'classifier__C': np.logspace(-4, 4, 9)},\n",
    "                {'classifier': [LogisticRegression(max_iter=10000)],\n",
    "                 'classifier__solver': ['lbfgs','saga'],\n",
    "                 'classifier__penalty': ['none']},\n",
    "                {'classifier': [LogisticRegression(max_iter=10000)]}\n",
    "                ]\n",
    "\n",
    "# Create grid search\n",
    "clf = GridSearchCV(pipe, search_space, cv=StratifiedKFold(n_splits=10),\n",
    "                   scoring=['accuracy', 'precision', 'f1_micro'], refit=False,\n",
    "                   verbose=0)\n",
    "\n",
    "# Fit grid search\n",
    "best_model = clf.fit(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "a4120924",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'mean_fit_time': array([0.78448954, 0.68094416, 0.7873116 , 0.69274485, 0.77690225,\n",
       "        0.6648411 , 0.82125957, 0.72304151, 0.82754579, 0.77510939,\n",
       "        0.95549963, 0.80591969, 0.960993  , 0.81434066, 0.91933064,\n",
       "        0.84659867, 0.96487954, 0.84253166, 0.09054177, 0.09921424,\n",
       "        0.10849853, 0.10646503, 0.10180056, 0.10059524, 0.11263793,\n",
       "        0.10580204, 0.10574818, 0.11453967, 0.71409128, 0.10780144]),\n",
       " 'std_fit_time': array([0.04209798, 0.04405663, 0.07528806, 0.03540744, 0.06747883,\n",
       "        0.05824731, 0.04803106, 0.05211329, 0.06806008, 0.11173712,\n",
       "        0.16091462, 0.12192324, 0.1463901 , 0.15664696, 0.17155203,\n",
       "        0.14320799, 0.16930209, 0.17885433, 0.00481448, 0.00445428,\n",
       "        0.00995482, 0.00433255, 0.00592979, 0.0044002 , 0.01913359,\n",
       "        0.00499686, 0.00605197, 0.01189295, 0.03741974, 0.00374943]),\n",
       " 'mean_score_time': array([0.0130353 , 0.01155958, 0.01142981, 0.01101227, 0.01136267,\n",
       "        0.01259608, 0.01429851, 0.01235039, 0.01217241, 0.01510224,\n",
       "        0.01196806, 0.0175142 , 0.01423588, 0.01133294, 0.01400821,\n",
       "        0.01290276, 0.0166285 , 0.01269977, 0.01442866, 0.01077027,\n",
       "        0.01177707, 0.01180656, 0.01183589, 0.01184194, 0.01181824,\n",
       "        0.00899436, 0.01113832, 0.01332912, 0.0106246 , 0.01267326]),\n",
       " 'std_score_time': array([0.00324479, 0.00274951, 0.00362945, 0.00243539, 0.00411918,\n",
       "        0.0028849 , 0.00254195, 0.00287327, 0.00317465, 0.00910993,\n",
       "        0.00473923, 0.01486371, 0.00379155, 0.00292934, 0.00342403,\n",
       "        0.00546073, 0.01126139, 0.00446737, 0.00400047, 0.00296098,\n",
       "        0.00349055, 0.00362608, 0.00293468, 0.00378909, 0.00465541,\n",
       "        0.00271562, 0.00352317, 0.00514502, 0.00281538, 0.00425862]),\n",
       " 'param_classifier': masked_array(data=[LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000),\n",
       "                    LogisticRegression(max_iter=10000)],\n",
       "              mask=[False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'param_classifier__C': masked_array(data=[0.0001, 0.0001, 0.001, 0.001, 0.01, 0.01, 0.1, 0.1,\n",
       "                    1.0, 1.0, 10.0, 10.0, 100.0, 100.0, 1000.0, 1000.0,\n",
       "                    10000.0, 10000.0, 0.0001, 0.001, 0.01, 0.1, 1.0, 10.0,\n",
       "                    100.0, 1000.0, 10000.0, --, --, --],\n",
       "              mask=[False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False,  True,  True,  True],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'param_classifier__penalty': masked_array(data=['l1', 'l2', 'l1', 'l2', 'l1', 'l2', 'l1', 'l2', 'l1',\n",
       "                    'l2', 'l1', 'l2', 'l1', 'l2', 'l1', 'l2', 'l1', 'l2',\n",
       "                    'l2', 'l2', 'l2', 'l2', 'l2', 'l2', 'l2', 'l2', 'l2',\n",
       "                    'none', 'none', --],\n",
       "              mask=[False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False,  True],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'param_classifier__solver': masked_array(data=['saga', 'saga', 'saga', 'saga', 'saga', 'saga', 'saga',\n",
       "                    'saga', 'saga', 'saga', 'saga', 'saga', 'saga', 'saga',\n",
       "                    'saga', 'saga', 'saga', 'saga', 'lbfgs', 'lbfgs',\n",
       "                    'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs',\n",
       "                    'lbfgs', 'lbfgs', 'saga', --],\n",
       "              mask=[False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False,  True],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'params': [{'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 0.0001,\n",
       "   'classifier__penalty': 'l1',\n",
       "   'classifier__solver': 'saga'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 0.0001,\n",
       "   'classifier__penalty': 'l2',\n",
       "   'classifier__solver': 'saga'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 0.001,\n",
       "   'classifier__penalty': 'l1',\n",
       "   'classifier__solver': 'saga'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 0.001,\n",
       "   'classifier__penalty': 'l2',\n",
       "   'classifier__solver': 'saga'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 0.01,\n",
       "   'classifier__penalty': 'l1',\n",
       "   'classifier__solver': 'saga'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 0.01,\n",
       "   'classifier__penalty': 'l2',\n",
       "   'classifier__solver': 'saga'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 0.1,\n",
       "   'classifier__penalty': 'l1',\n",
       "   'classifier__solver': 'saga'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 0.1,\n",
       "   'classifier__penalty': 'l2',\n",
       "   'classifier__solver': 'saga'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 1.0,\n",
       "   'classifier__penalty': 'l1',\n",
       "   'classifier__solver': 'saga'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 1.0,\n",
       "   'classifier__penalty': 'l2',\n",
       "   'classifier__solver': 'saga'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 10.0,\n",
       "   'classifier__penalty': 'l1',\n",
       "   'classifier__solver': 'saga'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 10.0,\n",
       "   'classifier__penalty': 'l2',\n",
       "   'classifier__solver': 'saga'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 100.0,\n",
       "   'classifier__penalty': 'l1',\n",
       "   'classifier__solver': 'saga'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 100.0,\n",
       "   'classifier__penalty': 'l2',\n",
       "   'classifier__solver': 'saga'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 1000.0,\n",
       "   'classifier__penalty': 'l1',\n",
       "   'classifier__solver': 'saga'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 1000.0,\n",
       "   'classifier__penalty': 'l2',\n",
       "   'classifier__solver': 'saga'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 10000.0,\n",
       "   'classifier__penalty': 'l1',\n",
       "   'classifier__solver': 'saga'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 10000.0,\n",
       "   'classifier__penalty': 'l2',\n",
       "   'classifier__solver': 'saga'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 0.0001,\n",
       "   'classifier__penalty': 'l2',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 0.001,\n",
       "   'classifier__penalty': 'l2',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 0.01,\n",
       "   'classifier__penalty': 'l2',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 0.1,\n",
       "   'classifier__penalty': 'l2',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 1.0,\n",
       "   'classifier__penalty': 'l2',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 10.0,\n",
       "   'classifier__penalty': 'l2',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 100.0,\n",
       "   'classifier__penalty': 'l2',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 1000.0,\n",
       "   'classifier__penalty': 'l2',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__C': 10000.0,\n",
       "   'classifier__penalty': 'l2',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__penalty': 'none',\n",
       "   'classifier__solver': 'lbfgs'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000),\n",
       "   'classifier__penalty': 'none',\n",
       "   'classifier__solver': 'saga'},\n",
       "  {'classifier': LogisticRegression(max_iter=10000)}],\n",
       " 'split0_test_accuracy': array([0.60042433, 0.73394625, 0.7407355 , 0.74342291, 0.74243281,\n",
       "        0.74200849, 0.74101839, 0.74115983, 0.74115983, 0.74115983,\n",
       "        0.74115983, 0.74115983, 0.74115983, 0.74115983, 0.74115983,\n",
       "        0.74115983, 0.74115983, 0.74115983, 0.73394625, 0.74342291,\n",
       "        0.74200849, 0.74115983, 0.74115983, 0.74115983, 0.74115983,\n",
       "        0.74115983, 0.74115983, 0.74115983, 0.74115983, 0.74115983]),\n",
       " 'split1_test_accuracy': array([0.60042433, 0.74214993, 0.74441301, 0.75091938, 0.74964639,\n",
       "        0.75049505, 0.74936351, 0.74936351, 0.74936351, 0.74936351,\n",
       "        0.74936351, 0.74936351, 0.74936351, 0.74936351, 0.74936351,\n",
       "        0.74936351, 0.74936351, 0.74936351, 0.74214993, 0.75091938,\n",
       "        0.75049505, 0.74936351, 0.74936351, 0.74936351, 0.74936351,\n",
       "        0.74936351, 0.74936351, 0.74936351, 0.74936351, 0.74936351]),\n",
       " 'split2_test_accuracy': array([0.6003678 , 0.74366954, 0.74777196, 0.75159146, 0.75003537,\n",
       "        0.75017683, 0.75003537, 0.74975244, 0.7498939 , 0.7498939 ,\n",
       "        0.7498939 , 0.7498939 , 0.7498939 , 0.7498939 , 0.7498939 ,\n",
       "        0.7498939 , 0.7498939 , 0.7498939 , 0.74366954, 0.75159146,\n",
       "        0.75017683, 0.74975244, 0.7498939 , 0.7498939 , 0.7498939 ,\n",
       "        0.7498939 , 0.7498939 , 0.7498939 , 0.7498939 , 0.7498939 ]),\n",
       " 'split3_test_accuracy': array([0.6003678 , 0.73956712, 0.7453671 , 0.74593295, 0.74692319,\n",
       "        0.74621587, 0.7464988 , 0.74635733, 0.74621587, 0.74621587,\n",
       "        0.74621587, 0.74621587, 0.74621587, 0.74621587, 0.74621587,\n",
       "        0.74621587, 0.74621587, 0.74621587, 0.73956712, 0.74593295,\n",
       "        0.74621587, 0.74635733, 0.74621587, 0.74621587, 0.74621587,\n",
       "        0.74621587, 0.74621587, 0.74621587, 0.74621587, 0.74621587]),\n",
       " 'split4_test_accuracy': array([0.6003678 , 0.74084029, 0.74494271, 0.75173292, 0.75173292,\n",
       "        0.75286462, 0.75300608, 0.75258169, 0.75258169, 0.75258169,\n",
       "        0.75258169, 0.75258169, 0.75258169, 0.75258169, 0.75258169,\n",
       "        0.75258169, 0.75258169, 0.75258169, 0.74084029, 0.75173292,\n",
       "        0.75286462, 0.75258169, 0.75258169, 0.75258169, 0.75258169,\n",
       "        0.75258169, 0.75258169, 0.75258169, 0.75258169, 0.75258169]),\n",
       " 'split5_test_accuracy': array([0.6003678 , 0.73588909, 0.74253784, 0.74480124, 0.74565002,\n",
       "        0.74692319, 0.74734757, 0.74777196, 0.74791342, 0.74791342,\n",
       "        0.74791342, 0.74791342, 0.74791342, 0.74791342, 0.74791342,\n",
       "        0.74791342, 0.74791342, 0.74791342, 0.73588909, 0.74480124,\n",
       "        0.74692319, 0.74777196, 0.74791342, 0.74791342, 0.74791342,\n",
       "        0.74791342, 0.74791342, 0.74791342, 0.74791342, 0.74791342]),\n",
       " 'split6_test_accuracy': array([0.6003678 , 0.73631348, 0.73970859, 0.74112321, 0.74253784,\n",
       "        0.74055736, 0.7404159 , 0.74027444, 0.7404159 , 0.74027444,\n",
       "        0.7404159 , 0.7404159 , 0.7404159 , 0.7404159 , 0.7404159 ,\n",
       "        0.7404159 , 0.7404159 , 0.74027444, 0.73631348, 0.74112321,\n",
       "        0.74055736, 0.74027444, 0.7404159 , 0.7404159 , 0.7404159 ,\n",
       "        0.7404159 , 0.7404159 , 0.7404159 , 0.7404159 , 0.7404159 ]),\n",
       " 'split7_test_accuracy': array([0.60050927, 0.73843542, 0.74451832, 0.74451832, 0.74607441,\n",
       "        0.74565002, 0.7464988 , 0.74664026, 0.74664026, 0.74664026,\n",
       "        0.7464988 , 0.7464988 , 0.7464988 , 0.7464988 , 0.7464988 ,\n",
       "        0.7464988 , 0.7464988 , 0.7464988 , 0.73843542, 0.74451832,\n",
       "        0.74565002, 0.74664026, 0.74664026, 0.7464988 , 0.7464988 ,\n",
       "        0.7464988 , 0.7464988 , 0.7464988 , 0.7464988 , 0.74664026]),\n",
       " 'split8_test_accuracy': array([0.60050927, 0.73065497, 0.73405008, 0.73744518, 0.73730372,\n",
       "        0.73744518, 0.7381525 , 0.73772811, 0.73786957, 0.73772811,\n",
       "        0.73772811, 0.73786957, 0.73786957, 0.73786957, 0.73786957,\n",
       "        0.73772811, 0.73786957, 0.73786957, 0.73065497, 0.73744518,\n",
       "        0.73744518, 0.73772811, 0.73772811, 0.73772811, 0.73772811,\n",
       "        0.73772811, 0.73772811, 0.73772811, 0.73772811, 0.73772811]),\n",
       " 'split9_test_accuracy': array([0.60050927, 0.7370208 , 0.74494271, 0.74664026, 0.74805489,\n",
       "        0.74805489, 0.74819635, 0.74805489, 0.74805489, 0.74805489,\n",
       "        0.74805489, 0.74805489, 0.74805489, 0.74805489, 0.74805489,\n",
       "        0.74805489, 0.74805489, 0.74805489, 0.7370208 , 0.74664026,\n",
       "        0.74805489, 0.74805489, 0.74805489, 0.74805489, 0.74805489,\n",
       "        0.74805489, 0.74805489, 0.74805489, 0.74805489, 0.74805489]),\n",
       " 'mean_test_accuracy': array([0.60042155, 0.73784869, 0.74289878, 0.74581278, 0.74603916,\n",
       "        0.74603915, 0.74605333, 0.74596845, 0.74601089, 0.74598259,\n",
       "        0.74598259, 0.74599674, 0.74599674, 0.74599674, 0.74599674,\n",
       "        0.74598259, 0.74599674, 0.74598259, 0.73784869, 0.74581278,\n",
       "        0.74603915, 0.74596845, 0.74599674, 0.74598259, 0.74598259,\n",
       "        0.74598259, 0.74598259, 0.74598259, 0.74598259, 0.74599674]),\n",
       " 'std_test_accuracy': array([6.12709395e-05, 3.72143540e-03, 3.69321657e-03, 4.42308299e-03,\n",
       "        4.09842493e-03, 4.56883284e-03, 4.48746662e-03, 4.48593657e-03,\n",
       "        4.45922367e-03, 4.50294487e-03, 4.48334802e-03, 4.45742865e-03,\n",
       "        4.45742865e-03, 4.45742865e-03, 4.45742865e-03, 4.48334802e-03,\n",
       "        4.45742865e-03, 4.47530640e-03, 3.72143540e-03, 4.42308299e-03,\n",
       "        4.56883284e-03, 4.48593657e-03, 4.48517729e-03, 4.48334802e-03,\n",
       "        4.48334802e-03, 4.48334802e-03, 4.48334802e-03, 4.48334802e-03,\n",
       "        4.48334802e-03, 4.48517729e-03]),\n",
       " 'rank_test_accuracy': array([30, 28, 27, 25,  2,  3,  1, 23,  5, 14, 14,  6,  6,  6,  6, 14,  6,\n",
       "        13, 28, 25,  3, 23,  6, 14, 14, 14, 14, 14, 14,  6]),\n",
       " 'split0_test_precision': array([0.        , 0.70521739, 0.69329696, 0.69286532, 0.68957704,\n",
       "        0.68908198, 0.68754717, 0.68780672, 0.68766503, 0.68766503,\n",
       "        0.68766503, 0.68766503, 0.68766503, 0.68766503, 0.68766503,\n",
       "        0.68766503, 0.68766503, 0.68766503, 0.70521739, 0.69286532,\n",
       "        0.68908198, 0.68780672, 0.68766503, 0.68766503, 0.68766503,\n",
       "        0.68766503, 0.68766503, 0.68766503, 0.68766503, 0.68766503]),\n",
       " 'split1_test_precision': array([0.        , 0.72386059, 0.70590615, 0.70636152, 0.70156668,\n",
       "        0.70240366, 0.69996202, 0.69981025, 0.69981025, 0.69981025,\n",
       "        0.69981025, 0.69981025, 0.69981025, 0.69981025, 0.69981025,\n",
       "        0.69981025, 0.69981025, 0.69981025, 0.72386059, 0.70636152,\n",
       "        0.70224933, 0.69981025, 0.69981025, 0.69981025, 0.69981025,\n",
       "        0.69981025, 0.69981025, 0.69981025, 0.69981025, 0.69981025]),\n",
       " 'split2_test_precision': array([0.        , 0.71710244, 0.69961686, 0.70192671, 0.69665428,\n",
       "        0.69632925, 0.69607116, 0.69555556, 0.69581326, 0.69581326,\n",
       "        0.69581326, 0.69581326, 0.69581326, 0.69581326, 0.69581326,\n",
       "        0.69581326, 0.69581326, 0.69581326, 0.71710244, 0.70192671,\n",
       "        0.69632925, 0.69555556, 0.69581326, 0.69581326, 0.69581326,\n",
       "        0.69581326, 0.69581326, 0.69581326, 0.69581326, 0.69581326]),\n",
       " 'split3_test_precision': array([0.        , 0.71043627, 0.70043019, 0.69705094, 0.69695817,\n",
       "        0.69519122, 0.69571808, 0.69545455, 0.69519122, 0.69519122,\n",
       "        0.69519122, 0.69519122, 0.69519122, 0.69519122, 0.69519122,\n",
       "        0.69519122, 0.69519122, 0.69519122, 0.71043627, 0.69705094,\n",
       "        0.69519122, 0.69545455, 0.69519122, 0.69519122, 0.69519122,\n",
       "        0.69519122, 0.69519122, 0.69519122, 0.69519122, 0.69519122]),\n",
       " 'split4_test_precision': array([0.        , 0.71596346, 0.70293884, 0.70640432, 0.70234493,\n",
       "        0.70263158, 0.70259106, 0.70180045, 0.70180045, 0.70180045,\n",
       "        0.70180045, 0.70180045, 0.70180045, 0.70180045, 0.70180045,\n",
       "        0.70180045, 0.70180045, 0.70180045, 0.71596346, 0.70640432,\n",
       "        0.70263158, 0.70180045, 0.70180045, 0.70180045, 0.70180045,\n",
       "        0.70180045, 0.70180045, 0.70180045, 0.70180045, 0.70180045]),\n",
       " 'split5_test_precision': array([0.        , 0.71157244, 0.69980119, 0.69718038, 0.6960672 ,\n",
       "        0.69710807, 0.69760365, 0.69809886, 0.69821361, 0.69821361,\n",
       "        0.69821361, 0.69821361, 0.69821361, 0.69821361, 0.69821361,\n",
       "        0.69821361, 0.69821361, 0.69821361, 0.71157244, 0.69718038,\n",
       "        0.69710807, 0.69809886, 0.69821361, 0.69821361, 0.69821361,\n",
       "        0.69821361, 0.69821361, 0.69821361, 0.69821361, 0.69821361]),\n",
       " 'split6_test_precision': array([0.        , 0.71083809, 0.6927593 , 0.69230769, 0.69157453,\n",
       "        0.68890583, 0.68878719, 0.68852459, 0.68864329, 0.68852459,\n",
       "        0.68864329, 0.68864329, 0.68864329, 0.68864329, 0.68864329,\n",
       "        0.68864329, 0.68864329, 0.68852459, 0.71083809, 0.69230769,\n",
       "        0.68890583, 0.68852459, 0.68864329, 0.68864329, 0.68864329,\n",
       "        0.68864329, 0.68864329, 0.68864329, 0.68864329, 0.68864329]),\n",
       " 'split7_test_precision': array([0.        , 0.71149675, 0.70311253, 0.69882813, 0.69872538,\n",
       "        0.69715603, 0.69815668, 0.69827255, 0.69827255, 0.69827255,\n",
       "        0.6980046 , 0.6980046 , 0.6980046 , 0.6980046 , 0.6980046 ,\n",
       "        0.6980046 , 0.6980046 , 0.6980046 , 0.71149675, 0.69882813,\n",
       "        0.69715603, 0.69827255, 0.69827255, 0.6980046 , 0.6980046 ,\n",
       "        0.6980046 , 0.6980046 , 0.6980046 , 0.6980046 , 0.69827255]),\n",
       " 'split8_test_precision': array([0.        , 0.69861831, 0.68582677, 0.68529862, 0.68101835,\n",
       "        0.6814093 , 0.68214152, 0.68151198, 0.68163113, 0.68151198,\n",
       "        0.68151198, 0.68163113, 0.68163113, 0.68163113, 0.68163113,\n",
       "        0.68151198, 0.68163113, 0.68163113, 0.69861831, 0.68529862,\n",
       "        0.6814093 , 0.68151198, 0.68151198, 0.68151198, 0.68151198,\n",
       "        0.68151198, 0.68151198, 0.68151198, 0.68151198, 0.68151198]),\n",
       " 'split9_test_precision': array([0.        , 0.70699271, 0.70169893, 0.69751434, 0.69701549,\n",
       "        0.69583177, 0.69565217, 0.69539153, 0.69539153, 0.69539153,\n",
       "        0.69539153, 0.69539153, 0.69539153, 0.69539153, 0.69539153,\n",
       "        0.69539153, 0.69539153, 0.69539153, 0.70699271, 0.69751434,\n",
       "        0.69583177, 0.69539153, 0.69539153, 0.69539153, 0.69539153,\n",
       "        0.69539153, 0.69539153, 0.69539153, 0.69539153, 0.69539153]),\n",
       " 'mean_test_precision': array([0.        , 0.71120984, 0.69853877, 0.6975738 , 0.6951502 ,\n",
       "        0.69460487, 0.69442307, 0.6942227 , 0.69424323, 0.69421945,\n",
       "        0.69420452, 0.69421644, 0.69421644, 0.69421644, 0.69421644,\n",
       "        0.69420452, 0.69421644, 0.69420457, 0.71120984, 0.6975738 ,\n",
       "        0.69458944, 0.6942227 , 0.69423132, 0.69420452, 0.69420452,\n",
       "        0.69420452, 0.69420452, 0.69420452, 0.69420452, 0.69423132]),\n",
       " 'std_test_precision': array([0.        , 0.00656165, 0.00577366, 0.00613789, 0.00599269,\n",
       "        0.00616401, 0.0059769 , 0.00599937, 0.00598734, 0.00602362,\n",
       "        0.00599499, 0.00596982, 0.00596982, 0.00596982, 0.00596982,\n",
       "        0.00599499, 0.00596982, 0.005981  , 0.00656165, 0.00613789,\n",
       "        0.00614463, 0.00599937, 0.00601249, 0.00599499, 0.00599499,\n",
       "        0.00599499, 0.00599499, 0.00599499, 0.00599499, 0.00601249]),\n",
       " 'rank_test_precision': array([30,  1,  3,  4,  6,  7,  9, 13, 10, 15, 22, 16, 16, 16, 16, 22, 16,\n",
       "        21,  1,  4,  8, 13, 11, 22, 22, 22, 22, 22, 22, 11]),\n",
       " 'split0_test_f1_micro': array([0.60042433, 0.73394625, 0.7407355 , 0.74342291, 0.74243281,\n",
       "        0.74200849, 0.74101839, 0.74115983, 0.74115983, 0.74115983,\n",
       "        0.74115983, 0.74115983, 0.74115983, 0.74115983, 0.74115983,\n",
       "        0.74115983, 0.74115983, 0.74115983, 0.73394625, 0.74342291,\n",
       "        0.74200849, 0.74115983, 0.74115983, 0.74115983, 0.74115983,\n",
       "        0.74115983, 0.74115983, 0.74115983, 0.74115983, 0.74115983]),\n",
       " 'split1_test_f1_micro': array([0.60042433, 0.74214993, 0.74441301, 0.75091938, 0.74964639,\n",
       "        0.75049505, 0.74936351, 0.74936351, 0.74936351, 0.74936351,\n",
       "        0.74936351, 0.74936351, 0.74936351, 0.74936351, 0.74936351,\n",
       "        0.74936351, 0.74936351, 0.74936351, 0.74214993, 0.75091938,\n",
       "        0.75049505, 0.74936351, 0.74936351, 0.74936351, 0.74936351,\n",
       "        0.74936351, 0.74936351, 0.74936351, 0.74936351, 0.74936351]),\n",
       " 'split2_test_f1_micro': array([0.6003678 , 0.74366954, 0.74777196, 0.75159146, 0.75003537,\n",
       "        0.75017683, 0.75003537, 0.74975244, 0.7498939 , 0.7498939 ,\n",
       "        0.7498939 , 0.7498939 , 0.7498939 , 0.7498939 , 0.7498939 ,\n",
       "        0.7498939 , 0.7498939 , 0.7498939 , 0.74366954, 0.75159146,\n",
       "        0.75017683, 0.74975244, 0.7498939 , 0.7498939 , 0.7498939 ,\n",
       "        0.7498939 , 0.7498939 , 0.7498939 , 0.7498939 , 0.7498939 ]),\n",
       " 'split3_test_f1_micro': array([0.6003678 , 0.73956712, 0.7453671 , 0.74593295, 0.74692319,\n",
       "        0.74621587, 0.7464988 , 0.74635733, 0.74621587, 0.74621587,\n",
       "        0.74621587, 0.74621587, 0.74621587, 0.74621587, 0.74621587,\n",
       "        0.74621587, 0.74621587, 0.74621587, 0.73956712, 0.74593295,\n",
       "        0.74621587, 0.74635733, 0.74621587, 0.74621587, 0.74621587,\n",
       "        0.74621587, 0.74621587, 0.74621587, 0.74621587, 0.74621587]),\n",
       " 'split4_test_f1_micro': array([0.6003678 , 0.74084029, 0.74494271, 0.75173292, 0.75173292,\n",
       "        0.75286462, 0.75300608, 0.75258169, 0.75258169, 0.75258169,\n",
       "        0.75258169, 0.75258169, 0.75258169, 0.75258169, 0.75258169,\n",
       "        0.75258169, 0.75258169, 0.75258169, 0.74084029, 0.75173292,\n",
       "        0.75286462, 0.75258169, 0.75258169, 0.75258169, 0.75258169,\n",
       "        0.75258169, 0.75258169, 0.75258169, 0.75258169, 0.75258169]),\n",
       " 'split5_test_f1_micro': array([0.6003678 , 0.73588909, 0.74253784, 0.74480124, 0.74565002,\n",
       "        0.74692319, 0.74734757, 0.74777196, 0.74791342, 0.74791342,\n",
       "        0.74791342, 0.74791342, 0.74791342, 0.74791342, 0.74791342,\n",
       "        0.74791342, 0.74791342, 0.74791342, 0.73588909, 0.74480124,\n",
       "        0.74692319, 0.74777196, 0.74791342, 0.74791342, 0.74791342,\n",
       "        0.74791342, 0.74791342, 0.74791342, 0.74791342, 0.74791342]),\n",
       " 'split6_test_f1_micro': array([0.6003678 , 0.73631348, 0.73970859, 0.74112321, 0.74253784,\n",
       "        0.74055736, 0.7404159 , 0.74027444, 0.7404159 , 0.74027444,\n",
       "        0.7404159 , 0.7404159 , 0.7404159 , 0.7404159 , 0.7404159 ,\n",
       "        0.7404159 , 0.7404159 , 0.74027444, 0.73631348, 0.74112321,\n",
       "        0.74055736, 0.74027444, 0.7404159 , 0.7404159 , 0.7404159 ,\n",
       "        0.7404159 , 0.7404159 , 0.7404159 , 0.7404159 , 0.7404159 ]),\n",
       " 'split7_test_f1_micro': array([0.60050927, 0.73843542, 0.74451832, 0.74451832, 0.74607441,\n",
       "        0.74565002, 0.7464988 , 0.74664026, 0.74664026, 0.74664026,\n",
       "        0.7464988 , 0.7464988 , 0.7464988 , 0.7464988 , 0.7464988 ,\n",
       "        0.7464988 , 0.7464988 , 0.7464988 , 0.73843542, 0.74451832,\n",
       "        0.74565002, 0.74664026, 0.74664026, 0.7464988 , 0.7464988 ,\n",
       "        0.7464988 , 0.7464988 , 0.7464988 , 0.7464988 , 0.74664026]),\n",
       " 'split8_test_f1_micro': array([0.60050927, 0.73065497, 0.73405008, 0.73744518, 0.73730372,\n",
       "        0.73744518, 0.7381525 , 0.73772811, 0.73786957, 0.73772811,\n",
       "        0.73772811, 0.73786957, 0.73786957, 0.73786957, 0.73786957,\n",
       "        0.73772811, 0.73786957, 0.73786957, 0.73065497, 0.73744518,\n",
       "        0.73744518, 0.73772811, 0.73772811, 0.73772811, 0.73772811,\n",
       "        0.73772811, 0.73772811, 0.73772811, 0.73772811, 0.73772811]),\n",
       " 'split9_test_f1_micro': array([0.60050927, 0.7370208 , 0.74494271, 0.74664026, 0.74805489,\n",
       "        0.74805489, 0.74819635, 0.74805489, 0.74805489, 0.74805489,\n",
       "        0.74805489, 0.74805489, 0.74805489, 0.74805489, 0.74805489,\n",
       "        0.74805489, 0.74805489, 0.74805489, 0.7370208 , 0.74664026,\n",
       "        0.74805489, 0.74805489, 0.74805489, 0.74805489, 0.74805489,\n",
       "        0.74805489, 0.74805489, 0.74805489, 0.74805489, 0.74805489]),\n",
       " 'mean_test_f1_micro': array([0.60042155, 0.73784869, 0.74289878, 0.74581278, 0.74603916,\n",
       "        0.74603915, 0.74605333, 0.74596845, 0.74601089, 0.74598259,\n",
       "        0.74598259, 0.74599674, 0.74599674, 0.74599674, 0.74599674,\n",
       "        0.74598259, 0.74599674, 0.74598259, 0.73784869, 0.74581278,\n",
       "        0.74603915, 0.74596845, 0.74599674, 0.74598259, 0.74598259,\n",
       "        0.74598259, 0.74598259, 0.74598259, 0.74598259, 0.74599674]),\n",
       " 'std_test_f1_micro': array([6.12709395e-05, 3.72143540e-03, 3.69321657e-03, 4.42308299e-03,\n",
       "        4.09842493e-03, 4.56883284e-03, 4.48746662e-03, 4.48593657e-03,\n",
       "        4.45922367e-03, 4.50294487e-03, 4.48334802e-03, 4.45742865e-03,\n",
       "        4.45742865e-03, 4.45742865e-03, 4.45742865e-03, 4.48334802e-03,\n",
       "        4.45742865e-03, 4.47530640e-03, 3.72143540e-03, 4.42308299e-03,\n",
       "        4.56883284e-03, 4.48593657e-03, 4.48517729e-03, 4.48334802e-03,\n",
       "        4.48334802e-03, 4.48334802e-03, 4.48334802e-03, 4.48334802e-03,\n",
       "        4.48334802e-03, 4.48517729e-03]),\n",
       " 'rank_test_f1_micro': array([30, 28, 27, 25,  2,  3,  1, 23,  5, 14, 14,  6,  6,  6,  6, 14,  6,\n",
       "        13, 28, 25,  3, 23,  6, 14, 14, 14, 14, 14, 14,  6])}"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_model.cv_results_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "6d28a919",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([30, 28, 27, 25,  2,  3,  1, 23,  5, 14, 14,  6,  6,  6,  6, 14,  6,\n",
       "       13, 28, 25,  3, 23,  6, 14, 14, 14, 14, 14, 14,  6])"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_model.cv_results_['rank_test_accuracy']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "0ff3283b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'classifier': LogisticRegression(max_iter=10000),\n",
       " 'classifier__C': 0.0001,\n",
       " 'classifier__penalty': 'l1',\n",
       " 'classifier__solver': 'saga'}"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_model.cv_results_['params'][ np.argmin(best_model.cv_results_['mean_test_accuracy'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "1b20e96a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>classifier</th>\n",
       "      <th>C</th>\n",
       "      <th>penalty</th>\n",
       "      <th>solver</th>\n",
       "      <th>score_acc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.600422</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.737849</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.742899</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.745813</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0100</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.746039</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0100</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.746039</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.1000</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.746053</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.1000</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.745968</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.746011</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.745983</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>10.0000</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.745983</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>10.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.745997</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>100.0000</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.745997</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>100.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.745997</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>1000.0000</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.745997</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>1000.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.745983</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>10000.0000</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.745997</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>10000.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.745983</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.737849</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.745813</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0100</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.746039</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.1000</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.745968</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.745997</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>10.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.745983</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>100.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.745983</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>1000.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.745983</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>10000.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.745983</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>NaN</td>\n",
       "      <td>none</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.745983</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>NaN</td>\n",
       "      <td>none</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.745983</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.745997</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                            classifier           C penalty solver  score_acc\n",
       "0   LogisticRegression(max_iter=10000)      0.0001      l1   saga   0.600422\n",
       "1   LogisticRegression(max_iter=10000)      0.0001      l2   saga   0.737849\n",
       "2   LogisticRegression(max_iter=10000)      0.0010      l1   saga   0.742899\n",
       "3   LogisticRegression(max_iter=10000)      0.0010      l2   saga   0.745813\n",
       "4   LogisticRegression(max_iter=10000)      0.0100      l1   saga   0.746039\n",
       "5   LogisticRegression(max_iter=10000)      0.0100      l2   saga   0.746039\n",
       "6   LogisticRegression(max_iter=10000)      0.1000      l1   saga   0.746053\n",
       "7   LogisticRegression(max_iter=10000)      0.1000      l2   saga   0.745968\n",
       "8   LogisticRegression(max_iter=10000)      1.0000      l1   saga   0.746011\n",
       "9   LogisticRegression(max_iter=10000)      1.0000      l2   saga   0.745983\n",
       "10  LogisticRegression(max_iter=10000)     10.0000      l1   saga   0.745983\n",
       "11  LogisticRegression(max_iter=10000)     10.0000      l2   saga   0.745997\n",
       "12  LogisticRegression(max_iter=10000)    100.0000      l1   saga   0.745997\n",
       "13  LogisticRegression(max_iter=10000)    100.0000      l2   saga   0.745997\n",
       "14  LogisticRegression(max_iter=10000)   1000.0000      l1   saga   0.745997\n",
       "15  LogisticRegression(max_iter=10000)   1000.0000      l2   saga   0.745983\n",
       "16  LogisticRegression(max_iter=10000)  10000.0000      l1   saga   0.745997\n",
       "17  LogisticRegression(max_iter=10000)  10000.0000      l2   saga   0.745983\n",
       "18  LogisticRegression(max_iter=10000)      0.0001      l2  lbfgs   0.737849\n",
       "19  LogisticRegression(max_iter=10000)      0.0010      l2  lbfgs   0.745813\n",
       "20  LogisticRegression(max_iter=10000)      0.0100      l2  lbfgs   0.746039\n",
       "21  LogisticRegression(max_iter=10000)      0.1000      l2  lbfgs   0.745968\n",
       "22  LogisticRegression(max_iter=10000)      1.0000      l2  lbfgs   0.745997\n",
       "23  LogisticRegression(max_iter=10000)     10.0000      l2  lbfgs   0.745983\n",
       "24  LogisticRegression(max_iter=10000)    100.0000      l2  lbfgs   0.745983\n",
       "25  LogisticRegression(max_iter=10000)   1000.0000      l2  lbfgs   0.745983\n",
       "26  LogisticRegression(max_iter=10000)  10000.0000      l2  lbfgs   0.745983\n",
       "27  LogisticRegression(max_iter=10000)         NaN    none  lbfgs   0.745983\n",
       "28  LogisticRegression(max_iter=10000)         NaN    none   saga   0.745983\n",
       "29  LogisticRegression(max_iter=10000)         NaN     NaN    NaN   0.745997"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results = pd.DataFrame( best_model.cv_results_['params'] )\n",
    "\n",
    "results['score_acc'] = best_model.cv_results_['mean_test_accuracy']\n",
    "cols = results.columns.to_series().str.split('__').apply(lambda x: x[-1])\n",
    "results.columns = cols\n",
    "\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "a9b8f04c",
   "metadata": {},
   "outputs": [],
   "source": [
    "acc_list.append(results[0:1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "9ff4ec73",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'classifier': LogisticRegression(max_iter=10000),\n",
       " 'classifier__C': 0.0001,\n",
       " 'classifier__penalty': 'l2',\n",
       " 'classifier__solver': 'saga'}"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_model.cv_results_['params'][ np.argmin(best_model.cv_results_['rank_test_precision'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "f5204e89",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>classifier</th>\n",
       "      <th>C</th>\n",
       "      <th>penalty</th>\n",
       "      <th>solver</th>\n",
       "      <th>score_precision</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.711210</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.698539</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.697574</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0100</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.695150</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0100</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.694605</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.1000</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.694423</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.1000</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.694223</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.694243</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.694219</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>10.0000</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.694205</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>10.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.694216</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>100.0000</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.694216</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>100.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.694216</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>1000.0000</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.694216</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>1000.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.694205</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>10000.0000</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.694216</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>10000.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.694205</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.711210</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.697574</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0100</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.694589</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.1000</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.694223</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.694231</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>10.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.694205</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>100.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.694205</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>1000.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.694205</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>10000.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.694205</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>NaN</td>\n",
       "      <td>none</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.694205</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>NaN</td>\n",
       "      <td>none</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.694205</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.694231</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                            classifier           C penalty solver  \\\n",
       "0   LogisticRegression(max_iter=10000)      0.0001      l1   saga   \n",
       "1   LogisticRegression(max_iter=10000)      0.0001      l2   saga   \n",
       "2   LogisticRegression(max_iter=10000)      0.0010      l1   saga   \n",
       "3   LogisticRegression(max_iter=10000)      0.0010      l2   saga   \n",
       "4   LogisticRegression(max_iter=10000)      0.0100      l1   saga   \n",
       "5   LogisticRegression(max_iter=10000)      0.0100      l2   saga   \n",
       "6   LogisticRegression(max_iter=10000)      0.1000      l1   saga   \n",
       "7   LogisticRegression(max_iter=10000)      0.1000      l2   saga   \n",
       "8   LogisticRegression(max_iter=10000)      1.0000      l1   saga   \n",
       "9   LogisticRegression(max_iter=10000)      1.0000      l2   saga   \n",
       "10  LogisticRegression(max_iter=10000)     10.0000      l1   saga   \n",
       "11  LogisticRegression(max_iter=10000)     10.0000      l2   saga   \n",
       "12  LogisticRegression(max_iter=10000)    100.0000      l1   saga   \n",
       "13  LogisticRegression(max_iter=10000)    100.0000      l2   saga   \n",
       "14  LogisticRegression(max_iter=10000)   1000.0000      l1   saga   \n",
       "15  LogisticRegression(max_iter=10000)   1000.0000      l2   saga   \n",
       "16  LogisticRegression(max_iter=10000)  10000.0000      l1   saga   \n",
       "17  LogisticRegression(max_iter=10000)  10000.0000      l2   saga   \n",
       "18  LogisticRegression(max_iter=10000)      0.0001      l2  lbfgs   \n",
       "19  LogisticRegression(max_iter=10000)      0.0010      l2  lbfgs   \n",
       "20  LogisticRegression(max_iter=10000)      0.0100      l2  lbfgs   \n",
       "21  LogisticRegression(max_iter=10000)      0.1000      l2  lbfgs   \n",
       "22  LogisticRegression(max_iter=10000)      1.0000      l2  lbfgs   \n",
       "23  LogisticRegression(max_iter=10000)     10.0000      l2  lbfgs   \n",
       "24  LogisticRegression(max_iter=10000)    100.0000      l2  lbfgs   \n",
       "25  LogisticRegression(max_iter=10000)   1000.0000      l2  lbfgs   \n",
       "26  LogisticRegression(max_iter=10000)  10000.0000      l2  lbfgs   \n",
       "27  LogisticRegression(max_iter=10000)         NaN    none  lbfgs   \n",
       "28  LogisticRegression(max_iter=10000)         NaN    none   saga   \n",
       "29  LogisticRegression(max_iter=10000)         NaN     NaN    NaN   \n",
       "\n",
       "    score_precision  \n",
       "0          0.000000  \n",
       "1          0.711210  \n",
       "2          0.698539  \n",
       "3          0.697574  \n",
       "4          0.695150  \n",
       "5          0.694605  \n",
       "6          0.694423  \n",
       "7          0.694223  \n",
       "8          0.694243  \n",
       "9          0.694219  \n",
       "10         0.694205  \n",
       "11         0.694216  \n",
       "12         0.694216  \n",
       "13         0.694216  \n",
       "14         0.694216  \n",
       "15         0.694205  \n",
       "16         0.694216  \n",
       "17         0.694205  \n",
       "18         0.711210  \n",
       "19         0.697574  \n",
       "20         0.694589  \n",
       "21         0.694223  \n",
       "22         0.694231  \n",
       "23         0.694205  \n",
       "24         0.694205  \n",
       "25         0.694205  \n",
       "26         0.694205  \n",
       "27         0.694205  \n",
       "28         0.694205  \n",
       "29         0.694231  "
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results = pd.DataFrame( best_model.cv_results_['params'] )\n",
    "\n",
    "results['score_precision'] = best_model.cv_results_['mean_test_precision']\n",
    "cols = results.columns.to_series().str.split('__').apply(lambda x: x[-1])\n",
    "results.columns = cols\n",
    "\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "43b18550",
   "metadata": {},
   "outputs": [],
   "source": [
    "roc_auc_ovr_List.append(results[1:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "24071971",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'classifier': LogisticRegression(max_iter=10000),\n",
       " 'classifier__C': 0.1,\n",
       " 'classifier__penalty': 'l1',\n",
       " 'classifier__solver': 'saga'}"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_model.cv_results_['params'][ np.argmin(best_model.cv_results_['rank_test_f1_micro'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "87b8ca97",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>classifier</th>\n",
       "      <th>C</th>\n",
       "      <th>penalty</th>\n",
       "      <th>solver</th>\n",
       "      <th>score_f1_micro</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.600422</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.737849</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.742899</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.745813</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0100</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.746039</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0100</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.746039</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.1000</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.746053</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.1000</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.745968</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.746011</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.745983</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>10.0000</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.745983</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>10.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.745997</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>100.0000</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.745997</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>100.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.745997</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>1000.0000</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.745997</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>1000.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.745983</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>10000.0000</td>\n",
       "      <td>l1</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.745997</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>10000.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.745983</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.737849</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.745813</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.0100</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.746039</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>0.1000</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.745968</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.745997</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>10.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.745983</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>100.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.745983</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>1000.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.745983</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>10000.0000</td>\n",
       "      <td>l2</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.745983</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>NaN</td>\n",
       "      <td>none</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.745983</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>NaN</td>\n",
       "      <td>none</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.745983</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>LogisticRegression(max_iter=10000)</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.745997</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                            classifier           C penalty solver  \\\n",
       "0   LogisticRegression(max_iter=10000)      0.0001      l1   saga   \n",
       "1   LogisticRegression(max_iter=10000)      0.0001      l2   saga   \n",
       "2   LogisticRegression(max_iter=10000)      0.0010      l1   saga   \n",
       "3   LogisticRegression(max_iter=10000)      0.0010      l2   saga   \n",
       "4   LogisticRegression(max_iter=10000)      0.0100      l1   saga   \n",
       "5   LogisticRegression(max_iter=10000)      0.0100      l2   saga   \n",
       "6   LogisticRegression(max_iter=10000)      0.1000      l1   saga   \n",
       "7   LogisticRegression(max_iter=10000)      0.1000      l2   saga   \n",
       "8   LogisticRegression(max_iter=10000)      1.0000      l1   saga   \n",
       "9   LogisticRegression(max_iter=10000)      1.0000      l2   saga   \n",
       "10  LogisticRegression(max_iter=10000)     10.0000      l1   saga   \n",
       "11  LogisticRegression(max_iter=10000)     10.0000      l2   saga   \n",
       "12  LogisticRegression(max_iter=10000)    100.0000      l1   saga   \n",
       "13  LogisticRegression(max_iter=10000)    100.0000      l2   saga   \n",
       "14  LogisticRegression(max_iter=10000)   1000.0000      l1   saga   \n",
       "15  LogisticRegression(max_iter=10000)   1000.0000      l2   saga   \n",
       "16  LogisticRegression(max_iter=10000)  10000.0000      l1   saga   \n",
       "17  LogisticRegression(max_iter=10000)  10000.0000      l2   saga   \n",
       "18  LogisticRegression(max_iter=10000)      0.0001      l2  lbfgs   \n",
       "19  LogisticRegression(max_iter=10000)      0.0010      l2  lbfgs   \n",
       "20  LogisticRegression(max_iter=10000)      0.0100      l2  lbfgs   \n",
       "21  LogisticRegression(max_iter=10000)      0.1000      l2  lbfgs   \n",
       "22  LogisticRegression(max_iter=10000)      1.0000      l2  lbfgs   \n",
       "23  LogisticRegression(max_iter=10000)     10.0000      l2  lbfgs   \n",
       "24  LogisticRegression(max_iter=10000)    100.0000      l2  lbfgs   \n",
       "25  LogisticRegression(max_iter=10000)   1000.0000      l2  lbfgs   \n",
       "26  LogisticRegression(max_iter=10000)  10000.0000      l2  lbfgs   \n",
       "27  LogisticRegression(max_iter=10000)         NaN    none  lbfgs   \n",
       "28  LogisticRegression(max_iter=10000)         NaN    none   saga   \n",
       "29  LogisticRegression(max_iter=10000)         NaN     NaN    NaN   \n",
       "\n",
       "    score_f1_micro  \n",
       "0         0.600422  \n",
       "1         0.737849  \n",
       "2         0.742899  \n",
       "3         0.745813  \n",
       "4         0.746039  \n",
       "5         0.746039  \n",
       "6         0.746053  \n",
       "7         0.745968  \n",
       "8         0.746011  \n",
       "9         0.745983  \n",
       "10        0.745983  \n",
       "11        0.745997  \n",
       "12        0.745997  \n",
       "13        0.745997  \n",
       "14        0.745997  \n",
       "15        0.745983  \n",
       "16        0.745997  \n",
       "17        0.745983  \n",
       "18        0.737849  \n",
       "19        0.745813  \n",
       "20        0.746039  \n",
       "21        0.745968  \n",
       "22        0.745997  \n",
       "23        0.745983  \n",
       "24        0.745983  \n",
       "25        0.745983  \n",
       "26        0.745983  \n",
       "27        0.745983  \n",
       "28        0.745983  \n",
       "29        0.745997  "
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results = pd.DataFrame( best_model.cv_results_['params'] )\n",
    "\n",
    "results['score_f1_micro'] = best_model.cv_results_['mean_test_f1_micro']\n",
    "cols = results.columns.to_series().str.split('__').apply(lambda x: x[-1])\n",
    "results.columns = cols\n",
    "\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "39fb7b98",
   "metadata": {},
   "outputs": [],
   "source": [
    "f1_micro_List.append(results[6:7])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "cd18e7d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time:  0:36:28.388900\n"
     ]
    }
   ],
   "source": [
    "end = datetime.now()\n",
    "time_taken = end - start\n",
    "print('Time: ',time_taken) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6491ce89",
   "metadata": {},
   "source": [
    "## Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "f82dc269",
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy = [x['score_acc'][0] for x in acc_list]\n",
    "accuracy_c = [x['C'][0] for x in acc_list]\n",
    "accuracy_penalty = [x['penalty'][0] for x in acc_list]\n",
    "roc = [x['score_precision'].reset_index(drop=True)[0] for x in roc_auc_ovr_List]\n",
    "roc_c = [x['C'].reset_index(drop=True)[0] for x in roc_auc_ovr_List]\n",
    "roc_penalty = [x['penalty'].reset_index(drop=True)[0] for x in roc_auc_ovr_List]\n",
    "f1 = [x['score_f1_micro'].reset_index(drop=True)[0] for x in f1_micro_List]\n",
    "f1_c = [x['C'].reset_index(drop=True)[0] for x in f1_micro_List]\n",
    "f1_penalty = [x['penalty'].reset_index(drop=True)[0] for x in f1_micro_List]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "5d58acc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {'Accuracy': accuracy, 'Accuracy C': accuracy_c, 'Accuracy Penalty': accuracy_penalty,\n",
    "        'Roc_auc_ovr': roc, 'Roc_auc_ovr C': roc_c, 'Roc_auc_ovr Penalty': roc_penalty,\n",
    "        'F1_micro':f1, 'F1_micro C': f1_c, 'F1_micro Penalty': f1_penalty}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "27c29018",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Accuracy C</th>\n",
       "      <th>Accuracy Penalty</th>\n",
       "      <th>Roc_auc_ovr</th>\n",
       "      <th>Roc_auc_ovr C</th>\n",
       "      <th>Roc_auc_ovr Penalty</th>\n",
       "      <th>F1_micro</th>\n",
       "      <th>F1_micro C</th>\n",
       "      <th>F1_micro Penalty</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.600535</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>l1</td>\n",
       "      <td>0.712024</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>l2</td>\n",
       "      <td>0.746704</td>\n",
       "      <td>0.01</td>\n",
       "      <td>l2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.599120</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>l1</td>\n",
       "      <td>0.712332</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>l2</td>\n",
       "      <td>0.747043</td>\n",
       "      <td>1000.00</td>\n",
       "      <td>l2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.599601</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>l1</td>\n",
       "      <td>0.711806</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>l2</td>\n",
       "      <td>0.746845</td>\n",
       "      <td>0.10</td>\n",
       "      <td>l1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.599332</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>l1</td>\n",
       "      <td>0.712440</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>l2</td>\n",
       "      <td>0.747652</td>\n",
       "      <td>1.00</td>\n",
       "      <td>l2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.600167</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>l1</td>\n",
       "      <td>0.711268</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>l2</td>\n",
       "      <td>0.746987</td>\n",
       "      <td>0.10</td>\n",
       "      <td>l1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.599389</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>l1</td>\n",
       "      <td>0.711137</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>l2</td>\n",
       "      <td>0.746407</td>\n",
       "      <td>10000.00</td>\n",
       "      <td>l1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.599785</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>l1</td>\n",
       "      <td>0.711292</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>l2</td>\n",
       "      <td>0.746464</td>\n",
       "      <td>0.10</td>\n",
       "      <td>l1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.599969</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>l1</td>\n",
       "      <td>0.710429</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>l2</td>\n",
       "      <td>0.746138</td>\n",
       "      <td>1.00</td>\n",
       "      <td>l1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.600110</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>l1</td>\n",
       "      <td>0.709766</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>l2</td>\n",
       "      <td>0.746011</td>\n",
       "      <td>0.10</td>\n",
       "      <td>l1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.600422</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>l1</td>\n",
       "      <td>0.711210</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>l2</td>\n",
       "      <td>0.746053</td>\n",
       "      <td>0.10</td>\n",
       "      <td>l1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Accuracy  Accuracy C Accuracy Penalty  Roc_auc_ovr  Roc_auc_ovr C  \\\n",
       "0  0.600535      0.0001               l1     0.712024         0.0001   \n",
       "1  0.599120      0.0001               l1     0.712332         0.0001   \n",
       "2  0.599601      0.0001               l1     0.711806         0.0001   \n",
       "3  0.599332      0.0001               l1     0.712440         0.0001   \n",
       "4  0.600167      0.0001               l1     0.711268         0.0001   \n",
       "5  0.599389      0.0001               l1     0.711137         0.0001   \n",
       "6  0.599785      0.0001               l1     0.711292         0.0001   \n",
       "7  0.599969      0.0001               l1     0.710429         0.0001   \n",
       "8  0.600110      0.0001               l1     0.709766         0.0001   \n",
       "9  0.600422      0.0001               l1     0.711210         0.0001   \n",
       "\n",
       "  Roc_auc_ovr Penalty  F1_micro  F1_micro C F1_micro Penalty  \n",
       "0                  l2  0.746704        0.01               l2  \n",
       "1                  l2  0.747043     1000.00               l2  \n",
       "2                  l2  0.746845        0.10               l1  \n",
       "3                  l2  0.747652        1.00               l2  \n",
       "4                  l2  0.746987        0.10               l1  \n",
       "5                  l2  0.746407    10000.00               l1  \n",
       "6                  l2  0.746464        0.10               l1  \n",
       "7                  l2  0.746138        1.00               l1  \n",
       "8                  l2  0.746011        0.10               l1  \n",
       "9                  l2  0.746053        0.10               l1  "
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainingResults  = pd.DataFrame(data = data)\n",
    "pd.options.display.max_colwidth = 100\n",
    "trainingResults"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "21d2830e",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainingResults.to_csv('LR_trainingResults.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
